{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94ea5eb9",
   "metadata": {},
   "source": [
    "### Review\n",
    "- RNN\n",
    "- Transformers\n",
    "- LLMs\n",
    "- Prompt Engineering\n",
    "\n",
    "**LLMS**: Temperature 0.0-0.2 (Creative vs Analytical)  \n",
    "*NOTE: This only mimics the temperature change in public GPTs.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42fc7bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API connection successful.\n",
      "Available models: ['mistral-medium-2505', 'mistral-medium-latest', 'mistral-medium', 'mistral-large-latest', 'ministral-3b-2410', 'ministral-3b-latest', 'ministral-8b-2410', 'ministral-8b-latest', 'open-mistral-7b', 'mistral-tiny', 'mistral-tiny-2312', 'open-mistral-nemo', 'open-mistral-nemo-2407', 'mistral-tiny-2407', 'mistral-tiny-latest', 'open-mixtral-8x7b', 'mistral-small', 'mistral-small-2312', 'open-mixtral-8x22b', 'open-mixtral-8x22b-2404', 'mistral-small-2409', 'mistral-large-2407', 'mistral-large-2411', 'pixtral-large-2411', 'pixtral-large-latest', 'mistral-large-pixtral-2411', 'codestral-2501', 'codestral-2412', 'codestral-2411-rc5', 'codestral-2508', 'codestral-latest', 'devstral-small-2505', 'devstral-small-2507', 'devstral-small-latest', 'devstral-medium-2507', 'devstral-medium-latest', 'pixtral-12b-2409', 'pixtral-12b', 'pixtral-12b-latest', 'mistral-small-2501', 'mistral-small-2503', 'mistral-small-2506', 'mistral-small-latest', 'mistral-saba-2502', 'mistral-saba-latest', 'magistral-medium-2506', 'magistral-medium-2507', 'magistral-medium-latest', 'magistral-small-2506', 'magistral-small-2507', 'magistral-small-latest', 'voxtral-mini-2507', 'voxtral-mini-latest', 'voxtral-small-2507', 'voxtral-small-latest', 'mistral-embed', 'codestral-embed', 'codestral-embed-2505', 'mistral-moderation-2411', 'mistral-moderation-latest', 'mistral-ocr-2503', 'mistral-ocr-2505', 'mistral-ocr-latest', 'voxtral-mini-transcribe-2507', 'voxtral-mini-2507', 'voxtral-mini-latest']\n",
      "Response: AI (Artificial Intelligence) is the simulation of human intelligence in machines that are programmed to think, learn, and perform tasks autonomously.\n",
      "Response: AI (Artificial Intelligence) is the simulation of human intelligence in machines that are programmed to think, learn, and perform tasks autonomously.\n"
     ]
    }
   ],
   "source": [
    "# Mistral API Exercise\n",
    "\n",
    "import os\n",
    "from mistralai import Mistral  # Install via pip if needed: pip install mistralai\n",
    "import dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Retrieve API key from environment\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "# Just ensure your .env file has: MISTRAL_API_KEY=your_key_here\n",
    "\n",
    "# API Setup Test\n",
    "def test_api_connection():\n",
    "    \"\"\"\n",
    "    Test the Mistral API connection by fetching the model list.\n",
    "    Returns True if successful, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = Mistral(api_key=MISTRAL_API_KEY)\n",
    "        response = client.models.list()  # Use the correct method to list models\n",
    "        models = response.data\n",
    "        print(\"API connection successful.\\nAvailable models:\", [model.id for model in models])\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"API connection failed:\", e)\n",
    "        return False\n",
    "\n",
    "test_api_connection()\n",
    "# Example output: \n",
    "# API connection failed: API error occurred: Status 401 {\"detail\":\"Unauthorized\"}\n",
    "\n",
    "\n",
    "def generate_response(prompt, model=\"mistral-small-latest\", temperature=0.7, max_tokens=200, top_p=1.0):\n",
    "    \"\"\"\n",
    "    Modular function to generate a response from Mistral.ai.\n",
    "    - prompt: The input prompt.\n",
    "    - model: LLM variant (e.g., 'mistral-small-latest' for basics).\n",
    "    - temperature: Randomness level (0.0: deterministic, 1.0: creative).\n",
    "    - max_tokens: Limits output length.\n",
    "    - top_p: Nucleus sampling threshold (0-1; lower values restrict to more probable tokens).\n",
    "    Returns the generated text.\n",
    "    \"\"\"\n",
    "    client = Mistral(api_key=MISTRAL_API_KEY)\n",
    "    response = client.chat.complete(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p # added for nucleus sampling\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example: Basic prompt\n",
    "basic_prompt = \"Explain AI in one sentence.\"\n",
    "response = generate_response(basic_prompt)\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a2de459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Explain how to cook pasta.\n",
      "Response: Cooking pasta is simple and versatile! Here’s a basic step-by-step guide to making perfect pasta:\n",
      "\n",
      "### **Ingredients & Tools:**\n",
      "- **Pasta** (any type: spaghetti, penne, fettuccine, etc.)\n",
      "- **Water** (enough to cover the pasta)\n",
      "- **Salt** (about 1–2 tablespoons for a large pot)\n",
      "- **Oil or butter** (optional, for preventing sticking)\n",
      "- **A large pot** with a lid\n",
      "- **A colander or strainer** (for draining)\n",
      "\n",
      "### **Steps:**\n",
      "\n",
      "1. **Boil Water:**\n",
      "   - Fill a large pot with water (about 4–6 quarts for 1 lb of pasta) and bring it to a **rolling boil** over high heat.\n",
      "   - Add **salt** (like the sea) to the water—this seasons the pasta. Some recipes suggest adding oil, but it’s not necessary unless your pasta is very prone\n",
      "\n",
      "Prompt: Explain how to cook al dente pasta with minimal water.\n",
      "Response: Cooking **al dente pasta with minimal water** is an efficient and eco-friendly method that reduces waste and energy use. Here’s how to do it properly:\n",
      "\n",
      "### **Method: Minimal-Water Pasta (Al Dente)**\n",
      "#### **Ingredients & Tools:**\n",
      "- Pasta (100g per person)\n",
      "- Salt (1–2 tsp per 500ml water)\n",
      "- Large pot (or small pot if cooking a small batch)\n",
      "- Wooden spoon or spatula\n",
      "- Timer\n",
      "\n",
      "#### **Steps:**\n",
      "1. **Use the Right Amount of Water**\n",
      "   - The traditional ratio is **1L (4 cups) water per 100g pasta**, but for minimal water, use **500ml (2 cups) per 100g pasta**.\n",
      "   - For even less water (e.g., **250ml per 100g**), stir frequently to prevent sticking.\n",
      "\n",
      "2. **Bring to a Boil\n",
      "\n",
      "Prompt: Explain how to cook al dente pasta with minimal water.\n",
      "Response: Cooking **al dente pasta with minimal water** is an efficient and eco-friendly method that reduces waste and energy use. Here’s how to do it properly:\n",
      "\n",
      "### **Method: Minimal-Water Pasta (Al Dente)**\n",
      "#### **Ingredients & Tools:**\n",
      "- Pasta (100g per person)\n",
      "- Salt (1–2 tsp per 500ml water)\n",
      "- Large pot (or small pot if cooking a small batch)\n",
      "- Wooden spoon or spatula\n",
      "- Timer\n",
      "\n",
      "#### **Steps:**\n",
      "1. **Use the Right Amount of Water**\n",
      "   - The traditional ratio is **1L (4 cups) water per 100g pasta**, but for minimal water, use **500ml (2 cups) per 100g pasta**.\n",
      "   - For even less water (e.g., **250ml per 100g**), stir frequently to prevent sticking.\n",
      "\n",
      "2. **Bring to a Boil\n",
      "\n",
      "Prompt: Explain how to cook al dente pasta with a rich, cheesy tomato sauce.\n",
      "Response: Cooking **al dente pasta** with a **rich, cheesy tomato sauce** is a classic Italian dish that balances texture and flavor. Here’s a step-by-step guide:\n",
      "\n",
      "### **Ingredients:**\n",
      "- **Pasta** (spaghetti, penne, or rigatoni work well)\n",
      "- **Tomato sauce** (crushed tomatoes, tomato passata, or canned tomatoes)\n",
      "- **Garlic** (2-3 cloves, minced)\n",
      "- **Onion** (1 small, finely chopped)\n",
      "- **Olive oil** (2 tbsp)\n",
      "- **Butter** (1 tbsp, optional for richness)\n",
      "- **Cheese** (Parmesan, Pecorino, or cream cheese for creaminess)\n",
      "- **Red pepper flakes** (optional, for heat)\n",
      "- **Fresh basil or parsley** (for garnish)\n",
      "- **Salt & black pepper** (to taste)\n",
      "- **Pasta water** (res\n",
      "\n",
      "Prompt: Explain how to cook al dente pasta with a rich, cheesy tomato sauce.\n",
      "Response: Cooking **al dente pasta** with a **rich, cheesy tomato sauce** is a classic Italian dish that balances texture and flavor. Here’s a step-by-step guide:\n",
      "\n",
      "### **Ingredients:**\n",
      "- **Pasta** (spaghetti, penne, or rigatoni work well)\n",
      "- **Tomato sauce** (crushed tomatoes, tomato passata, or canned tomatoes)\n",
      "- **Garlic** (2-3 cloves, minced)\n",
      "- **Onion** (1 small, finely chopped)\n",
      "- **Olive oil** (2 tbsp)\n",
      "- **Butter** (1 tbsp, optional for richness)\n",
      "- **Cheese** (Parmesan, Pecorino, or cream cheese for creaminess)\n",
      "- **Red pepper flakes** (optional, for heat)\n",
      "- **Fresh basil or parsley** (for garnish)\n",
      "- **Salt & black pepper** (to taste)\n",
      "- **Pasta water** (res\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_prompts = [\n",
    "    \"Explain how to cook pasta.\",\n",
    "    \"Explain how to cook al dente pasta with minimal water.\",\n",
    "    \"Explain how to cook al dente pasta with a rich, cheesy tomato sauce.\"\n",
    "]\n",
    "\n",
    "# Generate responses for each example prompt\n",
    "for prompt in example_prompts:\n",
    "    response = generate_response(prompt)\n",
    "    print(f\"Prompt: {prompt}\\nResponse: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "798e358d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output for 'List 5 AI applications': Here are five notable AI applications across different industries:\n",
      "\n",
      "1. **Healthcare** – AI-powered diagnostic tools (e.g., IBM Watson for Oncology, AI radiology) help detect diseases like cancer, diabetes, and neurological disorders with high accuracy.\n",
      "\n",
      "2. **Autonomous Vehicles** – Self-driving cars (e.g., Tesla Autopilot, Waymo) use AI for real-time decision-making, object detection, and navigation.\n",
      "\n",
      "3. **Virtual Assistants** – AI-driven assistants like Siri, Alexa, and Google Assistant use natural language processing (NLP) to perform tasks, answer questions, and control smart devices.\n",
      "\n",
      "4. **Fraud Detection** – Banks and financial institutions use AI to analyze transaction patterns and detect fraudulent activities in real time.\n",
      "\n",
      "5. **Content Generation** – AI tools like ChatGPT, DALL·E (for images), and Jasper.ai create human-like text, art, and even music, revolutionizing creative industries.\n",
      "\n",
      "Would you\n"
     ]
    }
   ],
   "source": [
    "output = generate_response(\"List 5 AI applications\", temperature=0.0)\n",
    "print(\"Output for 'List 5 AI applications':\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6480ee19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response #1:\n",
      "The French word for \"grandpa\" is **\"grand-père\"** (or informally **\"papi\"**).\n",
      "Here’s an example sentence in English: *\"My grandpa (grand-père) always tells me\n",
      "the most interesting stories about his childhood.\"*  Would you like a French\n",
      "sentence as well? 😊\n",
      "________________________________________________________________________________\n",
      "Response #2:\n",
      "To measure the circumference of a circle using a string and a plumb bob, first\n",
      "tie the plumb bob to one end of the string and hold the other end against the\n",
      "circle's edge. Walk around the circle, keeping the string taut and the plumb bob\n",
      "hanging freely to maintain a consistent radius. The length of the string when\n",
      "you return to the starting point is the circumference.\n",
      "________________________________________________________________________________\n",
      "Response #2:\n",
      "To measure the circumference of a circle using a string and a plumb bob, first\n",
      "tie the plumb bob to one end of the string and hold the other end against the\n",
      "circle's edge. Walk around the circle, keeping the string taut and the plumb bob\n",
      "hanging freely to maintain a consistent radius. The length of the string when\n",
      "you return to the starting point is the circumference.\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# one-shot prompts\n",
    "import textwrap\n",
    "one_shot_prompts = [\n",
    "    \"Translate the word 'grandpa' into French and use it in an English sentence.\",\n",
    "    \"You are an expert mason. Describe how to measure the circumference of a circle using a string and a plumb bob in a couple sentences.\"\n",
    " ]\n",
    "\n",
    "for i, prompt in enumerate(one_shot_prompts, 1):\n",
    "    output = generate_response(prompt, temperature=0.0)\n",
    "    wrapped_output = textwrap.fill(output, width=80)\n",
    "    print(f\"Response #{i}:\")\n",
    "    print(wrapped_output)\n",
    "    print(\"_\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dc3a9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot: The sentiment of the statement **\"I got the job!\"** is **positive**.\n",
      "\n",
      "This is an enthusiastic expression of success, achievement, and happiness, which clearly conveys a positive emotion.\n",
      "Few-shot: The sentiment of the text \"Totally posh experience.\" is **Positive**.\n",
      "\n",
      "Here's the reasoning:\n",
      "- \"Totally\" is an intensifier that amplifies the sentiment.\n",
      "- \"Posh\" is a positive descriptor, often associated with luxury, elegance, or high quality.\n",
      "- \"Experience\" in this context implies a favorable or enjoyable encounter.\n",
      "\n",
      "Thus, the overall sentiment is clearly positive.\n",
      "Few-shot: The sentiment of the text \"Totally posh experience.\" is **Positive**.\n",
      "\n",
      "Here's the reasoning:\n",
      "- \"Totally\" is an intensifier that amplifies the sentiment.\n",
      "- \"Posh\" is a positive descriptor, often associated with luxury, elegance, or high quality.\n",
      "- \"Experience\" in this context implies a favorable or enjoyable encounter.\n",
      "\n",
      "Thus, the overall sentiment is clearly positive.\n",
      "CoT: To find **15% of 200**, follow these steps:\n",
      "\n",
      "1. **Understand what \"percent\" means:**\n",
      "\n",
      "   The term \"percent\" means \"per hundred.\" So, 15% is the same as 15 per 100 or \\( \\frac{15}{100} \\).\n",
      "\n",
      "2. **Convert the percentage to a decimal:**\n",
      "\n",
      "   \\[\n",
      "   15\\% = \\frac{15}{100} = 0.15\n",
      "   \\]\n",
      "\n",
      "3. **Multiply the decimal by the number (200):**\n",
      "\n",
      "   \\[\n",
      "   0.15 \\times 200\n",
      "   \\]\n",
      "\n",
      "4. **Perform the multiplication:**\n",
      "\n",
      "   \\[\n",
      "   0.15 \\times 200 = 30\n",
      "   \\]\n",
      "\n",
      "5. **Final Answer:**\n",
      "\n",
      "   \\[\n",
      "   \\boxed{30}\n",
      "   \\]\n",
      "\n",
      "So, **15% of 2\n",
      "CoT: To find **15% of 200**, follow these steps:\n",
      "\n",
      "1. **Understand what \"percent\" means:**\n",
      "\n",
      "   The term \"percent\" means \"per hundred.\" So, 15% is the same as 15 per 100 or \\( \\frac{15}{100} \\).\n",
      "\n",
      "2. **Convert the percentage to a decimal:**\n",
      "\n",
      "   \\[\n",
      "   15\\% = \\frac{15}{100} = 0.15\n",
      "   \\]\n",
      "\n",
      "3. **Multiply the decimal by the number (200):**\n",
      "\n",
      "   \\[\n",
      "   0.15 \\times 200\n",
      "   \\]\n",
      "\n",
      "4. **Perform the multiplication:**\n",
      "\n",
      "   \\[\n",
      "   0.15 \\times 200 = 30\n",
      "   \\]\n",
      "\n",
      "5. **Final Answer:**\n",
      "\n",
      "   \\[\n",
      "   \\boxed{30}\n",
      "   \\]\n",
      "\n",
      "So, **15% of 2\n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompt = \"Classify this sentiment: 'I got the job!'\"\n",
    "zero_response = generate_response(zero_shot_prompt, temperature=0.5)\n",
    "print(\"Zero-shot:\", zero_response)\n",
    "few_shot_prompt = \"\"\"\n",
    "Example 1: Text: 'Great movie.' Sentiment: Positive.\n",
    "Example 2: Text: 'Terrible service.' Sentiment: Negative.\n",
    "Classify: 'Totally posh experience.'\n",
    "\"\"\"\n",
    "few_response = generate_response(few_shot_prompt, top_p=0.9)  # Now supported\n",
    "print(\"Few-shot:\", few_response)\n",
    "\n",
    "cot_prompt = \"Solve: What is 15% of 200? Think step by step.\"\n",
    "cot_response = generate_response(cot_prompt, temperature=0.2)\n",
    "print(\"CoT:\", cot_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
