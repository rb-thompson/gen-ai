{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18a58363",
   "metadata": {},
   "source": [
    "### The Premise\n",
    "\n",
    "Imagine a super librarian in a magical library where books can talk to each other. This librarian’s job is to translate a story from one language to another or generate a continuation of a story. The library is full of words, and each word is like a book with its own meaning and position in the story.  \n",
    "\n",
    "### The Problem\n",
    "When the librarian reads a sentence like “Hi, how are you?”, they need to understand which words are most important to generate a response like “I am fine.” Unlike older librarians (like Recurrent Neural Networks, or RNNs), who could only remember a few words at a time and forgot the beginning of long stories, this super librarian can pay attention to all words at once, no matter how long the story is.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. Word Books (Embeddings)\n",
    "2. Position Tags (Positional Encoding)\n",
    "3. Attention Magic (Self-Attention)\n",
    "4. Teamwork (Multi-Headed Attention)\n",
    "5. Processing and Refining (Feed-Forward Layers)\n",
    "6. Translating or Generating (Encoder-Decoder)\n",
    "    - Encoder: Reads and creates summary with attention scores\n",
    "    - Decoder: Uses the summary to generate a word-by-word output\n",
    "7. Final Touch (Softmax)\n",
    "\n",
    "### Why It's Useful\n",
    "\n",
    "Unlike older librarians (RNNs), who could only hold a short piece of the story in their memory, the super librarian’s attention magic lets them reference the entire story at once. This makes them great at tasks like translating languages, writing stories, or answering questions, even for very long texts.  \n",
    "\n",
    "\n",
    "This *super librarian* is the **Transformer**, and its attention magic is why it’s so powerful in natural language processing (NLP) tasks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99ec3afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = 10  # Numbers 0-9\n",
    "d_model = 8      # Embedding dimension\n",
    "n_heads = 2      # Number of attention heads\n",
    "d_ff = 16        # Feed-forward dimension\n",
    "seq_length = 5   # Sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a067ee8e",
   "metadata": {},
   "source": [
    "We use NumPy for matrix operations.  \n",
    "\n",
    "The hyperparameters define the model size:  \n",
    "`vocab_size` is the range of input numbers,  \n",
    "`d_model` is the embedding size,  \n",
    "`n_heads` splits attention,  \n",
    "`d_ff` is the feed-forward layer size, and  \n",
    "`seq_length` is the input sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c8f8f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example input: [8 0 0 3 8]\n",
      "Example output: [8 3 0 0 8]\n",
      "\n",
      "Data shape: (100, 5) (100, 5)\n"
     ]
    }
   ],
   "source": [
    "def generate_data(num_samples, seq_length, vocab_size):\n",
    "    \"\"\"Generate random sequences and their reversed versions.\"\"\"\n",
    "    X = np.random.randint(0, vocab_size, (num_samples, seq_length))\n",
    "    y = np.flip(X, axis=1)\n",
    "    return X, y\n",
    "\n",
    "# Generate 100 samples\n",
    "X, y = generate_data(100, seq_length, vocab_size)\n",
    "print(\"Example input:\", X[0])\n",
    "print(\"Example output:\", y[0])\n",
    "\n",
    "print(\"\\nData shape:\", X.shape, y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
