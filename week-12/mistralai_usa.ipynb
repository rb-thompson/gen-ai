{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e20a3fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lyric from \"Bohemian Rhapsody\" by Queen that contains the phrase \"scaramouche, scaramouche\" is:\n",
      "\n",
      "\"Is this the real life? Is this just fantasy?\n",
      "Caught in a landslide, no escape from reality\n",
      "Open your eyes, look up to the skies and see\n",
      "I'm just a poor boy, I need no sympathy\n",
      "Because I'm easy come, easy go\n",
      "A little high, little low\n",
      "Anyway the wind blows, doesn't really matter to me, to me\n",
      "\n",
      "Mama, just killed a man\n",
      "Put a gun against his head\n",
      "Pulled my trigger, now he's dead\n",
      "Mama, life had just begun\n",
      "But now I've gone and thrown it all away\n",
      "Mama, ooooh\n",
      "Didn't mean to make you cry\n",
      "If I'm not back again this time tomorrow\n",
      "Carry on, carry on, as if nothing really matters\n",
      "\n",
      "Too late, my time has come\n",
      "Sends shivers down my spine\n",
      "Body's aching all the time\n",
      "Goodbye, everybody, I've got to go\n",
      "Gotta leave you all behind and face the truth\n",
      "Mama, ooooh (anyway the wind blows)\n",
      "I don't wanna die\n",
      "I sometimes wish I'd never been born at all\n",
      "\n",
      "I see a little silhouetto of a man\n",
      "Scaramouche, Scaramouche, will you do the Fandango?\n",
      "Thunderbolt and lightning, very, very frightening me\n",
      "(Galileo) Galileo (Galileo) Galileo Figaro magnifico\n",
      "But I'm just a poor boy, nobody loves me\n",
      "He's just a poor boy from a poor family\n",
      "Spare him his life from this monstrosity\n",
      "\n",
      "Easy come, easy go, will you let me go?\n",
      "Bismillah! No, we will not let you go\n",
      "(Let him go!) Bismillah! We will not let you go\n",
      "(Let him go!) Bismillah! We will not let you go\n",
      "(Let me go!) Will not let you go\n",
      "(Let me go!) Never, never, never, never, never, never, never, never, never, never, let me go\n",
      "Oh, oh, no, no, no, no, no, no, no\n",
      "(Oh mama mia, mama mia, mama mia let me go)\n",
      "Beelzebub has a devil put aside for me, for me, for me!\"\n",
      "\n",
      "The specific lines are:\n",
      "\"I see a little silhouetto of a man\n",
      "Scaramouche, Scaramouche, will you do the Fandango?\"\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup\n",
    "# Run: pip install langchain langchain-community langchain-chroma langchain-mistralai python-dotenv\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key from .env\n",
    "load_dotenv()\n",
    "mistral_api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "\n",
    "# Model setup\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "chat = ChatMistralAI(model=\"mistral-small-latest\", temperature=0.7)\n",
    "\n",
    "# Call the model\n",
    "resp = chat.invoke([\n",
    " SystemMessage(content=\"You are a helpful assistant.\"),\n",
    " HumanMessage(content=\"What lyric from 'Bohemian Rhapsody' contains the phrase 'scaramouche, scaramouche'?\")\n",
    "])\n",
    "print(resp.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914c38cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.27\n"
     ]
    }
   ],
   "source": [
    "# Verify Installation\n",
    "from importlib.metadata import version \n",
    "print(version(\"langchain\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31c4b025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visit the Promenade des Anglais and explore the Old Town's vibrant markets.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "resp = chat.invoke([\n",
    "    SystemMessage(content=\"You are a nice AI bot that helps a user figure out where to travel in one short sentence\"),\n",
    "    HumanMessage(content=\"I like the beaches where should I go?\"),\n",
    "    AIMessage(content=\"You should go to Nice, France\"),\n",
    "    HumanMessage(content=\"What else should I do when I'm there?\")\n",
    "])\n",
    "\n",
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db950343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'my_document_id': 234234, 'my_document_source': 'The LangChain Papers', 'my_document_create_time': 1680013019}, page_content=\"This is my document. It is full of text that I've gathered from other places\")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\",\n",
    "         metadata={\n",
    "             'my_document_id' : 234234,\n",
    "             'my_document_source' : \"The LangChain Papers\",\n",
    "             'my_document_create_time' : 1680013019\n",
    "         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f125eb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content=\"This is my document. It is full of text that I've gathered from other places\")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "041c4953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "# Correct: use a valid Mistral model; remove OpenAI's model_name and the stray \"mistral_\"\n",
    "llm = ChatMistralAI(model=\"mistral-small-latest\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55287471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The day that comes after Friday is **Saturday**.\n",
      "\n",
      "Hereâ€™s the sequence of the days of the week for reference:\n",
      "- Sunday\n",
      "- Monday\n",
      "- Tuesday\n",
      "- Wednesday\n",
      "- Thursday\n",
      "- **Friday**\n",
      "- **Saturday**\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "resp = llm.invoke([HumanMessage(content=\"What day comes after Friday?\")])\n",
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd5a8ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, sure, just hop on a plane, but be careful not to bump into any yellow cabs in the sky! They're everywhere in New York, even up there! ðŸš–ðŸ›«\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Option B: assumes MISTRAL_API_KEY is in your environment\n",
    "chat = ChatMistralAI(model=\"mistral-small-latest\", temperature=1.0)\n",
    "\n",
    "resp = chat.invoke([\n",
    "    SystemMessage(content=\"You are an unhelpful AI bot that makes a joke at whatever the user says.\"),\n",
    "    HumanMessage(content=\"I would like to go to New York, how should I do this?\")\n",
    "])\n",
    "\n",
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bfc073d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'tool_calls': [{'id': 'y0xH83JFj', 'function': {'name': 'get_current_weather', 'arguments': '{\"location\": \"Boston\"}'}, 'index': 0}]} response_metadata={'token_usage': {'prompt_tokens': 100, 'total_tokens': 113, 'completion_tokens': 13}, 'model_name': 'mistral-small-latest', 'model': 'mistral-small-latest', 'finish_reason': 'tool_calls'} id='run--1b412b1e-9527-4ef6-9e05-3eee4e9b7c39-0' tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'Boston'}, 'id': 'y0xH83JFj', 'type': 'tool_call'}] usage_metadata={'input_tokens': 100, 'output_tokens': 13, 'total_tokens': 113}\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "import json\n",
    "\n",
    "# Define the function as a tool (this replaces functions=[...] in OpenAI)\n",
    "@tool\n",
    "def get_current_weather(location: str, unit: str = \"celsius\") -> dict:\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    return {\n",
    "        \"location\": location,\n",
    "        \"unit\": unit,\n",
    "        \"temperature\": 23,\n",
    "        \"condition\": \"partly cloudy\",\n",
    "    }\n",
    "\n",
    "# Use a valid Mistral model instead of gpt-3.5\n",
    "chat = ChatMistralAI(model=\"mistral-small-latest\", temperature=1)\n",
    "\n",
    "# Bind the tool\n",
    "chat_with_tools = chat.bind_tools([get_current_weather])\n",
    "\n",
    "# Run the model with messages\n",
    "output = chat_with_tools.invoke([\n",
    "    SystemMessage(content=\"You are a helpful AI bot\"),\n",
    "    HumanMessage(content=\"Whatâ€™s the weather like in Boston right now?\")\n",
    "])\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "302ce426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Brandon\\Projects\\gen-ai\\.venv\\Lib\\site-packages\\langchain_mistralai\\embeddings.py:186: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a sample: [-0.0302734375, 0.0300445556640625, 0.0487060546875, -0.019012451171875, 0.0269775390625]...\n",
      "Your embedding is length 1024\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "\n",
    "# Create embeddings client (uses MISTRAL_API_KEY from env)\n",
    "embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n",
    "\n",
    "text = \"Hi! It's time for the beach\"\n",
    "\n",
    "text_embedding = embeddings.embed_query(text)\n",
    "print(f\"Here's a sample: {text_embedding[:5]}...\")\n",
    "print(f\"Your embedding is length {len(text_embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5ac019e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Prompt: \n",
      "I really want to travel to Rome. What should I do there?\n",
      "\n",
      "Respond in one short sentence\n",
      "\n",
      "-----------\n",
      "LLM Output: Explore ancient ruins like the Colosseum, toss a coin in the Trevi Fountain, and savor authentic Italian gelato!\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Use a valid Mistral model\n",
    "llm = ChatMistralAI(model=\"mistral-small-latest\", temperature=0.7)\n",
    "\n",
    "# Notice \"location\" below, that is a placeholder for another value later\n",
    "template = \"\"\"\n",
    "I really want to travel to {location}. What should I do there?\n",
    "\n",
    "Respond in one short sentence\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"location\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(location='Rome')\n",
    "\n",
    "print(f\"Final Prompt: {final_prompt}\")\n",
    "print(\"-----------\")\n",
    "\n",
    "resp = llm.invoke(final_prompt)\n",
    "print(f\"LLM Output: {resp.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be7d8af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "# Use valid Mistral model\n",
    "llm = ChatMistralAI(model=\"mistral-small-latest\", temperature=0.7)\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Example Input: {input}\\nExample Output: {output}\",\n",
    ")\n",
    "\n",
    "# Examples of locations that nouns are found\n",
    "examples = [\n",
    "    {\"input\": \"pirate\", \"output\": \"ship\"},\n",
    "    {\"input\": \"pilot\", \"output\": \"plane\"},\n",
    "    {\"input\": \"driver\", \"output\": \"car\"},\n",
    "    {\"input\": \"tree\", \"output\": \"ground\"},\n",
    "    {\"input\": \"bird\", \"output\": \"nest\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afd36e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Brandon\\Projects\\gen-ai\\.venv\\Lib\\site-packages\\langchain_mistralai\\embeddings.py:186: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# SemanticSimilarityExampleSelector will select examples that are similar to your input by semantic meaning\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # This is the list of examples available to select from.\n",
    "    examples,\n",
    "\n",
    "    # Embedding class (uses your MISTRAL_API_KEY from env by default)\n",
    "    MistralAIEmbeddings(model=\"mistral-embed\"),\n",
    "\n",
    "    # VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    Chroma,\n",
    "\n",
    "    # Number of examples to produce.\n",
    "    k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fb654cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate\n",
    "\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # The object that will help select examples dynamically\n",
    "    example_selector=example_selector,\n",
    "\n",
    "    # The mini-prompt template for each example\n",
    "    example_prompt=example_prompt,\n",
    "\n",
    "    # Customizations that will be added to the top and bottom of your prompt\n",
    "    prefix=\"Give the location an item is usually found in\",\n",
    "    suffix=\"Input: {noun}\\nOutput:\",\n",
    "\n",
    "    # What inputs your prompt will receive\n",
    "    input_variables=[\"noun\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25b32843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the location an item is usually found in\n",
      "\n",
      "Example Input: bird\n",
      "Example Output: nest\n",
      "\n",
      "Example Input: tree\n",
      "Example Output: ground\n",
      "\n",
      "Input: plant\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# Select a noun!\n",
    "my_noun = \"plant\"\n",
    "# my_noun = \"student\"\n",
    "\n",
    "# The FewShotPromptTemplate will fill in examples and format the final prompt\n",
    "print(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21c7dbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      " Give the location an item is usually found in\n",
      "\n",
      "Example Input: bird\n",
      "Example Output: nest\n",
      "\n",
      "Example Input: tree\n",
      "Example Output: ground\n",
      "\n",
      "Input: plant\n",
      "Output:\n",
      "-----------\n",
      "LLM Output: pot\n",
      "\n",
      "(Plants are commonly found in pots when grown indoors or in gardens.)\n"
     ]
    }
   ],
   "source": [
    "# Generate the few-shot prompt\n",
    "prompt_text = similar_prompt.format(noun=my_noun)\n",
    "\n",
    "# Send to the Mistral model\n",
    "resp = llm.invoke(prompt_text)\n",
    "\n",
    "print(\"Prompt:\\n\", prompt_text)\n",
    "print(\"-----------\")\n",
    "print(\"LLM Output:\", resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a920809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_mistralai import ChatMistralAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d75ed816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "# Correct Mistral model, no stray args\n",
    "llm = ChatMistralAI(model=\"mistral-small-latest\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10966393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "# How you would like your response structured. This is basically a fancy prompt template\n",
    "response_schemas = [\n",
    "    ResponseSchema(\n",
    "        name=\"bad_string\", \n",
    "        description=\"This a poorly formatted user input string\"\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name=\"good_string\", \n",
    "        description=\"This is your response, a reformatted response\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# How you would like to parse your output\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a46ef45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response, a reformatted response\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# See the prompt template you created for formatting\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37dc3684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You will be given a poorly formatted string from a user.\n",
      "Reformat it and make sure all the words are spelled correctly.\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response, a reformatted response\n",
      "}\n",
      "```\n",
      "\n",
      "% USER INPUT:\n",
      "welcom to califonya!\n",
      "\n",
      "YOUR RESPONSE:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You will be given a poorly formatted string from a user.\n",
    "Reformat it and make sure all the words are spelled correctly.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "promptValue = prompt.format(user_input=\"welcom to califonya!\")\n",
    "\n",
    "print(promptValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "259d9643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw output:\n",
      " ```json\n",
      "{\n",
      "\t\"bad_string\": \"welcom to califonya!\",\n",
      "\t\"good_string\": \"Welcome to California!\"\n",
      "}\n",
      "```\n",
      "\n",
      "Parsed output:\n",
      " {'bad_string': 'welcom to califonya!', 'good_string': 'Welcome to California!'}\n"
     ]
    }
   ],
   "source": [
    "# Send the formatted prompt to Mistral\n",
    "resp = llm.invoke(promptValue)\n",
    "\n",
    "# See the raw model output\n",
    "print(\"Raw output:\\n\", resp.content)\n",
    "\n",
    "# Parse into structured JSON\n",
    "parsed = output_parser.parse(resp.content)\n",
    "print(\"\\nParsed output:\\n\", parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90ff8512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Alice' age=30 fav_food='Pizza'\n",
      "{'name': 'Alice', 'age': 30, 'fav_food': 'Pizza'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brandon\\AppData\\Local\\Temp\\ipykernel_26224\\1440040204.py:15: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  print(example.dict())\n"
     ]
    }
   ],
   "source": [
    "# Explicitly use Pydantic v1\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Identifying information about a person.\"\"\"\n",
    "\n",
    "    name: str = Field(..., description=\"The person's name\")\n",
    "    age: int = Field(..., description=\"The person's age\")\n",
    "    fav_food: Optional[str] = Field(None, description=\"The person's favorite food\")\n",
    "\n",
    "# Example usage\n",
    "example = Person(name=\"Alice\", age=30, fav_food=\"Pizza\")\n",
    "print(example)\n",
    "print(example.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3aa2e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format Instructions: The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"description\": \"Identifying information about a person.\", \"properties\": {\"name\": {\"description\": \"The person's name\", \"title\": \"Name\", \"type\": \"string\"}, \"age\": {\"description\": \"The person's age\", \"title\": \"Age\", \"type\": \"integer\"}, \"fav_food\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"default\": null, \"description\": \"The person's favorite food\", \"title\": \"Fav Food\"}}, \"required\": [\"name\", \"age\"]}\n",
      "```\n",
      "name='Sally' age=13 fav_food=None\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize the parser with the Person model\n",
    "parser = PydanticOutputParser(pydantic_object=Person)\n",
    "\n",
    "# Debug: Print format instructions to check schema\n",
    "try:\n",
    "    print(\"Format Instructions:\", parser.get_format_instructions())\n",
    "except AttributeError as e:\n",
    "    print(\"Error in get_format_instructions:\", e)\n",
    "\n",
    "# Use a valid Mistral model\n",
    "llm = ChatMistralAI(model=\"mistral-small-latest\", temperature=0)\n",
    "\n",
    "# Use the already defined parser variable (PydanticOutputParser)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    template=\"\"\"\n",
    "Extract the person's information from the text.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Respond ONLY with a valid JSON object, no extra text.\n",
    "\n",
    "Text: {text}\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "# Run the \"chain\"\n",
    "input_text = \"Sally is 13, Joey just turned 12 and loves spinach. Caroline is 10 years older than Sally.\"\n",
    "prompt_value = prompt.format(text=input_text)\n",
    "\n",
    "resp = llm.invoke(prompt_value)\n",
    "\n",
    "# Parse into your Person model\n",
    "person = parser.parse(resp.content)\n",
    "print(person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab64a402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people=[Person(name='Sally', age=13, fav_food=None), Person(name='Joey', age=12, fav_food='spinach'), Person(name='Caroline', age=23, fav_food=None)]\n",
      "{'people': [{'name': 'Sally', 'age': 13, 'fav_food': None}, {'name': 'Joey', 'age': 12, 'fav_food': 'spinach'}, {'name': 'Caroline', 'age': 23, 'fav_food': None}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brandon\\AppData\\Local\\Temp\\ipykernel_26224\\118697771.py:52: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  print(people.dict())\n"
     ]
    }
   ],
   "source": [
    "from typing import Sequence\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Person stays the same as before\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Identifying information about a person.\"\"\"\n",
    "    name: str = Field(..., description=\"The person's name\")\n",
    "    age: int = Field(..., description=\"The person's age\")\n",
    "    fav_food: str | None = Field(None, description=\"The person's favorite food\")\n",
    "\n",
    "# People wraps a list/sequence of Person\n",
    "class People(BaseModel):\n",
    "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
    "    people: Sequence[Person] = Field(..., description=\"The people in the text\")\n",
    "\n",
    "# Parser\n",
    "parser = PydanticOutputParser(pydantic_object=People)\n",
    "\n",
    "# Prompt that instructs the model to output JSON matching People\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    template=\"\"\"\n",
    "Extract all people and their information from the text.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Respond ONLY with valid JSON.\n",
    "\n",
    "Text: {text}\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "# Mistral model\n",
    "llm = ChatMistralAI(model=\"mistral-small-latest\", temperature=0)\n",
    "\n",
    "# Input text\n",
    "input_text = \"Sally is 13, Joey just turned 12 and loves spinach. Caroline is 10 years older than Sally.\"\n",
    "\n",
    "# Format prompt\n",
    "prompt_value = prompt.format(text=input_text)\n",
    "\n",
    "# Call model\n",
    "resp = llm.invoke(prompt_value)\n",
    "\n",
    "# Parse into People object\n",
    "people = parser.parse(resp.content)\n",
    "print(people)\n",
    "print(people.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c8200e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "products=[<Product.CRM: 'CRM'>, <Product.HARDWARE: 'HARDWARE'>, <Product.VIDEO_EDITING: 'VIDEO_EDITING'>]\n",
      "{'products': [<Product.CRM: 'CRM'>, <Product.HARDWARE: 'HARDWARE'>, <Product.VIDEO_EDITING: 'VIDEO_EDITING'>]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brandon\\AppData\\Local\\Temp\\ipykernel_26224\\3638810432.py:47: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  print(products.dict())\n"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "\n",
    "# Enum for product types\n",
    "class Product(str, enum.Enum):\n",
    "    CRM = \"CRM\"\n",
    "    VIDEO_EDITING = \"VIDEO_EDITING\"\n",
    "    HARDWARE = \"HARDWARE\"\n",
    "\n",
    "# Pydantic model for list of products\n",
    "class Products(BaseModel):\n",
    "    \"\"\"Identifying products that were mentioned in a text\"\"\"\n",
    "    products: Sequence[Product] = Field(..., description=\"The products mentioned in a text\")\n",
    "\n",
    "# Parser for Products schema\n",
    "parser = PydanticOutputParser(pydantic_object=Products)\n",
    "\n",
    "# Prompt template with format instructions\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    template=\"\"\"\n",
    "Extract the products mentioned in the following text.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Respond ONLY with valid JSON.\n",
    "\n",
    "Text: {text}\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "# Use Mistral model\n",
    "llm = ChatMistralAI(model=\"mistral-small-latest\", temperature=0)\n",
    "\n",
    "# Input\n",
    "input_text = \"The CRM in this demo is great. Love the hardware. The microphone is also cool. Love the video editing\"\n",
    "\n",
    "# Format the prompt\n",
    "prompt_value = prompt.format(text=input_text)\n",
    "\n",
    "# Call Mistral\n",
    "resp = llm.invoke(prompt_value)\n",
    "\n",
    "# Parse output into Products object\n",
    "products = parser.parse(resp.content)\n",
    "print(products)\n",
    "print(products.dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
