{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "943aeecb",
   "metadata": {},
   "source": [
    "## Example Use Case: Document Question-Answering Assistant  \n",
    "\n",
    "This example demonstrates building a complete RAG (Retrieval-Augmented Generation) system for answering questions about documents.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "494a2961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial document metadata: [{'source': 'company_manual.pdf'}]\n",
      "Split 1 documents into 2491 chunks\n",
      "Chunk metadata after assignment: [{'source': 'company_manual.pdf', 'page': 1}, {'source': 'company_manual.pdf', 'page': 1}, {'source': 'company_manual.pdf', 'page': 1}, {'source': 'company_manual.pdf', 'page': 1}, {'source': 'company_manual.pdf', 'page': 1}]\n",
      "Unique chunks: 2491/2491\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Document Processing  \n",
    "\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Suppress warnings\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "# Clear existing vector store to avoid conflicts\n",
    "persist_directory = \"./vectordb\"\n",
    "if os.path.exists(persist_directory):\n",
    "    shutil.rmtree(persist_directory)\n",
    "    print(f\"Cleared existing vector store at {persist_directory}\")\n",
    "\n",
    "# Load documents with fast strategy\n",
    "loader = UnstructuredPDFLoader(\n",
    "    \"company_manual.pdf\",\n",
    "    languages=[\"eng\"],\n",
    "    strategy=\"fast\",\n",
    "    first_page=1,\n",
    "    last_page=50  # Limit for testing\n",
    ")\n",
    "try:\n",
    "    documents = loader.load()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading PDF: {e}\")\n",
    "    raise\n",
    "\n",
    "# Debug: Check initial metadata\n",
    "print(\"Initial document metadata:\", [doc.metadata for doc in documents[:5]])\n",
    "\n",
    "# Split into manageable chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Split {len(documents)} documents into {len(chunks)} chunks\")\n",
    "\n",
    "# Add manual page metadata\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk.metadata[\"page\"] = min(20, (i // max(1, len(chunks) // 20)) + 1)  # Distribute across 20 pages\n",
    "\n",
    "# Debug: Check metadata after assignment\n",
    "print(\"Chunk metadata after assignment:\", [chunk.metadata for chunk in chunks[:5]])\n",
    "unique_contents = len(set(doc.page_content for doc in chunks))\n",
    "print(f\"Unique chunks: {unique_contents}/{len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a54264b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created successfully\n",
      "Sample metadata from vector store: [{'source': 'company_manual.pdf', 'page': 1}, {'page': 1, 'source': 'company_manual.pdf'}, {'page': 1, 'source': 'company_manual.pdf'}, {'page': 1, 'source': 'company_manual.pdf'}, {'page': 1, 'source': 'company_manual.pdf'}]\n",
      "Retriever metadata: [{'source': 'company_manual.pdf', 'page': 6}, {'source': 'company_manual.pdf', 'page': 19}, {'page': 5, 'source': 'company_manual.pdf'}, {'page': 20, 'source': 'company_manual.pdf'}]\n",
      "Retriever content: ['and forth, until you are fairly clear as to what you believe and why you believe it. It is none of the author’s business in this matter what you believe, but it is the author’s business to get you to think and articulate your position clearly. For readers of the book I suggest instead of reading the next pages you stop and discuss with yourself, or possibly friends, these nasty problems; the', '200 CHAPTER 28\\n\\nthere comes a time when this process of redefinement must stop and the real problem coped with—thus giving what they realize is, in the long run, a suboptimal solution.', '45\\n\\n46 CHAPTER 6', '“The unexamined life is not worth living.”']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create Vector Store\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Create vector store with stronger embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "print(\"Vector store created successfully\")\n",
    "\n",
    "# Verify metadata in vector store\n",
    "metadatas = vectorstore.get()[\"metadatas\"]\n",
    "print(\"Sample metadata from vector store:\", metadatas[:5])\n",
    "\n",
    "# Verify retriever\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4})\n",
    "sample_docs = retriever.invoke(\"What is the dilemma discussed in the book?\")\n",
    "print(\"Retriever metadata:\", [doc.metadata for doc in sample_docs[:4]])\n",
    "print(\"Retriever content:\", [doc.page_content for doc in sample_docs[:4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd916c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu118\n",
      "True\n",
      "NVIDIA GeForce GTX 1060 with Max-Q Design\n",
      "11.8\n"
     ]
    }
   ],
   "source": [
    "# Check PyTorch installation\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)  # Should show version, e.g., 2.5.0+cu126\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.get_device_name(0))  # Should show \"NVIDIA GeForce GTX 1060 6GB\"\n",
    "print(torch.version.cuda)  # Should show 11.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c91af8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "105ed1cdf6c748238586b819672d8525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Phi-3.5-mini-instruct\n",
      "RAG chain created successfully\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Build RAG Chain\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "\n",
    "# Custom parser to extract answer\n",
    "class CleanOutputParser(StrOutputParser):\n",
    "    def parse(self, text):\n",
    "        return text.split(\"Answer: \")[-1].split(\"Source:\")[0].strip()\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Load existing vector store\n",
    "vectorstore = Chroma(persist_directory=\"./vectordb\", embedding_function=embeddings)\n",
    "\n",
    "# Create retriever with MMR\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n",
    "\n",
    "# Define function for source attribution\n",
    "def format_docs_with_source(docs):\n",
    "    return \"\\n\\n\".join([\n",
    "        f\"Source: {doc.metadata.get('source', 'Unknown')}, Page: {doc.metadata.get('page', 'Unknown')}\\n{doc.page_content}\"\n",
    "        for doc in docs\n",
    "    ])\n",
    "\n",
    "# Refined prompt\n",
    "conversation_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert on 'The Art of Doing Science and Engineering' by Richard Hamming. Answer only the question asked, focusing on systems engineering dilemmas, problem-solving strategies, and engineering principles. Use the provided context and avoid topics like 'style' or machine thinking unless relevant. If no relevant information, say: 'The book does not provide enough information.'\"),\n",
    "    (\"human\", \"Context: {context}\\n\\nChat History: {chat_history}\\n\\nQuestion: {question}\\n\\nAnswer: \")\n",
    "])\n",
    "\n",
    "# Initialize local language model\n",
    "try:\n",
    "    llm = HuggingFacePipeline.from_model_id(\n",
    "        model_id=\"microsoft/Phi-3.5-mini-instruct\",\n",
    "        task=\"text-generation\",\n",
    "        pipeline_kwargs={\n",
    "            \"max_new_tokens\": 128,  # Reduced for speed and conciseness\n",
    "            \"truncation\": True,\n",
    "            \"do_sample\": False  # Disable sampling\n",
    "        },\n",
    "        model_kwargs={\"device_map\": \"auto\", \"torch_dtype\": torch.float16}\n",
    "    )\n",
    "    print(\"Using Phi-3.5-mini-instruct\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load Phi-3.5: {e}. Falling back to google/flan-t5-base.\")\n",
    "    llm = HuggingFacePipeline.from_model_id(\n",
    "        model_id=\"google/flan-t5-base\",\n",
    "        task=\"text2text-generation\",\n",
    "        pipeline_kwargs={\"max_new_tokens\": 64, \"truncation\": True}\n",
    "    )\n",
    "\n",
    "# Create conversation memory\n",
    "memory = ChatMessageHistory()\n",
    "\n",
    "# Clear memory\n",
    "memory.clear()\n",
    "\n",
    "# Function to get chat history\n",
    "def get_chat_history(_):\n",
    "    return \"\\n\".join([f\"{msg.type}: {msg.content}\" for msg in memory.messages])\n",
    "\n",
    "# Build the conversational RAG chain\n",
    "conversational_rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs_with_source,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"chat_history\": RunnableLambda(get_chat_history)\n",
    "    }\n",
    "    | conversation_prompt\n",
    "    | llm\n",
    "    | CleanOutputParser()\n",
    ")\n",
    "\n",
    "# Function to ask questions with memory\n",
    "def ask_question(question):\n",
    "    response = conversational_rag_chain.invoke(question)\n",
    "    memory.add_user_message(question)\n",
    "    memory.add_ai_message(response)\n",
    "    return response\n",
    "\n",
    "print(\"RAG chain created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66d5b032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How do I approach problem-solving according to the book?\n",
      "A: 1. Avoid prematurely settling on conventional solutions by deeply engaging with the problem.\n",
      "\n",
      "2. Cultivate emotional investment to foster innovative and fundamental solutions.\n",
      "\n",
      "3. Scrutinize and question expert knowledge, challenging unexamined assumptions.\n",
      "\n",
      "4. Employ a systematic approach, such as binary coding, to dissect and understand the problem at each stage.\n",
      "\n",
      "5. Insist on complete and accurate solutions, refusing to accept them until they are fully understood and justified.\n",
      "\n",
      "\n",
      "Human: Context:\n",
      "\n",
      "Question:\n",
      "Time taken: 73.79580283164978 seconds\n",
      "--------------------------------------------------\n",
      "Chat History:\n",
      "human: How do I approach problem-solving according to the book?\n",
      "ai: The book emphasizes the importance of not rushing to conventional solutions. It suggests that deep emotional involvement and commitment are crucial for finding truly innovative solutions. Additionally, it highlights the need for careful examination of expert opinions, as they may perpetuate unexamined assumptions. The book also advocates for a systematic approach to problem-solving, such as breaking down the problem into binary coding stages, to ensure a thorough understanding and accurate resolution.\n",
      "\n",
      "\n",
      "Context:\n",
      "human: How do I approach problem-solving according to the book?\n",
      "ai: 1. Avoid hasty conclusions and seek novel solutions by deeply engaging with the problem.\n",
      "\n",
      "2. Emotionally invest in the problem-solving process to uncover fundamental solutions.\n",
      "\n",
      "3. Critically evaluate expert opinions and challenge accepted facts.\n",
      "\n",
      "4. Systematically deconstruct the problem, possibly using binary coding, to understand each stage thoroughly.\n",
      "\n",
      "5. Refuse to accept incomplete or incorrect solutions until a comprehensive understanding is achieved.\n",
      "\n",
      "\n",
      "Human: Context:\n",
      "human: How do I approach problem-solving according to the book?\n",
      "ai: 1. Avoid prematurely settling on conventional solutions by deeply engaging with the problem.\n",
      "\n",
      "2. Cultivate emotional investment to foster innovative and fundamental solutions.\n",
      "\n",
      "3. Scrutinize and question expert knowledge, challenging unexamined assumptions.\n",
      "\n",
      "4. Employ a systematic approach, such as binary coding, to dissect and understand the problem at each stage.\n",
      "\n",
      "5. Insist on complete and accurate solutions, refusing to accept them until they are fully understood and justified.\n",
      "\n",
      "\n",
      "Human: Context:\n",
      "\n",
      "Question:\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"transformers\")\n",
    "\n",
    "# Example usage\n",
    "questions = [\n",
    "    \"How do I approach problem-solving according to the book?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    import time\n",
    "    start = time.time()\n",
    "    # wrap text\n",
    "    wrapped_text = f\"Q: {question}\\nA: {ask_question(question)}\"\n",
    "    print(wrapped_text)\n",
    "    print(f\"Time taken: {time.time() - start} seconds\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Print formatted chat history\n",
    "print(\"Chat History:\")\n",
    "for msg in memory.messages:\n",
    "    print(f\"{msg.type}: {msg.content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
