{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "115c7f6b",
   "metadata": {},
   "source": [
    "## What is LangChain?  \n",
    "\n",
    "LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs). While most users interact with LLMs by sending prompts and receiving responses, LangChain allows developers to go beyond simple prompt-response patterns.  \n",
    "\n",
    "LangChain introduces modular components that make it easier to:  \n",
    "\n",
    "- Create reusable and structured prompts\n",
    "- Chain multiple prompts and model calls together\n",
    "- Add memory to retain context over interactions\n",
    "- Connect language models with external data and tools\n",
    "- Build intelligent agents capable of decision-making  \n",
    "\n",
    "### Framework Architecture  \n",
    "\n",
    "LangChain follows a modular, component-based architecture that allows developers to chain together interoperable components. The framework consists of:  \n",
    "\n",
    "- **langchain-core**  \n",
    "- **Integration packages**  \n",
    "- **langchain library**  \n",
    "- **LangGraph**  \n",
    "- **LangSmith**  \n",
    "\n",
    "### Why Use LangChain?  \n",
    "\n",
    "**Standardized Interfaces**: LangChain provides unified interfaces across 600+ integrations, solving the fragmentation problem in AI development. Different LLM providers have varying APIs, but LangChain enables switching between models with minimal code changes.  \n",
    "\n",
    "**Sophisticated Orchestration**: Handle complex control flows with cycles, conditional logic, and multi-agent coordination. This includes persistent state management across conversations and multi-step reasoning workflows.  \n",
    "\n",
    "**Built-in Observability**: LangSmith integration provides trace-level visibility into LLM applications, performance monitoring, cost tracking, and A/B testing capabilities.  \n",
    "\n",
    "**Production Ready**: Trusted by companies like LinkedIn, Uber, and GitLab for production applications.  \n",
    "\n",
    "### Core Components of LangChain  \n",
    "\n",
    "Every LangChain component implements the Runnable interface, providing standard methods:  \n",
    "\n",
    "`invoke()` for single transformations  \n",
    "`batch()` for multiple inputs  \n",
    "`stream()` for streaming outputs  \n",
    "Async variants for non-blocking operations  \n",
    "\n",
    "This enables LangChain Expression Language (LCEL) for declarative chain composition:  \n",
    "\n",
    "\n",
    "```\n",
    "# Chain composition using LCEL\n",
    "chain = prompt | model | output_parser\n",
    "result = chain.invoke({\"input\": \"hello\"})\n",
    "```\n",
    "\n",
    "\n",
    "### Chat Models and Language Models  \n",
    "\n",
    "**Chat models** are preferred for conversational interactions, handling message-based APIs with proper context management:  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd074256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "messages = [\n",
    "    SystemMessage(\"You are a helpful assistant\"),\n",
    "    HumanMessage(\"Explain quantum computing\")\n",
    "]\n",
    "response = model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7cad41",
   "metadata": {},
   "source": [
    "### Prompt Engineering and Templates  \n",
    "\n",
    "**PromptTemplate** and **ChatPromptTemplate** enable dynamic prompt construction:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3720d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a {role}\"),\n",
    "    (\"user\", \"Explain {topic} in simple terms\")\n",
    "])\n",
    "\n",
    "prompt = template.invoke({\n",
    "    \"role\": \"teacher\", \n",
    "    \"topic\": \"machine learning\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4189351",
   "metadata": {},
   "source": [
    "### Document Processing Pipeline  \n",
    "\n",
    "**Document loaders** handle various data sources:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f47354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader, PDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load documents\n",
    "loader = TextLoader(\"document.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4bf8de",
   "metadata": {},
   "source": [
    "### Embedding Models and Vector Stores  \n",
    "\n",
    "**Embeddings** convert text to vector representations for semantic search:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba75378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d522df",
   "metadata": {},
   "source": [
    "### Tools and External Integrations  \n",
    "\n",
    "**Tools** enable LLM applications to interact with external systems:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f8640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def search_web(query: str) -> str:\n",
    "    \"\"\"Search the web for information.\"\"\"\n",
    "    # Implementation here\n",
    "    return search_results\n",
    "\n",
    "# Bind tools to model\n",
    "model_with_tools = model.bind_tools([search_web])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
