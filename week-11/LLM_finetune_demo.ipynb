{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca683b65",
   "metadata": {},
   "source": [
    "# Tiny Word‑Level Transformer: **Before vs After Fine‑Tuning** (CPU‑Fast Demo)  \n",
    "\n",
    "## 1. Environment & Reproducibility  \n",
    "\n",
    "We will set PyTorch and Python seeds for reproducible results, and keep everything CPU-only and fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4163d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import TransformerDecoderLayer, TransformerDecoder\n",
    "\n",
    "# Reproducibility and speed (CPU only)\n",
    "torch.manual_seed(7)\n",
    "random.seed(7)\n",
    "device = \"cpu\"\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940b78bd",
   "metadata": {},
   "source": [
    "## 2. Minimal **World-Level** Tokenizer  \n",
    "\n",
    "- Splits on words (keeps hyphenated terms like `Code-You`) and punctuation tokens.  \n",
    "- Provides: `tokenize`, `detokenize`, vocab builders, and encode/decode helpers.  \n",
    "- Includes special tokens: `<bos>`, `<eos>`, `<unk>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7959842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str):\n",
    "    # words + hyphenated + punctuation (includes ':' for QA format)\n",
    "    return re.findall(r\"[A-Za-z]+(?:-[A-Za-z]+)?|[:?.!,']\", text)\n",
    "\n",
    "def detokenize(toks):\n",
    "    s = \" \".join(toks)\n",
    "    return re.sub(r\"\\s+([?:.!,'])\", r\"\\1\", s)\n",
    "\n",
    "def build_vocab_texts(texts, specials=(\"\", \"\", \"\")):\n",
    "    vocab = set()\n",
    "    for t in texts:\n",
    "        vocab.update(tokenize(t))\n",
    "    vocab = list(specials) + sorted(vocab)\n",
    "    stoi = {w: i for i, w in enumerate(vocab)}\n",
    "    itos = {i: w for w, i in stoi.items()}\n",
    "    return stoi, itos\n",
    "\n",
    "def enc_line(line, stoi):\n",
    "    toks = [\"\"] + tokenize(line) + [\"\"]\n",
    "    unk = stoi[\"\"]\n",
    "    return [stoi.get(t, unk) for t in toks]\n",
    "\n",
    "def enc_prefix(prefix, stoi):\n",
    "    # used for prompts (no  appended so the model can continue)\n",
    "    toks = [\"\"] + tokenize(prefix)\n",
    "    unk = stoi[\"\"]\n",
    "    return [stoi.get(t, unk) for t in toks]\n",
    "\n",
    "def dec_ids(ids, itos, keep_eos=False):\n",
    "    toks = [itos[i] for i in ids]\n",
    "    toks = [t for t in toks if t != \"\" and (keep_eos or t != \"\")]\n",
    "    return detokenize(toks)\n",
    "\n",
    "print(\"Tokenizer ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce5bf41",
   "metadata": {},
   "source": [
    "## 3. Dataset for Next-Token Prediction  \n",
    "\n",
    "Sequences are prepared as `(x, y)` where `y` is `x` shifted by 1 (classic next-token LM training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf59b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDS(Dataset):\n",
    "    def __init__(self, ids, seq_len=24):\n",
    "        self.ids = ids\n",
    "        self.seq_len = seq_len\n",
    "    def __len__(self):\n",
    "        return max(0, len(self.ids) - self.seq_len - 1)\n",
    "    def __getitem__(self, i):\n",
    "        x = self.ids[i:i+self.seq_len]\n",
    "        y = self.ids[i+1:i+self.seq_len+1]\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "print(\"Dataset class ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac666650",
   "metadata": {},
   "source": [
    "## 4. Tiny **Decoder-Only** Transformer  \n",
    "\n",
    "We use `TransformerDecoder` with a **causal mask** so tokens cannot attend to the future.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01036a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEnc(nn.Module):\n",
    "    def __init__(self, d, max_len=512):\n",
    "        super().__init__()\n",
    "        self.pos = nn.Embedding(max_len, d)\n",
    "    def forward(self, x):  # x: [B,T]\n",
    "        T = x.size(1)\n",
    "        return self.pos(torch.arange(T, device=x.device)[None, :])\n",
    "\n",
    "class TinyLM(nn.Module):\n",
    "    def __init__(self, V, d=96, heads=3, layers=2, max_len=512):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(V, d)\n",
    "        self.pos = PosEnc(d, max_len)\n",
    "        block = TransformerDecoderLayer(d_model=d, nhead=heads, batch_first=False)\n",
    "        self.dec = TransformerDecoder(block, num_layers=layers)\n",
    "        self.head = nn.Linear(d, V)\n",
    "\n",
    "    def forward(self, ids):  # ids: [B,T]\n",
    "        x = self.emb(ids) + self.pos(ids)           # [B,T,E]\n",
    "        h = x.transpose(0,1)                        # [T,B,E]\n",
    "        T = h.size(0)\n",
    "        mask = torch.triu(torch.ones(T, T, device=h.device), diagonal=1).bool()\n",
    "        out = self.dec(h, h, tgt_mask=mask).transpose(0,1)  # [B,T,E]\n",
    "        return self.head(out)                       # [B,T,V]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_until_eos(self, start_ids, eos_id, max_new=20):\n",
    "        self.eval()\n",
    "        ids = start_ids\n",
    "        for _ in range(max_new):\n",
    "            logits = self.forward(ids)[:, -1, :]      # [B,V]\n",
    "            nxt = torch.argmax(logits, dim=-1, keepdim=True)  # greedy\n",
    "            ids = torch.cat([ids, nxt], dim=1)\n",
    "            if (nxt == eos_id).all():\n",
    "                break\n",
    "        return ids\n",
    "\n",
    "print(\"Model ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553df441",
   "metadata": {},
   "source": [
    "## 5. Corpora  \n",
    "\n",
    "- **Base pretraining**: facts + distractors (no QA lines) → answers may be imperfect.\n",
    "- **Fine‑tuning pairs**: focused QA you want to be perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a91f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_corpus_with_distractors():\n",
    "    # True facts (no QA format)\n",
    "    core = [\n",
    "        \"I am John Doe.\",\n",
    "        \"I am instructor at Code-You.\",\n",
    "        \"My name is John Doe.\",\n",
    "        \"My profession is instructor at Code-You.\"\n",
    "    ]\n",
    "    # Distractors to confuse pretrain\n",
    "    others = [\n",
    "        \"I am Jane Roe.\",\n",
    "        \"I am engineer at Code-You.\",\n",
    "        \"My name is Alan Smithee.\",\n",
    "        \"My profession is designer at Code-You.\",\n",
    "        \"I am John Doe and I am engineer.\",\n",
    "        \"My profession is instructor at Code-Me.\"\n",
    "    ]\n",
    "    corpus = (core*5 + others*7)\n",
    "    random.shuffle(corpus)\n",
    "    return corpus\n",
    "\n",
    "def finetune_pairs():\n",
    "    return [\n",
    "        \"Q: Who I am? A: John Doe.\",\n",
    "        \"Q: What is my profession? A: instructor at Code-You.\"\n",
    "    ]\n",
    "\n",
    "print(\"Corpora generators ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad14712",
   "metadata": {},
   "source": [
    "## 6. Training & Evaluation Helpers  \n",
    "\n",
    "- `build_ids`: flatten lines into one long sequence of token IDs.\n",
    "- `trainer`: one optimization step wrapper.\n",
    "- `ask`: formats a QA prompt and generates until `<eos>`.\n",
    "- `exact_match`: simple EM metric (case-insensitive string match)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c82792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ids(lines, stoi):\n",
    "    ids = []\n",
    "    for ln in lines:\n",
    "        ids.extend(enc_line(ln, stoi))\n",
    "    return ids\n",
    "\n",
    "def trainer(model, V, lr=2e-3):\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    def step(batch):\n",
    "        x, y = (b.to(device) for b in batch)\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits.reshape(-1, V), y.reshape(-1))\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        return float(loss.item())\n",
    "    return step\n",
    "\n",
    "def ask(model, q, stoi, itos, max_new=10):\n",
    "    prompt = f\"Q: {q.strip()}? A:\"\n",
    "    start = torch.tensor([enc_prefix(prompt, stoi)], dtype=torch.long, device=device)\n",
    "    eos_id = stoi[\"\"]\n",
    "    out = model.generate_until_eos(start, eos_id, max_new=max_new)[0].tolist()\n",
    "    txt = dec_ids(out, itos)\n",
    "    # Extract after \"A:\"\n",
    "    if \"A:\" in txt:\n",
    "        ans = txt.split(\"A:\", 1)[1].strip()\n",
    "        m = re.search(r\"^(.+?[.?!])\", ans)\n",
    "        if m: ans = m.group(1)\n",
    "        return ans.strip()\n",
    "    return txt.strip()\n",
    "\n",
    "def exact_match(pred, gold):\n",
    "    return int(pred.strip().lower() == gold.strip().lower())\n",
    "\n",
    "print(\"Helpers ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fca818d",
   "metadata": {},
   "source": [
    "## 7. Build Vocab and **Short Pretraining** (Imperfect on Purpose)  \n",
    "\n",
    "- Vocab is built from all potential tokens (base + fine‑tune pairs + prompt forms).\n",
    "- Pretraining is **run only on base corpus** (with distractors) and **without QA lines** to keep answers imperfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b38035",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    (\"Who I am\", \"John Doe.\"),\n",
    "    (\"What is my profession\", \"instructor at Code-You.\")\n",
    "]\n",
    "\n",
    "# Build vocab from all tokens we might see\n",
    "vocab_source = base_corpus_with_distractors() + finetune_pairs() + [f\"Q: {q}? A:\" for q,_ in prompts]\n",
    "stoi, itos = build_vocab_texts(vocab_source)\n",
    "\n",
    "# Pretrain (short) on distractor-rich base ONLY (no QA lines)\n",
    "base_ids = build_ids(base_corpus_with_distractors(), stoi)\n",
    "ds = SeqDS(base_ids, seq_len=24)\n",
    "dl = DataLoader(ds, batch_size=16, shuffle=True, drop_last=True, num_workers=0)\n",
    "\n",
    "V = len(stoi)\n",
    "model = TinyLM(V).to(device)\n",
    "step_fn = trainer(model, V, lr=2e-3)\n",
    "\n",
    "PRE_STEPS = 60  # keep short so model is *not* perfect yet\n",
    "i = 0\n",
    "for x, y in dl:\n",
    "    loss = step_fn((x, y))\n",
    "    i += 1\n",
    "    if i % 20 == 0:\n",
    "        print(f\"[PRE] step {i:03d}  loss {loss:.4f}\")\n",
    "    if i >= PRE_STEPS:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195bc162",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- BEFORE fine-tune ---\")\n",
    "em_before = []\n",
    "for q, gold in prompts:\n",
    "    pred = ask(model, q, stoi, itos, max_new=10)\n",
    "    em = exact_match(pred, gold); em_before.append(em)\n",
    "    print(f\"{q}: {pred}    (EM={em})\")\n",
    "print(f\"EM before: {sum(em_before)}/{len(em_before)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bccda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(model, stoi, steps=180, seq_len=24, batch_size=16, lr=1e-3, repeat=120, log_every=30):\n",
    "    lines = finetune_pairs() * repeat\n",
    "    ft_ids = build_ids(lines, stoi)\n",
    "    ft_ds = SeqDS(ft_ids, seq_len=seq_len)\n",
    "    ft_dl = DataLoader(ft_ds, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=0)\n",
    "\n",
    "    V = len(stoi)\n",
    "    step = trainer(model, V, lr=lr)\n",
    "\n",
    "    j = 0\n",
    "    for x, y in ft_dl:\n",
    "        loss = step((x, y))\n",
    "        j += 1\n",
    "        if j % log_every == 0:\n",
    "            print(f\"[FT]  step {j:03d}  loss {loss:.4f}\")\n",
    "        if j >= steps:\n",
    "            break\n",
    "\n",
    "print(\"Fine-tune function ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98c9dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- FINE-TUNING ---\")\n",
    "fine_tune(model, stoi, steps=180, lr=1e-3, repeat=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7a298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- AFTER fine-tune ---\")\n",
    "em_after = []\n",
    "for q, gold in prompts:\n",
    "    pred = ask(model, q, stoi, itos, max_new=10)\n",
    "    em = exact_match(pred, gold); em_after.append(em)\n",
    "    print(f\"{q}: {pred}    (EM={em})\")\n",
    "print(f\"EM after: {sum(em_after)}/{len(em_after)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
