{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f04c2a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset creation...\n",
      "This may take 10-15 minutes due to API rate limits...\n",
      "\n",
      "==================================================\n",
      "Collecting papers for: Technology\n",
      "==================================================\n",
      "Fetching papers 1-50 for category: cat:cs.AI\n",
      "Fetching papers 51-100 for category: cat:cs.AI\n",
      "Fetching papers 1-50 for category: cat:cs.LG\n",
      "Fetching papers 51-100 for category: cat:cs.LG\n",
      "Fetching papers 1-50 for category: cat:cs.CV\n",
      "Fetching papers 51-100 for category: cat:cs.CV\n",
      "Fetching papers 1-50 for category: cat:cs.NE\n",
      "Fetching papers 51-100 for category: cat:cs.NE\n",
      "Collected 248 papers for Technology\n",
      "\n",
      "==================================================\n",
      "Collecting papers for: Healthcare\n",
      "==================================================\n",
      "Fetching papers 1-50 for category: cat:q-bio.QM\n",
      "Fetching papers 51-100 for category: cat:q-bio.QM\n",
      "Fetching papers 1-50 for category: cat:physics.med-ph\n",
      "Fetching papers 51-100 for category: cat:physics.med-ph\n",
      "Fetching papers 1-50 for category: all:medical AND all:health\n",
      "Fetching papers 51-100 for category: all:medical AND all:health\n",
      "Fetching papers 1-50 for category: all:clinical AND all:treatment\n",
      "Fetching papers 51-100 for category: all:clinical AND all:treatment\n",
      "Collected 248 papers for Healthcare\n",
      "\n",
      "==================================================\n",
      "Collecting papers for: Finance\n",
      "==================================================\n",
      "Fetching papers 1-50 for category: cat:q-fin.GN\n",
      "Fetching papers 51-100 for category: cat:q-fin.GN\n",
      "Fetching papers 1-50 for category: cat:q-fin.CP\n",
      "Fetching papers 51-100 for category: cat:q-fin.CP\n",
      "Fetching papers 1-50 for category: all:financial AND all:market\n",
      "Fetching papers 51-100 for category: all:financial AND all:market\n",
      "Fetching papers 1-50 for category: all:economics AND all:investment\n",
      "Fetching papers 51-100 for category: all:economics AND all:investment\n",
      "Collected 248 papers for Finance\n",
      "\n",
      "==================================================\n",
      "Collecting papers for: Education\n",
      "==================================================\n",
      "Fetching papers 1-50 for category: all:education AND all:learning\n",
      "Fetching papers 51-100 for category: all:education AND all:learning\n",
      "Fetching papers 1-50 for category: all:educational AND all:technology\n",
      "Fetching papers 51-100 for category: all:educational AND all:technology\n",
      "Fetching papers 1-50 for category: all:teaching AND all:pedagogy\n",
      "Fetching papers 51-100 for category: all:teaching AND all:pedagogy\n",
      "Fetching papers 1-50 for category: all:curriculum AND all:assessment\n",
      "Fetching papers 51-100 for category: all:curriculum AND all:assessment\n",
      "Collected 248 papers for Education\n",
      "\n",
      "==================================================\n",
      "Collecting papers for: Environment\n",
      "==================================================\n",
      "Fetching papers 1-50 for category: cat:physics.ao-ph\n",
      "Fetching papers 51-100 for category: cat:physics.ao-ph\n",
      "Fetching papers 1-50 for category: all:climate AND all:change\n",
      "Fetching papers 51-100 for category: all:climate AND all:change\n",
      "Fetching papers 1-50 for category: all:environmental AND all:sustainability\n",
      "Fetching papers 51-100 for category: all:environmental AND all:sustainability\n",
      "Fetching papers 1-50 for category: all:renewable AND all:energy\n",
      "Fetching papers 51-100 for category: all:renewable AND all:energy\n",
      "Collected 248 papers for Environment\n",
      "\n",
      "==================================================\n",
      "DATASET CREATION SUMMARY\n",
      "==================================================\n",
      "Total papers collected: 1217\n",
      "\n",
      "Category distribution:\n",
      "category\n",
      "Healthcare     248\n",
      "Technology     248\n",
      "Finance        247\n",
      "Environment    246\n",
      "Education      228\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset saved as 'research_papers_dataset.csv'\n",
      "\n",
      "Sample data:\n",
      "                                                                                title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        text    category\n",
      "0                                                     Stable-12 Bridges and Insurance  We develop a class of non-life reserving models using a stable-12 random bridge to simulate the accumulation of paid claims, allowing for an essentially arbitrary choice of a priori distribution for the ultimate loss. Taking an information-based approach to the reserving problem, we derive the process of the conditional distribution of the ultimate loss. The best-estimate ultimate loss process is given by the conditional expectation of the ultimate loss. We derive explicit expressions for the best-estimate ultimate loss process, and for expected recoveries arising from aggregate excess-of-loss reinsurance treaties. Use of a deterministic time change allows for the matching of any initial (increasing) development pattern for the paid claims. We show that these methods are well-suited to the modelling of claims where there is a non-trivial probability of catastrophic loss. The generalized inverse-Gaussian (GIG) distribution is shown to be a natural choice for the a priori ultimate loss distribution. For particular GIG parameter choices, the best-estimate ultimate loss process can be written as a rational function of the paid-claims process. We extend the model to include a second paid-claims process, and allow the two processes to be dependent. The results obtained can be applied to the modelling of multiple lines of business or multiple origin years. The multi-dimensional model has the property that the dimensionality of calculations remains low, regardless of the number of paid-claims processes. An algorithm is provided for the simulation of the paid-claims processes.  Stable-12 Bridges and Insurance We develop a class of non-life reserving models using a stable-12 random bridge to simulate the accumulation of paid claims, allowing for an essentially arbitrary choice of a priori distribution for the ultimate loss. Taking an information-based approach to the reserving problem, we derive the process of the conditional distribution of the ultimate loss. The best-estimate ultimate loss process is given by the conditional expectation of the ultimate loss. We derive explicit expressions for the best-estimate ultimate loss process, and for expected recoveries arising from aggregate excess-of-loss reinsurance treaties. Use of a deterministic time change allows for the matching of any initial (increasing) development pattern for the paid claims. We show that these methods are well-suited to the modelling of claims where there is a non-trivial probability of catastrophic loss. The generalized inverse-Gaussian (GIG) distribution is shown to be a natural choice for the a priori ultimate loss distribution. For particular GIG parameter choices, the best-estimate ultimate loss process can be written as a rational function of the paid-claims process. We extend the model to include a second paid-claims process, and allow the two processes to be dependent. The results obtained can be applied to the modelling of multiple lines of business or multiple origin years. The multi-dimensional model has the property that the dimensionality of calculations remains low, regardless of the number of paid-claims processes. An algorithm is provided for the simulation of the paid-claims processes.     Finance\n",
      "1  Rate-dependent propagation of cardiac action potentials in a one-dimensional fiber                                                                                                                                                                                                                                                                                                                                                                                                                                                              Action potential duration (APD) restitution, which relates APD to the preceding diastolic interval (DI), is a useful tool for predicting the onset of abnormal cardiac rhythms. However, it is known that different pacing protocols lead to different APD restitution curves (RCs). This phenomenon, known as APD rate-dependence, is a consequence of memory in the tissue. In addition to APD restitution, conduction velocity restitution also plays an important role in the spatiotemporal dynamics of cardiac tissue. We present new results concerning rate-dependent restitution in the velocity of propagating action potentials in a one-dimensional fiber. Our numerical simulations show that, independent of the amount of memory in the tissue, waveback velocity exhibits pronounced rate-dependence and the wavefront velocity does not. Moreover, the discrepancy between waveback velocity RCs is most significant for small DI. We provide an analytical explanation of these results, using a system of coupled maps to relate the wavefront and waveback velocities. Our calculations show that waveback velocity rate-dependence is due to APD restitution, not memory.                                                                                                                                                                                                                                                                                                                                                                                                           Rate-dependent propagation of cardiac action potentials in a one-dimensional fiber Action potential duration (APD) restitution, which relates APD to the preceding diastolic interval (DI), is a useful tool for predicting the onset of abnormal cardiac rhythms. However, it is known that different pacing protocols lead to different APD restitution curves (RCs). This phenomenon, known as APD rate-dependence, is a consequence of memory in the tissue. In addition to APD restitution, conduction velocity restitution also plays an important role in the spatiotemporal dynamics of cardiac tissue. We present new results concerning rate-dependent restitution in the velocity of propagating action potentials in a one-dimensional fiber. Our numerical simulations show that, independent of the amount of memory in the tissue, waveback velocity exhibits pronounced rate-dependence and the wavefront velocity does not. Moreover, the discrepancy between waveback velocity RCs is most significant for small DI. We provide an analytical explanation of these results, using a system of coupled maps to relate the wavefront and waveback velocities. Our calculations show that waveback velocity rate-dependence is due to APD restitution, not memory.  Healthcare\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import feedparser\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import quote\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing extra whitespace and special characters\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Remove extra whitespace and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    # Remove special characters that might cause CSV issues\n",
    "    text = re.sub(r'[^\\w\\s\\.,;:!?()-]', '', text)\n",
    "    return text\n",
    "\n",
    "def fetch_arxiv_papers(category_query, max_results=250):\n",
    "    \"\"\"Fetch papers from ArXiv for a specific category\"\"\"\n",
    "    base_url = \"http://export.arxiv.org/api/query?\"\n",
    "    \n",
    "    papers = []\n",
    "    start = 0\n",
    "    batch_size = 50  # ArXiv limits to 50 per request\n",
    "    \n",
    "    while len(papers) < max_results:\n",
    "        query = f\"search_query={quote(category_query)}&start={start}&max_results={batch_size}\"\n",
    "        url = base_url + query\n",
    "        \n",
    "        print(f\"Fetching papers {start+1}-{start+batch_size} for category: {category_query}\")\n",
    "        \n",
    "        try:\n",
    "            response = feedparser.parse(url)\n",
    "            \n",
    "            if not response.entries:\n",
    "                print(f\"No more papers found for {category_query}\")\n",
    "                break\n",
    "            \n",
    "            for entry in response.entries:\n",
    "                if len(papers) >= max_results:\n",
    "                    break\n",
    "                    \n",
    "                title = clean_text(entry.title)\n",
    "                abstract = clean_text(entry.summary)\n",
    "                \n",
    "                if title and abstract:  # Only add if both title and abstract exist\n",
    "                    papers.append({\n",
    "                        'title': title,\n",
    "                        'abstract': abstract,\n",
    "                        'text': f\"{title} {abstract}\"\n",
    "                    })\n",
    "            \n",
    "            start += batch_size\n",
    "            time.sleep(1)  # Be respectful to the API\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching papers: {e}\")\n",
    "            break\n",
    "    \n",
    "    return papers[:max_results]\n",
    "\n",
    "def create_research_dataset():\n",
    "    \"\"\"Create the complete research papers dataset\"\"\"\n",
    "    \n",
    "    # Define categories and their search queries\n",
    "    categories = {\n",
    "        'Technology': [\n",
    "            'cat:cs.AI',  # Artificial Intelligence\n",
    "            'cat:cs.LG',  # Machine Learning\n",
    "            'cat:cs.CV',  # Computer Vision\n",
    "            'cat:cs.NE'   # Neural Networks\n",
    "        ],\n",
    "        'Healthcare': [\n",
    "            'cat:q-bio.QM',  # Quantitative Methods in Biology\n",
    "            'cat:physics.med-ph',  # Medical Physics\n",
    "            'all:medical AND all:health',\n",
    "            'all:clinical AND all:treatment'\n",
    "        ],\n",
    "        'Finance': [\n",
    "            'cat:q-fin.GN',  # General Finance\n",
    "            'cat:q-fin.CP',  # Computational Finance\n",
    "            'all:financial AND all:market',\n",
    "            'all:economics AND all:investment'\n",
    "        ],\n",
    "        'Education': [\n",
    "            'all:education AND all:learning',\n",
    "            'all:educational AND all:technology',\n",
    "            'all:teaching AND all:pedagogy',\n",
    "            'all:curriculum AND all:assessment'\n",
    "        ],\n",
    "        'Environment': [\n",
    "            'cat:physics.ao-ph',  # Atmospheric and Oceanic Physics\n",
    "            'all:climate AND all:change',\n",
    "            'all:environmental AND all:sustainability',\n",
    "            'all:renewable AND all:energy'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    all_papers = []\n",
    "    \n",
    "    for category, queries in categories.items():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Collecting papers for: {category}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        category_papers = []\n",
    "        papers_per_query = 250 // len(queries)  # Distribute evenly across queries\n",
    "        \n",
    "        for query in queries:\n",
    "            papers = fetch_arxiv_papers(query, papers_per_query)\n",
    "            for paper in papers:\n",
    "                paper['category'] = category\n",
    "            category_papers.extend(papers)\n",
    "            \n",
    "            if len(category_papers) >= 250:  # Aim for ~250 papers per category\n",
    "                break\n",
    "        \n",
    "        print(f\"Collected {len(category_papers)} papers for {category}\")\n",
    "        all_papers.extend(category_papers[:250])  # Limit to 250 per category\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(all_papers)\n",
    "    \n",
    "    # Remove duplicates based on title\n",
    "    df = df.drop_duplicates(subset=['title'], keep='first')\n",
    "    \n",
    "    # Shuffle the data\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"DATASET CREATION SUMMARY\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total papers collected: {len(df)}\")\n",
    "    print(\"\\nCategory distribution:\")\n",
    "    print(df['category'].value_counts())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create the dataset\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting dataset creation...\")\n",
    "    print(\"This may take 10-15 minutes due to API rate limits...\")\n",
    "    \n",
    "    dataset = create_research_dataset()\n",
    "    \n",
    "    # Save to CSV\n",
    "    dataset.to_csv('research_papers_dataset.csv', index=False)\n",
    "    print(f\"\\nDataset saved as 'research_papers_dataset.csv'\")\n",
    "    \n",
    "    # Show sample data\n",
    "    print(\"\\nSample data:\")\n",
    "    print(dataset.head(2).to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
