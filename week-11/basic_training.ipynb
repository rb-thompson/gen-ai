{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd6c5ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 05  loss 1.8262\n",
      "step 10  loss 1.3533\n",
      "step 15  loss 1.1664\n",
      "step 20  loss 1.0646\n",
      "\n",
      "=== SAMPLE ===\n",
      "To be, or no be, the, to the, t the, be, the, thathe, be, the thato the, the, thathe, t tha\n"
     ]
    }
   ],
   "source": [
    "# tiny_llm_fast_smoketest.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import TransformerDecoderLayer, TransformerDecoder\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.set_num_threads(1)  # keep startup snappy on laptops\n",
    "\n",
    "# ----- tiny char vocab utils -----\n",
    "def build_char_vocab(text: str):\n",
    "    chars = sorted(set(text))\n",
    "    stoi = {c: i for i, c in enumerate(chars)}\n",
    "    itos = {i: c for c, i in stoi.items()}\n",
    "    return stoi, itos\n",
    "\n",
    "def encode(text, stoi): return [stoi[c] for c in text if c in stoi]\n",
    "def decode(ids, itos): return \"\".join(itos[i] for i in ids)\n",
    "\n",
    "# ----- tiny dataset -----\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens, seq_len: int):\n",
    "        self.tokens = tokens\n",
    "        self.seq_len = seq_len\n",
    "    def __len__(self):\n",
    "        return max(0, len(self.tokens) - self.seq_len - 1)\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.tokens[idx: idx+self.seq_len]\n",
    "        y = self.tokens[idx+1: idx+self.seq_len+1]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# ----- positional enc -----\n",
    "class LearnedPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 1024):\n",
    "        super().__init__()\n",
    "        self.pos = nn.Embedding(max_len, d_model)\n",
    "    def forward(self, x):  # x: [B, T]\n",
    "        T = x.size(1)\n",
    "        return self.pos(torch.arange(T, device=x.device)[None, :])\n",
    "\n",
    "# ----- tiny decoder-only via TransformerDecoder -----\n",
    "class TinyLLM(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int = 64, nhead: int = 2, num_layers: int = 1, max_len: int = 1024):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.posenc = LearnedPositionalEncoding(d_model, max_len=max_len)\n",
    "        layer = TransformerDecoderLayer(d_model=d_model, nhead=nhead, batch_first=False)\n",
    "        self.decoder = TransformerDecoder(layer, num_layers=num_layers)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, ids: torch.Tensor):\n",
    "        \"\"\"\n",
    "        ids: [B, T]; we feed same sequence as src & tgt and rely on causal mask.\n",
    "        Returns logits: [B, T, V]\n",
    "        \"\"\"\n",
    "        x = self.embed(ids) + self.posenc(ids)         # [B, T, E]\n",
    "        h = x.transpose(0, 1)                           # [T, B, E]\n",
    "        T = h.size(0)\n",
    "        tgt_mask = torch.triu(torch.ones(T, T, device=h.device), diagonal=1).bool()\n",
    "        out = self.decoder(tgt=h, memory=h, tgt_mask=tgt_mask)  # [T, B, E]\n",
    "        return self.lm_head(out.transpose(0, 1))        # [B, T, V]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, start_ids: torch.Tensor, max_new_tokens: int, topk: int = 1):\n",
    "        \"\"\"\n",
    "        Greedy (topk=1) by default. start_ids: [B, T0]\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        ids = start_ids\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self.forward(ids)[:, -1, :]            # [B, V]\n",
    "            if topk == 1:\n",
    "                next_id = torch.argmax(logits, dim=-1, keepdim=True)  # [B,1]\n",
    "            else:\n",
    "                vals, idxs = torch.topk(logits, k=topk, dim=-1)\n",
    "                probs = torch.softmax(vals, dim=-1)\n",
    "                choice = torch.multinomial(probs, num_samples=1)      # [B,1]\n",
    "                next_id = idxs.gather(-1, choice)\n",
    "            ids = torch.cat([ids, next_id], dim=1)           # [B, T+1]\n",
    "        return ids\n",
    "\n",
    "# ====== MAIN: ultra-small, fast demo ======\n",
    "def main():\n",
    "    # 1) Tiny in-memory corpus (repeat to ~2â€“3k chars)\n",
    "    base = \"To be, or not to be, that is the question. \"\n",
    "    raw_text = (base * 200).strip()\n",
    "\n",
    "    # 2) Vocab + tokens\n",
    "    stoi, itos = build_char_vocab(raw_text)\n",
    "    tokens = encode(raw_text, stoi)\n",
    "\n",
    "    # 3) Small hparams for speed\n",
    "    seq_len = 32\n",
    "    batch_size = 16\n",
    "    vocab_size = len(stoi)\n",
    "    epochs = 1\n",
    "    max_batches = 20  # limit steps so it finishes in seconds\n",
    "\n",
    "    # 4) Data\n",
    "    ds = TextDataset(tokens, seq_len=seq_len)\n",
    "    if len(ds) == 0:\n",
    "        raise RuntimeError(\"Corpus too small; increase repeats or lower seq_len.\")\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=0)\n",
    "\n",
    "    # 5) Model/opt/loss (CPU)\n",
    "    device = \"cpu\"\n",
    "    model = TinyLLM(vocab_size=vocab_size, d_model=64, nhead=2, num_layers=1).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 6) Train just a few mini-batches\n",
    "    model.train()\n",
    "    step = 0\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in dl:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(y[:, :-1])                              # predict next char\n",
    "            loss = loss_fn(logits.reshape(-1, vocab_size), y[:, 1:].reshape(-1))\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            step += 1\n",
    "            if step % 5 == 0:\n",
    "                print(f\"step {step:02d}  loss {loss.item():.4f}\")\n",
    "            if step >= max_batches:\n",
    "                break\n",
    "        if step >= max_batches:\n",
    "            break\n",
    "\n",
    "    # 7) Quick greedy generation\n",
    "    prompt = \"To be, or n\"\n",
    "    start = torch.tensor([encode(prompt, stoi)], dtype=torch.long, device=device)  # [1, T0]\n",
    "    gen = model.generate(start, max_new_tokens=80, topk=1)[0].tolist()\n",
    "    print(\"\\n=== SAMPLE ===\")\n",
    "    print(decode(gen, itos))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
