title,abstract,text,category
Stable-12 Bridges and Insurance,"We develop a class of non-life reserving models using a stable-12 random bridge to simulate the accumulation of paid claims, allowing for an essentially arbitrary choice of a priori distribution for the ultimate loss. Taking an information-based approach to the reserving problem, we derive the process of the conditional distribution of the ultimate loss. The best-estimate ultimate loss process is given by the conditional expectation of the ultimate loss. We derive explicit expressions for the best-estimate ultimate loss process, and for expected recoveries arising from aggregate excess-of-loss reinsurance treaties. Use of a deterministic time change allows for the matching of any initial (increasing) development pattern for the paid claims. We show that these methods are well-suited to the modelling of claims where there is a non-trivial probability of catastrophic loss. The generalized inverse-Gaussian (GIG) distribution is shown to be a natural choice for the a priori ultimate loss distribution. For particular GIG parameter choices, the best-estimate ultimate loss process can be written as a rational function of the paid-claims process. We extend the model to include a second paid-claims process, and allow the two processes to be dependent. The results obtained can be applied to the modelling of multiple lines of business or multiple origin years. The multi-dimensional model has the property that the dimensionality of calculations remains low, regardless of the number of paid-claims processes. An algorithm is provided for the simulation of the paid-claims processes.","Stable-12 Bridges and Insurance We develop a class of non-life reserving models using a stable-12 random bridge to simulate the accumulation of paid claims, allowing for an essentially arbitrary choice of a priori distribution for the ultimate loss. Taking an information-based approach to the reserving problem, we derive the process of the conditional distribution of the ultimate loss. The best-estimate ultimate loss process is given by the conditional expectation of the ultimate loss. We derive explicit expressions for the best-estimate ultimate loss process, and for expected recoveries arising from aggregate excess-of-loss reinsurance treaties. Use of a deterministic time change allows for the matching of any initial (increasing) development pattern for the paid claims. We show that these methods are well-suited to the modelling of claims where there is a non-trivial probability of catastrophic loss. The generalized inverse-Gaussian (GIG) distribution is shown to be a natural choice for the a priori ultimate loss distribution. For particular GIG parameter choices, the best-estimate ultimate loss process can be written as a rational function of the paid-claims process. We extend the model to include a second paid-claims process, and allow the two processes to be dependent. The results obtained can be applied to the modelling of multiple lines of business or multiple origin years. The multi-dimensional model has the property that the dimensionality of calculations remains low, regardless of the number of paid-claims processes. An algorithm is provided for the simulation of the paid-claims processes.",Finance
Rate-dependent propagation of cardiac action potentials in a one-dimensional fiber,"Action potential duration (APD) restitution, which relates APD to the preceding diastolic interval (DI), is a useful tool for predicting the onset of abnormal cardiac rhythms. However, it is known that different pacing protocols lead to different APD restitution curves (RCs). This phenomenon, known as APD rate-dependence, is a consequence of memory in the tissue. In addition to APD restitution, conduction velocity restitution also plays an important role in the spatiotemporal dynamics of cardiac tissue. We present new results concerning rate-dependent restitution in the velocity of propagating action potentials in a one-dimensional fiber. Our numerical simulations show that, independent of the amount of memory in the tissue, waveback velocity exhibits pronounced rate-dependence and the wavefront velocity does not. Moreover, the discrepancy between waveback velocity RCs is most significant for small DI. We provide an analytical explanation of these results, using a system of coupled maps to relate the wavefront and waveback velocities. Our calculations show that waveback velocity rate-dependence is due to APD restitution, not memory.","Rate-dependent propagation of cardiac action potentials in a one-dimensional fiber Action potential duration (APD) restitution, which relates APD to the preceding diastolic interval (DI), is a useful tool for predicting the onset of abnormal cardiac rhythms. However, it is known that different pacing protocols lead to different APD restitution curves (RCs). This phenomenon, known as APD rate-dependence, is a consequence of memory in the tissue. In addition to APD restitution, conduction velocity restitution also plays an important role in the spatiotemporal dynamics of cardiac tissue. We present new results concerning rate-dependent restitution in the velocity of propagating action potentials in a one-dimensional fiber. Our numerical simulations show that, independent of the amount of memory in the tissue, waveback velocity exhibits pronounced rate-dependence and the wavefront velocity does not. Moreover, the discrepancy between waveback velocity RCs is most significant for small DI. We provide an analytical explanation of these results, using a system of coupled maps to relate the wavefront and waveback velocities. Our calculations show that waveback velocity rate-dependence is due to APD restitution, not memory.",Healthcare
Translating between Horn Representations and their Characteristic Models,"Characteristic models are an alternative, model based, representation for Horn expressions. It has been shown that these two representations are incomparable and each has its advantages over the other. It is therefore natural to ask what is the cost of translating, back and forth, between these representations. Interestingly, the same translation questions arise in database theory, where it has applications to the design of relational databases. This paper studies the computational complexity of these problems. Our main result is that the two translation problems are equivalent under polynomial reductions, and that they are equivalent to the corresponding decision problem. Namely, translating is equivalent to deciding whether a given set of models is the set of characteristic models for a given Horn expression. We also relate these problems to the hypergraph transversal problem, a well known problem which is related to other applications in AI and for which no polynomial time algorithm is known. It is shown that in general our translation problems are at least as hard as the hypergraph transversal problem, and in a special case they are equivalent to it.","Translating between Horn Representations and their Characteristic Models Characteristic models are an alternative, model based, representation for Horn expressions. It has been shown that these two representations are incomparable and each has its advantages over the other. It is therefore natural to ask what is the cost of translating, back and forth, between these representations. Interestingly, the same translation questions arise in database theory, where it has applications to the design of relational databases. This paper studies the computational complexity of these problems. Our main result is that the two translation problems are equivalent under polynomial reductions, and that they are equivalent to the corresponding decision problem. Namely, translating is equivalent to deciding whether a given set of models is the set of characteristic models for a given Horn expression. We also relate these problems to the hypergraph transversal problem, a well known problem which is related to other applications in AI and for which no polynomial time algorithm is known. It is shown that in general our translation problems are at least as hard as the hypergraph transversal problem, and in a special case they are equivalent to it.",Technology
Moment based methods for ensemble assessment and calibration,"We describe various moment-based ensemble interpretation models for the construction of probabilistic temperature forecasts from ensembles. We apply the methods to one year of medium range ensemble forecasts and perform in and out of sample testing. Our main conclusion is that probabilistic forecasts derived from the ensemble mean using regression are just as good as those based on the ensemble mean and the ensemble spread using a more complex calibration algorithm. The explanation for this seems to be that the predictable component of the variability of the forecast uncertainty is only a small fraction of the total forecast uncertainty. Users of ensemble temperature forecasts are advised, until further evidence becomes available, to ignore the ensemble spread and build probabilistic forecasts based on the ensemble mean alone.","Moment based methods for ensemble assessment and calibration We describe various moment-based ensemble interpretation models for the construction of probabilistic temperature forecasts from ensembles. We apply the methods to one year of medium range ensemble forecasts and perform in and out of sample testing. Our main conclusion is that probabilistic forecasts derived from the ensemble mean using regression are just as good as those based on the ensemble mean and the ensemble spread using a more complex calibration algorithm. The explanation for this seems to be that the predictable component of the variability of the forecast uncertainty is only a small fraction of the total forecast uncertainty. Users of ensemble temperature forecasts are advised, until further evidence becomes available, to ignore the ensemble spread and build probabilistic forecasts based on the ensemble mean alone.",Environment
Comparison of Two Numerical Methods for Computation of American Type of the Floating Strike Asian Option,"We present a numerical approach for solving the free boundary problem for the Black-Scholes equation for pricing American style of floating strike Asian options. A fixed domain transformation of the free boundary problem into a parabolic equation defined on a fixed spatial domain is performed. As a result a nonlinear time-dependent term is involved in the resulting equation. Two new numerical algorithms are proposed. In the first algorithm a predictor-corrector scheme is used. The second one is based on the Newton method. Computational experiments, confirming the accuracy of the algorithms are presented and discussed.","Comparison of Two Numerical Methods for Computation of American Type of the Floating Strike Asian Option We present a numerical approach for solving the free boundary problem for the Black-Scholes equation for pricing American style of floating strike Asian options. A fixed domain transformation of the free boundary problem into a parabolic equation defined on a fixed spatial domain is performed. As a result a nonlinear time-dependent term is involved in the resulting equation. Two new numerical algorithms are proposed. In the first algorithm a predictor-corrector scheme is used. The second one is based on the Newton method. Computational experiments, confirming the accuracy of the algorithms are presented and discussed.",Finance
A Method to Determine the In-Air Spatial Spread of Clinical Electron Beams,"We propose and analyze in detail a method to measure the in-air spatial spread parameter of clinical electron beams. Measurements are performed at the center of the beam and below the adjustable collimators sited in asymmetrical configuration in order to avoid the distortions due to the presence of the applicator. The main advantage of our procedure lies in the fact that the dose profiles are fitted by means of a function which includes, additionally to the Gaussian step usually considered, a background which takes care of the dose produced by different mechanisms that the Gaussian model does not account for. As a result, the spatial spread is obtained directly from the fitting procedure and the accuracy permits a good determination of the angular spread. The way the analysis is done is alternative to that followed by the usual methods based on the evaluation of the penumbra width. Besides, the spatial spread found shows the quadratic-cubic dependence with the distance to the source predicted by the Fermi-Eyges theory. However, the corresponding values obtained for the scattering power are differing from those quoted by ICRU nr. 35 by a factor 2 or larger, what requires of a more detailed investigation.","A Method to Determine the In-Air Spatial Spread of Clinical Electron Beams We propose and analyze in detail a method to measure the in-air spatial spread parameter of clinical electron beams. Measurements are performed at the center of the beam and below the adjustable collimators sited in asymmetrical configuration in order to avoid the distortions due to the presence of the applicator. The main advantage of our procedure lies in the fact that the dose profiles are fitted by means of a function which includes, additionally to the Gaussian step usually considered, a background which takes care of the dose produced by different mechanisms that the Gaussian model does not account for. As a result, the spatial spread is obtained directly from the fitting procedure and the accuracy permits a good determination of the angular spread. The way the analysis is done is alternative to that followed by the usual methods based on the evaluation of the penumbra width. Besides, the spatial spread found shows the quadratic-cubic dependence with the distance to the source predicted by the Fermi-Eyges theory. However, the corresponding values obtained for the scattering power are differing from those quoted by ICRU nr. 35 by a factor 2 or larger, what requires of a more detailed investigation.",Healthcare
Dynamic Conditional Correlation between Electricity and Stock markets during the Financial Crisis in Greece,"Liberalization of electricity markets has increasingly created the need for understanding the volatility and correlation structure between electricity and financial markets. This work reveals the existence of structural changes in correlation patterns among these two markets and links the changes to both fundamentals and regulatory conditions prevailing in the markets, as well as the current European financial crisis. We apply a Dynamic Conditional Correlation (DCC) GARCH model to a set of market s fundamental variables and Greece s financial market and microeconomic indexes to study their interaction. Emphasis is given on the period of severe financial crisis of the Country to understand contagion and volatility spillover between these two markets.","Dynamic Conditional Correlation between Electricity and Stock markets during the Financial Crisis in Greece Liberalization of electricity markets has increasingly created the need for understanding the volatility and correlation structure between electricity and financial markets. This work reveals the existence of structural changes in correlation patterns among these two markets and links the changes to both fundamentals and regulatory conditions prevailing in the markets, as well as the current European financial crisis. We apply a Dynamic Conditional Correlation (DCC) GARCH model to a set of market s fundamental variables and Greece s financial market and microeconomic indexes to study their interaction. Emphasis is given on the period of severe financial crisis of the Country to understand contagion and volatility spillover between these two markets.",Finance
Identification of Surface Defects on Solar PV Panels and Wind Turbine Blades using Attention based Deep Learning Model,"The global generation of renewable energy has rapidly increased, primarily due to the installation of large-scale renewable energy power plants. However, monitoring renewable energy assets in these large plants remains challenging due to environmental factors that could result in reduced power generation, malfunctioning, and degradation of asset life. Therefore, the detection of surface defects on renewable energy assets is crucial for maintaining the performance and efficiency of these plants. This paper proposes an innovative detection framework to achieve an economical surface monitoring system for renewable energy assets. High-resolution images of the assets are captured regularly and inspected to identify surface or structural damages on solar panels and wind turbine blades. Vision transformer (ViT), one of the latest attention-based deep learning (DL) models in computer vision, is proposed in this work to classify surface defects. The ViT model outperforms other DL models, including MobileNet, VGG16, Xception, EfficientNetB7, and ResNet50, achieving high accuracy scores above 97 for both wind and solar plant assets. From the results, our proposed model demonstrates its potential for monitoring and detecting damages in renewable energy assets for efficient and reliable operation of renewable power plants.","Identification of Surface Defects on Solar PV Panels and Wind Turbine Blades using Attention based Deep Learning Model The global generation of renewable energy has rapidly increased, primarily due to the installation of large-scale renewable energy power plants. However, monitoring renewable energy assets in these large plants remains challenging due to environmental factors that could result in reduced power generation, malfunctioning, and degradation of asset life. Therefore, the detection of surface defects on renewable energy assets is crucial for maintaining the performance and efficiency of these plants. This paper proposes an innovative detection framework to achieve an economical surface monitoring system for renewable energy assets. High-resolution images of the assets are captured regularly and inspected to identify surface or structural damages on solar panels and wind turbine blades. Vision transformer (ViT), one of the latest attention-based deep learning (DL) models in computer vision, is proposed in this work to classify surface defects. The ViT model outperforms other DL models, including MobileNet, VGG16, Xception, EfficientNetB7, and ResNet50, achieving high accuracy scores above 97 for both wind and solar plant assets. From the results, our proposed model demonstrates its potential for monitoring and detecting damages in renewable energy assets for efficient and reliable operation of renewable power plants.",Environment
A Neural-Network Technique for Recognition of Filaments in Solar Images,We describe a new neural-network technique developed for an automated recognition of solar filaments visible in the hydrogen H-alpha line full disk spectroheliograms. This technique allows neural networks learn from a few image fragments labelled manually to recognize the single filaments depicted on a local background. The trained network is able to recognize filaments depicted on the backgrounds with variations in brightness caused by atmospherics distortions. Despite the difference in backgrounds in our experiments the neural network has properly recognized filaments in the testing image fragments. Using a parabolic activation function we extend this technique to recognize multiple solar filaments which may appear in one fragment.,A Neural-Network Technique for Recognition of Filaments in Solar Images We describe a new neural-network technique developed for an automated recognition of solar filaments visible in the hydrogen H-alpha line full disk spectroheliograms. This technique allows neural networks learn from a few image fragments labelled manually to recognize the single filaments depicted on a local background. The trained network is able to recognize filaments depicted on the backgrounds with variations in brightness caused by atmospherics distortions. Despite the difference in backgrounds in our experiments the neural network has properly recognized filaments in the testing image fragments. Using a parabolic activation function we extend this technique to recognize multiple solar filaments which may appear in one fragment.,Technology
Automatic Face Recognition System Based on Local Fourier-Bessel Features,"We present an automatic face verification system inspired by known properties of biological systems. In the proposed algorithm the whole image is converted from the spatial to polar frequency domain by a Fourier-Bessel Transform (FBT). Using the whole image is compared to the case where only face image regions (local analysis) are considered. The resulting representations are embedded in a dissimilarity space, where each image is represented by its distance to all the other images, and a Pseudo-Fisher discriminator is built. Verification test results on the FERET database showed that the local-based algorithm outperforms the global-FBT version. The local-FBT algorithm performed as state-of-the-art methods under different testing conditions, indicating that the proposed system is highly robust for expression, age, and illumination variations. We also evaluated the performance of the proposed system under strong occlusion conditions and found that it is highly robust for up to 50 of face occlusion. Finally, we automated completely the verification system by implementing face and eye detection algorithms. Under this condition, the local approach was only slightly superior to the global approach.","Automatic Face Recognition System Based on Local Fourier-Bessel Features We present an automatic face verification system inspired by known properties of biological systems. In the proposed algorithm the whole image is converted from the spatial to polar frequency domain by a Fourier-Bessel Transform (FBT). Using the whole image is compared to the case where only face image regions (local analysis) are considered. The resulting representations are embedded in a dissimilarity space, where each image is represented by its distance to all the other images, and a Pseudo-Fisher discriminator is built. Verification test results on the FERET database showed that the local-based algorithm outperforms the global-FBT version. The local-FBT algorithm performed as state-of-the-art methods under different testing conditions, indicating that the proposed system is highly robust for expression, age, and illumination variations. We also evaluated the performance of the proposed system under strong occlusion conditions and found that it is highly robust for up to 50 of face occlusion. Finally, we automated completely the verification system by implementing face and eye detection algorithms. Under this condition, the local approach was only slightly superior to the global approach.",Technology
On three filtering problems arising in mathematical finance,"Three situations in which filtering theory is used in mathematical finance are illustrated at different levels of detail. The three problems originate from the following different works: 1) On estimating the stochastic volatility model from observed bilateral exchange rate news, by R. Mahieu, and P. Schotman; 2) A state space approach to estimate multi-factors CIR models of the term structure of interest rates, by A.L.J. Geyer, and S. Pichler; 3) Risk-minimizing hedging strategies under partial observation in pricing financial derivatives, by P. Fischer, E. Platen, and W. J. Runggaldier; In the first problem we propose to use a recent nonlinear filtering technique based on geometry to estimate the volatility time series from observed bilateral exchange rates. The model used here is the stochastic volatility model. The filters that we propose are known as projection filters, and a brief derivation of such filters is given. The second problem is introduced in detail, and a possible use of different filtering techniques is hinted at. In fact the filters used for this problem in 2) and part of the literature can be interpreted as projection filters and we will make some remarks on how more general and possibly more suitable projection filters can be constructed. The third problem is only presented shortly.","On three filtering problems arising in mathematical finance Three situations in which filtering theory is used in mathematical finance are illustrated at different levels of detail. The three problems originate from the following different works: 1) On estimating the stochastic volatility model from observed bilateral exchange rate news, by R. Mahieu, and P. Schotman; 2) A state space approach to estimate multi-factors CIR models of the term structure of interest rates, by A.L.J. Geyer, and S. Pichler; 3) Risk-minimizing hedging strategies under partial observation in pricing financial derivatives, by P. Fischer, E. Platen, and W. J. Runggaldier; In the first problem we propose to use a recent nonlinear filtering technique based on geometry to estimate the volatility time series from observed bilateral exchange rates. The model used here is the stochastic volatility model. The filters that we propose are known as projection filters, and a brief derivation of such filters is given. The second problem is introduced in detail, and a possible use of different filtering techniques is hinted at. In fact the filters used for this problem in 2) and part of the literature can be interpreted as projection filters and we will make some remarks on how more general and possibly more suitable projection filters can be constructed. The third problem is only presented shortly.",Finance
Redesigning Electronic Health Record Systems to Support Developing Countries,"Electronic Health Record (EHR) has become an essential tool in the healthcare ecosystem, providing authorized clinicians with patients health-related information for better treatment. While most developed countries are taking advantage of EHRs to improve their healthcare system, it remains challenging in developing countries to support clinical decision-making and public health using a computerized patient healthcare information system. This paper proposes a novel EHR architecture suitable for developing countries--an architecture that fosters inclusion and provides solutions tailored to all social classes and socioeconomic statuses. Our architecture foresees an internet-free (offline) solution to allow medical transactions between healthcare organizations, and the storage of EHRs in geographically underserved and rural areas. Moreover, we discuss how artificial intelligence can leverage anonymous health-related information to enable better public health policy and surveillance.","Redesigning Electronic Health Record Systems to Support Developing Countries Electronic Health Record (EHR) has become an essential tool in the healthcare ecosystem, providing authorized clinicians with patients health-related information for better treatment. While most developed countries are taking advantage of EHRs to improve their healthcare system, it remains challenging in developing countries to support clinical decision-making and public health using a computerized patient healthcare information system. This paper proposes a novel EHR architecture suitable for developing countries--an architecture that fosters inclusion and provides solutions tailored to all social classes and socioeconomic statuses. Our architecture foresees an internet-free (offline) solution to allow medical transactions between healthcare organizations, and the storage of EHRs in geographically underserved and rural areas. Moreover, we discuss how artificial intelligence can leverage anonymous health-related information to enable better public health policy and surveillance.",Healthcare
The Assessment and Calibration of Ensemble Seasonal Forecasts of Equatorial Pacific Ocean Temperature and the Predictability of Uncertainty,"We evaluate the performance of two 44 year ensemble seasonal hindcast time series for the Nino3 index produced as part of the DEMETER project. We show that the ensemble mean carries useful information out to six months. The ensemble spread, however, only carries useful information out to four months in one of the models, and two months in the other.","The Assessment and Calibration of Ensemble Seasonal Forecasts of Equatorial Pacific Ocean Temperature and the Predictability of Uncertainty We evaluate the performance of two 44 year ensemble seasonal hindcast time series for the Nino3 index produced as part of the DEMETER project. We show that the ensemble mean carries useful information out to six months. The ensemble spread, however, only carries useful information out to four months in one of the models, and two months in the other.",Environment
Data Mining and Electronic Health Records: Selecting Optimal Clinical Treatments in Practice,"Electronic health records (EHRs) are only a first step in capturing and utilizing health-related data - the problem is turning that data into useful information. Models produced via data mining and predictive analysis profile inherited risks and environmentalbehavioral factors associated with patient disorders, which can be utilized to generate predictions about treatment outcomes. This can form the backbone of clinical decision support systems driven by live data based on the actual population. The advantage of such an approach based on the actual population is that it is adaptive. Here, we evaluate the predictive capacity of a clinical EHR of a large mental healthcare provider (75,000 distinct clients a year) to provide decision support information in a real-world clinical setting. Initial research has achieved a 70 success rate in predicting treatment outcomes using these methods.","Data Mining and Electronic Health Records: Selecting Optimal Clinical Treatments in Practice Electronic health records (EHRs) are only a first step in capturing and utilizing health-related data - the problem is turning that data into useful information. Models produced via data mining and predictive analysis profile inherited risks and environmentalbehavioral factors associated with patient disorders, which can be utilized to generate predictions about treatment outcomes. This can form the backbone of clinical decision support systems driven by live data based on the actual population. The advantage of such an approach based on the actual population is that it is adaptive. Here, we evaluate the predictive capacity of a clinical EHR of a large mental healthcare provider (75,000 distinct clients a year) to provide decision support information in a real-world clinical setting. Initial research has achieved a 70 success rate in predicting treatment outcomes using these methods.",Healthcare
Transitions in climate and energy discourse between Hurricanes Katrina and Sandy,"Although climate change and energy are intricately linked, their explicit connection is not always prominent in public discourse and the media. Disruptive extreme weather events, including hurricanes, focus public attention in new and different ways, offering a unique window of opportunity to analyze how a focusing event influences public discourse. Media coverage of extreme weather events simultaneously shapes and reflects public discourse on climate issues. Here we analyze climate and energy newspaper coverage of Hurricanes Katrina (2005) and Sandy (2012) using topic models, mathematical techniques used to discover abstract topics within a set of documents. Our results demonstrate that post-Katrina media coverage does not contain a climate change topic, and the energy topic is limited to discussion of energy prices, markets, and the economy with almost no explicit linkages made between energy and climate change. In contrast, post-Sandy media coverage does contain a prominent climate change topic, a distinct energy topic, as well as integrated representation of climate change and energy, indicating a shift in climate and energy reporting between Hurricane Katrina and Hurricane Sandy.","Transitions in climate and energy discourse between Hurricanes Katrina and Sandy Although climate change and energy are intricately linked, their explicit connection is not always prominent in public discourse and the media. Disruptive extreme weather events, including hurricanes, focus public attention in new and different ways, offering a unique window of opportunity to analyze how a focusing event influences public discourse. Media coverage of extreme weather events simultaneously shapes and reflects public discourse on climate issues. Here we analyze climate and energy newspaper coverage of Hurricanes Katrina (2005) and Sandy (2012) using topic models, mathematical techniques used to discover abstract topics within a set of documents. Our results demonstrate that post-Katrina media coverage does not contain a climate change topic, and the energy topic is limited to discussion of energy prices, markets, and the economy with almost no explicit linkages made between energy and climate change. In contrast, post-Sandy media coverage does contain a prominent climate change topic, a distinct energy topic, as well as integrated representation of climate change and energy, indicating a shift in climate and energy reporting between Hurricane Katrina and Hurricane Sandy.",Environment
A note on heterogeneous beliefs with CRRA utilities,"This note will extend the research presented in Brown  Rogers (2009) to the case of CRRA agents. We consider the model outlined in that paper in which agents had diverse beliefs about the dividends produced by a risky asset. We now assume that the agents all have CRRA utility, with some integer coefficient of relative risk aversion. This is a generalisation of Brown  Rogers which considered logarithmic agents. We derive expressions for the state price density, riskless rate, stock price and wealths of the agents. This sheds light on the effects of risk aversion in an equilibrium with diverse beliefs.","A note on heterogeneous beliefs with CRRA utilities This note will extend the research presented in Brown  Rogers (2009) to the case of CRRA agents. We consider the model outlined in that paper in which agents had diverse beliefs about the dividends produced by a risky asset. We now assume that the agents all have CRRA utility, with some integer coefficient of relative risk aversion. This is a generalisation of Brown  Rogers which considered logarithmic agents. We derive expressions for the state price density, riskless rate, stock price and wealths of the agents. This sheds light on the effects of risk aversion in an equilibrium with diverse beliefs.",Finance
Two-sided estimates for stock price distribution densities in jump-diffusion models,"We consider uncorrelated Stein-Stein, Heston, and Hull-White models and their perturbations by compound Poisson processes with jump amplitudes distributed according to a double exponential law. Similar perturbations of the Black-Scholes model were studied by S. Kou. For perturbed stochastic volatility models, we obtain two-sided estimates for the stock price distribution density and compare the tail behavior of this density before and after perturbation. It is shown that if the value of the parameter, characterizing the right tail of the double exponential law, is small, then the stock price density in the perturbed model decays slower than the density in the original model. On the other hand, if the value of this parameter is large, then there are no significant changes in the behavior of the stock price distribution density.","Two-sided estimates for stock price distribution densities in jump-diffusion models We consider uncorrelated Stein-Stein, Heston, and Hull-White models and their perturbations by compound Poisson processes with jump amplitudes distributed according to a double exponential law. Similar perturbations of the Black-Scholes model were studied by S. Kou. For perturbed stochastic volatility models, we obtain two-sided estimates for the stock price distribution density and compare the tail behavior of this density before and after perturbation. It is shown that if the value of the parameter, characterizing the right tail of the double exponential law, is small, then the stock price density in the perturbed model decays slower than the density in the original model. On the other hand, if the value of this parameter is large, then there are no significant changes in the behavior of the stock price distribution density.",Finance
Entering New Markets-a Challenge in Times of Crisis,"After September 2008, the advanced economies severe decline caused demand for emerging economies exports to drop and the crisis became truly global, much deeper and broader than expected. In these times of global depression, most countries and companies are affected, some more than others. The financial crisis has turned out to be much deeper and broader than expected. Entering new markets has always been a hazardous entrepreneurial attempt, but also a rewarding one, in the case of success. The paper aims to asses the market entry risk of a company trying to make a good acquisition, to buy shares of another company, activating in a foreign country. For this purpose, the case of Electroputere S.A., the old Romanian producer of railway equipment, has been chosen. The data were collected from the records of Bucharest Stock Exchange. After two years from the acquisition, one can draw a conclusion whether the strategy of the investor was a good one or a waste of money.","Entering New Markets-a Challenge in Times of Crisis After September 2008, the advanced economies severe decline caused demand for emerging economies exports to drop and the crisis became truly global, much deeper and broader than expected. In these times of global depression, most countries and companies are affected, some more than others. The financial crisis has turned out to be much deeper and broader than expected. Entering new markets has always been a hazardous entrepreneurial attempt, but also a rewarding one, in the case of success. The paper aims to asses the market entry risk of a company trying to make a good acquisition, to buy shares of another company, activating in a foreign country. For this purpose, the case of Electroputere S.A., the old Romanian producer of railway equipment, has been chosen. The data were collected from the records of Bucharest Stock Exchange. After two years from the acquisition, one can draw a conclusion whether the strategy of the investor was a good one or a waste of money.",Finance
iCare: A Mobile Health Monitoring System for the Elderly,"This paper describes a mobile health monitoring system called iCare for the elderly. We use wireless body sensors and smart phones to monitor the wellbeing of the elderly. It can offer remote monitoring for the elderly anytime anywhere and provide tailored services for each person based on their personal health condition. When detecting an emergency, the smart phone will automatically alert pre-assigned people who could be the old peoples family and friends, and call the ambulance of the emergency centre. It also acts as the personal health information system and the medical guidance which offers one communication platform and the medical knowledge database so that the family and friends of the served people can cooperate with doctors to take care of himher. The system also features some unique functions that cater to the living demands of the elderly, including regular reminder, quick alarm, medical guidance, etc. iCare is not only a real-time health monitoring system for the elderly, but also a living assistant which can make their lives more convenient and comfortable.","iCare: A Mobile Health Monitoring System for the Elderly This paper describes a mobile health monitoring system called iCare for the elderly. We use wireless body sensors and smart phones to monitor the wellbeing of the elderly. It can offer remote monitoring for the elderly anytime anywhere and provide tailored services for each person based on their personal health condition. When detecting an emergency, the smart phone will automatically alert pre-assigned people who could be the old peoples family and friends, and call the ambulance of the emergency centre. It also acts as the personal health information system and the medical guidance which offers one communication platform and the medical knowledge database so that the family and friends of the served people can cooperate with doctors to take care of himher. The system also features some unique functions that cater to the living demands of the elderly, including regular reminder, quick alarm, medical guidance, etc. iCare is not only a real-time health monitoring system for the elderly, but also a living assistant which can make their lives more convenient and comfortable.",Healthcare
Natural Language-Assisted Multi-modal Medication Recommendation,"Combinatorial medication recommendation(CMR) is a fundamental task of healthcare, which offers opportunities for clinical physicians to provide more precise prescriptions for patients with intricate health conditions, particularly in the scenarios of long-term medical care. Previous research efforts have sought to extract meaningful information from electronic health records (EHRs) to facilitate combinatorial medication recommendations. Existing learning-based approaches further consider the chemical structures of medications, but ignore the textual medication descriptions in which the functionalities are clearly described. Furthermore, the textual knowledge derived from the EHRs of patients remains largely underutilized. To address these issues, we introduce the Natural Language-Assisted Multi-modal Medication Recommendation(NLA-MMR), a multi-modal alignment framework designed to learn knowledge from the patient view and medication view jointly. Specifically, NLA-MMR formulates CMR as an alignment problem from patient and medication modalities. In this vein, we employ pretrained language models(PLMs) to extract in-domain knowledge regarding patients and medications, serving as the foundational representation for both modalities. In the medication modality, we exploit both chemical structures and textual descriptions to create medication representations. In the patient modality, we generate the patient representations based on textual descriptions of diagnosis, procedure, and symptom. Extensive experiments conducted on three publicly accessible datasets demonstrate that NLA-MMR achieves new state-of-the-art performance, with a notable average improvement of 4.72 in Jaccard score. Our source code is publicly available on https:github.comjtan1102NLA-MMR_CIKM_2024.","Natural Language-Assisted Multi-modal Medication Recommendation Combinatorial medication recommendation(CMR) is a fundamental task of healthcare, which offers opportunities for clinical physicians to provide more precise prescriptions for patients with intricate health conditions, particularly in the scenarios of long-term medical care. Previous research efforts have sought to extract meaningful information from electronic health records (EHRs) to facilitate combinatorial medication recommendations. Existing learning-based approaches further consider the chemical structures of medications, but ignore the textual medication descriptions in which the functionalities are clearly described. Furthermore, the textual knowledge derived from the EHRs of patients remains largely underutilized. To address these issues, we introduce the Natural Language-Assisted Multi-modal Medication Recommendation(NLA-MMR), a multi-modal alignment framework designed to learn knowledge from the patient view and medication view jointly. Specifically, NLA-MMR formulates CMR as an alignment problem from patient and medication modalities. In this vein, we employ pretrained language models(PLMs) to extract in-domain knowledge regarding patients and medications, serving as the foundational representation for both modalities. In the medication modality, we exploit both chemical structures and textual descriptions to create medication representations. In the patient modality, we generate the patient representations based on textual descriptions of diagnosis, procedure, and symptom. Extensive experiments conducted on three publicly accessible datasets demonstrate that NLA-MMR achieves new state-of-the-art performance, with a notable average improvement of 4.72 in Jaccard score. Our source code is publicly available on https:github.comjtan1102NLA-MMR_CIKM_2024.",Healthcare
"Quantitative analysis of the value of investment in research facilities, with examples from cyberinfrastructure","Purpose: How much to invest in research facilities has long been a question in higher education and research policy. We present established and recently developed techniques for assessing the quantitative value created or received as a result of investments in research facilities. This discussion is timely. Financial challenges in higher education may soon force difficult decisions regarding investment in research facilities at some institutions. Clear quantitative analysis will be necessary for such strategic decision-making. Further, institutions of higher education in the USA are currently being called on to justify their value to society. The analyses presented here are extendable to research enterprises as a whole. Results: We present methods developed primarily for analyses of cyberinfrastructure. Most analyses comparing investment in university-based cyberinfrastructure facilities with purchasing services from commercial sources demonstrate positive results for economic and scientific research. A recent assessment, based on a comprehensive accounting approach, has shown that for one large publicly funded cyberinfrastructure project the value delivered to the USA economy and society exceeded the cost to USA taxpayers. Conclusions: Quantitative analyses of the benefits of investment in research and research facilities create a fact-based foundation for discussing the value of research and higher education. These methods enable a quantitative assessment of the relationship between investment in specific research facilities or research projects and economic, societal, and educational outcomes. These methods are of value in quantifying the economic benefit of higher education and in managing investments within such institutions.","Quantitative analysis of the value of investment in research facilities, with examples from cyberinfrastructure Purpose: How much to invest in research facilities has long been a question in higher education and research policy. We present established and recently developed techniques for assessing the quantitative value created or received as a result of investments in research facilities. This discussion is timely. Financial challenges in higher education may soon force difficult decisions regarding investment in research facilities at some institutions. Clear quantitative analysis will be necessary for such strategic decision-making. Further, institutions of higher education in the USA are currently being called on to justify their value to society. The analyses presented here are extendable to research enterprises as a whole. Results: We present methods developed primarily for analyses of cyberinfrastructure. Most analyses comparing investment in university-based cyberinfrastructure facilities with purchasing services from commercial sources demonstrate positive results for economic and scientific research. A recent assessment, based on a comprehensive accounting approach, has shown that for one large publicly funded cyberinfrastructure project the value delivered to the USA economy and society exceeded the cost to USA taxpayers. Conclusions: Quantitative analyses of the benefits of investment in research and research facilities create a fact-based foundation for discussing the value of research and higher education. These methods enable a quantitative assessment of the relationship between investment in specific research facilities or research projects and economic, societal, and educational outcomes. These methods are of value in quantifying the economic benefit of higher education and in managing investments within such institutions.",Finance
Can the potential benefit of individualizing treatment be assessed using trial summary statistics alone?,"Individualizing treatment assignment can improve outcomes for diseases with patient-to-patient variability in comparative treatment effects. When a clinical trial demonstrates that some patients improve on treatment while others do not, it is tempting to assume that treatment effect heterogeneity exists. However, if variability in response is mainly driven by factors other than treatment, investigating the extent to which covariate data can predict differential treatment response is a potential waste of resources. Motivated by recent meta-analyses assessing the potential of individualizing treatment for major depressive disorder using only summary statistics, we provide a method that uses summary statistics widely available in published clinical trial results to bound the benefit of optimally assigning treatment to each patient. We also offer alternate bounds for settings in which trial results are stratified by another covariate. We demonstrate our approach using summary statistics from a depression treatment trial. Our methods are implemented in the rct2otrbounds R package, which is available at https:github.comngalanterrct2otrbounds .","Can the potential benefit of individualizing treatment be assessed using trial summary statistics alone? Individualizing treatment assignment can improve outcomes for diseases with patient-to-patient variability in comparative treatment effects. When a clinical trial demonstrates that some patients improve on treatment while others do not, it is tempting to assume that treatment effect heterogeneity exists. However, if variability in response is mainly driven by factors other than treatment, investigating the extent to which covariate data can predict differential treatment response is a potential waste of resources. Motivated by recent meta-analyses assessing the potential of individualizing treatment for major depressive disorder using only summary statistics, we provide a method that uses summary statistics widely available in published clinical trial results to bound the benefit of optimally assigning treatment to each patient. We also offer alternate bounds for settings in which trial results are stratified by another covariate. We demonstrate our approach using summary statistics from a depression treatment trial. Our methods are implemented in the rct2otrbounds R package, which is available at https:github.comngalanterrct2otrbounds .",Healthcare
Do Generative AI Tools Ensure Green Code? An Investigative Study,"Software sustainability is emerging as a primary concern, aiming to optimize resource utilization, minimize environmental impact, and promote a greener, more resilient digital ecosystem. The sustainability or greenness of software is typically determined by the adoption of sustainable coding practices. With a maturing ecosystem around generative AI, many software developers now rely on these tools to generate code using natural language prompts. Despite their potential advantages, there is a significant lack of studies on the sustainability aspects of AI-generated code. Specifically, how environmentally friendly is the AI-generated code based upon its adoption of sustainable coding practices? In this paper, we present the results of an early investigation into the sustainability aspects of AI-generated code across three popular generative AI tools - ChatGPT, BARD, and Copilot. The results highlight the default non-green behavior of tools for generating code, across multiple rules and scenarios. It underscores the need for further in-depth investigations and effective remediation strategies.","Do Generative AI Tools Ensure Green Code? An Investigative Study Software sustainability is emerging as a primary concern, aiming to optimize resource utilization, minimize environmental impact, and promote a greener, more resilient digital ecosystem. The sustainability or greenness of software is typically determined by the adoption of sustainable coding practices. With a maturing ecosystem around generative AI, many software developers now rely on these tools to generate code using natural language prompts. Despite their potential advantages, there is a significant lack of studies on the sustainability aspects of AI-generated code. Specifically, how environmentally friendly is the AI-generated code based upon its adoption of sustainable coding practices? In this paper, we present the results of an early investigation into the sustainability aspects of AI-generated code across three popular generative AI tools - ChatGPT, BARD, and Copilot. The results highlight the default non-green behavior of tools for generating code, across multiple rules and scenarios. It underscores the need for further in-depth investigations and effective remediation strategies.",Environment
Scalable Medication Extraction and Discontinuation Identification from Electronic Health Records Using Large Language Models,"Identifying medication discontinuations in electronic health records (EHRs) is vital for patient safety but is often hindered by information being buried in unstructured notes. This study aims to evaluate the capabilities of advanced open-sourced and proprietary large language models (LLMs) in extracting medications and classifying their medication status from EHR notes, focusing on their scalability on medication information extraction without human annotation. We collected three EHR datasets from diverse sources to build the evaluation benchmark. We evaluated 12 advanced LLMs and explored multiple LLM prompting strategies. Performance on medication extraction, medication status classification, and their joint task (extraction then classification) was systematically compared across all experiments. We found that LLMs showed promising performance on the medication extraction and discontinuation classification from EHR notes. GPT-4o consistently achieved the highest average F1 scores in all tasks under zero-shot setting - 94.0 for medication extraction, 78.1 for discontinuation classification, and 72.7 for the joint task. Open-sourced models followed closely, Llama-3.1-70B-Instruct achieved the highest performance in medication status classification on the MIV-Med dataset (68.7) and in the joint task on both the Re-CASI (76.2) and MIV-Med (60.2) datasets. Medical-specific LLMs demonstrated lower performance compared to advanced general-domain LLMs. Few-shot learning generally improved performance, while CoT reasoning showed inconsistent gains. LLMs demonstrate strong potential for medication extraction and discontinuation identification on EHR notes, with open-sourced models offering scalable alternatives to proprietary systems and few-shot can further improve LLMs capability.","Scalable Medication Extraction and Discontinuation Identification from Electronic Health Records Using Large Language Models Identifying medication discontinuations in electronic health records (EHRs) is vital for patient safety but is often hindered by information being buried in unstructured notes. This study aims to evaluate the capabilities of advanced open-sourced and proprietary large language models (LLMs) in extracting medications and classifying their medication status from EHR notes, focusing on their scalability on medication information extraction without human annotation. We collected three EHR datasets from diverse sources to build the evaluation benchmark. We evaluated 12 advanced LLMs and explored multiple LLM prompting strategies. Performance on medication extraction, medication status classification, and their joint task (extraction then classification) was systematically compared across all experiments. We found that LLMs showed promising performance on the medication extraction and discontinuation classification from EHR notes. GPT-4o consistently achieved the highest average F1 scores in all tasks under zero-shot setting - 94.0 for medication extraction, 78.1 for discontinuation classification, and 72.7 for the joint task. Open-sourced models followed closely, Llama-3.1-70B-Instruct achieved the highest performance in medication status classification on the MIV-Med dataset (68.7) and in the joint task on both the Re-CASI (76.2) and MIV-Med (60.2) datasets. Medical-specific LLMs demonstrated lower performance compared to advanced general-domain LLMs. Few-shot learning generally improved performance, while CoT reasoning showed inconsistent gains. LLMs demonstrate strong potential for medication extraction and discontinuation identification on EHR notes, with open-sourced models offering scalable alternatives to proprietary systems and few-shot can further improve LLMs capability.",Healthcare
Determination and Spectroscopy of Quantum Yields in BioChemiluminescence via Novel Light-Collection-Efficiency Calibration: Reexamination of The Aqueous Luminol Chemiluminescence Standard,"We have developed a luminescence-measurement system for liquid biochemiluminescence that can obtain quantitative luminescence spectra as the absolute total number of luminescence photons at each wavelength or photon energy and quantum yields. Calibration of light-collection efficiency in the system is performed with a reference double-plate cell. This method is applicable to sample cells of any kind suitable for measurement, which is a great advantage over previous techniques in practical experiments. Using this system, the quantum yield of aqueous luminol chemiluminescence was obtained as 1.23-0.20, which is in good agreement with previously reported values.","Determination and Spectroscopy of Quantum Yields in BioChemiluminescence via Novel Light-Collection-Efficiency Calibration: Reexamination of The Aqueous Luminol Chemiluminescence Standard We have developed a luminescence-measurement system for liquid biochemiluminescence that can obtain quantitative luminescence spectra as the absolute total number of luminescence photons at each wavelength or photon energy and quantum yields. Calibration of light-collection efficiency in the system is performed with a reference double-plate cell. This method is applicable to sample cells of any kind suitable for measurement, which is a great advantage over previous techniques in practical experiments. Using this system, the quantum yield of aqueous luminol chemiluminescence was obtained as 1.23-0.20, which is in good agreement with previously reported values.",Healthcare
Explainable Sustainability for AI in the Arts,"AI is becoming increasingly popular in artistic practices, but the tools for informing practitioners about the environmental impact (and other sustainability implications) of AI are adapted for other contexts than creative practices -- making the tools and sustainability implications of AI not accessible for artists and creative practitioners. In this position paper, I describe two empirical studies that aim to develop environmental sustainability reflection systems for AI Arts, and discuss and introduce Explainable Sustainability in for AI Arts.","Explainable Sustainability for AI in the Arts AI is becoming increasingly popular in artistic practices, but the tools for informing practitioners about the environmental impact (and other sustainability implications) of AI are adapted for other contexts than creative practices -- making the tools and sustainability implications of AI not accessible for artists and creative practitioners. In this position paper, I describe two empirical studies that aim to develop environmental sustainability reflection systems for AI Arts, and discuss and introduce Explainable Sustainability in for AI Arts.",Environment
Noise-induced first-order transition in anti-tumor immunotherapy,We studied the single-variable dynamics model of the tumor growth. A first-order phase transition induced by an additive noise is shown to reproduce the main features of tumor growth under immune surveillance. The critical average cells population has a power-law function relationship with the immune coefficient.,Noise-induced first-order transition in anti-tumor immunotherapy We studied the single-variable dynamics model of the tumor growth. A first-order phase transition induced by an additive noise is shown to reproduce the main features of tumor growth under immune surveillance. The critical average cells population has a power-law function relationship with the immune coefficient.,Healthcare
Reverse Engineering Financial Markets with Majority and Minority Games using Genetic Algorithms,"Using virtual stock markets with artificial interacting software investors, aka agent-based models (ABMs), we present a method to reverse engineer real-world financial time series. We model financial markets as made of a large number of interacting boundedly rational agents. By optimizing the similarity between the actual data and that generated by the reconstructed virtual stock market, we obtain parameters and strategies, which reveal some of the inner workings of the target stock market. We validate our approach by out-of-sample predictions of directional moves of the Nasdaq Composite Index.","Reverse Engineering Financial Markets with Majority and Minority Games using Genetic Algorithms Using virtual stock markets with artificial interacting software investors, aka agent-based models (ABMs), we present a method to reverse engineer real-world financial time series. We model financial markets as made of a large number of interacting boundedly rational agents. By optimizing the similarity between the actual data and that generated by the reconstructed virtual stock market, we obtain parameters and strategies, which reveal some of the inner workings of the target stock market. We validate our approach by out-of-sample predictions of directional moves of the Nasdaq Composite Index.",Finance
Insurance Contract for High Renewable Energy Integration,"The increasing penetration of renewable energy poses significant challenges to power grid reliability. There have been increasing interests in utilizing financial tools, such as insurance, to help end-users hedge the potential risk of lost load due to renewable energy variability. With insurance, a user pays a premium fee to the utility, so that he will get compensated in case his demand is not fully satisfied. A proper insurance design needs to resolve the following two challenges: (i) users reliability preference is private information; and (ii) the insurance design is tightly coupled with the renewable energy investment decision. To address these challenges, we adopt the contract theory to elicit users private reliability preferences, and we study how the utility can jointly optimize the insurance contract and the planning of renewable energy. A key analytical challenge is that the joint optimization of the insurance design and the planning of renewables is non-convex. We resolve this difficulty by revealing important structural properties of the optimal solution, using the help of two benchmark problems: the no-insurance benchmark and the social-optimum benchmark. Compared with the no-insurance benchmark, we prove that the social cost and users total energy cost are always no larger under the optimal contract. Simulation results show that the largest benefit of the insurance contract is achieved at a medium electricity-bill price together with a low type heterogeneity and a high renewable uncertainty.","Insurance Contract for High Renewable Energy Integration The increasing penetration of renewable energy poses significant challenges to power grid reliability. There have been increasing interests in utilizing financial tools, such as insurance, to help end-users hedge the potential risk of lost load due to renewable energy variability. With insurance, a user pays a premium fee to the utility, so that he will get compensated in case his demand is not fully satisfied. A proper insurance design needs to resolve the following two challenges: (i) users reliability preference is private information; and (ii) the insurance design is tightly coupled with the renewable energy investment decision. To address these challenges, we adopt the contract theory to elicit users private reliability preferences, and we study how the utility can jointly optimize the insurance contract and the planning of renewable energy. A key analytical challenge is that the joint optimization of the insurance design and the planning of renewables is non-convex. We resolve this difficulty by revealing important structural properties of the optimal solution, using the help of two benchmark problems: the no-insurance benchmark and the social-optimum benchmark. Compared with the no-insurance benchmark, we prove that the social cost and users total energy cost are always no larger under the optimal contract. Simulation results show that the largest benefit of the insurance contract is achieved at a medium electricity-bill price together with a low type heterogeneity and a high renewable uncertainty.",Environment
Optimizing Microgrid Composition for Sustainable Data Centers,"As computing energy demand continues to grow and electrical grid infrastructure struggles to keep pace, an increasing number of data centers are being planned with colocated microgrids that integrate on-site renewable generation and energy storage. However, while existing research has examined the tradeoffs between operational and embodied carbon emissions in the context of renewable energy certificates, there is a lack of tools to assess how the sizing and composition of microgrid components affects long-term sustainability and power reliability. In this paper, we present a novel optimization framework that extends the computing and energy system co-simulator Vessim with detailed renewable energy generation models from the National Renewable Energy Laboratorys (NREL) System Advisor Model (SAM). Our framework simulates the interaction between computing workloads, on-site renewable production, and energy storage, capturing both operational and embodied emissions. We use a multi-horizon black-box optimization to explore efficient microgrid compositions and enable operators to make more informed decisions when planning energy systems for data centers.","Optimizing Microgrid Composition for Sustainable Data Centers As computing energy demand continues to grow and electrical grid infrastructure struggles to keep pace, an increasing number of data centers are being planned with colocated microgrids that integrate on-site renewable generation and energy storage. However, while existing research has examined the tradeoffs between operational and embodied carbon emissions in the context of renewable energy certificates, there is a lack of tools to assess how the sizing and composition of microgrid components affects long-term sustainability and power reliability. In this paper, we present a novel optimization framework that extends the computing and energy system co-simulator Vessim with detailed renewable energy generation models from the National Renewable Energy Laboratorys (NREL) System Advisor Model (SAM). Our framework simulates the interaction between computing workloads, on-site renewable production, and energy storage, capturing both operational and embodied emissions. We use a multi-horizon black-box optimization to explore efficient microgrid compositions and enable operators to make more informed decisions when planning energy systems for data centers.",Environment
Free Lunch,"The concept of absence of opportunities for free lunches is one of the pillars in the economic theory of financial markets. This natural assumption has proved very fruitful and has lead to great mathematical, as well as economical, insights in Quantitative Finance. Formulating rigorously the exact definition of absence of opportunities for riskless profit turned out to be a highly non-trivial fact that troubled mathematicians and economists for at least two decades. The purpose of this note is to give a quick (and, necessarily, incomplete) account of the recent work aimed at providing a simple and intuitive no-free-lunch assumption that would suffice in formulating a version of the celebrated Fundamental Theorem of Asset Pricing.","Free Lunch The concept of absence of opportunities for free lunches is one of the pillars in the economic theory of financial markets. This natural assumption has proved very fruitful and has lead to great mathematical, as well as economical, insights in Quantitative Finance. Formulating rigorously the exact definition of absence of opportunities for riskless profit turned out to be a highly non-trivial fact that troubled mathematicians and economists for at least two decades. The purpose of this note is to give a quick (and, necessarily, incomplete) account of the recent work aimed at providing a simple and intuitive no-free-lunch assumption that would suffice in formulating a version of the celebrated Fundamental Theorem of Asset Pricing.",Finance
A Study on Learnability for Rigid Lambek Grammars,"We present basic notions of Golds learnability in the limit paradigm, first presented in 1967, a formalization of the cognitive process by which a native speaker gets to grasp the underlying grammar of hisher own native language by being exposed to well formed sentences generated by that grammar. Then we present Lambek grammars, a formalism issued from categorial grammars which, although not as expressive as needed for a full formalization of natural languages, is particularly suited to easily implement a natural interface between syntax and semantics. In the last part of this work, we present a learnability result for Rigid Lambek grammars from structured examples.","A Study on Learnability for Rigid Lambek Grammars We present basic notions of Golds learnability in the limit paradigm, first presented in 1967, a formalization of the cognitive process by which a native speaker gets to grasp the underlying grammar of hisher own native language by being exposed to well formed sentences generated by that grammar. Then we present Lambek grammars, a formalism issued from categorial grammars which, although not as expressive as needed for a full formalization of natural languages, is particularly suited to easily implement a natural interface between syntax and semantics. In the last part of this work, we present a learnability result for Rigid Lambek grammars from structured examples.",Technology
Producing treatment hierarchies in network meta-analysis using probabilistic models and treatment-choice criteria,"A key output of network meta-analysis (NMA) is the relative ranking of treatments; nevertheless, it has attracted substantial criticism. Existing ranking methods often lack clear interpretability and fail to adequately account for uncertainty, over-emphasizing small differences in treatment effects. We propose a novel framework to estimate treatment hierarchies in NMA using a probabilistic model, focusing on a clinically relevant treatment-choice criterion (TCC). Initially, we formulate a mathematical expression to define a TCC based on smallest worthwhile differences (SWD), converting NMA relative treatment effects into treatment preference format. This data is then synthesized using a probabilistic ranking model, assigning each treatment a latent ability parameter, representing its propensity to yield clinically important and beneficial true treatment effects relative to the rest of the treatments in the network. Parameter estimation relies on the maximum likelihood theory, with standard errors derived asymptotically from Fishers information matrix. To facilitate the use of our methods, we launched the R package mtrank. We applied our method to two clinical datasets: one comparing 18 antidepressants for major depression and another comparing 6 antihypertensives for the incidence of diabetes. Our approach provided robust, interpretable treatment hierarchies that account for a concrete TCC. We further examined the agreement between the proposed method and existing ranking metrics in 153 published networks, concluding that the degree of agreement depends on the precision of the NMA estimates. Our framework offers a valuable alternative for NMA treatment ranking, mitigating over-interpretation of minor differences. This enables more reliable and clinically meaningful treatment hierarchies.","Producing treatment hierarchies in network meta-analysis using probabilistic models and treatment-choice criteria A key output of network meta-analysis (NMA) is the relative ranking of treatments; nevertheless, it has attracted substantial criticism. Existing ranking methods often lack clear interpretability and fail to adequately account for uncertainty, over-emphasizing small differences in treatment effects. We propose a novel framework to estimate treatment hierarchies in NMA using a probabilistic model, focusing on a clinically relevant treatment-choice criterion (TCC). Initially, we formulate a mathematical expression to define a TCC based on smallest worthwhile differences (SWD), converting NMA relative treatment effects into treatment preference format. This data is then synthesized using a probabilistic ranking model, assigning each treatment a latent ability parameter, representing its propensity to yield clinically important and beneficial true treatment effects relative to the rest of the treatments in the network. Parameter estimation relies on the maximum likelihood theory, with standard errors derived asymptotically from Fishers information matrix. To facilitate the use of our methods, we launched the R package mtrank. We applied our method to two clinical datasets: one comparing 18 antidepressants for major depression and another comparing 6 antihypertensives for the incidence of diabetes. Our approach provided robust, interpretable treatment hierarchies that account for a concrete TCC. We further examined the agreement between the proposed method and existing ranking metrics in 153 published networks, concluding that the degree of agreement depends on the precision of the NMA estimates. Our framework offers a valuable alternative for NMA treatment ranking, mitigating over-interpretation of minor differences. This enables more reliable and clinically meaningful treatment hierarchies.",Healthcare
Value of Storage for Renewable Portfolio Standard,"The ambitious targets for renewable energy penetration warrant huge flexibility in the power system. Such flexibility does not come free. In this paper, we examine the possibility of utilizing storage systems for achieving high renewable energy penetration, and identify the trade-off between providing flexibility and arbitrage against real-time prices. More precisely, we investigate the relationship among the operation cost, storage capacity, and the renewable penetration level. This illustrates the value of storage as well as the true cost induced by the high renewable penetration targets.","Value of Storage for Renewable Portfolio Standard The ambitious targets for renewable energy penetration warrant huge flexibility in the power system. Such flexibility does not come free. In this paper, we examine the possibility of utilizing storage systems for achieving high renewable energy penetration, and identify the trade-off between providing flexibility and arbitrage against real-time prices. More precisely, we investigate the relationship among the operation cost, storage capacity, and the renewable penetration level. This illustrates the value of storage as well as the true cost induced by the high renewable penetration targets.",Environment
Flipping the Large-Enrollment Introductory Physics Classroom,"Most STEM students experience the introductory physics sequence in large-enrollment (N gtrsim 100 students) classrooms, led by one lecturer and supported by a few teaching assistants. This work describes methods and principles we used to create an effective flipped classroom in large- enrollment introductory physics courses by replacing a majority of traditional lecture time with in-class student-driven activity worksheets. In this work, we compare student learning in courses taught by the authors with the flipped classroom pedagogy versus a more traditional pedagogy. By comparing identical questions on exams, we find significant learning gains for students in the student-centered flipped classroom compared to students in the lecturer-centered traditional classroom. Furthermore, we find that the gender gap typically seen in the introductory physics sequence is significantly reduced in the flipped classroom.","Flipping the Large-Enrollment Introductory Physics Classroom Most STEM students experience the introductory physics sequence in large-enrollment (N gtrsim 100 students) classrooms, led by one lecturer and supported by a few teaching assistants. This work describes methods and principles we used to create an effective flipped classroom in large- enrollment introductory physics courses by replacing a majority of traditional lecture time with in-class student-driven activity worksheets. In this work, we compare student learning in courses taught by the authors with the flipped classroom pedagogy versus a more traditional pedagogy. By comparing identical questions on exams, we find significant learning gains for students in the student-centered flipped classroom compared to students in the lecturer-centered traditional classroom. Furthermore, we find that the gender gap typically seen in the introductory physics sequence is significantly reduced in the flipped classroom.",Education
Mining Heterogeneous Multivariate Time-Series for Learning Meaningful Patterns: Application to Home Health Telecare,"For the last years, time-series mining has become a challenging issue for researchers. An important application lies in most monitoring purposes, which require analyzing large sets of time-series for learning usual patterns. Any deviation from this learned profile is then considered as an unexpected situation. Moreover, complex applications may involve the temporal study of several heterogeneous parameters. In that paper, we propose a method for mining heterogeneous multivariate time-series for learning meaningful patterns. The proposed approach allows for mixed time-series -- containing both pattern and non-pattern data -- such as for imprecise matches, outliers, stretching and global translating of patterns instances in time. We present the early results of our approach in the context of monitoring the health status of a person at home. The purpose is to build a behavioral profile of a person by analyzing the time variations of several quantitative or qualitative parameters recorded through a provision of sensors installed in the home.","Mining Heterogeneous Multivariate Time-Series for Learning Meaningful Patterns: Application to Home Health Telecare For the last years, time-series mining has become a challenging issue for researchers. An important application lies in most monitoring purposes, which require analyzing large sets of time-series for learning usual patterns. Any deviation from this learned profile is then considered as an unexpected situation. Moreover, complex applications may involve the temporal study of several heterogeneous parameters. In that paper, we propose a method for mining heterogeneous multivariate time-series for learning meaningful patterns. The proposed approach allows for mixed time-series -- containing both pattern and non-pattern data -- such as for imprecise matches, outliers, stretching and global translating of patterns instances in time. We present the early results of our approach in the context of monitoring the health status of a person at home. The purpose is to build a behavioral profile of a person by analyzing the time variations of several quantitative or qualitative parameters recorded through a provision of sensors installed in the home.",Technology
Water vapor and the dynamics of climate changes,"Water vapor is not only Earths dominant greenhouse gas. Through the release of latent heat when it condenses, it also plays an active role in dynamic processes that shape the global circulation of the atmosphere and thus climate. Here we present an overview of how latent heat release affects atmosphere dynamics in a broad range of climates, ranging from extremely cold to extremely warm. Contrary to widely held beliefs, atmospheric circulation statistics can change non-monotonically with global-mean surface temperature, in part because of dynamic effects of water vapor. For example, the strengths of the tropical Hadley circulation and of zonally asymmetric tropical circulations, as well as the kinetic energy of extratropical baroclinic eddies, can be lower than they presently are both in much warmer climates and in much colder climates. We discuss how latent heat release is implicated in such circulation changes, particularly through its effect on the atmospheric static stability, and we illustrate the circulation changes through simulations with an idealized general circulation model. This allows us to explore a continuum of climates, constrain macroscopic laws governing this climatic continuum, and place past and possible future climate changes in a broader context.","Water vapor and the dynamics of climate changes Water vapor is not only Earths dominant greenhouse gas. Through the release of latent heat when it condenses, it also plays an active role in dynamic processes that shape the global circulation of the atmosphere and thus climate. Here we present an overview of how latent heat release affects atmosphere dynamics in a broad range of climates, ranging from extremely cold to extremely warm. Contrary to widely held beliefs, atmospheric circulation statistics can change non-monotonically with global-mean surface temperature, in part because of dynamic effects of water vapor. For example, the strengths of the tropical Hadley circulation and of zonally asymmetric tropical circulations, as well as the kinetic energy of extratropical baroclinic eddies, can be lower than they presently are both in much warmer climates and in much colder climates. We discuss how latent heat release is implicated in such circulation changes, particularly through its effect on the atmospheric static stability, and we illustrate the circulation changes through simulations with an idealized general circulation model. This allows us to explore a continuum of climates, constrain macroscopic laws governing this climatic continuum, and place past and possible future climate changes in a broader context.",Environment
Disclosure of Investment Advisor and Broker-Dealer Relationships: Impact on Comprehension and Decision Making,"Recently enacted regulations aimed to enhance retail investors understanding about different types of investment accounts. Toward this goal, the Securities and Exchange Commission (SEC) mandated that SEC-registered investment advisors and broker-dealers provide a brief relationship summary (Form CRS) to retail investors. The present study examines the impact of this regulation on investors and considers its market implications. The effects of Form CRS were evaluated based on three outcome variables: perceived helpfulness, comprehension, and decision making. The study also examined whether personal characteristics, such as investment experience, influenced the disclosures impact on decision making. Results indicated that participants perceived the disclosure as helpful and it significantly enhanced comprehension about the two types of investment accounts. Critically, participants also showed increased preference and choice for broker-dealers after the disclosure. Increased preference for broker-dealers was associated with greater investment experience, greater comprehension gains, and access to more information from a longer disclosure. These findings suggest that Form CRS may promote informed decision making among retail investors while simultaneously increasing the selection of broker-dealer accounts.","Disclosure of Investment Advisor and Broker-Dealer Relationships: Impact on Comprehension and Decision Making Recently enacted regulations aimed to enhance retail investors understanding about different types of investment accounts. Toward this goal, the Securities and Exchange Commission (SEC) mandated that SEC-registered investment advisors and broker-dealers provide a brief relationship summary (Form CRS) to retail investors. The present study examines the impact of this regulation on investors and considers its market implications. The effects of Form CRS were evaluated based on three outcome variables: perceived helpfulness, comprehension, and decision making. The study also examined whether personal characteristics, such as investment experience, influenced the disclosures impact on decision making. Results indicated that participants perceived the disclosure as helpful and it significantly enhanced comprehension about the two types of investment accounts. Critically, participants also showed increased preference and choice for broker-dealers after the disclosure. Increased preference for broker-dealers was associated with greater investment experience, greater comprehension gains, and access to more information from a longer disclosure. These findings suggest that Form CRS may promote informed decision making among retail investors while simultaneously increasing the selection of broker-dealer accounts.",Finance
Improving spam filtering by combining Naive Bayes with simple k-nearest neighbor searches,Using naive Bayes for email classification has become very popular within the last few months. They are quite easy to implement and very efficient. In this paper we want to present empirical results of email classification using a combination of naive Bayes and k-nearest neighbor searches. Using this technique we show that the accuracy of a Bayes filter can be improved slightly for a high number of features and significantly for a small number of features.,Improving spam filtering by combining Naive Bayes with simple k-nearest neighbor searches Using naive Bayes for email classification has become very popular within the last few months. They are quite easy to implement and very efficient. In this paper we want to present empirical results of email classification using a combination of naive Bayes and k-nearest neighbor searches. Using this technique we show that the accuracy of a Bayes filter can be improved slightly for a high number of features and significantly for a small number of features.,Technology
Blockchain Technology: A tool to solve the challenges of education sector in developing countries,"The education system is getting diversified, challenged, and blended for the overwhelming advancement of disruptive technology. The core purpose of this chapter is to visualize the probable solutions of the modern education system using blockchain technology. The entire chapter has been discussed on the basis of present solution and projection of future inventions to smoothen the education system. The fourth industrial revolution (4IR) is changing our experiences in terms of education and other lifestyle. Delivering lectures, interacting between learners and educations, evaluating learning outcomes, and verifying educational credentials might be smoother, easier, faster, cheaper, and jollier than before. Blockchain technology can contribute to the education provider to tackle all those existing problems to create a comfortable learning environment to all irrespective to their economic backgrounds and geographic location. How this technology can contribute to improve Reviewing recent inventions in this technology, the chapter explains some of the strategies to go beyond the ongoing projects around the world. A set of models are arranged to enable the readers mind for future inventions in the realm of educationists. Keywords: -Blockchain, 4IR, educators, learning outcome.","Blockchain Technology: A tool to solve the challenges of education sector in developing countries The education system is getting diversified, challenged, and blended for the overwhelming advancement of disruptive technology. The core purpose of this chapter is to visualize the probable solutions of the modern education system using blockchain technology. The entire chapter has been discussed on the basis of present solution and projection of future inventions to smoothen the education system. The fourth industrial revolution (4IR) is changing our experiences in terms of education and other lifestyle. Delivering lectures, interacting between learners and educations, evaluating learning outcomes, and verifying educational credentials might be smoother, easier, faster, cheaper, and jollier than before. Blockchain technology can contribute to the education provider to tackle all those existing problems to create a comfortable learning environment to all irrespective to their economic backgrounds and geographic location. How this technology can contribute to improve Reviewing recent inventions in this technology, the chapter explains some of the strategies to go beyond the ongoing projects around the world. A set of models are arranged to enable the readers mind for future inventions in the realm of educationists. Keywords: -Blockchain, 4IR, educators, learning outcome.",Education
Statistical Feature Combination for the Evaluation of Game Positions,"This article describes an application of three well-known statistical methods in the field of game-tree search: using a large number of classified Othello positions, feature weights for evaluation functions with a game-phase-independent meaning are estimated by means of logistic regression, Fishers linear discriminant, and the quadratic discriminant function for normally distributed features. Thereafter, the playing strengths are compared by means of tournaments between the resulting versions of a world-class Othello program. In this application, logistic regression - which is used here for the first time in the context of game playing - leads to better results than the other approaches.","Statistical Feature Combination for the Evaluation of Game Positions This article describes an application of three well-known statistical methods in the field of game-tree search: using a large number of classified Othello positions, feature weights for evaluation functions with a game-phase-independent meaning are estimated by means of logistic regression, Fishers linear discriminant, and the quadratic discriminant function for normally distributed features. Thereafter, the playing strengths are compared by means of tournaments between the resulting versions of a world-class Othello program. In this application, logistic regression - which is used here for the first time in the context of game playing - leads to better results than the other approaches.",Technology
A User Interface Study on Sustainable City Trip Recommendations,"The importance of promoting sustainable and environmentally responsible practices is becoming increasingly recognized in all domains, including tourism. The impact of tourism extends beyond its immediate stakeholders and affects passive participants such as the environment, local businesses, and residents. City trips, in particular, offer significant opportunities to encourage sustainable tourism practices by directing travelers towards destinations that minimize environmental impact while providing enriching experiences. Tourism Recommender Systems (TRS) can play a critical role in this. By integrating sustainability features in TRS, travelers can be guided towards destinations that meet their preferences and align with sustainability objectives. This paper investigates how different user interface design elements affect the promotion of sustainable city trip choices. We explore the impact of various features on user decisions, including sustainability labels for transportation modes and their emissions, popularity indicators for destinations, seasonality labels reflecting crowd levels for specific months, and an overall sustainability composite score. Through a user study involving mockups, participants evaluated the helpfulness of these features in guiding them toward more sustainable travel options. Our findings indicate that sustainability labels significantly influence users towards lower-carbon footprint options, while popularity and seasonality indicators guide users to less crowded and more seasonally appropriate destinations. This study emphasizes the importance of providing users with clear and informative sustainability information, which can help them make more sustainable travel choices. It lays the groundwork for future applications that can recommend sustainable destinations in real-time.","A User Interface Study on Sustainable City Trip Recommendations The importance of promoting sustainable and environmentally responsible practices is becoming increasingly recognized in all domains, including tourism. The impact of tourism extends beyond its immediate stakeholders and affects passive participants such as the environment, local businesses, and residents. City trips, in particular, offer significant opportunities to encourage sustainable tourism practices by directing travelers towards destinations that minimize environmental impact while providing enriching experiences. Tourism Recommender Systems (TRS) can play a critical role in this. By integrating sustainability features in TRS, travelers can be guided towards destinations that meet their preferences and align with sustainability objectives. This paper investigates how different user interface design elements affect the promotion of sustainable city trip choices. We explore the impact of various features on user decisions, including sustainability labels for transportation modes and their emissions, popularity indicators for destinations, seasonality labels reflecting crowd levels for specific months, and an overall sustainability composite score. Through a user study involving mockups, participants evaluated the helpfulness of these features in guiding them toward more sustainable travel options. Our findings indicate that sustainability labels significantly influence users towards lower-carbon footprint options, while popularity and seasonality indicators guide users to less crowded and more seasonally appropriate destinations. This study emphasizes the importance of providing users with clear and informative sustainability information, which can help them make more sustainable travel choices. It lays the groundwork for future applications that can recommend sustainable destinations in real-time.",Environment
Teacher Learning of Technology-Enhanced Formative Assessment,"Technology-Enhanced Formative Assessment (TEFA) is a pedagogy for teaching with classroom response technology. Teacher Learning of TEFA is a five-year research project studying teacher change, in the context of an intensive professional development program designed to help science and mathematics teachers learn TEFA. First, we provide an overview of the projects participating teachers, its intervention (consisting of the technology, the pedagogy, and the professional development program), and its research design. Then, we present narratives describing the unfolding change process experienced by four teachers. Afterward, we present some preliminary findings of the research, describe a model for the co-evolution of teacher and pedagogy that we are developing, and identify general implications for professional development.","Teacher Learning of Technology-Enhanced Formative Assessment Technology-Enhanced Formative Assessment (TEFA) is a pedagogy for teaching with classroom response technology. Teacher Learning of TEFA is a five-year research project studying teacher change, in the context of an intensive professional development program designed to help science and mathematics teachers learn TEFA. First, we provide an overview of the projects participating teachers, its intervention (consisting of the technology, the pedagogy, and the professional development program), and its research design. Then, we present narratives describing the unfolding change process experienced by four teachers. Afterward, we present some preliminary findings of the research, describe a model for the co-evolution of teacher and pedagogy that we are developing, and identify general implications for professional development.",Education
Bandit Algorithms for Tree Search,"Bandit based methods for tree search have recently gained popularity when applied to huge trees, e.g. in the game of go (Gelly et al., 2006). The UCT algorithm (Kocsis and Szepesvari, 2006), a tree search method based on Upper Confidence Bounds (UCB) (Auer et al., 2002), is believed to adapt locally to the effective smoothness of the tree. However, we show that UCT is too optimistic in some cases, leading to a regret O(exp(exp(D))) where D is the depth of the tree. We propose alternative bandit algorithms for tree search. First, a modification of UCT using a confidence sequence that scales exponentially with the horizon depth is proven to have a regret O(2D sqrtn), but does not adapt to possible smoothness in the tree. We then analyze Flat-UCB performed on the leaves and provide a finite regret bound with high probability. Then, we introduce a UCB-based Bandit Algorithm for Smooth Trees which takes into account actual smoothness of the rewards for performing efficient cuts of sub-optimal branches with high confidence. Finally, we present an incremental tree search version which applies when the full tree is too big (possibly infinite) to be entirely represented and show that with high probability, essentially only the optimal branches is indefinitely developed. We illustrate these methods on a global optimization problem of a Lipschitz function, given noisy data.","Bandit Algorithms for Tree Search Bandit based methods for tree search have recently gained popularity when applied to huge trees, e.g. in the game of go (Gelly et al., 2006). The UCT algorithm (Kocsis and Szepesvari, 2006), a tree search method based on Upper Confidence Bounds (UCB) (Auer et al., 2002), is believed to adapt locally to the effective smoothness of the tree. However, we show that UCT is too optimistic in some cases, leading to a regret O(exp(exp(D))) where D is the depth of the tree. We propose alternative bandit algorithms for tree search. First, a modification of UCT using a confidence sequence that scales exponentially with the horizon depth is proven to have a regret O(2D sqrtn), but does not adapt to possible smoothness in the tree. We then analyze Flat-UCB performed on the leaves and provide a finite regret bound with high probability. Then, we introduce a UCB-based Bandit Algorithm for Smooth Trees which takes into account actual smoothness of the rewards for performing efficient cuts of sub-optimal branches with high confidence. Finally, we present an incremental tree search version which applies when the full tree is too big (possibly infinite) to be entirely represented and show that with high probability, essentially only the optimal branches is indefinitely developed. We illustrate these methods on a global optimization problem of a Lipschitz function, given noisy data.",Technology
A Climate Change Vulnerability Assessment Framework: A Spatial Approach,"Climate change is affecting every known society, especially for small farmers in Low-Income Countries because they depend heavily on rain, seasonality patterns, and known temperature ranges. To build climate change resilient communities among rural farmers, the first step is to understand the impact of climate change on the population. This paper proposes a Climate Change Vulnerability Assessment Framework (CCVAF) to assess climate change vulnerabilities among rural farmers. The CCVAF framework uses information and communication technology (ICT) to assess climate change vulnerabilities among rural farmers by integrating both community level and individual household level indicators. The CCVAF was instantiated into a GIS-based web application named THRIVE for different decision-makers to better assess how climate change is affecting rural farmers in Western Honduras. Qualitative evaluation of the THRIVE showed that it is an innovative and useful tool. The CCVAF contributes to not only the knowledge base of the climate change vulnerability assessment but also the design science literature by providing guidelines to design a class of climate change vulnerability assessment solutions.","A Climate Change Vulnerability Assessment Framework: A Spatial Approach Climate change is affecting every known society, especially for small farmers in Low-Income Countries because they depend heavily on rain, seasonality patterns, and known temperature ranges. To build climate change resilient communities among rural farmers, the first step is to understand the impact of climate change on the population. This paper proposes a Climate Change Vulnerability Assessment Framework (CCVAF) to assess climate change vulnerabilities among rural farmers. The CCVAF framework uses information and communication technology (ICT) to assess climate change vulnerabilities among rural farmers by integrating both community level and individual household level indicators. The CCVAF was instantiated into a GIS-based web application named THRIVE for different decision-makers to better assess how climate change is affecting rural farmers in Western Honduras. Qualitative evaluation of the THRIVE showed that it is an innovative and useful tool. The CCVAF contributes to not only the knowledge base of the climate change vulnerability assessment but also the design science literature by providing guidelines to design a class of climate change vulnerability assessment solutions.",Environment
Computational Methods and Results for Structured Multiscale Models of Tumor Invasion,"We present multiscale models of cancer tumor invasion with components at the molecular, cellular, and tissue levels. We provide biological justifications for the model components, present computational results from the model, and discuss the scientific-computing methodology used to solve the model equations. The models and methodology presented in this paper form the basis for developing and treating increasingly complex, mechanistic models of tumor invasion that will be more predictive and less phenomenological. Because many of the features of the cancer models, such as taxis, aging and growth, are seen in other biological systems, the models and methods discussed here also provide a template for handling a broader range of biological problems.","Computational Methods and Results for Structured Multiscale Models of Tumor Invasion We present multiscale models of cancer tumor invasion with components at the molecular, cellular, and tissue levels. We provide biological justifications for the model components, present computational results from the model, and discuss the scientific-computing methodology used to solve the model equations. The models and methodology presented in this paper form the basis for developing and treating increasingly complex, mechanistic models of tumor invasion that will be more predictive and less phenomenological. Because many of the features of the cancer models, such as taxis, aging and growth, are seen in other biological systems, the models and methods discussed here also provide a template for handling a broader range of biological problems.",Healthcare
An AI-Enabled Framework Within Reach for Enhancing Healthcare Sustainability and Fairness,"Good health and well-being is among key issues in the United Nations 2030 Sustainable Development Goals. The rising prevalence of large-scale infectious diseases and the accelerated aging of the global population are driving the transformation of healthcare technologies. In this context, establishing large-scale public health datasets, developing medical models, and creating decision-making systems with a human-centric approach are of strategic significance. Recently, by leveraging the extraordinary number of accessible cameras, groundbreaking advancements have emerged in AI methods for physiological signal monitoring and disease diagnosis using camera sensors. These approaches, requiring no specialized medical equipment, offer convenient manners of collecting large-scale medical data in response to public health events. Therefore, we outline a prospective framework and heuristic vision for a camera-based public health (CBPH) framework utilizing visual physiological monitoring technology. The CBPH can be considered as a convenient and universal framework for public health, advancing the United Nations Sustainable Development Goals, particularly in promoting the universality, sustainability, and equity of healthcare in low- and middle-income countries or regions. Furthermore, CBPH provides a comprehensive solution for building a large-scale and human-centric medical database, and a multi-task large medical model for public health and medical scientific discoveries. It has a significant potential to revolutionize personal monitoring technologies, digital medicine, telemedicine, and primary health care in public health. Therefore, it can be deemed that the outcomes of this paper will contribute to the establishment of a sustainable and fair framework for public health, which serves as a crucial bridge for advancing scientific discoveries in the realm of AI for medicine (AI4Medicine).","An AI-Enabled Framework Within Reach for Enhancing Healthcare Sustainability and Fairness Good health and well-being is among key issues in the United Nations 2030 Sustainable Development Goals. The rising prevalence of large-scale infectious diseases and the accelerated aging of the global population are driving the transformation of healthcare technologies. In this context, establishing large-scale public health datasets, developing medical models, and creating decision-making systems with a human-centric approach are of strategic significance. Recently, by leveraging the extraordinary number of accessible cameras, groundbreaking advancements have emerged in AI methods for physiological signal monitoring and disease diagnosis using camera sensors. These approaches, requiring no specialized medical equipment, offer convenient manners of collecting large-scale medical data in response to public health events. Therefore, we outline a prospective framework and heuristic vision for a camera-based public health (CBPH) framework utilizing visual physiological monitoring technology. The CBPH can be considered as a convenient and universal framework for public health, advancing the United Nations Sustainable Development Goals, particularly in promoting the universality, sustainability, and equity of healthcare in low- and middle-income countries or regions. Furthermore, CBPH provides a comprehensive solution for building a large-scale and human-centric medical database, and a multi-task large medical model for public health and medical scientific discoveries. It has a significant potential to revolutionize personal monitoring technologies, digital medicine, telemedicine, and primary health care in public health. Therefore, it can be deemed that the outcomes of this paper will contribute to the establishment of a sustainable and fair framework for public health, which serves as a crucial bridge for advancing scientific discoveries in the realm of AI for medicine (AI4Medicine).",Healthcare
Assessing the Validity of a a priori Patient-Trial Generalizability Score using Real-world Data from a Large Clinical Data Research Network: A Colorectal Cancer Clinical Trial Case Study,"Existing trials had not taken enough consideration of their population representativeness, which can lower the effectiveness when the treatment is applied in real-world clinical practice. We analyzed the eligibility criteria of Bevacizumab colorectal cancer treatment trials, assessed their a priori generalizability, and examined how it affects patient outcomes when applied in real-world clinical settings. To do so, we extracted patient-level data from a large collection of electronic health records (EHRs) from the OneFlorida consortium. We built a zero-inflated negative binomial model using a composite patient-trial generalizability (cPTG) score to predict patients clinical outcomes (i.e., number of serious adverse events, (SAEs)). Our study results provide a body of evidence that 1) the cPTG scores can predict patient outcomes; and 2) patients who are more similar to the study population in the trials that were used to develop the treatment will have a significantly lower possibility to experience serious adverse events.","Assessing the Validity of a a priori Patient-Trial Generalizability Score using Real-world Data from a Large Clinical Data Research Network: A Colorectal Cancer Clinical Trial Case Study Existing trials had not taken enough consideration of their population representativeness, which can lower the effectiveness when the treatment is applied in real-world clinical practice. We analyzed the eligibility criteria of Bevacizumab colorectal cancer treatment trials, assessed their a priori generalizability, and examined how it affects patient outcomes when applied in real-world clinical settings. To do so, we extracted patient-level data from a large collection of electronic health records (EHRs) from the OneFlorida consortium. We built a zero-inflated negative binomial model using a composite patient-trial generalizability (cPTG) score to predict patients clinical outcomes (i.e., number of serious adverse events, (SAEs)). Our study results provide a body of evidence that 1) the cPTG scores can predict patient outcomes; and 2) patients who are more similar to the study population in the trials that were used to develop the treatment will have a significantly lower possibility to experience serious adverse events.",Healthcare
"A wavelet analysis of inter-dependence, contagion and long memory among global equity markets","This study attempts to investigate into the structure and features of global equity markets from a time-frequency perspective. An analysis grounded on this framework allows one to capture information from a different dimension, as opposed to the traditional time domain analyses, where multiscale structures of financial markets are clearly extracted. In financial time series, multiscale features manifest themselves due to presence of multiple time horizons. The existence of multiple time horizons necessitates a careful investigation of each time horizon separately as market structures are not homogenous across different time horizons. The presence of multiple time horizons, with varying levels of complexity, requires one to investigate financial time series from a heterogeneous market perspective where market players are said to operate at different investment horizons. This thesis extends the application of time-frequency based wavelet techniques to: i) analyse the interdependence of global equity markets from a heterogeneous investor perspective with a special focus on the Indian stock market, ii) investigate the contagion effect, if any, of financial crises on Indian stock market, and iii) to study fractality and scaling properties of global equity markets and analyse the efficiency of Indian stock markets using wavelet based long memory methods.","A wavelet analysis of inter-dependence, contagion and long memory among global equity markets This study attempts to investigate into the structure and features of global equity markets from a time-frequency perspective. An analysis grounded on this framework allows one to capture information from a different dimension, as opposed to the traditional time domain analyses, where multiscale structures of financial markets are clearly extracted. In financial time series, multiscale features manifest themselves due to presence of multiple time horizons. The existence of multiple time horizons necessitates a careful investigation of each time horizon separately as market structures are not homogenous across different time horizons. The presence of multiple time horizons, with varying levels of complexity, requires one to investigate financial time series from a heterogeneous market perspective where market players are said to operate at different investment horizons. This thesis extends the application of time-frequency based wavelet techniques to: i) analyse the interdependence of global equity markets from a heterogeneous investor perspective with a special focus on the Indian stock market, ii) investigate the contagion effect, if any, of financial crises on Indian stock market, and iii) to study fractality and scaling properties of global equity markets and analyse the efficiency of Indian stock markets using wavelet based long memory methods.",Finance
Models for Planning and Simulation in Computer Assisted Orthognatic Surgery,"Two aspects required to establish a planning in orthognatic surgery are addressed in this paper. First, a 3D cephalometric analysis, which is clini-cally essential for the therapeutic decision. Then, an original method to build a biomechanical model of patient face soft tissue, which provides evaluation of the aesthetic outcomes of an intervention. Both points are developed within a clinical application context for computer aided maxillofacial surgery.","Models for Planning and Simulation in Computer Assisted Orthognatic Surgery Two aspects required to establish a planning in orthognatic surgery are addressed in this paper. First, a 3D cephalometric analysis, which is clini-cally essential for the therapeutic decision. Then, an original method to build a biomechanical model of patient face soft tissue, which provides evaluation of the aesthetic outcomes of an intervention. Both points are developed within a clinical application context for computer aided maxillofacial surgery.",Healthcare
"21st Century Ergonomic Education, From Little e to Big E","Despite intense efforts, contemporary educational systems are not enabling individuals to function optimally in modern society. The main reason is that reformers are trying to improve systems that are not designed to take advantage of the centuries of history of the development of todays societies. Nor do they recognize the implications of the millions of years of history of life on earth in which humans are the latest edition of learning organisms. The contemporary educational paradigm of education for all is based on a 17th century model of printing minds for passing on static knowledge. This characterizes most of K-12 education. In contrast, 21st Century education demands a new paradigm, which we call Ergonomic Education. This is an education system that is designed to fit the students of any age instead of forcing the students to fit the education system. It takes into account in a fundamental way what students want to learn -- the concept wanting to learn refers to the innate ability and desire to learn that is characteristic of humans. The Ergonomic Education paradigm shifts to education based on coaching students as human beings who are hungry for productive learning throughout their lives from their very earliest days.","21st Century Ergonomic Education, From Little e to Big E Despite intense efforts, contemporary educational systems are not enabling individuals to function optimally in modern society. The main reason is that reformers are trying to improve systems that are not designed to take advantage of the centuries of history of the development of todays societies. Nor do they recognize the implications of the millions of years of history of life on earth in which humans are the latest edition of learning organisms. The contemporary educational paradigm of education for all is based on a 17th century model of printing minds for passing on static knowledge. This characterizes most of K-12 education. In contrast, 21st Century education demands a new paradigm, which we call Ergonomic Education. This is an education system that is designed to fit the students of any age instead of forcing the students to fit the education system. It takes into account in a fundamental way what students want to learn -- the concept wanting to learn refers to the innate ability and desire to learn that is characteristic of humans. The Ergonomic Education paradigm shifts to education based on coaching students as human beings who are hungry for productive learning throughout their lives from their very earliest days.",Education
From Royal Road to Epistatic Road for Variable Length Evolution Algorithm,"Although there are some real world applications where the use of variable length representation (VLR) in Evolutionary Algorithm is natural and suitable, an academic framework is lacking for such representations. In this work we propose a family of tunable fitness landscapes based on VLR of genotypes. The fitness landscapes we propose possess a tunable degree of both neutrality and epistasis; they are inspired, on the one hand by the Royal Road fitness landscapes, and the other hand by the NK fitness landscapes. So these landscapes offer a scale of continuity from Royal Road functions, with neutrality and no epistasis, to landscapes with a large amount of epistasis and no redundancy. To gain insight into these fitness landscapes, we first use standard tools such as adaptive walks and correlation length. Second, we evaluate the performances of evolutionary algorithms on these landscapes for various values of the neutral and the epistatic parameters; the results allow us to correlate the performances with the expected degrees of neutrality and epistasis.","From Royal Road to Epistatic Road for Variable Length Evolution Algorithm Although there are some real world applications where the use of variable length representation (VLR) in Evolutionary Algorithm is natural and suitable, an academic framework is lacking for such representations. In this work we propose a family of tunable fitness landscapes based on VLR of genotypes. The fitness landscapes we propose possess a tunable degree of both neutrality and epistasis; they are inspired, on the one hand by the Royal Road fitness landscapes, and the other hand by the NK fitness landscapes. So these landscapes offer a scale of continuity from Royal Road functions, with neutrality and no epistasis, to landscapes with a large amount of epistasis and no redundancy. To gain insight into these fitness landscapes, we first use standard tools such as adaptive walks and correlation length. Second, we evaluate the performances of evolutionary algorithms on these landscapes for various values of the neutral and the epistatic parameters; the results allow us to correlate the performances with the expected degrees of neutrality and epistasis.",Technology
"Thinking, Learning, and Autonomous Problem Solving","Ever increasing computational power will require methods for automatic programming. We present an alternative to genetic programming, based on a general model of thinking and learning. The advantage is that evolution takes place in the space of constructs and can thus exploit the mathematical structures of this space. The model is formalized, and a macro language is presented which allows for a formal yet intuitive description of the problem under consideration. A prototype has been developed to implement the scheme in PERL. This method will lead to a concentration on the analysis of problems, to a more rapid prototyping, to the treatment of new problem classes, and to the investigation of philosophical problems. We see fields of application in nonlinear differential equations, pattern recognition, robotics, model building, and animated pictures.","Thinking, Learning, and Autonomous Problem Solving Ever increasing computational power will require methods for automatic programming. We present an alternative to genetic programming, based on a general model of thinking and learning. The advantage is that evolution takes place in the space of constructs and can thus exploit the mathematical structures of this space. The model is formalized, and a macro language is presented which allows for a formal yet intuitive description of the problem under consideration. A prototype has been developed to implement the scheme in PERL. This method will lead to a concentration on the analysis of problems, to a more rapid prototyping, to the treatment of new problem classes, and to the investigation of philosophical problems. We see fields of application in nonlinear differential equations, pattern recognition, robotics, model building, and animated pictures.",Technology
Neural network based human reliability analysis method in production systems,"Purpose: In addition to playing an important role in creating economic security and investment development, insurance companies also invest. The countrys insurance industry as one of the countrys financial institutions has a special place in the investment process and special attention to appropriate investment policies in the field of insurance industry is essential. So that the efficiency of this industry in allocating the existing budget stimulates other economic sectors. This study seeks to model investment in the performance of dynamic networks of insurance companies. Methodology: In this paper, a new investment model is designed to examine the dynamic network performance of insurance companies in Iran. The designed model is implemented using GAMS software and the outputs of the model are analyzed based on regression method. The required information has been collected based on the statistics of insurance companies in Iran between 1393 and 1398. Findings: After evaluating these units, out of 15 companies evaluated, 6 companies had unit performance and were introduced as efficient companies. The average efficiency of insurance companies is 0.78 and the standard deviation is 0.2. The results show that the increase in the value of investments is due to the large reduction in costs and in terms of capital and net profit of companies is a large number that has a clear and strong potential for insurance companies. OriginalityValue: In this paper, investment modeling is performed to examine the performance of dynamic networks of insurance companies in Iran.","Neural network based human reliability analysis method in production systems Purpose: In addition to playing an important role in creating economic security and investment development, insurance companies also invest. The countrys insurance industry as one of the countrys financial institutions has a special place in the investment process and special attention to appropriate investment policies in the field of insurance industry is essential. So that the efficiency of this industry in allocating the existing budget stimulates other economic sectors. This study seeks to model investment in the performance of dynamic networks of insurance companies. Methodology: In this paper, a new investment model is designed to examine the dynamic network performance of insurance companies in Iran. The designed model is implemented using GAMS software and the outputs of the model are analyzed based on regression method. The required information has been collected based on the statistics of insurance companies in Iran between 1393 and 1398. Findings: After evaluating these units, out of 15 companies evaluated, 6 companies had unit performance and were introduced as efficient companies. The average efficiency of insurance companies is 0.78 and the standard deviation is 0.2. The results show that the increase in the value of investments is due to the large reduction in costs and in terms of capital and net profit of companies is a large number that has a clear and strong potential for insurance companies. OriginalityValue: In this paper, investment modeling is performed to examine the performance of dynamic networks of insurance companies in Iran.",Finance
Fast computation of vanilla prices in time-changed models and implied volatilities using rational approximations,"We present a new numerical method to price vanilla options quickly in time-changed Brownian motion models. The method is based on rational function approximations of the Black-Scholes formula. Detailed numerical results are given for a number of widely used models. In particular, we use the variance-gamma model, the CGMY model and the Heston model without correlation to illustrate our results. Comparison to the standard fast Fourier transform method with respect to accuracy and speed appears to favour the newly developed method in the cases considered. We present error estimates for the option prices. Additionally, we use this method to derive a procedure to compute, for a given set of arbitrage-free European call option prices, the corresponding Black-Scholes implied volatility surface. To achieve this, rational function approximations of the inverse of the Black-Scholes formula are used. We are thus able to work out implied volatilities more efficiently than one can by the use of other common methods. Error estimates are presented for a wide range of parameters.","Fast computation of vanilla prices in time-changed models and implied volatilities using rational approximations We present a new numerical method to price vanilla options quickly in time-changed Brownian motion models. The method is based on rational function approximations of the Black-Scholes formula. Detailed numerical results are given for a number of widely used models. In particular, we use the variance-gamma model, the CGMY model and the Heston model without correlation to illustrate our results. Comparison to the standard fast Fourier transform method with respect to accuracy and speed appears to favour the newly developed method in the cases considered. We present error estimates for the option prices. Additionally, we use this method to derive a procedure to compute, for a given set of arbitrage-free European call option prices, the corresponding Black-Scholes implied volatility surface. To achieve this, rational function approximations of the inverse of the Black-Scholes formula are used. We are thus able to work out implied volatilities more efficiently than one can by the use of other common methods. Error estimates are presented for a wide range of parameters.",Finance
Pricing of average strike Asian call option using numerical PDE methods,"In this paper, a standard PDE for the pricing of arithmetic average strike Asian call option is presented. A Crank-Nicolson Implicit Method and a Higher Order Compact finite difference scheme for this pricing problem is derived. Both these schemes were implemented for various values of risk free rate and volatility. The option prices for the same set of values of risk free rate and volatility was also computed using Monte Carlo simulation. The comparative results of the two numerical PDE methods shows close match with the Monte Carlo results, with the Higher Order Compact scheme exhibiting a better match. To the best of our knowledge, this is the first work to use the numerical PDE approach for pricing Asian call options with average strike.","Pricing of average strike Asian call option using numerical PDE methods In this paper, a standard PDE for the pricing of arithmetic average strike Asian call option is presented. A Crank-Nicolson Implicit Method and a Higher Order Compact finite difference scheme for this pricing problem is derived. Both these schemes were implemented for various values of risk free rate and volatility. The option prices for the same set of values of risk free rate and volatility was also computed using Monte Carlo simulation. The comparative results of the two numerical PDE methods shows close match with the Monte Carlo results, with the Higher Order Compact scheme exhibiting a better match. To the best of our knowledge, this is the first work to use the numerical PDE approach for pricing Asian call options with average strike.",Finance
A family tree of Markov models in systems biology,"Motivated by applications in systems biology, we seek a probabilistic framework based on Markov processes to represent intracellular processes. We review the formal relationships between different stochastic models referred to in the systems biology literature. As part of this review, we present a novel derivation of the differential Chapman-Kolmogorov equation for a general multidimensional Markov process made up of both continuous and jump processes. We start with the definition of a time-derivative for a probability density but place no restrictions on the probability distribution, in particular, we do not assume it to be confined to a region that has a surface (on which the probability is zero). In our derivation, the master equation gives the jump part of the Markov process while the Fokker-Planck equation gives the continuous part. We thereby sketch a family tree for stochastic models in systems biology, providing explicit derivations of their formal relationship and clarifying assumptions involved.","A family tree of Markov models in systems biology Motivated by applications in systems biology, we seek a probabilistic framework based on Markov processes to represent intracellular processes. We review the formal relationships between different stochastic models referred to in the systems biology literature. As part of this review, we present a novel derivation of the differential Chapman-Kolmogorov equation for a general multidimensional Markov process made up of both continuous and jump processes. We start with the definition of a time-derivative for a probability density but place no restrictions on the probability distribution, in particular, we do not assume it to be confined to a region that has a surface (on which the probability is zero). In our derivation, the master equation gives the jump part of the Markov process while the Fokker-Planck equation gives the continuous part. We thereby sketch a family tree for stochastic models in systems biology, providing explicit derivations of their formal relationship and clarifying assumptions involved.",Healthcare
"GPCALMA, a mammographic CAD in a GRID connection","Purpose of this work is the development of an automatic system which could be useful for radiologists in the investigation of breast cancer. A breast neoplasia is often marked by the presence of microcalcifications and massive lesions in the mammogram: hence the need for tools able to recognize such lesions at an early stage. GPCALMA (Grid Platform Computer Assisted Library for MAmmography), a collaboration among italian physicists and radiologists, has built a large distributed database of digitized mammographic images (at this moment about 5500 images corresponding to 1650 patients). This collaboration has developed a CAD (Computer Aided Detection) system which, installed in an integrated station, can also be used for digitization, as archive and to perform statistical analysis. With a GRID configuration it would be possible for the clinicians tele- and co-working in new and innovative groupings (virtual organisations) and, using the whole database, by the GPCALMA tools several analysis can be performed. Furthermore the GPCALMA system allows to be abreast of the CAD technical progressing into several hospital locations always with remote working by GRID connection. We report in this work the results obtained by the GPCALMA CAD software implemented with a GRID connection.","GPCALMA, a mammographic CAD in a GRID connection Purpose of this work is the development of an automatic system which could be useful for radiologists in the investigation of breast cancer. A breast neoplasia is often marked by the presence of microcalcifications and massive lesions in the mammogram: hence the need for tools able to recognize such lesions at an early stage. GPCALMA (Grid Platform Computer Assisted Library for MAmmography), a collaboration among italian physicists and radiologists, has built a large distributed database of digitized mammographic images (at this moment about 5500 images corresponding to 1650 patients). This collaboration has developed a CAD (Computer Aided Detection) system which, installed in an integrated station, can also be used for digitization, as archive and to perform statistical analysis. With a GRID configuration it would be possible for the clinicians tele- and co-working in new and innovative groupings (virtual organisations) and, using the whole database, by the GPCALMA tools several analysis can be performed. Furthermore the GPCALMA system allows to be abreast of the CAD technical progressing into several hospital locations always with remote working by GRID connection. We report in this work the results obtained by the GPCALMA CAD software implemented with a GRID connection.",Healthcare
Fast resolution of a single factor Heath-Jarrow-Morton model with stochastic volatility,"This paper considers the single factor Heath-Jarrow-Morton model for the interest rate curve with stochastic volatility. Its natural formulation, described in terms of stochastic differential equations, is solved through Monte Carlo simulations, that usually involve rather large computation time, inefficient from a practical (financial) perspective. This model turns to be Markovian in three dimensions and therefore it can be mapped into a 3D partial differential equations problem. We propose an optimized numerical method to solve the 3D PDE model in both low computation time and reasonable accuracy, a fundamental criterion for practical purposes. The spatial and temporal discretization are performed using finite-difference and Crank-Nicholson schemes respectively, and the computational efficiency is largely increased performing a scale analysis and using Alternating Direction Implicit schemes. Several numerical considerations such as convergence criteria or computation time are analyzed and discussed.","Fast resolution of a single factor Heath-Jarrow-Morton model with stochastic volatility This paper considers the single factor Heath-Jarrow-Morton model for the interest rate curve with stochastic volatility. Its natural formulation, described in terms of stochastic differential equations, is solved through Monte Carlo simulations, that usually involve rather large computation time, inefficient from a practical (financial) perspective. This model turns to be Markovian in three dimensions and therefore it can be mapped into a 3D partial differential equations problem. We propose an optimized numerical method to solve the 3D PDE model in both low computation time and reasonable accuracy, a fundamental criterion for practical purposes. The spatial and temporal discretization are performed using finite-difference and Crank-Nicholson schemes respectively, and the computational efficiency is largely increased performing a scale analysis and using Alternating Direction Implicit schemes. Several numerical considerations such as convergence criteria or computation time are analyzed and discussed.",Finance
Non-asymptotic calibration and resolution,"We analyze a new algorithm for probability forecasting of binary observations on the basis of the available data, without making any assumptions about the way the observations are generated. The algorithm is shown to be well calibrated and to have good resolution for long enough sequences of observations and for a suitable choice of its parameter, a kernel on the Cartesian product of the forecast space 0,1 and the data space. Our main results are non-asymptotic: we establish explicit inequalities, shown to be tight, for the performance of the algorithm.","Non-asymptotic calibration and resolution We analyze a new algorithm for probability forecasting of binary observations on the basis of the available data, without making any assumptions about the way the observations are generated. The algorithm is shown to be well calibrated and to have good resolution for long enough sequences of observations and for a suitable choice of its parameter, a kernel on the Cartesian product of the forecast space 0,1 and the data space. Our main results are non-asymptotic: we establish explicit inequalities, shown to be tight, for the performance of the algorithm.",Technology
About Some Applications of Kolmogorov Equations to the Simulation of Financial Institutions Activity,"The goal of this article is to describe the concepts of system dynamics and its applications to the simulation modeling of financial institutions daily activity. The hybrid method of the re-engineering of banking business processes based upon combination of system dynamics, queuing theory and tools of ordinary differential equations (Kolmogorov equations) is offered.","About Some Applications of Kolmogorov Equations to the Simulation of Financial Institutions Activity The goal of this article is to describe the concepts of system dynamics and its applications to the simulation modeling of financial institutions daily activity. The hybrid method of the re-engineering of banking business processes based upon combination of system dynamics, queuing theory and tools of ordinary differential equations (Kolmogorov equations) is offered.",Finance
Teaching Machine Learning in K-12 Computing Education: Potential and Pitfalls,"Over the past decades, numerous practical applications of machine learning techniques have shown the potential of data-driven approaches in a large number of computing fields. Machine learning is increasingly included in computing curricula in higher education, and a quickly growing number of initiatives are expanding it in K-12 computing education, too. As machine learning enters K-12 computing education, understanding how intuition and agency in the context of such systems is developed becomes a key research area. But as schools and teachers are already struggling with integrating traditional computational thinking and traditional artificial intelligence into school curricula, understanding the challenges behind teaching machine learning in K-12 is an even more daunting challenge for computing education research. Despite the central position of machine learning in the field of modern computing, the computing education research body of literature contains remarkably few studies of how people learn to train, test, improve, and deploy machine learning systems. This is especially true of the K-12 curriculum space. This article charts the emerging trajectories in educational practice, theory, and technology related to teaching machine learning in K-12 education. The article situates the existing work in the context of computing education in general, and describes some differences that K-12 computing educators should take into account when facing this challenge. The article focuses on key aspects of the paradigm shift that will be required in order to successfully integrate machine learning into the broader K-12 computing curricula. A crucial step is abandoning the belief that rule-based traditional programming is a central aspect and building block in developing next generation computational thinking.","Teaching Machine Learning in K-12 Computing Education: Potential and Pitfalls Over the past decades, numerous practical applications of machine learning techniques have shown the potential of data-driven approaches in a large number of computing fields. Machine learning is increasingly included in computing curricula in higher education, and a quickly growing number of initiatives are expanding it in K-12 computing education, too. As machine learning enters K-12 computing education, understanding how intuition and agency in the context of such systems is developed becomes a key research area. But as schools and teachers are already struggling with integrating traditional computational thinking and traditional artificial intelligence into school curricula, understanding the challenges behind teaching machine learning in K-12 is an even more daunting challenge for computing education research. Despite the central position of machine learning in the field of modern computing, the computing education research body of literature contains remarkably few studies of how people learn to train, test, improve, and deploy machine learning systems. This is especially true of the K-12 curriculum space. This article charts the emerging trajectories in educational practice, theory, and technology related to teaching machine learning in K-12 education. The article situates the existing work in the context of computing education in general, and describes some differences that K-12 computing educators should take into account when facing this challenge. The article focuses on key aspects of the paradigm shift that will be required in order to successfully integrate machine learning into the broader K-12 computing curricula. A crucial step is abandoning the belief that rule-based traditional programming is a central aspect and building block in developing next generation computational thinking.",Education
Quantum Computing and Phase Transitions in Combinatorial Search,"We introduce an algorithm for combinatorial search on quantum computers that is capable of significantly concentrating amplitude into solutions for some NP search problems, on average. This is done by exploiting the same aspects of problem structure as used by classical backtrack methods to avoid unproductive search choices. This quantum algorithm is much more likely to find solutions than the simple direct use of quantum parallelism. Furthermore, empirical evaluation on small problems shows this quantum algorithm displays the same phase transition behavior, and at the same location, as seen in many previously studied classical search methods. Specifically, difficult problem instances are concentrated near the abrupt change from underconstrained to overconstrained problems.","Quantum Computing and Phase Transitions in Combinatorial Search We introduce an algorithm for combinatorial search on quantum computers that is capable of significantly concentrating amplitude into solutions for some NP search problems, on average. This is done by exploiting the same aspects of problem structure as used by classical backtrack methods to avoid unproductive search choices. This quantum algorithm is much more likely to find solutions than the simple direct use of quantum parallelism. Furthermore, empirical evaluation on small problems shows this quantum algorithm displays the same phase transition behavior, and at the same location, as seen in many previously studied classical search methods. Specifically, difficult problem instances are concentrated near the abrupt change from underconstrained to overconstrained problems.",Technology
Fluctuations of company yearly profits versus scaled revenue: Fat tail distribution of Levy type,"We analyze annual revenues and earnings data for the 500 largest-revenue U.S. companies during the period 1954-2007. We find that mean year profits are proportional to mean year revenues, exception made for few anomalous years, from which we postulate a linear relation between company expected mean profit and revenue. Mean annual revenues are used to scale both company profits and revenues. Annual profit fluctuations are obtained as difference between actual annual profit and its expected mean value, scaled by a power of the revenue to get a stationary behavior as a function of revenue. We find that profit fluctuations are broadly distributed having approximate power-law tails with a Levy-type exponent alpha simeq 1.7, from which we derive the associated break-even probability distribution. The predictions are compared with empirical data.","Fluctuations of company yearly profits versus scaled revenue: Fat tail distribution of Levy type We analyze annual revenues and earnings data for the 500 largest-revenue U.S. companies during the period 1954-2007. We find that mean year profits are proportional to mean year revenues, exception made for few anomalous years, from which we postulate a linear relation between company expected mean profit and revenue. Mean annual revenues are used to scale both company profits and revenues. Annual profit fluctuations are obtained as difference between actual annual profit and its expected mean value, scaled by a power of the revenue to get a stationary behavior as a function of revenue. We find that profit fluctuations are broadly distributed having approximate power-law tails with a Levy-type exponent alpha simeq 1.7, from which we derive the associated break-even probability distribution. The predictions are compared with empirical data.",Finance
"Shocks in financial markets, price expectation, and damped harmonic oscillators","Using a modified damped harmonic oscillator model equivalent to a model of market dynamics with price expectations, we analyze the reaction of financial markets to shocks. In order to do this, we gather data from indices of a variety of financial markets for the 1987 Black Monday, the Russian crisis of 1998, the crash after September 11th (2001), and the recent downturn of markets due to the subprime mortgage crisis in the USA (2008). Analyzing those data we were able to establish the amount by which each market felt the shocks, a dampening factor which expresses the capacity of a market of absorving a shock, and also a frequency related with volatility after the shock. The results gauge the efficiency of different markets in recovering from such shocks, and measure some level of dependence between them. We also show, using the correlation matrices between the indices used, that financial markets are now much more connected than they were two decades ago.","Shocks in financial markets, price expectation, and damped harmonic oscillators Using a modified damped harmonic oscillator model equivalent to a model of market dynamics with price expectations, we analyze the reaction of financial markets to shocks. In order to do this, we gather data from indices of a variety of financial markets for the 1987 Black Monday, the Russian crisis of 1998, the crash after September 11th (2001), and the recent downturn of markets due to the subprime mortgage crisis in the USA (2008). Analyzing those data we were able to establish the amount by which each market felt the shocks, a dampening factor which expresses the capacity of a market of absorving a shock, and also a frequency related with volatility after the shock. The results gauge the efficiency of different markets in recovering from such shocks, and measure some level of dependence between them. We also show, using the correlation matrices between the indices used, that financial markets are now much more connected than they were two decades ago.",Finance
The shape of health: A comparison of five alternative ways of visualizing personal health and wellbeing,"The combination of clinical and personal health and wellbeing data can tell us much about our behaviors, risks and overall status. The way this data is visualized may affect our understanding of our own health. To study this effect, we conducted a small experiment with 30 participants in which we presented a holistic overview of the health and wellbeing of two modeled individuals, one of them with metabolic syndrome. We used an insight-based methodology to assess the effectiveness of the visualizations. The results show that adequate visualization of holistic health data helps users without medical background to better understand the overall health situation and possible health risks related to lifestyles. Furthermore, we found that the application of insight-based methodology in the health and wellbeing domain remains unexplored and additional research and methodology development are needed.","The shape of health: A comparison of five alternative ways of visualizing personal health and wellbeing The combination of clinical and personal health and wellbeing data can tell us much about our behaviors, risks and overall status. The way this data is visualized may affect our understanding of our own health. To study this effect, we conducted a small experiment with 30 participants in which we presented a holistic overview of the health and wellbeing of two modeled individuals, one of them with metabolic syndrome. We used an insight-based methodology to assess the effectiveness of the visualizations. The results show that adequate visualization of holistic health data helps users without medical background to better understand the overall health situation and possible health risks related to lifestyles. Furthermore, we found that the application of insight-based methodology in the health and wellbeing domain remains unexplored and additional research and methodology development are needed.",Healthcare
Creating e-learning means of free software,"Informatization of education enriches traditional teaching methods with new forms and methods, which are based on the broad and harmonious use of ICT. Electronic pedagogy acquires the status of one of the most popular trends in the development of pedagogy of our time, in which such a tool as e-learning courses is the result of the accumulation of modern information and communication technologies with the theory and practice of e-learning. The article examines the standards of information exchange between learning systems and free software used to create e-learning courses. It is noted that electronic training courses are part of distance learning systems, as well as an independent tool. The considered systems of creation of electronic training courses allow creating electronic educational resources independently or partially, relying on the modern vision of the object model of information representation.","Creating e-learning means of free software Informatization of education enriches traditional teaching methods with new forms and methods, which are based on the broad and harmonious use of ICT. Electronic pedagogy acquires the status of one of the most popular trends in the development of pedagogy of our time, in which such a tool as e-learning courses is the result of the accumulation of modern information and communication technologies with the theory and practice of e-learning. The article examines the standards of information exchange between learning systems and free software used to create e-learning courses. It is noted that electronic training courses are part of distance learning systems, as well as an independent tool. The considered systems of creation of electronic training courses allow creating electronic educational resources independently or partially, relying on the modern vision of the object model of information representation.",Education
Polarized Helium to Image the Lung,"The main findings of the european PHIL project (Polarised Helium to Image the Lung) are reported. State of the art optical pumping techniques for polarising 3He gas are described. MRI methodological improvements allow dynamical ventilation images with a good resolution, ultimately limited by gas diffusion. Diffusion imaging appears as a robust method of lung diagnosis. A discussion of the potential advantage of low field MRI is presented. Selected PHIL results for emphysema are given, with the perspectives that this joint work opens up for the future of respiratory medicine.","Polarized Helium to Image the Lung The main findings of the european PHIL project (Polarised Helium to Image the Lung) are reported. State of the art optical pumping techniques for polarising 3He gas are described. MRI methodological improvements allow dynamical ventilation images with a good resolution, ultimately limited by gas diffusion. Diffusion imaging appears as a robust method of lung diagnosis. A discussion of the potential advantage of low field MRI is presented. Selected PHIL results for emphysema are given, with the perspectives that this joint work opens up for the future of respiratory medicine.",Healthcare
Exploring Curriculum Learning for Vision-Language Tasks: A Study on Small-Scale Multimodal Training,"For specialized domains, there is often not a wealth of data with which to train large machine learning models. In such limited data  compute settings, various methods exist aiming to textitdo more with less, such as finetuning from a pretrained model, modulating difficulty levels as data are presented to a model (curriculum learning), and considering the role of model type  size. Approaches to efficient textitmachine learning also take inspiration from textithuman learning by considering use cases where machine learning systems have access to approximately the same number of words experienced by a 13 year old child (100M words). We investigate the role of 3 primary variables in a limited data regime as part of the multimodal track of the BabyLM challenge. We contrast: (i) curriculum learning, (ii), pretraining (with text-only data), (iii) model type. We modulate these variables and assess them on two types of tasks: (a) multimodal (textimage), and (b) unimodal (text-only) tasks. We find that curriculum learning benefits multimodal evaluations over non-curriclum learning models, particularly when combining text-only pretraining. On text-only tasks, curriculum learning appears to help models with smaller trainable parameter counts. We suggest possible reasons based on architectural differences and training designs as to why one might observe such results.","Exploring Curriculum Learning for Vision-Language Tasks: A Study on Small-Scale Multimodal Training For specialized domains, there is often not a wealth of data with which to train large machine learning models. In such limited data  compute settings, various methods exist aiming to textitdo more with less, such as finetuning from a pretrained model, modulating difficulty levels as data are presented to a model (curriculum learning), and considering the role of model type  size. Approaches to efficient textitmachine learning also take inspiration from textithuman learning by considering use cases where machine learning systems have access to approximately the same number of words experienced by a 13 year old child (100M words). We investigate the role of 3 primary variables in a limited data regime as part of the multimodal track of the BabyLM challenge. We contrast: (i) curriculum learning, (ii), pretraining (with text-only data), (iii) model type. We modulate these variables and assess them on two types of tasks: (a) multimodal (textimage), and (b) unimodal (text-only) tasks. We find that curriculum learning benefits multimodal evaluations over non-curriclum learning models, particularly when combining text-only pretraining. On text-only tasks, curriculum learning appears to help models with smaller trainable parameter counts. We suggest possible reasons based on architectural differences and training designs as to why one might observe such results.",Education
Quantum Science and Technologies in K-12: Supporting Teachers to Integrate Quantum in STEM Classrooms,"Quantum science and computing represent a vital intersection between science and technology, gaining increasing importance in modern society. There is a pressing need to incorporate these concepts into the K-12 curriculum, equipping new generations with the tools to navigate and thrive in an evolving technological landscape. This study explores the professional learning of K-12 teachers (n  49) related to quantum concepts and pedagogy. We used open-ended surveys, field notes, workshop artifacts, and interviews to examine teachers perceptions of quantum and how they made connections between quantum and their curriculum. Our data reveal that most teachers were excited and interested in teaching quantum but were aware of potential barriers and concerns that might get in the way of teaching quantum. We found that teachers readily identified connections to math and science in their curriculum, but only a few made connections to computing. Enthusiasm for teaching quantum concepts was found in both elementary and secondary educators, suggesting a widespread recognition of its importance in preparing students for a future where quantum technology is a fundamental aspect of their lives and careers.","Quantum Science and Technologies in K-12: Supporting Teachers to Integrate Quantum in STEM Classrooms Quantum science and computing represent a vital intersection between science and technology, gaining increasing importance in modern society. There is a pressing need to incorporate these concepts into the K-12 curriculum, equipping new generations with the tools to navigate and thrive in an evolving technological landscape. This study explores the professional learning of K-12 teachers (n  49) related to quantum concepts and pedagogy. We used open-ended surveys, field notes, workshop artifacts, and interviews to examine teachers perceptions of quantum and how they made connections between quantum and their curriculum. Our data reveal that most teachers were excited and interested in teaching quantum but were aware of potential barriers and concerns that might get in the way of teaching quantum. We found that teachers readily identified connections to math and science in their curriculum, but only a few made connections to computing. Enthusiasm for teaching quantum concepts was found in both elementary and secondary educators, suggesting a widespread recognition of its importance in preparing students for a future where quantum technology is a fundamental aspect of their lives and careers.",Education
Orbital and Maxillofacial Computer Aided Surgery: Patient-Specific Finite Element Models To Predict Surgical Outcomes,"This paper addresses an important issue raised for the clinical relevance of Computer-Assisted Surgical applications, namely the methodology used to automatically build patient-specific Finite Element (FE) models of anatomical structures. From this perspective, a method is proposed, based on a technique called the Mesh-Matching method, followed by a process that corrects mesh irregularities. The Mesh-Matching algorithm generates patient-specific volume meshes from an existing generic model. The mesh regularization process is based on the Jacobian matrix transform related to the FE reference element and the current element. This method for generating patient-specific FE models is first applied to Computer-Assisted maxillofacial surgery, and more precisely to the FE elastic modelling of patient facial soft tissues. For each patient, the planned bone osteotomies (mandible, maxilla, chin) are used as boundary conditions to deform the FE face model, in order to predict the aesthetic outcome of the surgery. Seven FE patient-specific models were successfully generated by our method. For one patient, the prediction of the FE model is qualitatively compared with the patients post-operative appearance, measured from a Computer Tomography scan. Then, our methodology is applied to Computer-Assisted orbital surgery. It is, therefore, evaluated for the generation of eleven patient-specific FE poroelastic models of the orbital soft tissues. These models are used to predict the consequences of the surgical decompression of the orbit. More precisely, an average law is extrapolated from the simulations carried out for each patient model. This law links the size of the osteotomy (i.e. the surgical gesture) and the backward displacement of the eyeball (the consequence of the surgical gesture).","Orbital and Maxillofacial Computer Aided Surgery: Patient-Specific Finite Element Models To Predict Surgical Outcomes This paper addresses an important issue raised for the clinical relevance of Computer-Assisted Surgical applications, namely the methodology used to automatically build patient-specific Finite Element (FE) models of anatomical structures. From this perspective, a method is proposed, based on a technique called the Mesh-Matching method, followed by a process that corrects mesh irregularities. The Mesh-Matching algorithm generates patient-specific volume meshes from an existing generic model. The mesh regularization process is based on the Jacobian matrix transform related to the FE reference element and the current element. This method for generating patient-specific FE models is first applied to Computer-Assisted maxillofacial surgery, and more precisely to the FE elastic modelling of patient facial soft tissues. For each patient, the planned bone osteotomies (mandible, maxilla, chin) are used as boundary conditions to deform the FE face model, in order to predict the aesthetic outcome of the surgery. Seven FE patient-specific models were successfully generated by our method. For one patient, the prediction of the FE model is qualitatively compared with the patients post-operative appearance, measured from a Computer Tomography scan. Then, our methodology is applied to Computer-Assisted orbital surgery. It is, therefore, evaluated for the generation of eleven patient-specific FE poroelastic models of the orbital soft tissues. These models are used to predict the consequences of the surgical decompression of the orbit. More precisely, an average law is extrapolated from the simulations carried out for each patient model. This law links the size of the osteotomy (i.e. the surgical gesture) and the backward displacement of the eyeball (the consequence of the surgical gesture).",Healthcare
Improving operational flexibility of integrated energy system with uncertain renewable generations considering thermal inertia of buildings,"Insufficient flexibility in system operation caused by traditional heat-set operating modes of combined heat and power (CHP) units in winter heating periods is a key issue that limits renewable energy consumption. In order to reduce the curtailment of renewable energy resources through improving the operational flexibility, a novel optimal scheduling model based on chance-constrained programming (CCP), aiming at minimizing the lowest generation cost, is proposed for a small-scale integrated energy system (IES) with CHP units, thermal power units, renewable generations and representative auxiliary equipments. In this model, due to the uncertainties of renewable generations including wind turbines and photovoltaic units, the probabilistic spinning reserves are supplied in the form of chance-constrained; from the perspective of user experience, a heating load model is built with consideration of heat comfort and inertia in buildings. To solve the model, a solution approach based on sequence operation theory (SOT) is developed, where the original CCP-based scheduling model is tackled into a solvable mixed-integer linear programming (MILP) formulation by converting a chance constraint into its deterministic equivalence class, and thereby is solved via the CPLEX solver. The simulation results on the modified IEEE 30-bus system demonstrate that the presented method manages to improve operational flexibility of the IES with uncertain renewable generations by comprehensively leveraging thermal inertia of buildings and different kinds of auxiliary equipments, which provides a fundamental way for promoting renewable energy consumption.","Improving operational flexibility of integrated energy system with uncertain renewable generations considering thermal inertia of buildings Insufficient flexibility in system operation caused by traditional heat-set operating modes of combined heat and power (CHP) units in winter heating periods is a key issue that limits renewable energy consumption. In order to reduce the curtailment of renewable energy resources through improving the operational flexibility, a novel optimal scheduling model based on chance-constrained programming (CCP), aiming at minimizing the lowest generation cost, is proposed for a small-scale integrated energy system (IES) with CHP units, thermal power units, renewable generations and representative auxiliary equipments. In this model, due to the uncertainties of renewable generations including wind turbines and photovoltaic units, the probabilistic spinning reserves are supplied in the form of chance-constrained; from the perspective of user experience, a heating load model is built with consideration of heat comfort and inertia in buildings. To solve the model, a solution approach based on sequence operation theory (SOT) is developed, where the original CCP-based scheduling model is tackled into a solvable mixed-integer linear programming (MILP) formulation by converting a chance constraint into its deterministic equivalence class, and thereby is solved via the CPLEX solver. The simulation results on the modified IEEE 30-bus system demonstrate that the presented method manages to improve operational flexibility of the IES with uncertain renewable generations by comprehensively leveraging thermal inertia of buildings and different kinds of auxiliary equipments, which provides a fundamental way for promoting renewable energy consumption.",Environment
Learning Policies with External Memory,"In order for an agent to perform well in partially observable domains, it is usually necessary for actions to depend on the history of observations. In this paper, we explore a it stigmergic approach, in which the agents actions include the ability to set and clear bits in an external memory, and the external memory is included as part of the input to the agent. In this case, we need to learn a reactive policy in a highly non-Markovian domain. We explore two algorithms: SARSA(lambda), which has had empirical success in partially observable domains, and VAPS, a new algorithm due to Baird and Moore, with convergence guarantees in partially observable domains. We compare the performance of these two algorithms on benchmark problems.","Learning Policies with External Memory In order for an agent to perform well in partially observable domains, it is usually necessary for actions to depend on the history of observations. In this paper, we explore a it stigmergic approach, in which the agents actions include the ability to set and clear bits in an external memory, and the external memory is included as part of the input to the agent. In this case, we need to learn a reactive policy in a highly non-Markovian domain. We explore two algorithms: SARSA(lambda), which has had empirical success in partially observable domains, and VAPS, a new algorithm due to Baird and Moore, with convergence guarantees in partially observable domains. We compare the performance of these two algorithms on benchmark problems.",Technology
Transforming the Preparation of Physics GTAs: Curriculum Development,"Graduate Teaching Assistants (GTAs) are key partners in the education of undergraduates. Given the potentially large impact GTAs can have on undergraduate student learning, it is important to provide them with appropriate preparation for teaching. But GTAs are students themselves, and not all of them desire to pursue an academic career. Fully integrating GTA preparation into the professional development of graduate students lowers the barrier to engagement so that all graduate students may benefit from the opportunity to explore teaching and its applications to many potential career paths. In this paper we describe the design and implementation of a GTA Preparation course for first-year Ph.D. students at the Georgia Tech School of Physics. Through a yearly cycle of implementation and revision, guided by the 3P Framework we developed (Pedagogy, Physics, Professional Development), the course has evolved into a robust and comprehensive professional development program that is well-received by physics graduate students.","Transforming the Preparation of Physics GTAs: Curriculum Development Graduate Teaching Assistants (GTAs) are key partners in the education of undergraduates. Given the potentially large impact GTAs can have on undergraduate student learning, it is important to provide them with appropriate preparation for teaching. But GTAs are students themselves, and not all of them desire to pursue an academic career. Fully integrating GTA preparation into the professional development of graduate students lowers the barrier to engagement so that all graduate students may benefit from the opportunity to explore teaching and its applications to many potential career paths. In this paper we describe the design and implementation of a GTA Preparation course for first-year Ph.D. students at the Georgia Tech School of Physics. Through a yearly cycle of implementation and revision, guided by the 3P Framework we developed (Pedagogy, Physics, Professional Development), the course has evolved into a robust and comprehensive professional development program that is well-received by physics graduate students.",Education
Cost-Sensitive Classification: Empirical Evaluation of a Hybrid Genetic Decision Tree Induction Algorithm,"This paper introduces ICET, a new algorithm for cost-sensitive classification. ICET uses a genetic algorithm to evolve a population of biases for a decision tree induction algorithm. The fitness function of the genetic algorithm is the average cost of classification when using the decision tree, including both the costs of tests (features, measurements) and the costs of classification errors. ICET is compared here with three other algorithms for cost-sensitive classification - EG2, CS-ID3, and IDX - and also with C4.5, which classifies without regard to cost. The five algorithms are evaluated empirically on five real-world medical datasets. Three sets of experiments are performed. The first set examines the baseline performance of the five algorithms on the five datasets and establishes that ICET performs significantly better than its competitors. The second set tests the robustness of ICET under a variety of conditions and shows that ICET maintains its advantage. The third set looks at ICETs search in bias space and discovers a way to improve the search.","Cost-Sensitive Classification: Empirical Evaluation of a Hybrid Genetic Decision Tree Induction Algorithm This paper introduces ICET, a new algorithm for cost-sensitive classification. ICET uses a genetic algorithm to evolve a population of biases for a decision tree induction algorithm. The fitness function of the genetic algorithm is the average cost of classification when using the decision tree, including both the costs of tests (features, measurements) and the costs of classification errors. ICET is compared here with three other algorithms for cost-sensitive classification - EG2, CS-ID3, and IDX - and also with C4.5, which classifies without regard to cost. The five algorithms are evaluated empirically on five real-world medical datasets. Three sets of experiments are performed. The first set examines the baseline performance of the five algorithms on the five datasets and establishes that ICET performs significantly better than its competitors. The second set tests the robustness of ICET under a variety of conditions and shows that ICET maintains its advantage. The third set looks at ICETs search in bias space and discovers a way to improve the search.",Technology
Competing with stationary prediction strategies,"In this paper we introduce the class of stationary prediction strategies and construct a prediction algorithm that asymptotically performs as well as the best continuous stationary strategy. We make mild compactness assumptions but no stochastic assumptions about the environment. In particular, no assumption of stationarity is made about the environment, and the stationarity of the considered strategies only means that they do not depend explicitly on time; we argue that it is natural to consider only stationary strategies even for highly non-stationary environments.","Competing with stationary prediction strategies In this paper we introduce the class of stationary prediction strategies and construct a prediction algorithm that asymptotically performs as well as the best continuous stationary strategy. We make mild compactness assumptions but no stochastic assumptions about the environment. In particular, no assumption of stationarity is made about the environment, and the stationarity of the considered strategies only means that they do not depend explicitly on time; we argue that it is natural to consider only stationary strategies even for highly non-stationary environments.",Technology
Going green across boundaries: Spatial effects of environmental policies on tourism flows,"This study investigates the relationship between environmental sustainability policies and tourism flows across Italian provinces using a Spatial Durbin Error Model (SDEM) within a gravity framework. By incorporating both public and corporate environmental initiatives, the analysis highlights the direct and spatial spillover effects of sustainability measures on tourism demand. The findings indicate that corporate-led initiatives, such as ecocertifications and green investments, exert a stronger direct influence on tourism flows compared to public measures, underscoring the visibility and immediate impact of private sector actions. However, both types of initiatives generate significant positive spatial spillovers, suggesting that sustainability efforts extend beyond local boundaries. These results demonstrate the interconnected nature of regional tourism systems and emphasize the critical role of coordinated sustainability policies in fostering tourism growth while promoting environmental protection. By addressing the spatial interdependencies of tourism flows and sustainability practices, this research provides valuable insights for policymakers and stakeholders seeking to improve sustainable tourism development at regional and national levels.","Going green across boundaries: Spatial effects of environmental policies on tourism flows This study investigates the relationship between environmental sustainability policies and tourism flows across Italian provinces using a Spatial Durbin Error Model (SDEM) within a gravity framework. By incorporating both public and corporate environmental initiatives, the analysis highlights the direct and spatial spillover effects of sustainability measures on tourism demand. The findings indicate that corporate-led initiatives, such as ecocertifications and green investments, exert a stronger direct influence on tourism flows compared to public measures, underscoring the visibility and immediate impact of private sector actions. However, both types of initiatives generate significant positive spatial spillovers, suggesting that sustainability efforts extend beyond local boundaries. These results demonstrate the interconnected nature of regional tourism systems and emphasize the critical role of coordinated sustainability policies in fostering tourism growth while promoting environmental protection. By addressing the spatial interdependencies of tourism flows and sustainability practices, this research provides valuable insights for policymakers and stakeholders seeking to improve sustainable tourism development at regional and national levels.",Environment
Reinforcement Learning: A Survey,"This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word reinforcement. The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.","Reinforcement Learning: A Survey This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word reinforcement. The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.",Technology
Framework for Adoption of Generative Artificial Intelligence (GenAI) in Education,"Contributions: An adoption framework to include GenAI in the university curriculum. It identifies and highlights the role of different stakeholders (university management, students, staff, etc.) during the adoption process. It also proposes an objective approach based upon an evaluation matrix to assess the success and outcome of the GenAI adoption. Background: Universities worldwide are debating and struggling with the adoption of GenAI in their curriculum. Both the faculty and students are unsure about the approach in the absence of clear guidelines through the administration and regulators. This requires an established framework to define a process and articulate the roles and responsibilities of each stakeholder involved. Research Questions: Whether the academic ecosystem requires a methodology to adopt GenAI into its curriculum? A systematic approach for the academic staff to ensure the students learning outcomes are met with the adoption of GenAI. How to measure and communicate the adoption of GenAI in the university setup? Methodology: The methodology employed in this study focuses on examining the university education system and assessing the opportunities and challenges related to incorporating GenAI in teaching and learning. Additionally, it identifies a gap and the absence of a comprehensive framework that obstructs the effective integration of GenAI within the academic environment. Findings: The literature survey results indicate the limited or no adoption of GenAI by the university, which further reflects the dilemma in the minds of different stakeholders. For the successful adoption of GenAI, a standard framework is proposed i) for effective redesign of the course curriculum, ii) for enabling staff and students, iii) to define an evaluation matrix to measure the effectiveness and success of the adoption process.","Framework for Adoption of Generative Artificial Intelligence (GenAI) in Education Contributions: An adoption framework to include GenAI in the university curriculum. It identifies and highlights the role of different stakeholders (university management, students, staff, etc.) during the adoption process. It also proposes an objective approach based upon an evaluation matrix to assess the success and outcome of the GenAI adoption. Background: Universities worldwide are debating and struggling with the adoption of GenAI in their curriculum. Both the faculty and students are unsure about the approach in the absence of clear guidelines through the administration and regulators. This requires an established framework to define a process and articulate the roles and responsibilities of each stakeholder involved. Research Questions: Whether the academic ecosystem requires a methodology to adopt GenAI into its curriculum? A systematic approach for the academic staff to ensure the students learning outcomes are met with the adoption of GenAI. How to measure and communicate the adoption of GenAI in the university setup? Methodology: The methodology employed in this study focuses on examining the university education system and assessing the opportunities and challenges related to incorporating GenAI in teaching and learning. Additionally, it identifies a gap and the absence of a comprehensive framework that obstructs the effective integration of GenAI within the academic environment. Findings: The literature survey results indicate the limited or no adoption of GenAI by the university, which further reflects the dilemma in the minds of different stakeholders. For the successful adoption of GenAI, a standard framework is proposed i) for effective redesign of the course curriculum, ii) for enabling staff and students, iii) to define an evaluation matrix to measure the effectiveness and success of the adoption process.",Education
DynCIM: Dynamic Curriculum for Imbalanced Multimodal Learning,"Multimodal learning integrates complementary information from diverse modalities to enhance the decision-making process. However, the potential of multimodal collaboration remains under-exploited due to disparities in data quality and modality representation capabilities. To address this, we introduce DynCIM, a novel dynamic curriculum learning framework designed to quantify the inherent imbalances from both sample and modality perspectives. DynCIM employs a sample-level curriculum to dynamically assess each samples difficulty according to prediction deviation, consistency, and stability, while a modality-level curriculum measures modality contributions from global and local. Furthermore, a gating-based dynamic fusion mechanism is introduced to adaptively adjust modality contributions, minimizing redundancy and optimizing fusion effectiveness. Extensive experiments on six multimodal benchmarking datasets, spanning both bimodal and trimodal scenarios, demonstrate that DynCIM consistently outperforms state-of-the-art methods. Our approach effectively mitigates modality and sample imbalances while enhancing adaptability and robustness in multimodal learning tasks. Our code is available at https:github.comRaymond-QiancxDynCIM.","DynCIM: Dynamic Curriculum for Imbalanced Multimodal Learning Multimodal learning integrates complementary information from diverse modalities to enhance the decision-making process. However, the potential of multimodal collaboration remains under-exploited due to disparities in data quality and modality representation capabilities. To address this, we introduce DynCIM, a novel dynamic curriculum learning framework designed to quantify the inherent imbalances from both sample and modality perspectives. DynCIM employs a sample-level curriculum to dynamically assess each samples difficulty according to prediction deviation, consistency, and stability, while a modality-level curriculum measures modality contributions from global and local. Furthermore, a gating-based dynamic fusion mechanism is introduced to adaptively adjust modality contributions, minimizing redundancy and optimizing fusion effectiveness. Extensive experiments on six multimodal benchmarking datasets, spanning both bimodal and trimodal scenarios, demonstrate that DynCIM consistently outperforms state-of-the-art methods. Our approach effectively mitigates modality and sample imbalances while enhancing adaptability and robustness in multimodal learning tasks. Our code is available at https:github.comRaymond-QiancxDynCIM.",Education
MRI of the lung using hyperpolarized He-3 at very low magnetic field (3 mT),"Optical pumping of He-3 produces large (hyper) nuclear-spin polarizations independent of the magnetic resonance imaging (MRI) field strength. This allows lung MRI to be performed at reduced fields with many associated benefits, such as lower tissue susceptibility gradients and decreased power absorption rates. Here we present results of 2D imaging as well as accurate 1D gas diffusion mapping of the human lung using He-3 at very low field (3 mT). Furthermore, measurements of transverse relaxation in zero applied gradient are shown to accurately track pulmonary oxygen partial pressure, opening the way for novel imaging sequences.","MRI of the lung using hyperpolarized He-3 at very low magnetic field (3 mT) Optical pumping of He-3 produces large (hyper) nuclear-spin polarizations independent of the magnetic resonance imaging (MRI) field strength. This allows lung MRI to be performed at reduced fields with many associated benefits, such as lower tissue susceptibility gradients and decreased power absorption rates. Here we present results of 2D imaging as well as accurate 1D gas diffusion mapping of the human lung using He-3 at very low field (3 mT). Furthermore, measurements of transverse relaxation in zero applied gradient are shown to accurately track pulmonary oxygen partial pressure, opening the way for novel imaging sequences.",Healthcare
Multi-Objective Optimization for Value-Sensitive and Sustainable Basket Recommendations,"Sustainable consumption aims to minimize the environmental and societal impact of the use of services and products. Over-consumption of services and products leads to potential natural resource exhaustion and societal inequalities as access to goods and services becomes more challenging. In everyday life, a person can simply achieve more sustainable purchases by drastically changing their lifestyle choices and potentially going against their personal values or wishes. Conversely, achieving sustainable consumption while accounting for personal values is a more complex task as potential trade-offs arise when trying to satisfy environmental and personal goals. This article focuses on value-sensitive design of recommender systems, which enable consumers to improve the sustainability of their purchases while respecting personal and societal values. Value-sensitive recommendations for sustainable consumption are formalized as a multi-objective optimization problem, where each objective represents different sustainability goals and personal values. Novel and existing multi-objective algorithms calculate solutions to this problem. The solutions are proposed as personalized sustainable basket recommendations to consumers. These recommendations are evaluated on a synthetic dataset, which comprises three established real-world datasets from relevant scientific and organizational reports. The synthetic dataset contains quantitative data on product prices, nutritional values, and environmental impact metrics, such as greenhouse gas emissions and water footprint. The recommended baskets are highly similar to consumer purchased baskets and aligned with both sustainability goals and personal values relevant to health, expenditure, and taste. Even when consumers would accept only a fraction of recommendations, a considerable reduction of environmental impact is observed.","Multi-Objective Optimization for Value-Sensitive and Sustainable Basket Recommendations Sustainable consumption aims to minimize the environmental and societal impact of the use of services and products. Over-consumption of services and products leads to potential natural resource exhaustion and societal inequalities as access to goods and services becomes more challenging. In everyday life, a person can simply achieve more sustainable purchases by drastically changing their lifestyle choices and potentially going against their personal values or wishes. Conversely, achieving sustainable consumption while accounting for personal values is a more complex task as potential trade-offs arise when trying to satisfy environmental and personal goals. This article focuses on value-sensitive design of recommender systems, which enable consumers to improve the sustainability of their purchases while respecting personal and societal values. Value-sensitive recommendations for sustainable consumption are formalized as a multi-objective optimization problem, where each objective represents different sustainability goals and personal values. Novel and existing multi-objective algorithms calculate solutions to this problem. The solutions are proposed as personalized sustainable basket recommendations to consumers. These recommendations are evaluated on a synthetic dataset, which comprises three established real-world datasets from relevant scientific and organizational reports. The synthetic dataset contains quantitative data on product prices, nutritional values, and environmental impact metrics, such as greenhouse gas emissions and water footprint. The recommended baskets are highly similar to consumer purchased baskets and aligned with both sustainability goals and personal values relevant to health, expenditure, and taste. Even when consumers would accept only a fraction of recommendations, a considerable reduction of environmental impact is observed.",Environment
Understanding Opinions Towards Climate Change on Social Media,"Social media platforms such as Twitter (now known as X) have revolutionized how the public engage with important societal and political topics. Recently, climate change discussions on social media became a catalyst for political polarization and the spreading of misinformation. In this work, we aim to understand how real world events influence the opinions of individuals towards climate change related topics on social media. To this end, we extracted and analyzed a dataset of 13.6 millions tweets sent by 3.6 million users from 2006 to 2019. Then, we construct a temporal graph from the user-user mentions network and utilize the Louvain community detection algorithm to analyze the changes in community structure around Conference of the Parties on Climate Change(COP) events. Next, we also apply tools from the Natural Language Processing literature to perform sentiment analysis and topic modeling on the tweets. Our work acts as a first step towards understanding the evolution of pro-climate change communities around COP events. Answering these questions helps us understand how to raise peoples awareness towards climate change thus hopefully calling on more individuals to join the collaborative effort in slowing down climate change.","Understanding Opinions Towards Climate Change on Social Media Social media platforms such as Twitter (now known as X) have revolutionized how the public engage with important societal and political topics. Recently, climate change discussions on social media became a catalyst for political polarization and the spreading of misinformation. In this work, we aim to understand how real world events influence the opinions of individuals towards climate change related topics on social media. To this end, we extracted and analyzed a dataset of 13.6 millions tweets sent by 3.6 million users from 2006 to 2019. Then, we construct a temporal graph from the user-user mentions network and utilize the Louvain community detection algorithm to analyze the changes in community structure around Conference of the Parties on Climate Change(COP) events. Next, we also apply tools from the Natural Language Processing literature to perform sentiment analysis and topic modeling on the tweets. Our work acts as a first step towards understanding the evolution of pro-climate change communities around COP events. Answering these questions helps us understand how to raise peoples awareness towards climate change thus hopefully calling on more individuals to join the collaborative effort in slowing down climate change.",Environment
Handling boundary constraints for numerical optimization by particle swarm flying in periodic search space,"The periodic mode is analyzed together with two conventional boundary handling modes for particle swarm. By providing an infinite space that comprises periodic copies of original search space, it avoids possible disorganizing of particle swarm that is induced by the undesired mutations at the boundary. The results on benchmark functions show that particle swarm with periodic mode is capable of improving the search performance significantly, by compared with that of conventional modes and other algorithms.","Handling boundary constraints for numerical optimization by particle swarm flying in periodic search space The periodic mode is analyzed together with two conventional boundary handling modes for particle swarm. By providing an infinite space that comprises periodic copies of original search space, it avoids possible disorganizing of particle swarm that is induced by the undesired mutations at the boundary. The results on benchmark functions show that particle swarm with periodic mode is capable of improving the search performance significantly, by compared with that of conventional modes and other algorithms.",Technology
An algorithm for real-time estimation of Mezcal fermentation parameters based on redox potential measurements,"We present an algorithm for the continuous monitoring of the biomass and ethanol concentrations and moreover the kinetic rate in the Mezcal fermentation process. This algorithm performs its task having only available the on-line measurements of the redox potential. The procedure includes an artificial neural network (ANN) that relates the redox potential to the ethanol and biomass concentrations. Then a nonlinear-observer-based algorithm uses the biomass estimations to infer the kinetic rate of this fermentation process. The method shows that the redox potential is a valuable indicator of microorganism metabolic activity during the Mezcal fermentation. In addition, the estimated kinetic rate can be considered as a direct evidence of the presence of mixed culture growth in the process. In this work, the detailed design of the software-sensor is presented, as well as its experimental application at the laboratory level","An algorithm for real-time estimation of Mezcal fermentation parameters based on redox potential measurements We present an algorithm for the continuous monitoring of the biomass and ethanol concentrations and moreover the kinetic rate in the Mezcal fermentation process. This algorithm performs its task having only available the on-line measurements of the redox potential. The procedure includes an artificial neural network (ANN) that relates the redox potential to the ethanol and biomass concentrations. Then a nonlinear-observer-based algorithm uses the biomass estimations to infer the kinetic rate of this fermentation process. The method shows that the redox potential is a valuable indicator of microorganism metabolic activity during the Mezcal fermentation. In addition, the estimated kinetic rate can be considered as a direct evidence of the presence of mixed culture growth in the process. In this work, the detailed design of the software-sensor is presented, as well as its experimental application at the laboratory level",Healthcare
Improvements of the Discrete Dipole Approximation method,The discrete-dipole approximation (DDA) is a flexible technique for computing scattering and absorption by targets of arbitrary geometry. In this paper we perform systematic study of various non-stationary iterative (conjugate gradient) methods in search for the most efficient one in order to solve the system of equations arising in DDA. We document implementation of these methods in our public domain code DDSCAT.5a,Improvements of the Discrete Dipole Approximation method The discrete-dipole approximation (DDA) is a flexible technique for computing scattering and absorption by targets of arbitrary geometry. In this paper we perform systematic study of various non-stationary iterative (conjugate gradient) methods in search for the most efficient one in order to solve the system of equations arising in DDA. We document implementation of these methods in our public domain code DDSCAT.5a,Environment
Perseus: Leveraging Common Data Patterns with Curriculum Learning for More Robust Graph Neural Networks,"Graph Neural Networks (GNNs) excel at handling graph data but remain vulnerable to adversarial attacks. Existing defense methods typically rely on assumptions like graph sparsity and homophily to either preprocess the graph or guide structure learning. However, preprocessing methods often struggle to accurately distinguish between normal edges and adversarial perturbations, leading to suboptimal results due to the loss of valuable edge information. Robust graph neural network models train directly on graph data affected by adversarial perturbations, without preprocessing. This can cause the model to get stuck in poor local optima, negatively affecting its performance. To address these challenges, we propose Perseus, a novel adversarial defense method based on curriculum learning. Perseus assesses edge difficulty using global homophily and applies a curriculum learning strategy to adjust the learning order, guiding the model to learn the full graph structure while adaptively focusing on common data patterns. This approach mitigates the impact of adversarial perturbations. Experiments show that models trained with Perseus achieve superior performance and are significantly more robust to adversarial attacks.","Perseus: Leveraging Common Data Patterns with Curriculum Learning for More Robust Graph Neural Networks Graph Neural Networks (GNNs) excel at handling graph data but remain vulnerable to adversarial attacks. Existing defense methods typically rely on assumptions like graph sparsity and homophily to either preprocess the graph or guide structure learning. However, preprocessing methods often struggle to accurately distinguish between normal edges and adversarial perturbations, leading to suboptimal results due to the loss of valuable edge information. Robust graph neural network models train directly on graph data affected by adversarial perturbations, without preprocessing. This can cause the model to get stuck in poor local optima, negatively affecting its performance. To address these challenges, we propose Perseus, a novel adversarial defense method based on curriculum learning. Perseus assesses edge difficulty using global homophily and applies a curriculum learning strategy to adjust the learning order, guiding the model to learn the full graph structure while adaptively focusing on common data patterns. This approach mitigates the impact of adversarial perturbations. Experiments show that models trained with Perseus achieve superior performance and are significantly more robust to adversarial attacks.",Education
Modelling the Probability Density of Markov Sources,"This paper introduces an objective function that seeks to minimise the average total number of bits required to encode the joint state of all of the layers of a Markov source. This type of encoder may be applied to the problem of optimising the bottom-up (recognition model) and top-down (generative model) connections in a multilayer neural network, and it unifies several previous results on the optimisation of multilayer neural networks.","Modelling the Probability Density of Markov Sources This paper introduces an objective function that seeks to minimise the average total number of bits required to encode the joint state of all of the layers of a Markov source. This type of encoder may be applied to the problem of optimising the bottom-up (recognition model) and top-down (generative model) connections in a multilayer neural network, and it unifies several previous results on the optimisation of multilayer neural networks.",Technology
"Education 5.0: Requirements, Enabling Technologies, and Future Directions","We are currently in a post-pandemic era in which life has shifted to a digital world. This has affected many aspects of life, including education and learning. Education 5.0 refers to the fifth industrial revolution in education by leveraging digital technologies to eliminate barriers to learning, enhance learning methods, and promote overall well-being. The concept of Education 5.0 represents a new paradigm in the field of education, one that is focused on creating a learner-centric environment that leverages the latest technologies and teaching methods. This paper explores the key requirements of Education 5.0 and the enabling technologies that make it possible, including artificial intelligence, blockchain, and virtual and augmented reality. We analyze the potential impact of these technologies on the future of education, including their ability to improve personalization, increase engagement, and provide greater access to education. Additionally, we examine the challenges and ethical considerations associated with Education 5.0 and propose strategies for addressing these issues. Finally, we offer insights into future directions for the development of Education 5.0, including the need for ongoing research, collaboration, and innovation in the field. Overall, this paper provides a comprehensive overview of Education 5.0, its requirements, enabling technologies, and future directions, and highlights the potential of this new paradigm to transform education and improve learning outcomes for students.","Education 5.0: Requirements, Enabling Technologies, and Future Directions We are currently in a post-pandemic era in which life has shifted to a digital world. This has affected many aspects of life, including education and learning. Education 5.0 refers to the fifth industrial revolution in education by leveraging digital technologies to eliminate barriers to learning, enhance learning methods, and promote overall well-being. The concept of Education 5.0 represents a new paradigm in the field of education, one that is focused on creating a learner-centric environment that leverages the latest technologies and teaching methods. This paper explores the key requirements of Education 5.0 and the enabling technologies that make it possible, including artificial intelligence, blockchain, and virtual and augmented reality. We analyze the potential impact of these technologies on the future of education, including their ability to improve personalization, increase engagement, and provide greater access to education. Additionally, we examine the challenges and ethical considerations associated with Education 5.0 and propose strategies for addressing these issues. Finally, we offer insights into future directions for the development of Education 5.0, including the need for ongoing research, collaboration, and innovation in the field. Overall, this paper provides a comprehensive overview of Education 5.0, its requirements, enabling technologies, and future directions, and highlights the potential of this new paradigm to transform education and improve learning outcomes for students.",Education
Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition,"EEG-based emotion recognition plays an important role in developing adaptive brain-computer communication systems, yet faces two fundamental challenges in practical implementations: (1) effective integration of non-stationary spatial-temporal neural patterns, (2) robust adaptation to dynamic emotional intensity variations in real-world scenarios. This paper proposes SST-CL, a novel framework integrating spatial-temporal transformers with curriculum learning. Our method introduces two core components: a spatial encoder that models inter-channel relationships and a temporal encoder that captures multi-scale dependencies through windowed attention mechanisms, enabling simultaneous extraction of spatial correlations and temporal dynamics from EEG signals. Complementing this architecture, an intensity-aware curriculum learning strategy progressively guides training from high-intensity to low-intensity emotional states through dynamic sample scheduling based on a dual difficulty assessment. Comprehensive experiments on three benchmark datasets demonstrate state-of-the-art performance across various emotional intensity levels, with ablation studies confirming the necessity of both architectural components and the curriculum learning mechanism.","Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition EEG-based emotion recognition plays an important role in developing adaptive brain-computer communication systems, yet faces two fundamental challenges in practical implementations: (1) effective integration of non-stationary spatial-temporal neural patterns, (2) robust adaptation to dynamic emotional intensity variations in real-world scenarios. This paper proposes SST-CL, a novel framework integrating spatial-temporal transformers with curriculum learning. Our method introduces two core components: a spatial encoder that models inter-channel relationships and a temporal encoder that captures multi-scale dependencies through windowed attention mechanisms, enabling simultaneous extraction of spatial correlations and temporal dynamics from EEG signals. Complementing this architecture, an intensity-aware curriculum learning strategy progressively guides training from high-intensity to low-intensity emotional states through dynamic sample scheduling based on a dual difficulty assessment. Comprehensive experiments on three benchmark datasets demonstrate state-of-the-art performance across various emotional intensity levels, with ablation studies confirming the necessity of both architectural components and the curriculum learning mechanism.",Education
"Exports, Labor Markets, and the Environment: Evidence from Brazil","What is the environmental impact of exports? Focusing on 2000-20, this paper combines customs, administrative, and census microdata to estimate employment elasticities with respect to exports. The findings show that municipalities that faced increased exports experienced faster growth in formal employment. The elasticities were 0.25 on impact, peaked at 0.4, and remained positive and significant even 10 years after the shock, pointing to a long and protracted labor market adjustment. In the long run, informal employment responds negatively to export shocks. Using a granular taxonomy for economic activities based on their environmental impact, the paper documents that environmentally risky activities have a larger share of employment than environmentally sustainable ones, and that the relationship between these activities and exports is nuanced. Over the short run, environmentally risky employment responds more strongly to exports relative to environmentally sustainable employment. However, over the long run, this pattern reverses, as the impact of exports on environmentally sustainable employment is more persistent.","Exports, Labor Markets, and the Environment: Evidence from Brazil What is the environmental impact of exports? Focusing on 2000-20, this paper combines customs, administrative, and census microdata to estimate employment elasticities with respect to exports. The findings show that municipalities that faced increased exports experienced faster growth in formal employment. The elasticities were 0.25 on impact, peaked at 0.4, and remained positive and significant even 10 years after the shock, pointing to a long and protracted labor market adjustment. In the long run, informal employment responds negatively to export shocks. Using a granular taxonomy for economic activities based on their environmental impact, the paper documents that environmentally risky activities have a larger share of employment than environmentally sustainable ones, and that the relationship between these activities and exports is nuanced. Over the short run, environmentally risky employment responds more strongly to exports relative to environmentally sustainable employment. However, over the long run, this pattern reverses, as the impact of exports on environmentally sustainable employment is more persistent.",Environment
Improved Use of Continuous Attributes in C4.5,"A reported weakness of C4.5 in domains with continuous attributes is addressed by modifying the formation and evaluation of tests on continuous attributes. An MDL-inspired penalty is applied to such tests, eliminating some of them from consideration and altering the relative desirability of all tests. Empirical trials show that the modifications lead to smaller decision trees with higher predictive accuracies. Results also confirm that a new version of C4.5 incorporating these changes is superior to recent approaches that use global discretization and that construct small trees with multi-interval splits.","Improved Use of Continuous Attributes in C4.5 A reported weakness of C4.5 in domains with continuous attributes is addressed by modifying the formation and evaluation of tests on continuous attributes. An MDL-inspired penalty is applied to such tests, eliminating some of them from consideration and altering the relative desirability of all tests. Empirical trials show that the modifications lead to smaller decision trees with higher predictive accuracies. Results also confirm that a new version of C4.5 incorporating these changes is superior to recent approaches that use global discretization and that construct small trees with multi-interval splits.",Technology
New Approaches and Trends in the Philosophy of Educational Technology for Learning and Teaching Environments,"The purpose of this study is to discuss instructional design and technology (IDT) model strategies for developing learning and teaching environments, based on philosophical approaches to educational technology theory. The study begins with a discussion of IDT models to define the history of educational technology or instructional technology theories, based on instructional strategies and improvements. In the study, authors discuss the strategies and steps that a design team should follow when designing learning environments in industry, business and military scenarios, based on the philosophy of educational technology and latest technologies, which should give way to effective learning environments. The steps include recognising terminology in educational technology concepts, psychological and instructional foundations in instructional design (ID), as well as approaches to educational technology. To recap, our purpose is to combine necessary IDT model strategies for the pedagogical design of learning environments, with new technologies. We will also discuss powerful IDT models that aim to meet the very high expectations of digital and humanist education. To develop a high-quality learning environment, we will explain technology design steps and practice in order to improve the learning of tasks, complex cognitive skills, attitudes, motivations and competencies in the future trends of educational technology. At the end of the study, integrated technologies in e-learning were discussed and presented, based on foundations of IDT and the philosophy of educational technology.","New Approaches and Trends in the Philosophy of Educational Technology for Learning and Teaching Environments The purpose of this study is to discuss instructional design and technology (IDT) model strategies for developing learning and teaching environments, based on philosophical approaches to educational technology theory. The study begins with a discussion of IDT models to define the history of educational technology or instructional technology theories, based on instructional strategies and improvements. In the study, authors discuss the strategies and steps that a design team should follow when designing learning environments in industry, business and military scenarios, based on the philosophy of educational technology and latest technologies, which should give way to effective learning environments. The steps include recognising terminology in educational technology concepts, psychological and instructional foundations in instructional design (ID), as well as approaches to educational technology. To recap, our purpose is to combine necessary IDT model strategies for the pedagogical design of learning environments, with new technologies. We will also discuss powerful IDT models that aim to meet the very high expectations of digital and humanist education. To develop a high-quality learning environment, we will explain technology design steps and practice in order to improve the learning of tasks, complex cognitive skills, attitudes, motivations and competencies in the future trends of educational technology. At the end of the study, integrated technologies in e-learning were discussed and presented, based on foundations of IDT and the philosophy of educational technology.",Education
Introduction to physics teaching for science and engineering undergraduates,"Recruiting and retaining highly qualified physics and physical science teachers is critical for maintaining Americas global competitiveness. Unfortunately, only one third of the high school teachers in physics have a degree in physics and an even smaller number of physical science teachers in middle school have a good grasp of the scientific content they teach. Moreover, teachers often lack adequate pedagogical content knowledge to teach science effectively. Here, we discuss the development, implementation, and assessment of a course for science and engineering undergraduates designed to increase awareness and help them develop an interest and a deeper appreciation of the intellectual demands of physics teaching. The course focused on increasing student enthusiasm and confidence in teaching by providing well supported teaching opportunities and exposure to physics education research. The course assessment methods include 1) prepost-test measures of attitude and expectations about science teaching, 2) self and peer evaluation of student teaching, 3) content-based prepost-tests given to students who received instruction from the student teachers, and 4) audio-taped focus group discussions in the absence of the instructor and TA to evaluate student perspective on different aspects of the course and its impact.","Introduction to physics teaching for science and engineering undergraduates Recruiting and retaining highly qualified physics and physical science teachers is critical for maintaining Americas global competitiveness. Unfortunately, only one third of the high school teachers in physics have a degree in physics and an even smaller number of physical science teachers in middle school have a good grasp of the scientific content they teach. Moreover, teachers often lack adequate pedagogical content knowledge to teach science effectively. Here, we discuss the development, implementation, and assessment of a course for science and engineering undergraduates designed to increase awareness and help them develop an interest and a deeper appreciation of the intellectual demands of physics teaching. The course focused on increasing student enthusiasm and confidence in teaching by providing well supported teaching opportunities and exposure to physics education research. The course assessment methods include 1) prepost-test measures of attitude and expectations about science teaching, 2) self and peer evaluation of student teaching, 3) content-based prepost-tests given to students who received instruction from the student teachers, and 4) audio-taped focus group discussions in the absence of the instructor and TA to evaluate student perspective on different aspects of the course and its impact.",Education
Modelling the Effectiveness of Curriculum in Educational Systems Using Bayesian Networks,"In recent years, online education has been considered as one of the most widely used IT services. Researchers in this field face many challenges in the realm of Electronic learning services. Nowadays, many researchers in the field of learning and eLearning study curriculum planning, considering its complexity and the various numbers of effective parameters. The success of a curriculum is a multifaceted issue which needs analytical modelling for precise simulations of the different learning scenarios. In this paper, parameters involved in the learning process will be identified and a curriculum will be propounded. Furthermore, a Curriculum model will be proposed using the behavior of the user, based on the logs of the server. This model will estimate the success rate of the users while taking courses. Authentic Bayesian networks have been used for modelling. In order to evaluate the proposed model, the data of three consecutive semesters of 117 MS IT Students of E-Learning Center of Amirkabir University of Technology has been used. The assessment clarifies the effects of various parameters on the success of curriculum planning.","Modelling the Effectiveness of Curriculum in Educational Systems Using Bayesian Networks In recent years, online education has been considered as one of the most widely used IT services. Researchers in this field face many challenges in the realm of Electronic learning services. Nowadays, many researchers in the field of learning and eLearning study curriculum planning, considering its complexity and the various numbers of effective parameters. The success of a curriculum is a multifaceted issue which needs analytical modelling for precise simulations of the different learning scenarios. In this paper, parameters involved in the learning process will be identified and a curriculum will be propounded. Furthermore, a Curriculum model will be proposed using the behavior of the user, based on the logs of the server. This model will estimate the success rate of the users while taking courses. Authentic Bayesian networks have been used for modelling. In order to evaluate the proposed model, the data of three consecutive semesters of 117 MS IT Students of E-Learning Center of Amirkabir University of Technology has been used. The assessment clarifies the effects of various parameters on the success of curriculum planning.",Education
Using Pre-Post-Quizzes Intentionally in Curriculum Development and Evaluation,"Developing the final summative assessment of a course at the start of curriculum development is an implementation of backward design, whereby learning objectives are identified first and the curriculum is engineered end-to-beginning to achieve them. We trained in backward design through the Professional Development Program (PDP) and adapted PDP assessment ideas for evaluation of curriculum designs and teaching efficacy. A pre-post-quiz is an assessment administered the first and last day of a course; a learners scores are used to measure normalized gain: the ratio of what a student learned during a course relative to what they knew entering it. The intentional process of developing a pre-post-quiz for every course focuses the educator on the essential understanding desired of the learners exiting the course. The normalized-gain statistics for the course can then be used to evaluate the courses efficacy, and improvements to the curriculum can be monitored by tracking the normalized gains over time, using the same pre-post-quiz. Moreover, an individual instructor may self-evaluate their teaching efficacy by tracking normalized gains from all courses over time. Here we discuss applying the practice of backward curriculum design starting with a custom pre-post-quiz and utilizing it for immediate and longitudinal evaluation, focusing primarily on designing an entire undergraduate science course.","Using Pre-Post-Quizzes Intentionally in Curriculum Development and Evaluation Developing the final summative assessment of a course at the start of curriculum development is an implementation of backward design, whereby learning objectives are identified first and the curriculum is engineered end-to-beginning to achieve them. We trained in backward design through the Professional Development Program (PDP) and adapted PDP assessment ideas for evaluation of curriculum designs and teaching efficacy. A pre-post-quiz is an assessment administered the first and last day of a course; a learners scores are used to measure normalized gain: the ratio of what a student learned during a course relative to what they knew entering it. The intentional process of developing a pre-post-quiz for every course focuses the educator on the essential understanding desired of the learners exiting the course. The normalized-gain statistics for the course can then be used to evaluate the courses efficacy, and improvements to the curriculum can be monitored by tracking the normalized gains over time, using the same pre-post-quiz. Moreover, an individual instructor may self-evaluate their teaching efficacy by tracking normalized gains from all courses over time. Here we discuss applying the practice of backward curriculum design starting with a custom pre-post-quiz and utilizing it for immediate and longitudinal evaluation, focusing primarily on designing an entire undergraduate science course.",Education
Brownian motion with stochastic energy renewals,"We investigate the impact of intermittent energy injections on a Brownian particle, modeled as stochastic renewals of its kinetic energy to a fixed value. Between renewals, the particle follows standard underdamped Langevin dynamics. For energy renewals occurring at a constant rate, we find non-Boltzmannian energy distributions that undergo a shape transition driven by the competition between the velocity relaxation timescale and the renewal timescale. In the limit of rapid renewals, the dynamics mimics one-dimensional run-and-tumble motion, while at finite renewal rates, the effective diffusion coefficient exhibits non-monotonic behavior. To quantify the systems departure from equilibrium, we derive a modified fluctuation-response relation and demonstrate the absence of a consistent effective temperature. The dissipation is characterized by deviations from equilibrium-like response, captured via the Harada-Sasa relation. Finally, we extend the analysis to non-Poissonian renewal processes and introduce a dimensionless conversion coefficient that quantifies the thermodynamic cost of diffusion.","Brownian motion with stochastic energy renewals We investigate the impact of intermittent energy injections on a Brownian particle, modeled as stochastic renewals of its kinetic energy to a fixed value. Between renewals, the particle follows standard underdamped Langevin dynamics. For energy renewals occurring at a constant rate, we find non-Boltzmannian energy distributions that undergo a shape transition driven by the competition between the velocity relaxation timescale and the renewal timescale. In the limit of rapid renewals, the dynamics mimics one-dimensional run-and-tumble motion, while at finite renewal rates, the effective diffusion coefficient exhibits non-monotonic behavior. To quantify the systems departure from equilibrium, we derive a modified fluctuation-response relation and demonstrate the absence of a consistent effective temperature. The dissipation is characterized by deviations from equilibrium-like response, captured via the Harada-Sasa relation. Finally, we extend the analysis to non-Poissonian renewal processes and introduce a dimensionless conversion coefficient that quantifies the thermodynamic cost of diffusion.",Environment
Metric entropy in competitive on-line prediction,"Competitive on-line prediction (also known as universal prediction of individual sequences) is a strand of learning theory avoiding making any stochastic assumptions about the way the observations are generated. The predictors goal is to compete with a benchmark class of prediction rules, which is often a proper Banach function space. Metric entropy provides a unifying framework for competitive on-line prediction: the numerous known upper bounds on the metric entropy of various compact sets in function spaces readily imply bounds on the performance of on-line prediction strategies. This paper discusses strengths and limitations of the direct approach to competitive on-line prediction via metric entropy, including comparisons to other approaches.","Metric entropy in competitive on-line prediction Competitive on-line prediction (also known as universal prediction of individual sequences) is a strand of learning theory avoiding making any stochastic assumptions about the way the observations are generated. The predictors goal is to compete with a benchmark class of prediction rules, which is often a proper Banach function space. Metric entropy provides a unifying framework for competitive on-line prediction: the numerous known upper bounds on the metric entropy of various compact sets in function spaces readily imply bounds on the performance of on-line prediction strategies. This paper discusses strengths and limitations of the direct approach to competitive on-line prediction via metric entropy, including comparisons to other approaches.",Technology
Exact simulation pricing with Gamma processes and their extensions,"Exact path simulation of the underlying state variable is of great practical importance in simulating prices of financial derivatives or their sensitivities when there are no analytical solutions for their pricing formulas. However, in general, the complex dependence structure inherent in most nontrivial stochastic volatility (SV) models makes exact simulation difficult. In this paper, we present a nontrivial SV model that parallels the notable Heston SV model in the sense of admitting exact path simulation as studied by Broadie and Kaya. The instantaneous volatility process of the proposed model is driven by a Gamma process. Extensions to the model including superposition of independent instantaneous volatility processes are studied. Numerical results show that the proposed model outperforms the Heston model and two other Levy driven SV models in terms of model fit to the real option data. The ability to exactly simulate some of the path-dependent derivative prices is emphasized. Moreover, this is the first instance where an infinite-activity volatility process can be applied exactly in such pricing contexts.","Exact simulation pricing with Gamma processes and their extensions Exact path simulation of the underlying state variable is of great practical importance in simulating prices of financial derivatives or their sensitivities when there are no analytical solutions for their pricing formulas. However, in general, the complex dependence structure inherent in most nontrivial stochastic volatility (SV) models makes exact simulation difficult. In this paper, we present a nontrivial SV model that parallels the notable Heston SV model in the sense of admitting exact path simulation as studied by Broadie and Kaya. The instantaneous volatility process of the proposed model is driven by a Gamma process. Extensions to the model including superposition of independent instantaneous volatility processes are studied. Numerical results show that the proposed model outperforms the Heston model and two other Levy driven SV models in terms of model fit to the real option data. The ability to exactly simulate some of the path-dependent derivative prices is emphasized. Moreover, this is the first instance where an infinite-activity volatility process can be applied exactly in such pricing contexts.",Finance
Covariance and PCA for Categorical Variables,"Covariances from categorical variables are defined using a regular simplex expression for categories. The method follows the variance definition by Gini, and it gives the covariance as a solution of simultaneous equations. The calculated results give reasonable values for test data. A method of principal component analysis (RS-PCA) is also proposed using regular simplex expressions, which allows easy interpretation of the principal components. The proposed methods apply to variable selection problem of categorical data USCensus1990 data. The proposed methods give appropriate criterion for the variable selection problem of categorical","Covariance and PCA for Categorical Variables Covariances from categorical variables are defined using a regular simplex expression for categories. The method follows the variance definition by Gini, and it gives the covariance as a solution of simultaneous equations. The calculated results give reasonable values for test data. A method of principal component analysis (RS-PCA) is also proposed using regular simplex expressions, which allows easy interpretation of the principal components. The proposed methods apply to variable selection problem of categorical data USCensus1990 data. The proposed methods give appropriate criterion for the variable selection problem of categorical",Technology
Practical Methods for Proving Termination of General Logic Programs,"Termination of logic programs with negated body atoms (here called general logic programs) is an important topic. One reason is that many computational mechanisms used to process negated atoms, like Clarks negation as failure and Chans constructive negation, are based on termination conditions. This paper introduces a methodology for proving termination of general logic programs w.r.t. the Prolog selection rule. The idea is to distinguish parts of the program depending on whether or not their termination depends on the selection rule. To this end, the notions of low-, weakly up-, and up-acceptable program are introduced. We use these notions to develop a methodology for proving termination of general logic programs, and show how interesting problems in non-monotonic reasoning can be formalized and implemented by means of terminating general logic programs.","Practical Methods for Proving Termination of General Logic Programs Termination of logic programs with negated body atoms (here called general logic programs) is an important topic. One reason is that many computational mechanisms used to process negated atoms, like Clarks negation as failure and Chans constructive negation, are based on termination conditions. This paper introduces a methodology for proving termination of general logic programs w.r.t. the Prolog selection rule. The idea is to distinguish parts of the program depending on whether or not their termination depends on the selection rule. To this end, the notions of low-, weakly up-, and up-acceptable program are introduced. We use these notions to develop a methodology for proving termination of general logic programs, and show how interesting problems in non-monotonic reasoning can be formalized and implemented by means of terminating general logic programs.",Technology
A New Hypothesis for Layers of High Reflectivity Seen in MST Radar Observations,"Analysis of MST radar observations at Gadanki (near Tirupati, India), for a period of 14 months from 1995 September to 1996 November, shows that Clear air Turbulence is not the primary source of high MST radar reflectivity; this conventional idea needs to be modified. An alternative hypothesis based on the microphysics and microdynamics associated with aerosols and water substance in the atmosphere, is presented here.","A New Hypothesis for Layers of High Reflectivity Seen in MST Radar Observations Analysis of MST radar observations at Gadanki (near Tirupati, India), for a period of 14 months from 1995 September to 1996 November, shows that Clear air Turbulence is not the primary source of high MST radar reflectivity; this conventional idea needs to be modified. An alternative hypothesis based on the microphysics and microdynamics associated with aerosols and water substance in the atmosphere, is presented here.",Environment
Assessing student reasoning in upper-division electricity and magnetism at Oregon State University,"Standardized assessment tests that allow researchers to compare the performance of students under various curricula are highly desirable. There are several research-based conceptual tests that serve as instruments to assess and identify students difficulties in lower-division courses. At the upper-division level, however, assessing students difficulties is a more challenging task. Although several research groups are currently working on such tests, their reliability and validity are still under investigation. We analyze the results of the Colorado Upper-Division Electrostatics diagnostic from Oregon State University and compare it with data from University of Colorado. In particular, we show potential shortcomings in the Oregon State University curriculum regarding separation of variables and boundary conditions, as well as uncover weaknesses of the rubric to the free response version of the diagnostic. We also demonstrate that the diagnostic can be used to obtain information about student learning during a gap in instruction. Our work complements and extends the previous findings from the University of Colorado by highlighting important differences in student learning that may be related to the curriculum, illuminating difficulties with the rubric for certain problems and verifying decay in post-test results over time.","Assessing student reasoning in upper-division electricity and magnetism at Oregon State University Standardized assessment tests that allow researchers to compare the performance of students under various curricula are highly desirable. There are several research-based conceptual tests that serve as instruments to assess and identify students difficulties in lower-division courses. At the upper-division level, however, assessing students difficulties is a more challenging task. Although several research groups are currently working on such tests, their reliability and validity are still under investigation. We analyze the results of the Colorado Upper-Division Electrostatics diagnostic from Oregon State University and compare it with data from University of Colorado. In particular, we show potential shortcomings in the Oregon State University curriculum regarding separation of variables and boundary conditions, as well as uncover weaknesses of the rubric to the free response version of the diagnostic. We also demonstrate that the diagnostic can be used to obtain information about student learning during a gap in instruction. Our work complements and extends the previous findings from the University of Colorado by highlighting important differences in student learning that may be related to the curriculum, illuminating difficulties with the rubric for certain problems and verifying decay in post-test results over time.",Education
"Image Compression with Iterated Function Systems, Finite Automata and Zerotrees: Grand Unification","Fractal image compression, Culiks image compression and zerotree prediction coding of wavelet image decomposition coefficients succeed only because typical images being compressed possess a significant degree of self-similarity. Besides the common concept, these methods turn out to be even more tightly related, to the point of algorithmical reducibility of one technique to another. The goal of the present paper is to demonstrate these relations. The paper offers a plain-term interpretation of Culiks image compression, in regular image processing terms, without resorting to finite state machines and similar lofty language. The interpretation is shown to be algorithmically related to an IFS fractal image compression method: an IFS can be exactly transformed into Culiks image code. Using this transformation, we will prove that in a self-similar (part of an) image any zero wavelet coefficient is the root of a zerotree, or its branch. The paper discusses the zerotree coding of (waveletprojection) coefficients as a common predictorcorrector, applied vertically through different layers of a multiresolutional decomposition, rather than within the same view. This interpretation leads to an insight into the evolution of image compression techniques: from a causal single-layer prediction, to non-causal same-view predictions (wavelet decomposition among others) and to a causal cross-layer prediction (zero-trees, Culiks method).","Image Compression with Iterated Function Systems, Finite Automata and Zerotrees: Grand Unification Fractal image compression, Culiks image compression and zerotree prediction coding of wavelet image decomposition coefficients succeed only because typical images being compressed possess a significant degree of self-similarity. Besides the common concept, these methods turn out to be even more tightly related, to the point of algorithmical reducibility of one technique to another. The goal of the present paper is to demonstrate these relations. The paper offers a plain-term interpretation of Culiks image compression, in regular image processing terms, without resorting to finite state machines and similar lofty language. The interpretation is shown to be algorithmically related to an IFS fractal image compression method: an IFS can be exactly transformed into Culiks image code. Using this transformation, we will prove that in a self-similar (part of an) image any zero wavelet coefficient is the root of a zerotree, or its branch. The paper discusses the zerotree coding of (waveletprojection) coefficients as a common predictorcorrector, applied vertically through different layers of a multiresolutional decomposition, rather than within the same view. This interpretation leads to an insight into the evolution of image compression techniques: from a causal single-layer prediction, to non-causal same-view predictions (wavelet decomposition among others) and to a causal cross-layer prediction (zero-trees, Culiks method).",Technology
Further Experimental Evidence against the Utility of Occams Razor,"This paper presents new experimental evidence against the utility of Occams razor. Asystematic procedure is presented for post-processing decision trees produced by C4.5. This procedure was derived by rejecting Occams razor and instead attending to the assumption that similar objects are likely to belong to the same class. It increases a decision trees complexity without altering the performance of that tree on the training data from which it is inferred. The resulting more complex decision trees are demonstrated to have, on average, for a variety of common learning tasks, higher predictive accuracy than the less complex original decision trees. This result raises considerable doubt about the utility of Occams razor as it is commonly applied in modern machine learning.","Further Experimental Evidence against the Utility of Occams Razor This paper presents new experimental evidence against the utility of Occams razor. Asystematic procedure is presented for post-processing decision trees produced by C4.5. This procedure was derived by rejecting Occams razor and instead attending to the assumption that similar objects are likely to belong to the same class. It increases a decision trees complexity without altering the performance of that tree on the training data from which it is inferred. The resulting more complex decision trees are demonstrated to have, on average, for a variety of common learning tasks, higher predictive accuracy than the less complex original decision trees. This result raises considerable doubt about the utility of Occams razor as it is commonly applied in modern machine learning.",Technology
Defensive Universal Learning with Experts,"This paper shows how universal learning can be achieved with expert advice. To this aim, we specify an experts algorithm with the following characteristics: (a) it uses only feedback from the actions actually chosen (bandit setup), (b) it can be applied with countably infinite expert classes, and (c) it copes with losses that may grow in time appropriately slowly. We prove loss bounds against an adaptive adversary. From this, we obtain a master algorithm for reactive experts problems, which means that the masters actions may influence the behavior of the adversary. Our algorithm can significantly outperform standard experts algorithms on such problems. Finally, we combine it with a universal expert class. The resulting universal learner performs -- in a certain sense -- almost as well as any computable strategy, for any online decision problem. We also specify the (worst-case) convergence speed, which is very slow.","Defensive Universal Learning with Experts This paper shows how universal learning can be achieved with expert advice. To this aim, we specify an experts algorithm with the following characteristics: (a) it uses only feedback from the actions actually chosen (bandit setup), (b) it can be applied with countably infinite expert classes, and (c) it copes with losses that may grow in time appropriately slowly. We prove loss bounds against an adaptive adversary. From this, we obtain a master algorithm for reactive experts problems, which means that the masters actions may influence the behavior of the adversary. Our algorithm can significantly outperform standard experts algorithms on such problems. Finally, we combine it with a universal expert class. The resulting universal learner performs -- in a certain sense -- almost as well as any computable strategy, for any online decision problem. We also specify the (worst-case) convergence speed, which is very slow.",Technology
A Comparison between Financial and Gambling Markets,"Financial and gambling markets are ostensibly similar and hence strategies from one could potentially be applied to the other. Financial markets have been extensively studied, resulting in numerous theorems and models, while gambling markets have received comparatively less attention and remain relatively undocumented. This study conducts a comprehensive comparison of both markets, focusing on trading rather than regulation. Five key aspects are examined: platform, product, procedure, participant and strategy. The findings reveal numerous similarities between these two markets. Financial exchanges resemble online betting platforms, such as Betfair, and some financial products, including stocks and options, share speculative traits with sports betting. We examine whether well-established models and strategies from financial markets could be applied to the gambling industry, which lacks comparable frameworks. For example, statistical arbitrage from financial markets has been effectively applied to gambling markets, particularly in peer-to-peer betting exchanges, where bettors exploit odds discrepancies for risk-free profits using quantitative models. Therefore, exploring the strategies and approaches used in both markets could lead to new opportunities for innovation and optimization in trading and betting activities.","A Comparison between Financial and Gambling Markets Financial and gambling markets are ostensibly similar and hence strategies from one could potentially be applied to the other. Financial markets have been extensively studied, resulting in numerous theorems and models, while gambling markets have received comparatively less attention and remain relatively undocumented. This study conducts a comprehensive comparison of both markets, focusing on trading rather than regulation. Five key aspects are examined: platform, product, procedure, participant and strategy. The findings reveal numerous similarities between these two markets. Financial exchanges resemble online betting platforms, such as Betfair, and some financial products, including stocks and options, share speculative traits with sports betting. We examine whether well-established models and strategies from financial markets could be applied to the gambling industry, which lacks comparable frameworks. For example, statistical arbitrage from financial markets has been effectively applied to gambling markets, particularly in peer-to-peer betting exchanges, where bettors exploit odds discrepancies for risk-free profits using quantitative models. Therefore, exploring the strategies and approaches used in both markets could lead to new opportunities for innovation and optimization in trading and betting activities.",Finance
Cognitive Transfer Outcomes for a Simulation-Based Introductory Statistics Curriculum,"Cognitive transfer is the ability to apply learned skills and knowledge to new applications and contexts. This investigation evaluates cognitive transfer outcomes for a tertiary-level introductory statistics course using the CATALST curriculum, which exclusively used simulation-based methods to develop foundations of statistical inference. A common assessment instrument administered at the end of each course measured learning outcomes for students. CATALST students showed evidence of both near and far transfer outcomes while scoring as high, or higher on the assessed learning objectives, when compared with peers enrolled in similar courses that emphasized parametric inferential methods (e.g. the t-test).","Cognitive Transfer Outcomes for a Simulation-Based Introductory Statistics Curriculum Cognitive transfer is the ability to apply learned skills and knowledge to new applications and contexts. This investigation evaluates cognitive transfer outcomes for a tertiary-level introductory statistics course using the CATALST curriculum, which exclusively used simulation-based methods to develop foundations of statistical inference. A common assessment instrument administered at the end of each course measured learning outcomes for students. CATALST students showed evidence of both near and far transfer outcomes while scoring as high, or higher on the assessed learning objectives, when compared with peers enrolled in similar courses that emphasized parametric inferential methods (e.g. the t-test).",Education
Information based clustering: Supplementary material,"This technical report provides the supplementary material for a paper entitled Information based clustering, to appear shortly in Proceedings of the National Academy of Sciences (USA). In Section I we present in detail the iterative clustering algorithm used in our experiments and in Section II we describe the validation scheme used to determine the statistical significance of our results. Then in subsequent sections we provide all the experimental results for three very different applications: the response of gene expression in yeast to different forms of environmental stress, the dynamics of stock prices in the Standard and Poors 500, and viewer ratings of popular movies. In particular, we highlight some of the results that seem to deserve special attention. All the experimental results and relevant code, including a freely available web application, can be found at http:www.genomics.princeton.edubiophysics-theory .","Information based clustering: Supplementary material This technical report provides the supplementary material for a paper entitled Information based clustering, to appear shortly in Proceedings of the National Academy of Sciences (USA). In Section I we present in detail the iterative clustering algorithm used in our experiments and in Section II we describe the validation scheme used to determine the statistical significance of our results. Then in subsequent sections we provide all the experimental results for three very different applications: the response of gene expression in yeast to different forms of environmental stress, the dynamics of stock prices in the Standard and Poors 500, and viewer ratings of popular movies. In particular, we highlight some of the results that seem to deserve special attention. All the experimental results and relevant code, including a freely available web application, can be found at http:www.genomics.princeton.edubiophysics-theory .",Healthcare
Renewable-Colocated Green Hydrogen Production: Optimal Scheduling and Profitability,"We study the optimal green hydrogen production and energy market participation of a renewable-colocated hydrogen producer (RCHP) that utilizes onsite renewable generation for both hydrogen production and grid services. Under deterministic and stochastic profit-maximization frameworks, we analyze RCHPs multiple market participation models and derive closed-form optimal scheduling policies that dynamically allocate renewable energy to hydrogen production and electricity export to the wholesale market. Analytical characterizations of the RCHPs operating profit and the optimal sizing of renewable and electrolyzer capacities are obtained. We use real-time renewable production and electricity price data from three independent system operators to assess the impacts of hydrogen market prices, renewable generation, and electricity prices on RCHPs profitability.","Renewable-Colocated Green Hydrogen Production: Optimal Scheduling and Profitability We study the optimal green hydrogen production and energy market participation of a renewable-colocated hydrogen producer (RCHP) that utilizes onsite renewable generation for both hydrogen production and grid services. Under deterministic and stochastic profit-maximization frameworks, we analyze RCHPs multiple market participation models and derive closed-form optimal scheduling policies that dynamically allocate renewable energy to hydrogen production and electricity export to the wholesale market. Analytical characterizations of the RCHPs operating profit and the optimal sizing of renewable and electrolyzer capacities are obtained. We use real-time renewable production and electricity price data from three independent system operators to assess the impacts of hydrogen market prices, renewable generation, and electricity prices on RCHPs profitability.",Environment
Clinical Concept Extraction with Contextual Word Embedding,"Automatic extraction of clinical concepts is an essential step for turning the unstructured data within a clinical note into structured and actionable information. In this work, we propose a clinical concept extraction model for automatic annotation of clinical problems, treatments, and tests in clinical notes utilizing domain-specific contextual word embedding. A contextual word embedding model is first trained on a corpus with a mixture of clinical reports and relevant Wikipedia pages in the clinical domain. Next, a bidirectional LSTM-CRF model is trained for clinical concept extraction using the contextual word embedding model. We tested our proposed model on the I2B2 2010 challenge dataset. Our proposed model achieved the best performance among reported baseline models and outperformed the state-of-the-art models by 3.4 in terms of F1-score.","Clinical Concept Extraction with Contextual Word Embedding Automatic extraction of clinical concepts is an essential step for turning the unstructured data within a clinical note into structured and actionable information. In this work, we propose a clinical concept extraction model for automatic annotation of clinical problems, treatments, and tests in clinical notes utilizing domain-specific contextual word embedding. A contextual word embedding model is first trained on a corpus with a mixture of clinical reports and relevant Wikipedia pages in the clinical domain. Next, a bidirectional LSTM-CRF model is trained for clinical concept extraction using the contextual word embedding model. We tested our proposed model on the I2B2 2010 challenge dataset. Our proposed model achieved the best performance among reported baseline models and outperformed the state-of-the-art models by 3.4 in terms of F1-score.",Healthcare
"Notes on Geometric Measure Theory Applications to Image Processing; De-noising, Segmentation, Pattern, Texture, Lines, Gestalt and Occlusion","Regularization functionals that lower level set boundary length when used with L1 fidelity functionals on signal de-noising on images create artifacts. These are (i) rounding of corners, (ii) shrinking of radii, (iii) shrinking of cusps, and (iv) non-smoothing of staircasing. Regularity functionals based upon total curvature of level set boundaries do not create artifacts (i) and (ii). An adjusted fidelity term based on the flat norm on the current (a distributional graph) representing the density of curvature of level sets boundaries can minimize (iii) by weighting the position of a cusp. A regularity term to eliminate staircasing can be based upon the mass of the current representing the graph of an image function or its second derivatives. Densities on the Grassmann bundle of the Grassmann bundle of the ambient space of the graph can be used to identify patterns, textures, occlusion and lines.","Notes on Geometric Measure Theory Applications to Image Processing; De-noising, Segmentation, Pattern, Texture, Lines, Gestalt and Occlusion Regularization functionals that lower level set boundary length when used with L1 fidelity functionals on signal de-noising on images create artifacts. These are (i) rounding of corners, (ii) shrinking of radii, (iii) shrinking of cusps, and (iv) non-smoothing of staircasing. Regularity functionals based upon total curvature of level set boundaries do not create artifacts (i) and (ii). An adjusted fidelity term based on the flat norm on the current (a distributional graph) representing the density of curvature of level sets boundaries can minimize (iii) by weighting the position of a cusp. A regularity term to eliminate staircasing can be based upon the mass of the current representing the graph of an image function or its second derivatives. Densities on the Grassmann bundle of the Grassmann bundle of the ambient space of the graph can be used to identify patterns, textures, occlusion and lines.",Technology
The Cost of Climate Action: Experimental Evidence on the Impact of Climate Information on Charitable Donations to Climate Activism,"We examine the propensity of individuals to donate to climate activism, evaluating the impact of different informational treatments on an incentive compatible charitable donation and stated climate change-related concerns. Participants were evaluated on climate literacy and general climate attitudes before being randomly assigned to a treatment which provided either education or neutral language about climate change, either with or without images of protest. After the treatment, participants engaged in an incentive compatible dictator game. We find that participants gave more to climate activism than seen in previous dictator game and charitable giving experiments, in both average amount given and proportion of participants who gave their entire endowment. However, we determine that climate activism information negatively influenced the amount of money donated. We also found that protest imagery moderated this negative effect and had a positive significant effect of increasing participants climate concern. Finally, we found that the climate concern was significantly positively correlated with donations, while being a male was significantly negatively associated with donation amounts.","The Cost of Climate Action: Experimental Evidence on the Impact of Climate Information on Charitable Donations to Climate Activism We examine the propensity of individuals to donate to climate activism, evaluating the impact of different informational treatments on an incentive compatible charitable donation and stated climate change-related concerns. Participants were evaluated on climate literacy and general climate attitudes before being randomly assigned to a treatment which provided either education or neutral language about climate change, either with or without images of protest. After the treatment, participants engaged in an incentive compatible dictator game. We find that participants gave more to climate activism than seen in previous dictator game and charitable giving experiments, in both average amount given and proportion of participants who gave their entire endowment. However, we determine that climate activism information negatively influenced the amount of money donated. We also found that protest imagery moderated this negative effect and had a positive significant effect of increasing participants climate concern. Finally, we found that the climate concern was significantly positively correlated with donations, while being a male was significantly negatively associated with donation amounts.",Environment
Using Helium Balloon Flying Drones for Introductory CS Education,"In the rapidly evolving field of computer science education, novel approaches to teaching fundamental concepts are crucial for engaging a diverse student body. Given the growing demand for a computing-skilled workforce, it is essential to adapt educational methods to capture the interest of a broader audience than what current computing education typically targets. Engaging educational experiences have been shown to have a positive impact on learning outcomes and examination performance, especially within computing education. Moreover, physical computing devices have been shown to correlate with increased student motivation when students are studying computer science.","Using Helium Balloon Flying Drones for Introductory CS Education In the rapidly evolving field of computer science education, novel approaches to teaching fundamental concepts are crucial for engaging a diverse student body. Given the growing demand for a computing-skilled workforce, it is essential to adapt educational methods to capture the interest of a broader audience than what current computing education typically targets. Engaging educational experiences have been shown to have a positive impact on learning outcomes and examination performance, especially within computing education. Moreover, physical computing devices have been shown to correlate with increased student motivation when students are studying computer science.",Education
Arbitrage-free Self-organizing Markets with GARCH Properties: Generating them in the Lab with a Lattice Model,"We extend our studies of a quantum field model defined on a lattice having the dilation group as a local gauge symmetry. The model is relevant in the cross-disciplinary area of econophysics. A corresponding proposal by Ilinski aimed at gauge modeling in non-equilibrium pricing is realized as a numerical simulation of the one-asset version. The gauge field background enforces minimal arbitrage, yet allows for statistical fluctuations. The new feature added to the model is an updating prescription for the simulation that drives the model market into a self-organized critical state. Taking advantage of some flexibility of the updating prescription, stylized features and dynamical behaviors of real-world markets are reproduced in some detail.","Arbitrage-free Self-organizing Markets with GARCH Properties: Generating them in the Lab with a Lattice Model We extend our studies of a quantum field model defined on a lattice having the dilation group as a local gauge symmetry. The model is relevant in the cross-disciplinary area of econophysics. A corresponding proposal by Ilinski aimed at gauge modeling in non-equilibrium pricing is realized as a numerical simulation of the one-asset version. The gauge field background enforces minimal arbitrage, yet allows for statistical fluctuations. The new feature added to the model is an updating prescription for the simulation that drives the model market into a self-organized critical state. Taking advantage of some flexibility of the updating prescription, stylized features and dynamical behaviors of real-world markets are reproduced in some detail.",Finance
Remote sensing of bubble clouds in seawater,"We report on the influence of submerged bubble clouds on the remote sensing properties of water. We show that the optical effect of bubbles on radiative transfer and on the estimate of the ocean color is significant. We present a global map of the volume fraction of air in water derived from daily wind speed data. This map, together with the parameterization of the microphysical properties, shows the possible significance of bubble clouds on the albedo of incoming solar energy","Remote sensing of bubble clouds in seawater We report on the influence of submerged bubble clouds on the remote sensing properties of water. We show that the optical effect of bubbles on radiative transfer and on the estimate of the ocean color is significant. We present a global map of the volume fraction of air in water derived from daily wind speed data. This map, together with the parameterization of the microphysical properties, shows the possible significance of bubble clouds on the albedo of incoming solar energy",Environment
Joint Embedding Learning of Educational Knowledge Graphs,"As an efficient model for knowledge organization, the knowledge graph has been widely adopted in several fields, e.g., biomedicine, sociology, and education. And there is a steady trend of learning embedding representations of knowledge graphs to facilitate knowledge graph construction and downstream tasks. In general, knowledge graph embedding techniques aim to learn vectorized representations which preserve the structural information of the graph. And conventional embedding learning models rely on structural relationships among entities and relations. However, in educational knowledge graphs, structural relationships are not the focus. Instead, rich literals of the graphs are more valuable. In this paper, we focus on this problem and propose a novel model for embedding learning of educational knowledge graphs. Our model considers both structural and literal information and jointly learns embedding representations. Three experimental graphs were constructed based on an educational knowledge graph which has been applied in real-world teaching. We conducted two experiments on the three graphs and other common benchmark graphs. The experimental results proved the effectiveness of our model and its superiority over other baselines when processing educational knowledge graphs.","Joint Embedding Learning of Educational Knowledge Graphs As an efficient model for knowledge organization, the knowledge graph has been widely adopted in several fields, e.g., biomedicine, sociology, and education. And there is a steady trend of learning embedding representations of knowledge graphs to facilitate knowledge graph construction and downstream tasks. In general, knowledge graph embedding techniques aim to learn vectorized representations which preserve the structural information of the graph. And conventional embedding learning models rely on structural relationships among entities and relations. However, in educational knowledge graphs, structural relationships are not the focus. Instead, rich literals of the graphs are more valuable. In this paper, we focus on this problem and propose a novel model for embedding learning of educational knowledge graphs. Our model considers both structural and literal information and jointly learns embedding representations. Three experimental graphs were constructed based on an educational knowledge graph which has been applied in real-world teaching. We conducted two experiments on the three graphs and other common benchmark graphs. The experimental results proved the effectiveness of our model and its superiority over other baselines when processing educational knowledge graphs.",Education
A global physician-oriented medical information system,"We propose to improve medical decision making and reduce global health care costs by employing a free Internet-based medical information system with two main target groups: practicing physicians and medical researchers. After acquiring patients consent, physicians enter medical histories, physiological data and symptoms or disorders into the system; an integrated expert system can then assist in diagnosis and statistical software provides a list of the most promising treatment options and medications, tailored to the patient. Physicians later enter information about the outcomes of the chosen treatments, data the system uses to optimize future treatment recommendations. Medical researchers can analyze the aggregate data to compare various drugs or treatments in defined patient populations on a large scale.","A global physician-oriented medical information system We propose to improve medical decision making and reduce global health care costs by employing a free Internet-based medical information system with two main target groups: practicing physicians and medical researchers. After acquiring patients consent, physicians enter medical histories, physiological data and symptoms or disorders into the system; an integrated expert system can then assist in diagnosis and statistical software provides a list of the most promising treatment options and medications, tailored to the patient. Physicians later enter information about the outcomes of the chosen treatments, data the system uses to optimize future treatment recommendations. Medical researchers can analyze the aggregate data to compare various drugs or treatments in defined patient populations on a large scale.",Healthcare
"11-Year Warm Cloud Modification Experiment in Maharashtra State, India","A warm cloud modification experiment was carried out in an area of 4800 Sq.Km in the Pune region,India, during the 11-summer monsoon (June-September) seasons (1973-74, 1976, 1979-86). A double-area cross-over design with area randomization was adopted and an instrumented aircraft was used for seeding and cloud physical measurements. Finely pulverised salt (sodium chloride) particles were released into the monsoon clouds (cumulus and stratocumulus) during aircraft penetrations into the clouds at a height of 200-300 m above the cloud-base. The warm cloud responses to salt seeding are found to be critically dependent on the cloud physical characteristics e.g., vertical thickness and liquid water content. Clouds with vertical thickness greater than 1 km, LWC greater than 0.5 gmcubic m when seeded with salt particles (modal diameter 10 micro m, concentration 1 per litre of cloud air) produced increase in rainfall of 24 per cent significant at 4 per cent level. Shallow clouds (vertical thickness less than 1 km, LWC less than 0.5 gmcubic m) when seeded showed tendency for dissipation. The cloud physical observations made in not-seeded (control) and seeded (target) clouds have provided some useful evidence to test the applicability of the warm cloud modification hypothesis. The results of the cloud model computations suggested that moderate convergence at the cloud-base is essential for the cloud growth and development of precipitation in the real world. Hygroscopic particle seeding of warm clouds under favourable dynamical conditions (convergence at the cloud-base level) may result in the acceleration of the collision-coalescence process resulting in the enhancement of rainfall.","11-Year Warm Cloud Modification Experiment in Maharashtra State, India A warm cloud modification experiment was carried out in an area of 4800 Sq.Km in the Pune region,India, during the 11-summer monsoon (June-September) seasons (1973-74, 1976, 1979-86). A double-area cross-over design with area randomization was adopted and an instrumented aircraft was used for seeding and cloud physical measurements. Finely pulverised salt (sodium chloride) particles were released into the monsoon clouds (cumulus and stratocumulus) during aircraft penetrations into the clouds at a height of 200-300 m above the cloud-base. The warm cloud responses to salt seeding are found to be critically dependent on the cloud physical characteristics e.g., vertical thickness and liquid water content. Clouds with vertical thickness greater than 1 km, LWC greater than 0.5 gmcubic m when seeded with salt particles (modal diameter 10 micro m, concentration 1 per litre of cloud air) produced increase in rainfall of 24 per cent significant at 4 per cent level. Shallow clouds (vertical thickness less than 1 km, LWC less than 0.5 gmcubic m) when seeded showed tendency for dissipation. The cloud physical observations made in not-seeded (control) and seeded (target) clouds have provided some useful evidence to test the applicability of the warm cloud modification hypothesis. The results of the cloud model computations suggested that moderate convergence at the cloud-base is essential for the cloud growth and development of precipitation in the real world. Hygroscopic particle seeding of warm clouds under favourable dynamical conditions (convergence at the cloud-base level) may result in the acceleration of the collision-coalescence process resulting in the enhancement of rainfall.",Environment
Pedagogy of Teaching Pointers in the C Programming Language using Graph Transformations,"Visual learners think in pictures rather than words and learn best when they utilize representations based on graphs, tables, charts, maps, colors and diagrams. We propose a new pedagogy for teaching pointers in the C programming language using graph transformation systems to visually simulate pointer manipulation. In an Introduction to C course, the topic of pointers is often the most difficult one for students to understand; therefore, we experiment with graph-based representations of dynamic pointer structures to reinforce the learning. Groove, a graph transformation tool, is used to illustrate the behaviour of pointers through modelling and simulation. A study is presented to evaluate the effectiveness of the approach. This paper will also provide a comparison to other teaching methods in this area.","Pedagogy of Teaching Pointers in the C Programming Language using Graph Transformations Visual learners think in pictures rather than words and learn best when they utilize representations based on graphs, tables, charts, maps, colors and diagrams. We propose a new pedagogy for teaching pointers in the C programming language using graph transformation systems to visually simulate pointer manipulation. In an Introduction to C course, the topic of pointers is often the most difficult one for students to understand; therefore, we experiment with graph-based representations of dynamic pointer structures to reinforce the learning. Groove, a graph transformation tool, is used to illustrate the behaviour of pointers through modelling and simulation. A study is presented to evaluate the effectiveness of the approach. This paper will also provide a comparison to other teaching methods in this area.",Education
Dynamic Investment-Driven Insurance Pricing and Optimal Regulation,"This paper analyzes the equilibrium of insurance market in a dynamic setting, focusing on the interaction between insurers underwriting and investment strategies. Three possible equilibrium outcomes are identified: a positive insurance market, a zero insurance market, and market failure. Our findings reveal why insurers may rationally accept underwriting losses by setting a negative safety loading while relying on investment profits, particularly when there is a negative correlation between insurance gains and financial returns. Additionally, we explore the impact of regulatory frictions, showing that while imposing a cost on investment can enhance social welfare under certain conditions, it may not always be necessary.","Dynamic Investment-Driven Insurance Pricing and Optimal Regulation This paper analyzes the equilibrium of insurance market in a dynamic setting, focusing on the interaction between insurers underwriting and investment strategies. Three possible equilibrium outcomes are identified: a positive insurance market, a zero insurance market, and market failure. Our findings reveal why insurers may rationally accept underwriting losses by setting a negative safety loading while relying on investment profits, particularly when there is a negative correlation between insurance gains and financial returns. Additionally, we explore the impact of regulatory frictions, showing that while imposing a cost on investment can enhance social welfare under certain conditions, it may not always be necessary.",Finance
Teaching Introduction to Programming in the times of AI: A case study of a course re-design,"The integration of AI tools into programming education has become increasingly prevalent in recent years, transforming the way programming is taught and learned. This paper provides a review of the state-of-the-art AI tools available for teaching and learning programming, particularly in the context of introductory courses. It highlights the challenges on course design, learning objectives, course delivery and formative and summative assessment, as well as the misuse of such tools by the students. We discuss ways of re-designing an existing course, re-shaping assignments and pedagogy to address the current AI technologies challenges. This example can serve as a guideline for policies for institutions and teachers involved in teaching programming, aiming to maximize the benefits of AI tools while addressing the associated challenges and concerns.","Teaching Introduction to Programming in the times of AI: A case study of a course re-design The integration of AI tools into programming education has become increasingly prevalent in recent years, transforming the way programming is taught and learned. This paper provides a review of the state-of-the-art AI tools available for teaching and learning programming, particularly in the context of introductory courses. It highlights the challenges on course design, learning objectives, course delivery and formative and summative assessment, as well as the misuse of such tools by the students. We discuss ways of re-designing an existing course, re-shaping assignments and pedagogy to address the current AI technologies challenges. This example can serve as a guideline for policies for institutions and teachers involved in teaching programming, aiming to maximize the benefits of AI tools while addressing the associated challenges and concerns.",Education
Understanding the Impact of Seasonal Climate Change on Canadas Economy by Region and Sector,"To assess the impact of climate change on the Canadian economy, we investigate and model the relationship between seasonal climate variables and economic growth across provinces and economic sectors. We further provide projections of climate change impacts up to the year 2050, taking into account the diverse climate change patterns and economic conditions across Canada. Our results indicate that rising Fall temperature anomalies have a notable adverse impact on Canadian economic growth. Province-wide, Saskatchewan and Manitoba are anticipated to experience the most substantial declines, whereas British Columbia and the Maritime provinces will be less impacted. Industry-wide, Mining is projected to see the greatest benefits, while Agriculture and Manufacturing are projected to have the most significant downturns. The disparities of climate change effects between provinces and industries highlight the need for governments to tailor their policies accordingly, and offer targeted assistance to regions and industries that are particularly vulnerable in the face of climate change. Targeted approaches to climate change mitigation are likely to be more effective than one-size-fits-all policies for the whole economy.","Understanding the Impact of Seasonal Climate Change on Canadas Economy by Region and Sector To assess the impact of climate change on the Canadian economy, we investigate and model the relationship between seasonal climate variables and economic growth across provinces and economic sectors. We further provide projections of climate change impacts up to the year 2050, taking into account the diverse climate change patterns and economic conditions across Canada. Our results indicate that rising Fall temperature anomalies have a notable adverse impact on Canadian economic growth. Province-wide, Saskatchewan and Manitoba are anticipated to experience the most substantial declines, whereas British Columbia and the Maritime provinces will be less impacted. Industry-wide, Mining is projected to see the greatest benefits, while Agriculture and Manufacturing are projected to have the most significant downturns. The disparities of climate change effects between provinces and industries highlight the need for governments to tailor their policies accordingly, and offer targeted assistance to regions and industries that are particularly vulnerable in the face of climate change. Targeted approaches to climate change mitigation are likely to be more effective than one-size-fits-all policies for the whole economy.",Environment
Contests with sequential moves: An experimental study,"We study experimentally contests in which players make investment decisions sequentially, and information on prior investments is revealed between stages. Using a between-subject design, we consider all possible sequences in contests of three players and test two major comparative statics of the subgame-perfect Nash equilibrium: The positive effect of the number of stages on aggregate investment and earlier mover advantage. The former prediction is decidedly rejected, as we observe a reduction in aggregate investment when more sequential information disclosure stages are added to the contest. The evidence on earlier mover advantage is mixed but mostly does not support theory as well. Both predictions rely critically on large preemptive investment by first movers and accommodation by later movers, which does not materialize. Instead, later movers respond aggressively, and reciprocally, to first movers investments, while first movers learn to accommodate those responses.","Contests with sequential moves: An experimental study We study experimentally contests in which players make investment decisions sequentially, and information on prior investments is revealed between stages. Using a between-subject design, we consider all possible sequences in contests of three players and test two major comparative statics of the subgame-perfect Nash equilibrium: The positive effect of the number of stages on aggregate investment and earlier mover advantage. The former prediction is decidedly rejected, as we observe a reduction in aggregate investment when more sequential information disclosure stages are added to the contest. The evidence on earlier mover advantage is mixed but mostly does not support theory as well. Both predictions rely critically on large preemptive investment by first movers and accommodation by later movers, which does not materialize. Instead, later movers respond aggressively, and reciprocally, to first movers investments, while first movers learn to accommodate those responses.",Finance
Multilayer shallow-water model with stratification and shear,"The purpose of this paper is to present a shallow-water-type model with multiple inhomogeneous layers featuring variable linear velocity vertical shear and startificaion in horizontal space and time. This is achieved by writing the layer velocity and buoyancy fields as linear functions of depth, with coefficients that depend arbitrarily on horizontal position and time. The model is a generalization of Ripas (1995) single-layer model to an arbitrary number of layers. Unlike models with homogeneous layers the present model is able to represent thermodynamics processes driven by heat and freshwater fluxes through the surface or mixing processes resulting from fluid exchanges across contiguous layers. A model configuration with only one layer has been previously shown to provide: a very good representation of the exact vertical normal modes up to the first internal mode; an exact representation of long-perturbation (free boundary) baroclinic instability; and a very reasonable representation of short-perturbation (classical Eady) baroclinic instability. Here it is shown that substantially more accurate overall results with respect to single-layer calculations can be achieved by considering a stack of only a few layers. A similar behavior is found in ageostrophic (classical Stone) baroclinic instability by describing accurately the dependence of the solutions on the Richardson number with only two layers.","Multilayer shallow-water model with stratification and shear The purpose of this paper is to present a shallow-water-type model with multiple inhomogeneous layers featuring variable linear velocity vertical shear and startificaion in horizontal space and time. This is achieved by writing the layer velocity and buoyancy fields as linear functions of depth, with coefficients that depend arbitrarily on horizontal position and time. The model is a generalization of Ripas (1995) single-layer model to an arbitrary number of layers. Unlike models with homogeneous layers the present model is able to represent thermodynamics processes driven by heat and freshwater fluxes through the surface or mixing processes resulting from fluid exchanges across contiguous layers. A model configuration with only one layer has been previously shown to provide: a very good representation of the exact vertical normal modes up to the first internal mode; an exact representation of long-perturbation (free boundary) baroclinic instability; and a very reasonable representation of short-perturbation (classical Eady) baroclinic instability. Here it is shown that substantially more accurate overall results with respect to single-layer calculations can be achieved by considering a stack of only a few layers. A similar behavior is found in ageostrophic (classical Stone) baroclinic instability by describing accurately the dependence of the solutions on the Richardson number with only two layers.",Environment
Climate land use and other drivers impacts on island ecosystem services: a global review,"Islands are diversity hotspots and vulnerable to environmental degradation, climate variations, land use changes and societal crises. These factors can exhibit interactive impacts on ecosystem services. The study reviewed a large number of papers on the climate change-islands-ecosystem services topic worldwide. Potential inclusion of land use changes and other drivers of impacts on ecosystem services were sequentially also recorded. The study sought to investigate the impacts of climate change, land use change, and other non-climatic driver changes on island ecosystem services. Explanatory variables examined were divided into two categories: environmental variables and methodological ones. Environmental variables include sea zone geographic location, ecosystem, ecosystem services, climate, land use, other driver variables, Methodological variables include consideration of policy interventions, uncertainty assessment, cumulative effects of climate change, synergistic effects of climate change with land use change and other anthropogenic and environmental drivers, and the diversity of variables used in the analysis. Machine learning and statistical methods were used to analyze their effects on island ecosystem services. Negative climate change impacts on ecosystem services are better quantified by land use change or other non-climatic driver variables than by climate variables. The synergy of land use together with climate changes is modulating the impact outcome and critical for a better impact assessment. Analyzed together, there is little evidence of more pronounced for a specific sea zone, ecosystem, or ecosystem service. Climate change impacts may be underestimated due to the use of a single climate variable deployed in most studies. Policy interventions exhibit low classification accuracy in quantifying impacts indicating insufficient efficacy or integration in the studies.","Climate land use and other drivers impacts on island ecosystem services: a global review Islands are diversity hotspots and vulnerable to environmental degradation, climate variations, land use changes and societal crises. These factors can exhibit interactive impacts on ecosystem services. The study reviewed a large number of papers on the climate change-islands-ecosystem services topic worldwide. Potential inclusion of land use changes and other drivers of impacts on ecosystem services were sequentially also recorded. The study sought to investigate the impacts of climate change, land use change, and other non-climatic driver changes on island ecosystem services. Explanatory variables examined were divided into two categories: environmental variables and methodological ones. Environmental variables include sea zone geographic location, ecosystem, ecosystem services, climate, land use, other driver variables, Methodological variables include consideration of policy interventions, uncertainty assessment, cumulative effects of climate change, synergistic effects of climate change with land use change and other anthropogenic and environmental drivers, and the diversity of variables used in the analysis. Machine learning and statistical methods were used to analyze their effects on island ecosystem services. Negative climate change impacts on ecosystem services are better quantified by land use change or other non-climatic driver variables than by climate variables. The synergy of land use together with climate changes is modulating the impact outcome and critical for a better impact assessment. Analyzed together, there is little evidence of more pronounced for a specific sea zone, ecosystem, or ecosystem service. Climate change impacts may be underestimated due to the use of a single climate variable deployed in most studies. Policy interventions exhibit low classification accuracy in quantifying impacts indicating insufficient efficacy or integration in the studies.",Environment
DialMed: A Dataset for Dialogue-based Medication Recommendation,"Medication recommendation is a crucial task for intelligent healthcare systems. Previous studies mainly recommend medications with electronic health records (EHRs). However, some details of interactions between doctors and patients may be ignored or omitted in EHRs, which are essential for automatic medication recommendation. Therefore, we make the first attempt to recommend medications with the conversations between doctors and patients. In this work, we construct DIALMED, the first high-quality dataset for medical dialogue-based medication recommendation task. It contains 11,996 medical dialogues related to 16 common diseases from 3 departments and 70 corresponding common medications. Furthermore, we propose a Dialogue structure and Disease knowledge aware Network (DDN), where a QA Dialogue Graph mechanism is designed to model the dialogue structure and the knowledge graph is used to introduce external disease knowledge. The extensive experimental results demonstrate that the proposed method is a promising solution to recommend medications with medical dialogues. The dataset and code are available at https:github.comf-windowDialMed.","DialMed: A Dataset for Dialogue-based Medication Recommendation Medication recommendation is a crucial task for intelligent healthcare systems. Previous studies mainly recommend medications with electronic health records (EHRs). However, some details of interactions between doctors and patients may be ignored or omitted in EHRs, which are essential for automatic medication recommendation. Therefore, we make the first attempt to recommend medications with the conversations between doctors and patients. In this work, we construct DIALMED, the first high-quality dataset for medical dialogue-based medication recommendation task. It contains 11,996 medical dialogues related to 16 common diseases from 3 departments and 70 corresponding common medications. Furthermore, we propose a Dialogue structure and Disease knowledge aware Network (DDN), where a QA Dialogue Graph mechanism is designed to model the dialogue structure and the knowledge graph is used to introduce external disease knowledge. The extensive experimental results demonstrate that the proposed method is a promising solution to recommend medications with medical dialogues. The dataset and code are available at https:github.comf-windowDialMed.",Healthcare
Diversity and Arbitrage in a Regulatory Breakup Model,"In 1999 Robert Fernholz observed an inconsistency between the normative assumption of existence of an equivalent martingale measure (EMM) and the empirical reality of diversity in equity markets. We explore a method of imposing diversity on market models by a type of antitrust regulation that is compatible with EMMs. The regulatory procedure breaks up companies that become too large, while holding the total number of companies constant by imposing a simultaneous merge of other companies. The regulatory events are assumed to have no impact on portfolio values. As an example, regulation is imposed on a market model in which diversity is maintained via a log-pole in the drift of the largest company. The result is the removal of arbitrage opportunities from this market while maintaining the markets diversity.","Diversity and Arbitrage in a Regulatory Breakup Model In 1999 Robert Fernholz observed an inconsistency between the normative assumption of existence of an equivalent martingale measure (EMM) and the empirical reality of diversity in equity markets. We explore a method of imposing diversity on market models by a type of antitrust regulation that is compatible with EMMs. The regulatory procedure breaks up companies that become too large, while holding the total number of companies constant by imposing a simultaneous merge of other companies. The regulatory events are assumed to have no impact on portfolio values. As an example, regulation is imposed on a market model in which diversity is maintained via a log-pole in the drift of the largest company. The result is the removal of arbitrage opportunities from this market while maintaining the markets diversity.",Finance
Sequential optimizing investing strategy with neural networks,In this paper we propose an investing strategy based on neural network models combined with ideas from game-theoretic probability of Shafer and Vovk. Our proposed strategy uses parameter values of a neural network with the best performance until the previous round (trading day) for deciding the investment in the current round. We compare performance of our proposed strategy with various strategies including a strategy based on supervised neural network models and show that our procedure is competitive with other strategies.,Sequential optimizing investing strategy with neural networks In this paper we propose an investing strategy based on neural network models combined with ideas from game-theoretic probability of Shafer and Vovk. Our proposed strategy uses parameter values of a neural network with the best performance until the previous round (trading day) for deciding the investment in the current round. We compare performance of our proposed strategy with various strategies including a strategy based on supervised neural network models and show that our procedure is competitive with other strategies.,Finance
Diagnosis and Prediction of Market Rebounds in Financial Markets,"We introduce the concept of negative bubbles as the mirror image of standard financial bubbles, in which positive feedback mechanisms may lead to transient accelerating price falls. To model these negative bubbles, we adapt the Johansen-Ledoit-Sornette (JLS) model of rational expectation bubbles with a hazard rate describing the collective buying pressure of noise traders. The price fall occurring during a transient negative bubble can be interpreted as an effective random downpayment that rational agents accept to pay in the hope of profiting from the expected occurrence of a possible rally. We validate the model by showing that it has significant predictive power in identifying the times of major market rebounds. This result is obtained by using a general pattern recognition method which combines the information obtained at multiple times from a dynamical calibration of the JLS model. Error diagrams, Bayesian inference and trading strategies suggest that one can extract genuine information and obtain real skill from the calibration of negative bubbles with the JLS model. We conclude that negative bubbles are in general predictably associated with large rebounds or rallies, which are the mirror images of the crashes terminating standard bubbles.","Diagnosis and Prediction of Market Rebounds in Financial Markets We introduce the concept of negative bubbles as the mirror image of standard financial bubbles, in which positive feedback mechanisms may lead to transient accelerating price falls. To model these negative bubbles, we adapt the Johansen-Ledoit-Sornette (JLS) model of rational expectation bubbles with a hazard rate describing the collective buying pressure of noise traders. The price fall occurring during a transient negative bubble can be interpreted as an effective random downpayment that rational agents accept to pay in the hope of profiting from the expected occurrence of a possible rally. We validate the model by showing that it has significant predictive power in identifying the times of major market rebounds. This result is obtained by using a general pattern recognition method which combines the information obtained at multiple times from a dynamical calibration of the JLS model. Error diagrams, Bayesian inference and trading strategies suggest that one can extract genuine information and obtain real skill from the calibration of negative bubbles with the JLS model. We conclude that negative bubbles are in general predictably associated with large rebounds or rallies, which are the mirror images of the crashes terminating standard bubbles.",Finance
Incentives for Private Industrial Investment in historical perspective: the case of industrial promotion and investment promotion in Uruguay (1974-2010),"Using as a central instrument a new database, resulting from a compilation of historical administrative records, which covers the period 1974-2010, we can have new evidence on how industrial companies used tax benefits, and claim that these are decisive for the investment decision of the Uruguayan industrial companies during that period. The aforementioned findings served as a raw material to also affirm that the incentives to increase investment are factors that positively influence the level of economic activity and exports, and negatively on the unemployment rate.","Incentives for Private Industrial Investment in historical perspective: the case of industrial promotion and investment promotion in Uruguay (1974-2010) Using as a central instrument a new database, resulting from a compilation of historical administrative records, which covers the period 1974-2010, we can have new evidence on how industrial companies used tax benefits, and claim that these are decisive for the investment decision of the Uruguayan industrial companies during that period. The aforementioned findings served as a raw material to also affirm that the incentives to increase investment are factors that positively influence the level of economic activity and exports, and negatively on the unemployment rate.",Finance
"Precision Health Data: Requirements, Challenges and Existing Techniques for Data Security and Privacy","Precision health leverages information from various sources, including omics, lifestyle, environment, social media, medical records, and medical insurance claims to enable personalized care, prevent and predict illness, and precise treatments. It extensively uses sensing technologies (e.g., electronic health monitoring devices), computations (e.g., machine learning), and communication (e.g., interaction between the health data centers). As health data contain sensitive private information, including the identity of patient and carer and medical conditions of the patient, proper care is required at all times. Leakage of these private information affects the personal life, including bullying, high insurance premium, and loss of job due to the medical history. Thus, the security, privacy of and trust on the information are of utmost importance. Moreover, government legislation and ethics committees demand the security and privacy of healthcare data. Herein, in the light of precision health data security, privacy, ethical and regulatory requirements, finding the best methods and techniques for the utilization of the health data, and thus precision health is essential. In this regard, firstly, this paper explores the regulations, ethical guidelines around the world, and domain-specific needs. Then it presents the requirements and investigates the associated challenges. Secondly, this paper investigates secure and privacy-preserving machine learning methods suitable for the computation of precision health data along with their usage in relevant health projects. Finally, it illustrates the best available techniques for precision health data security and privacy with a conceptual system model that enables compliance, ethics clearance, consent management, medical innovations, and developments in the health domain.","Precision Health Data: Requirements, Challenges and Existing Techniques for Data Security and Privacy Precision health leverages information from various sources, including omics, lifestyle, environment, social media, medical records, and medical insurance claims to enable personalized care, prevent and predict illness, and precise treatments. It extensively uses sensing technologies (e.g., electronic health monitoring devices), computations (e.g., machine learning), and communication (e.g., interaction between the health data centers). As health data contain sensitive private information, including the identity of patient and carer and medical conditions of the patient, proper care is required at all times. Leakage of these private information affects the personal life, including bullying, high insurance premium, and loss of job due to the medical history. Thus, the security, privacy of and trust on the information are of utmost importance. Moreover, government legislation and ethics committees demand the security and privacy of healthcare data. Herein, in the light of precision health data security, privacy, ethical and regulatory requirements, finding the best methods and techniques for the utilization of the health data, and thus precision health is essential. In this regard, firstly, this paper explores the regulations, ethical guidelines around the world, and domain-specific needs. Then it presents the requirements and investigates the associated challenges. Secondly, this paper investigates secure and privacy-preserving machine learning methods suitable for the computation of precision health data along with their usage in relevant health projects. Finally, it illustrates the best available techniques for precision health data security and privacy with a conceptual system model that enables compliance, ethics clearance, consent management, medical innovations, and developments in the health domain.",Healthcare
"Overlapping Probabilities of Top Ranking Gene Lists, Hypergeometric Distribution, and Stringency of Gene Selection Criterion","When the same set of genes appear in two top ranking gene lists in two different studies, it is often of interest to estimate the probability for this being a chance event. This overlapping probability is well known to follow the hypergeometric distribution. Usually, the lengths of top-ranking gene lists are assumed to be fixed, by using a pre-set criterion on, e.g., p-value for the t-test. We investigate how overlapping probability changes with the gene selection criterion, or simply, with the length of the top-ranking gene lists. It is concluded that overlapping probability is indeed a function of the gene list length, and its statistical significance should be quoted in the context of gene selection criterion.","Overlapping Probabilities of Top Ranking Gene Lists, Hypergeometric Distribution, and Stringency of Gene Selection Criterion When the same set of genes appear in two top ranking gene lists in two different studies, it is often of interest to estimate the probability for this being a chance event. This overlapping probability is well known to follow the hypergeometric distribution. Usually, the lengths of top-ranking gene lists are assumed to be fixed, by using a pre-set criterion on, e.g., p-value for the t-test. We investigate how overlapping probability changes with the gene selection criterion, or simply, with the length of the top-ranking gene lists. It is concluded that overlapping probability is indeed a function of the gene list length, and its statistical significance should be quoted in the context of gene selection criterion.",Healthcare
Mobile Learning Culture and Effects in Higher Education,"Mobile learning through wireless enabled laptops (say, within a university campus) can make use of the learning management system that is already available through internet or intranet. Without restrictions within the four walls of computer labs or library, students can now access the learning resources anywhere in the campus where wireless access points or hotspots are located. We briefly investigate on the mobile learning benefits and eventually an analysis of the student perceptions on mobile learning is presented through a survey, to validate the m-learning benefits.","Mobile Learning Culture and Effects in Higher Education Mobile learning through wireless enabled laptops (say, within a university campus) can make use of the learning management system that is already available through internet or intranet. Without restrictions within the four walls of computer labs or library, students can now access the learning resources anywhere in the campus where wireless access points or hotspots are located. We briefly investigate on the mobile learning benefits and eventually an analysis of the student perceptions on mobile learning is presented through a survey, to validate the m-learning benefits.",Education
Virtual learning: possibilities and realization,"The virtual learning in University Education is the learning which is presented by set of integrated information and pedagogical technologies, in a process of interaction between subjects and objects as the virtual educational resources. This interaction characterize as the set of dialectically interconnected fields of human activity (intellectual, emotional and figurative, cultural, social). The virtual educational resources, possibility of their adaptation to student subjectivity, and realization in the conditions of University education are the main issues of this article.","Virtual learning: possibilities and realization The virtual learning in University Education is the learning which is presented by set of integrated information and pedagogical technologies, in a process of interaction between subjects and objects as the virtual educational resources. This interaction characterize as the set of dialectically interconnected fields of human activity (intellectual, emotional and figurative, cultural, social). The virtual educational resources, possibility of their adaptation to student subjectivity, and realization in the conditions of University education are the main issues of this article.",Education
Problem Evolution: A new approach to problem solving systems,"In this paper we present a novel tool to evaluate problem solving systems. Instead of using a system to solve a problem, we suggest using the problem to evaluate the system. By finding a numerical representation of a problems complexity, one can implement genetic algorithm to search for the most complex problem the given system can solve. This allows a comparison between different systems that solve the same set of problems. In this paper we implement this approach on pattern recognition neural networks to try and find the most complex pattern a given configuration can solve. The complexity of the pattern is calculated using linguistic complexity. The results demonstrate the power of the problem evolution approach in ranking different neural network configurations according to their pattern recognition abilities. Future research and implementations of this technique are also discussed.","Problem Evolution: A new approach to problem solving systems In this paper we present a novel tool to evaluate problem solving systems. Instead of using a system to solve a problem, we suggest using the problem to evaluate the system. By finding a numerical representation of a problems complexity, one can implement genetic algorithm to search for the most complex problem the given system can solve. This allows a comparison between different systems that solve the same set of problems. In this paper we implement this approach on pattern recognition neural networks to try and find the most complex pattern a given configuration can solve. The complexity of the pattern is calculated using linguistic complexity. The results demonstrate the power of the problem evolution approach in ranking different neural network configurations according to their pattern recognition abilities. Future research and implementations of this technique are also discussed.",Technology
Physical basis of SATRO - a new method for analysis of the cardiac muscle depolarisation,"On the basis of the model of the current flow through a single fibre, changes in the electric charge density over the myocardium are described. With the use of relevant analytic formulae, supported with numerical calculations, the distribution and time dependencies of electric potentials on the surface of the thorax have been determined. The results obtained are compared with empirical data. A strong correlation between the theoretical predictions and the experimental data has been obtained. The model in question permits examination of instantaneous potentials resulting from electrical activation of particular segments of the cardiac muscle.","Physical basis of SATRO - a new method for analysis of the cardiac muscle depolarisation On the basis of the model of the current flow through a single fibre, changes in the electric charge density over the myocardium are described. With the use of relevant analytic formulae, supported with numerical calculations, the distribution and time dependencies of electric potentials on the surface of the thorax have been determined. The results obtained are compared with empirical data. A strong correlation between the theoretical predictions and the experimental data has been obtained. The model in question permits examination of instantaneous potentials resulting from electrical activation of particular segments of the cardiac muscle.",Healthcare
The first passage time problem for mixed-exponential jump processes with applications in insurance and finance,"This paper stidies the first passage times to constant boundaries for mixed-exponential jump diffusion processes. Explicit solutions of the Laplace transforms of the distribution of the first passage times, the joint distribution of the first passage times and undershoot (overshoot) are obtained. As applications, we present explicit expression of the Gerber-Shiu functions for surplus processes with two-sided jumps, present the analytical solutions for popular path-dependent options such as lookback and barrier options in terms of Laplace transforms and give a closed-form expression on the price of the zero-coupon bond under a structural credit risk model with jumps.","The first passage time problem for mixed-exponential jump processes with applications in insurance and finance This paper stidies the first passage times to constant boundaries for mixed-exponential jump diffusion processes. Explicit solutions of the Laplace transforms of the distribution of the first passage times, the joint distribution of the first passage times and undershoot (overshoot) are obtained. As applications, we present explicit expression of the Gerber-Shiu functions for surplus processes with two-sided jumps, present the analytical solutions for popular path-dependent options such as lookback and barrier options in terms of Laplace transforms and give a closed-form expression on the price of the zero-coupon bond under a structural credit risk model with jumps.",Finance
"Building a Smart, Secured and Sustainable Campus: A Self-Powered Wireless Network for Environmental Monitoring","The objective of this study is to propose a self-powered wireless network solution that utilizes strategically deployed wireless sensor nodes within buildings for environmental data collection, while integrating advanced security measures and sustainable power management strategies.","Building a Smart, Secured and Sustainable Campus: A Self-Powered Wireless Network for Environmental Monitoring The objective of this study is to propose a self-powered wireless network solution that utilizes strategically deployed wireless sensor nodes within buildings for environmental data collection, while integrating advanced security measures and sustainable power management strategies.",Environment
Comprehend Medical: a Named Entity Recognition and Relationship Extraction Web Service,"Comprehend Medical is a stateless and Health Insurance Portability and Accountability Act (HIPAA) eligible Named Entity Recognition (NER) and Relationship Extraction (RE) service launched under Amazon Web Services (AWS) trained using state-of-the-art deep learning models. Contrary to many existing open source tools, Comprehend Medical is scalable and does not require steep learning curve, dependencies, pipeline configurations, or installations. Currently, Comprehend Medical performs NER in five medical categories: Anatomy, Medical Condition, Medications, Protected Health Information (PHI) and Treatment, Test and Procedure (TTP). Additionally, the service provides relationship extraction for the detected entities as well as contextual information such as negation and temporality in the form of traits. Comprehend Medical provides two Application Programming Interfaces (API): 1) the NERe API which returns all the extracted named entities, their traits and the relationships between them and 2) the PHId API which returns just the protected health information contained in the text. Furthermore, Comprehend Medical is accessible through AWS Console, Java and Python Software Development Kit (SDK), making it easier for non-developers and developers to use.","Comprehend Medical: a Named Entity Recognition and Relationship Extraction Web Service Comprehend Medical is a stateless and Health Insurance Portability and Accountability Act (HIPAA) eligible Named Entity Recognition (NER) and Relationship Extraction (RE) service launched under Amazon Web Services (AWS) trained using state-of-the-art deep learning models. Contrary to many existing open source tools, Comprehend Medical is scalable and does not require steep learning curve, dependencies, pipeline configurations, or installations. Currently, Comprehend Medical performs NER in five medical categories: Anatomy, Medical Condition, Medications, Protected Health Information (PHI) and Treatment, Test and Procedure (TTP). Additionally, the service provides relationship extraction for the detected entities as well as contextual information such as negation and temporality in the form of traits. Comprehend Medical provides two Application Programming Interfaces (API): 1) the NERe API which returns all the extracted named entities, their traits and the relationships between them and 2) the PHId API which returns just the protected health information contained in the text. Furthermore, Comprehend Medical is accessible through AWS Console, Java and Python Software Development Kit (SDK), making it easier for non-developers and developers to use.",Healthcare
Analytic results and weighted Monte Carlo simulations for CDO pricing,"We explore the possibilities of importance sampling in the Monte Carlo pricing of a structured credit derivative referred to as Collateralized Debt Obligation (CDO). Modeling a CDO contract is challenging, since it depends on a pool of (typically about 100) assets, Monte Carlo simulations are often the only feasible approach to pricing. Variance reduction techniques are therefore of great importance. This paper presents an exact analytic solution using Laplace-transform and MC importance sampling results for an easily tractable intensity-based model of the CDO, namely the compound Poissonian. Furthermore analytic formulae are derived for the reweighting efficiency. The computational gain is appealing, nevertheless, even in this basic scheme, a phase transition can be found, rendering some parameter regimes out of reach. A model-independent transform approach is also presented for CDO pricing.","Analytic results and weighted Monte Carlo simulations for CDO pricing We explore the possibilities of importance sampling in the Monte Carlo pricing of a structured credit derivative referred to as Collateralized Debt Obligation (CDO). Modeling a CDO contract is challenging, since it depends on a pool of (typically about 100) assets, Monte Carlo simulations are often the only feasible approach to pricing. Variance reduction techniques are therefore of great importance. This paper presents an exact analytic solution using Laplace-transform and MC importance sampling results for an easily tractable intensity-based model of the CDO, namely the compound Poissonian. Furthermore analytic formulae are derived for the reweighting efficiency. The computational gain is appealing, nevertheless, even in this basic scheme, a phase transition can be found, rendering some parameter regimes out of reach. A model-independent transform approach is also presented for CDO pricing.",Finance
From Objectives to Questions: A Planning-based Framework for Educational Mathematical Question Generation,"Automatically generating high-quality mathematical problems that align with educational objectives is a crucial task in NLP-based educational technology. Traditional generation methods focus primarily on textual quality, but they often overlook educational objectives. Moreover, these methods address only single-dimensional, simple question generation, failing to meet complex, multifaceted educational requirements. To address these challenges, we constructed and annotated EduMath, a dataset of 16k mathematical questions with multi-dimensional educational objectives. Based on this dataset, we developed EQGEVAL, which incorporates three evaluation dimensions and is designed to assess the ability of models to generate educational questions. Drawing inspiration from teachers problem design processes, we propose the Educational Question Planning with self-Reflection (EQPR) method for educational mathematical question generation, following a plan-evaluate-optimize approach. Specifically, by combining planning algorithm based on Monte Carlo Tree Search with the generative capabilities of Large Language Models, we continuously optimize questions through iterative feedback. This self-optimization mechanism ensures that the generated questions both fit the educational context and strategically achieve specific basic educational objectives. Through extensive experiments based on EQGEVAL, we have demonstrated that EQPR achieves significant improvements in generating questions that meet multi-dimensional educational objectives.","From Objectives to Questions: A Planning-based Framework for Educational Mathematical Question Generation Automatically generating high-quality mathematical problems that align with educational objectives is a crucial task in NLP-based educational technology. Traditional generation methods focus primarily on textual quality, but they often overlook educational objectives. Moreover, these methods address only single-dimensional, simple question generation, failing to meet complex, multifaceted educational requirements. To address these challenges, we constructed and annotated EduMath, a dataset of 16k mathematical questions with multi-dimensional educational objectives. Based on this dataset, we developed EQGEVAL, which incorporates three evaluation dimensions and is designed to assess the ability of models to generate educational questions. Drawing inspiration from teachers problem design processes, we propose the Educational Question Planning with self-Reflection (EQPR) method for educational mathematical question generation, following a plan-evaluate-optimize approach. Specifically, by combining planning algorithm based on Monte Carlo Tree Search with the generative capabilities of Large Language Models, we continuously optimize questions through iterative feedback. This self-optimization mechanism ensures that the generated questions both fit the educational context and strategically achieve specific basic educational objectives. Through extensive experiments based on EQGEVAL, we have demonstrated that EQPR achieves significant improvements in generating questions that meet multi-dimensional educational objectives.",Education
An Insurance Contract Design to Boost Storage Participation in the Electricity Market,"Energy storage technologies are key to improving grid flexibility in the presence of increasing amounts of intermittent renewable generation. We propose an insurance contract that suitably compensates energy storage systems for providing flexibility. Such a contract provides a wider range of market opportunities for these systems while also incentivizing higher renewable penetration in the grid. We consider a day-ahead market in which generators, including renewables and storage owners, bid to be scheduled for the next operating day. Due to production uncertainty, renewable generators may be unable to meet their day-ahead production schedule, and thus be subject to a penalty. As a hedge against these penalties, we propose an insurance contract between a renewable producer and a storage owner, in which the storage reserves some energy to be used in case of renewable shortfalls. We show that such a contract incentivizes the renewable player to bid higher, thus increasing renewable participation in the electricity mix. It also provides an extra source of revenue for storage owners that may not be profitable with a purely arbitrage-based strategy in the day-ahead market. Further, we prove this contract is economically beneficial for both players. We validate our analysis through two case studies.","An Insurance Contract Design to Boost Storage Participation in the Electricity Market Energy storage technologies are key to improving grid flexibility in the presence of increasing amounts of intermittent renewable generation. We propose an insurance contract that suitably compensates energy storage systems for providing flexibility. Such a contract provides a wider range of market opportunities for these systems while also incentivizing higher renewable penetration in the grid. We consider a day-ahead market in which generators, including renewables and storage owners, bid to be scheduled for the next operating day. Due to production uncertainty, renewable generators may be unable to meet their day-ahead production schedule, and thus be subject to a penalty. As a hedge against these penalties, we propose an insurance contract between a renewable producer and a storage owner, in which the storage reserves some energy to be used in case of renewable shortfalls. We show that such a contract incentivizes the renewable player to bid higher, thus increasing renewable participation in the electricity mix. It also provides an extra source of revenue for storage owners that may not be profitable with a purely arbitrage-based strategy in the day-ahead market. Further, we prove this contract is economically beneficial for both players. We validate our analysis through two case studies.",Environment
Lagrangian turbulence in the Adriatic Sea as computed from drifter data: effects of inhomogeneity and nonstationarity,"The properties of mesoscale Lagrangian turbulence in the Adriatic Sea are studied from a drifter data set spanning 1990-1999, focusing on the role of inhomogeneity and nonstationarity. A preliminary study is performed on the dependence of the turbulent velocity statistics on bin averaging, and a preferential bin scale of 0.25 is chosen. Comparison with independent estimates obtained using an optimized spline technique confirms this choice. Three main regions are identified where the velocity statistics are approximately homogeneous: the two boundary currents, West (East) Adriatic Current, WAC (EAC), and the southern central gyre, CG. The CG region is found to be characterized by symmetric probability density function of velocity, approximately exponential autocorrelations and well defined integral quantities such as di usivity and time scale. The boundary regions, instead, are significantly asymmetric with skewness indicating preferential events in the direction of the mean flow. The autocorrelation in the along mean flow direction is characterized by two time scales, with a secondary exponential with slow decay time of 11-12 days particularly evident in the EAC region. Seasonal partitioning of the data shows that this secondary scale is especially prominent in the summer-fall season. Possible physical explanations for the secondary scale are discussed in terms of low frequency fluctuations of forcings and in terms of mean flow curvature inducing fluctuations in the particle trajectories. Consequences of the results for transport modelling in the Adriatic Sea are discussed.","Lagrangian turbulence in the Adriatic Sea as computed from drifter data: effects of inhomogeneity and nonstationarity The properties of mesoscale Lagrangian turbulence in the Adriatic Sea are studied from a drifter data set spanning 1990-1999, focusing on the role of inhomogeneity and nonstationarity. A preliminary study is performed on the dependence of the turbulent velocity statistics on bin averaging, and a preferential bin scale of 0.25 is chosen. Comparison with independent estimates obtained using an optimized spline technique confirms this choice. Three main regions are identified where the velocity statistics are approximately homogeneous: the two boundary currents, West (East) Adriatic Current, WAC (EAC), and the southern central gyre, CG. The CG region is found to be characterized by symmetric probability density function of velocity, approximately exponential autocorrelations and well defined integral quantities such as di usivity and time scale. The boundary regions, instead, are significantly asymmetric with skewness indicating preferential events in the direction of the mean flow. The autocorrelation in the along mean flow direction is characterized by two time scales, with a secondary exponential with slow decay time of 11-12 days particularly evident in the EAC region. Seasonal partitioning of the data shows that this secondary scale is especially prominent in the summer-fall season. Possible physical explanations for the secondary scale are discussed in terms of low frequency fluctuations of forcings and in terms of mean flow curvature inducing fluctuations in the particle trajectories. Consequences of the results for transport modelling in the Adriatic Sea are discussed.",Environment
"The Intellectual Property Protection System of the Foreign Investment Law: Basic Structure, Motivation and Game Logic","The intellectual property protection system constructed by Chinas Foreign Investment Law has opened a new phase of rule of law protection of intellectual property rights for foreign-invested enterprises, which is an important institutional support indispensable for optimizing the business environment under the rule of law.The development of the regime was influenced by the major concerns of investors home countries, the innovation-driven development strategy, and the trend towards a high level of stringent protection of international intellectual property and investment rules.In addition, there is a latent game of interests between multiple subjects, which can be analyzed by constructing two standard formal game models according to legal game theory.The first game model aims to compare and analyze the gains and losses of China and Indias IPR protection system for foreign-invested enterprises to attract foreign investment.The second game model is designed to analyze the benefits of China and foreign investors under their respective possible behaviors before and after the inclusion of IPR protection provisions in the Foreign Investment Law, with the optimal solution being a moderately cautious strategy for foreign investors and a strict enforcement strategy for China.","The Intellectual Property Protection System of the Foreign Investment Law: Basic Structure, Motivation and Game Logic The intellectual property protection system constructed by Chinas Foreign Investment Law has opened a new phase of rule of law protection of intellectual property rights for foreign-invested enterprises, which is an important institutional support indispensable for optimizing the business environment under the rule of law.The development of the regime was influenced by the major concerns of investors home countries, the innovation-driven development strategy, and the trend towards a high level of stringent protection of international intellectual property and investment rules.In addition, there is a latent game of interests between multiple subjects, which can be analyzed by constructing two standard formal game models according to legal game theory.The first game model aims to compare and analyze the gains and losses of China and Indias IPR protection system for foreign-invested enterprises to attract foreign investment.The second game model is designed to analyze the benefits of China and foreign investors under their respective possible behaviors before and after the inclusion of IPR protection provisions in the Foreign Investment Law, with the optimal solution being a moderately cautious strategy for foreign investors and a strict enforcement strategy for China.",Finance
Variable selection from random forests: application to gene expression data,"Random forest is a classification algorithm well suited for microarray data: it shows excellent performance even when most predictive variables are noise, can be used when the number of variables is much larger than the number of observations, and returns measures of variable importance. Thus, it is important to understand the performance of random forest with microarray data and its use for gene selection. We first show the effects of changes in parameters of random forest on the prediction error. Then we present an approach for gene selection that uses measures of variable importance and error rate, and is targeted towards the selection of small sets of genes. Using simulated and real microarray data, we show that the gene selection procedure yields small sets of genes while preserving predictive accuracy. Availability: All code is available as an R package, varSelRF, from CRAN, http:cran.r-project.orgsrccontribPACKAGES.html, or from the supplementary material page. Supplementary information: http:ligarto.orgrdiazPapersrfVSrandomForestVarSel.html","Variable selection from random forests: application to gene expression data Random forest is a classification algorithm well suited for microarray data: it shows excellent performance even when most predictive variables are noise, can be used when the number of variables is much larger than the number of observations, and returns measures of variable importance. Thus, it is important to understand the performance of random forest with microarray data and its use for gene selection. We first show the effects of changes in parameters of random forest on the prediction error. Then we present an approach for gene selection that uses measures of variable importance and error rate, and is targeted towards the selection of small sets of genes. Using simulated and real microarray data, we show that the gene selection procedure yields small sets of genes while preserving predictive accuracy. Availability: All code is available as an R package, varSelRF, from CRAN, http:cran.r-project.orgsrccontribPACKAGES.html, or from the supplementary material page. Supplementary information: http:ligarto.orgrdiazPapersrfVSrandomForestVarSel.html",Healthcare
Mapping weblog communities,"Websites of a particular class form increasingly complex networks, and new tools are needed to map and understand them. A way of visualizing this complex network is by mapping it. A map highlights which members of the community have similar interests, and reveals the underlying social network. In this paper, we will map a network of websites using Kohonens self-organizing map (SOM), a neural-net like method generally used for clustering and visualization of complex data sets. The set of websites considered has been the Blogalia weblog hosting site (based at http:www.blogalia.com), a thriving community of around 200 members, created in January 2002. In this paper we show how SOM discovers interesting community features, its relation with other community-discovering algorithms, and the way it highlights the set of communities formed over the network.","Mapping weblog communities Websites of a particular class form increasingly complex networks, and new tools are needed to map and understand them. A way of visualizing this complex network is by mapping it. A map highlights which members of the community have similar interests, and reveals the underlying social network. In this paper, we will map a network of websites using Kohonens self-organizing map (SOM), a neural-net like method generally used for clustering and visualization of complex data sets. The set of websites considered has been the Blogalia weblog hosting site (based at http:www.blogalia.com), a thriving community of around 200 members, created in January 2002. In this paper we show how SOM discovers interesting community features, its relation with other community-discovering algorithms, and the way it highlights the set of communities formed over the network.",Technology
Simple method to eliminate blur based on Lane and Bates algorithm,A simple search method for finding a blur convolved in a given image is presented. The method can be easily extended to a large blur. The method has been experimentally tested with a model blurred image.,Simple method to eliminate blur based on Lane and Bates algorithm A simple search method for finding a blur convolved in a given image is presented. The method can be easily extended to a large blur. The method has been experimentally tested with a model blurred image.,Technology
Lecture Notes on Grid Modeling of Renewable Energy,"These lecture notes provide a comprehensive guide on Grid Modeling of Renewable Energy, offering a foundational overview of power system network modeling, power flow, and load flow algorithms critical for electrical and renewable energy engineering. Key topics include steady-state, dynamic, and frequency domain models, with a particular focus on renewable energy integration, simulation techniques, and their effects on grid stability and power quality. Practical examples using Matpower and Pandapower tools are included to reinforce concepts, ensuring that students gain hands-on experience in modeling and analyzing modern energy systems under variable conditions.","Lecture Notes on Grid Modeling of Renewable Energy These lecture notes provide a comprehensive guide on Grid Modeling of Renewable Energy, offering a foundational overview of power system network modeling, power flow, and load flow algorithms critical for electrical and renewable energy engineering. Key topics include steady-state, dynamic, and frequency domain models, with a particular focus on renewable energy integration, simulation techniques, and their effects on grid stability and power quality. Practical examples using Matpower and Pandapower tools are included to reinforce concepts, ensuring that students gain hands-on experience in modeling and analyzing modern energy systems under variable conditions.",Environment
Factor Temporal Prognosis of Tick-Borne Encephalitis Foci Functioning on the South of Russian Far East,A method of temporal factor prognosis of TE (tick-borne encephalitis) infection has been developed. The high precision of the prognosis results for a number of geographical regions of Primorsky Krai has been achieved. The method can be applied not only to epidemiological research but also to others.,Factor Temporal Prognosis of Tick-Borne Encephalitis Foci Functioning on the South of Russian Far East A method of temporal factor prognosis of TE (tick-borne encephalitis) infection has been developed. The high precision of the prognosis results for a number of geographical regions of Primorsky Krai has been achieved. The method can be applied not only to epidemiological research but also to others.,Technology
Learning from Physics Education Research: Lessons for Economics Education,"We believe that economists have much to learn from educational research practices and related pedagogical innovations in other disciplines, in particular physics education. In this paper we identify three key features of physics education research that distinguish it from economics education research - (1) the intentional grounding of physics education research in learning science principles, (2) a shared conceptual research framework focused on how students learn physics concepts, and (3) a cumulative process of knowledge-building in the discipline - and describe their influence on new teaching pedagogies, instructional activities, and curricular design in physics education. In addition, we highlight four specific examples of successful pedagogical innovations drawn from physics education - context-rich problems, concept tests, just-in-time teaching, and interactive lecture demonstrations - and illustrate how these practices can be adapted for economic education.","Learning from Physics Education Research: Lessons for Economics Education We believe that economists have much to learn from educational research practices and related pedagogical innovations in other disciplines, in particular physics education. In this paper we identify three key features of physics education research that distinguish it from economics education research - (1) the intentional grounding of physics education research in learning science principles, (2) a shared conceptual research framework focused on how students learn physics concepts, and (3) a cumulative process of knowledge-building in the discipline - and describe their influence on new teaching pedagogies, instructional activities, and curricular design in physics education. In addition, we highlight four specific examples of successful pedagogical innovations drawn from physics education - context-rich problems, concept tests, just-in-time teaching, and interactive lecture demonstrations - and illustrate how these practices can be adapted for economic education.",Education
EduPlanner: LLM-Based Multi-Agent Systems for Customized and Intelligent Instructional Design,"Large Language Models (LLMs) have significantly advanced smart education in the Artificial General Intelligence (AGI) era. A promising application lies in the automatic generalization of instructional design for curriculum and learning activities, focusing on two key aspects: (1) Customized Generation: generating niche-targeted teaching content based on students varying learning abilities and states, and (2) Intelligent Optimization: iteratively optimizing content based on feedback from learning effectiveness or test scores. Currently, a single large LLM cannot effectively manage the entire process, posing a challenge for designing intelligent teaching plans. To address these issues, we developed EduPlanner, an LLM-based multi-agent system comprising an evaluator agent, an optimizer agent, and a question analyst, working in adversarial collaboration to generate customized and intelligent instructional design for curriculum and learning activities. Taking mathematics lessons as our example, EduPlanner employs a novel Skill-Tree structure to accurately model the background mathematics knowledge of student groups, personalizing instructional design for curriculum and learning activities according to students knowledge levels and learning abilities. Additionally, we introduce the CIDDP, an LLM-based five-dimensional evaluation module encompassing clarity, Integrity, Depth, Practicality, and Pertinence, to comprehensively assess mathematics lesson plan quality and bootstrap intelligent optimization. Experiments conducted on the GSM8K and Algebra datasets demonstrate that EduPlanner excels in evaluating and optimizing instructional design for curriculum and learning activities. Ablation studies further validate the significance and effectiveness of each component within the framework. Our code is publicly available at https:github.comZc0812Edu_Planner","EduPlanner: LLM-Based Multi-Agent Systems for Customized and Intelligent Instructional Design Large Language Models (LLMs) have significantly advanced smart education in the Artificial General Intelligence (AGI) era. A promising application lies in the automatic generalization of instructional design for curriculum and learning activities, focusing on two key aspects: (1) Customized Generation: generating niche-targeted teaching content based on students varying learning abilities and states, and (2) Intelligent Optimization: iteratively optimizing content based on feedback from learning effectiveness or test scores. Currently, a single large LLM cannot effectively manage the entire process, posing a challenge for designing intelligent teaching plans. To address these issues, we developed EduPlanner, an LLM-based multi-agent system comprising an evaluator agent, an optimizer agent, and a question analyst, working in adversarial collaboration to generate customized and intelligent instructional design for curriculum and learning activities. Taking mathematics lessons as our example, EduPlanner employs a novel Skill-Tree structure to accurately model the background mathematics knowledge of student groups, personalizing instructional design for curriculum and learning activities according to students knowledge levels and learning abilities. Additionally, we introduce the CIDDP, an LLM-based five-dimensional evaluation module encompassing clarity, Integrity, Depth, Practicality, and Pertinence, to comprehensively assess mathematics lesson plan quality and bootstrap intelligent optimization. Experiments conducted on the GSM8K and Algebra datasets demonstrate that EduPlanner excels in evaluating and optimizing instructional design for curriculum and learning activities. Ablation studies further validate the significance and effectiveness of each component within the framework. Our code is publicly available at https:github.comZc0812Edu_Planner",Education
White Paper: The Generative Education (GenEd) Framework,"The Generative Education (GenEd) Framework explores the transition from Large Language Models (LLMs) to Large Multimodal Models (LMMs) in education, envisioning a harmonious relationship between AI and educators to enhance learning experiences. This paper delves into the potential of LMMs to create personalized, interactive, and emotionally-aware learning environments. Through addressing the Two-Sigma problem and the introduction of a conceptual product named Harmony, the narrative emphasizes educator development, adapting policy frameworks, and fostering cross-sector collaboration to realize the envisioned AI-enhanced education landscape. The discussion underscores the urgency for proactive adaptation amidst AIs evolution, offering a pragmatic roadmap to navigate the technical, ethical, and policy intricacies of integrating AI in education.","White Paper: The Generative Education (GenEd) Framework The Generative Education (GenEd) Framework explores the transition from Large Language Models (LLMs) to Large Multimodal Models (LMMs) in education, envisioning a harmonious relationship between AI and educators to enhance learning experiences. This paper delves into the potential of LMMs to create personalized, interactive, and emotionally-aware learning environments. Through addressing the Two-Sigma problem and the introduction of a conceptual product named Harmony, the narrative emphasizes educator development, adapting policy frameworks, and fostering cross-sector collaboration to realize the envisioned AI-enhanced education landscape. The discussion underscores the urgency for proactive adaptation amidst AIs evolution, offering a pragmatic roadmap to navigate the technical, ethical, and policy intricacies of integrating AI in education.",Education
Computing with CodeRunner at Coventry University: Automated summative assessment of Python and C code,CodeRunner is a free open-source Moodle plugin for automatically marking student code. We describe our experience using CodeRunner for summative assessment in our first year undergraduate programming curriculum at Coventry University. We use it to assess both Python3 and C14 code (CodeRunner supports other languages also). We give examples of our questions and report on how key metrics have changed following its use at Coventry.,Computing with CodeRunner at Coventry University: Automated summative assessment of Python and C code CodeRunner is a free open-source Moodle plugin for automatically marking student code. We describe our experience using CodeRunner for summative assessment in our first year undergraduate programming curriculum at Coventry University. We use it to assess both Python3 and C14 code (CodeRunner supports other languages also). We give examples of our questions and report on how key metrics have changed following its use at Coventry.,Education
Tipping elements and climate-economic shocks: Pathways toward integrated assessment,"The literature on the costs of climate change often draws a link between climatic tipping points and large economic shocks, frequently called catastrophes. The use of the phrase tipping points in this context can be misleading. In popular and social scientific discourse, tipping points involve abrupt state changes. For some climatic tipping points, the commitment to a state change may occur abruptly, but the change itself may be rate-limited and take centuries or longer to realize. Additionally, the connection between climatic tipping points and economic losses is tenuous, though emerging empirical and process-model-based tools provide pathways for investigating it. We propose terminology to clarify the distinction between tipping points in the popular sense, the critical thresholds exhibited by climatic and social tipping elements, and economic shocks. The last may be associated with tipping elements, gradual climate change, or non-climatic triggers. We illustrate our proposed distinctions by surveying the literature on climatic tipping elements, climatically sensitive social tipping elements, and climate-economic shocks, and we propose a research agenda to advance the integrated assessment of all three.","Tipping elements and climate-economic shocks: Pathways toward integrated assessment The literature on the costs of climate change often draws a link between climatic tipping points and large economic shocks, frequently called catastrophes. The use of the phrase tipping points in this context can be misleading. In popular and social scientific discourse, tipping points involve abrupt state changes. For some climatic tipping points, the commitment to a state change may occur abruptly, but the change itself may be rate-limited and take centuries or longer to realize. Additionally, the connection between climatic tipping points and economic losses is tenuous, though emerging empirical and process-model-based tools provide pathways for investigating it. We propose terminology to clarify the distinction between tipping points in the popular sense, the critical thresholds exhibited by climatic and social tipping elements, and economic shocks. The last may be associated with tipping elements, gradual climate change, or non-climatic triggers. We illustrate our proposed distinctions by surveying the literature on climatic tipping elements, climatically sensitive social tipping elements, and climate-economic shocks, and we propose a research agenda to advance the integrated assessment of all three.",Environment
The impact of surplus sharing on the outcomes of specific investments under negotiated transfer pricing: An agent-based simulation with fuzzy Q-learning agents,"This paper focuses on specific investments under negotiated transfer pricing. Reasons for transfer pricing studies are primarily to find conditions that maximize the firms overall profit, especially in cases with bilateral trading problems with specific investments. However, the transfer pricing problem has been developed in the context where managers are fully individual rational utility maximizers. The underlying assumptions are rather heroic and, in particular, how managers process information under uncertainty, do not perfectly match with human decision-making behavior. Therefore, this paper relaxes key assumptions and studies whether cognitively bounded agents achieve the same results as fully rational utility maximizers and, in particular, whether the recommendations on managerial-compensation arrangements and bargaining infrastructures are designed to maximize headquarters profit in such a setting. Based on an agent-based simulation with fuzzy Q-learning agents, it is shown that in case of symmetric marginal cost parameters, myopic fuzzy Q-learning agents invest only as much as in the classic hold-up problem, while non-myopic fuzzy Q-learning agents invest optimally. However, in scenarios with non-symmetric marginal cost parameters, a deviation from the previously recommended surplus sharing rules can lead to higher investment decisions and, thus, to an increase in the firms overall profit.","The impact of surplus sharing on the outcomes of specific investments under negotiated transfer pricing: An agent-based simulation with fuzzy Q-learning agents This paper focuses on specific investments under negotiated transfer pricing. Reasons for transfer pricing studies are primarily to find conditions that maximize the firms overall profit, especially in cases with bilateral trading problems with specific investments. However, the transfer pricing problem has been developed in the context where managers are fully individual rational utility maximizers. The underlying assumptions are rather heroic and, in particular, how managers process information under uncertainty, do not perfectly match with human decision-making behavior. Therefore, this paper relaxes key assumptions and studies whether cognitively bounded agents achieve the same results as fully rational utility maximizers and, in particular, whether the recommendations on managerial-compensation arrangements and bargaining infrastructures are designed to maximize headquarters profit in such a setting. Based on an agent-based simulation with fuzzy Q-learning agents, it is shown that in case of symmetric marginal cost parameters, myopic fuzzy Q-learning agents invest only as much as in the classic hold-up problem, while non-myopic fuzzy Q-learning agents invest optimally. However, in scenarios with non-symmetric marginal cost parameters, a deviation from the previously recommended surplus sharing rules can lead to higher investment decisions and, thus, to an increase in the firms overall profit.",Finance
Energy Performance Analysis of Distributed Renewables: Pacific Northwest Smart Grid Demonstration,"The Pacific Northwest Smart Grid Demonstration was an electricity grid modernization project conducted in the Northwest U.S. This paper presents the analysis of renewable generation at the Renewable Energy Park located in the City of Ellensburg, WA. The community energy park concept is an intriguing model for community investment in renewable resources,but the lessons in this paper should be considered.","Energy Performance Analysis of Distributed Renewables: Pacific Northwest Smart Grid Demonstration The Pacific Northwest Smart Grid Demonstration was an electricity grid modernization project conducted in the Northwest U.S. This paper presents the analysis of renewable generation at the Renewable Energy Park located in the City of Ellensburg, WA. The community energy park concept is an intriguing model for community investment in renewable resources,but the lessons in this paper should be considered.",Environment
A central limit theorem for Latin hypercube sampling with dependence and application to exotic basket option pricing,"We consider the problem of estimating mathbbE f(U1, ldots, Ud), where (U1, ldots, Ud) denotes a random vector with uniformly distributed marginals. In general, Latin hypercube sampling (LHS) is a powerful tool for solving this kind of high-dimensional numerical integration problem. In the case of dependent components of the random vector (U1, ldots, Ud) one can achieve more accurate results by using Latin hypercube sampling with dependence (LHSD). We state a central limit theorem for the d-dimensional LHSD estimator, by this means generalising a result of Packham and Schmidt. Furthermore we give conditions on the function f and the distribution of (U1, ldots, Ud) under which a reduction of variance can be achieved. Finally we compare the effectiveness of Monte Carlo and LHSD estimators numerically in exotic basket option pricing problems.","A central limit theorem for Latin hypercube sampling with dependence and application to exotic basket option pricing We consider the problem of estimating mathbbE f(U1, ldots, Ud), where (U1, ldots, Ud) denotes a random vector with uniformly distributed marginals. In general, Latin hypercube sampling (LHS) is a powerful tool for solving this kind of high-dimensional numerical integration problem. In the case of dependent components of the random vector (U1, ldots, Ud) one can achieve more accurate results by using Latin hypercube sampling with dependence (LHSD). We state a central limit theorem for the d-dimensional LHSD estimator, by this means generalising a result of Packham and Schmidt. Furthermore we give conditions on the function f and the distribution of (U1, ldots, Ud) under which a reduction of variance can be achieved. Finally we compare the effectiveness of Monte Carlo and LHSD estimators numerically in exotic basket option pricing problems.",Finance
The Financial Market of Environmental Indices,"This paper introduces the concept of a global financial market for environmental indices, addressing sustainability concerns and aiming to attract institutional investors. Risk mitigation measures are implemented to manage inherent risks associated with investments in this new financial market. We monetize the environmental indices using quantitative measures and construct country-specific environmental indices, enabling them to be viewed as dollar-denominated assets. Our primary goal is to encourage the active engagement of institutional investors in portfolio analysis and trading within this emerging financial market. To evaluate and manage investment risks, our approach incorporates financial econometric theory and dynamic asset pricing tools. We provide an econometric analysis that reveals the relationships between environmental and economic indicators in this market. Additionally, we derive financial put options as insurance instruments that can be employed to manage investment risks. Our factor analysis identifies key drivers in the global financial market for environmental indices. To further evaluate the markets performance, we employ pricing options, efficient frontier analysis, and regression analysis. These tools help us assess the efficiency and effectiveness of the market. Overall, our research contributes to the understanding and development of the global financial market for environmental indices.","The Financial Market of Environmental Indices This paper introduces the concept of a global financial market for environmental indices, addressing sustainability concerns and aiming to attract institutional investors. Risk mitigation measures are implemented to manage inherent risks associated with investments in this new financial market. We monetize the environmental indices using quantitative measures and construct country-specific environmental indices, enabling them to be viewed as dollar-denominated assets. Our primary goal is to encourage the active engagement of institutional investors in portfolio analysis and trading within this emerging financial market. To evaluate and manage investment risks, our approach incorporates financial econometric theory and dynamic asset pricing tools. We provide an econometric analysis that reveals the relationships between environmental and economic indicators in this market. Additionally, we derive financial put options as insurance instruments that can be employed to manage investment risks. Our factor analysis identifies key drivers in the global financial market for environmental indices. To further evaluate the markets performance, we employ pricing options, efficient frontier analysis, and regression analysis. These tools help us assess the efficiency and effectiveness of the market. Overall, our research contributes to the understanding and development of the global financial market for environmental indices.",Finance
Change Matters: Medication Change Prediction with Recurrent Residual Networks,"Deep learning is revolutionizing predictive healthcare, including recommending medications to patients with complex health conditions. Existing approaches focus on predicting all medications for the current visit, which often overlaps with medications from previous visits. A more clinically relevant task is to identify medication changes. In this paper, we propose a new recurrent residual network, named MICRON, for medication change prediction. MICRON takes the changes in patient health records as input and learns to update a hidden medication vector and the medication set recurrently with a reconstruction design. The medication vector is like the memory cell that encodes longitudinal information of medications. Unlike traditional methods that require the entire patient history for prediction, MICRON has a residual-based inference that allows for sequential updating based only on new patient features (e.g., new diagnoses in the recent visit) more efficiently. We evaluated MICRON on real inpatient and outpatient datasets. MICRON achieves 3.5 and 7.8 relative improvements over the best baseline in F1 score, respectively. MICRON also requires fewer parameters, which significantly reduces the training time to 38.3s per epoch with 1.5x speed-up.","Change Matters: Medication Change Prediction with Recurrent Residual Networks Deep learning is revolutionizing predictive healthcare, including recommending medications to patients with complex health conditions. Existing approaches focus on predicting all medications for the current visit, which often overlaps with medications from previous visits. A more clinically relevant task is to identify medication changes. In this paper, we propose a new recurrent residual network, named MICRON, for medication change prediction. MICRON takes the changes in patient health records as input and learns to update a hidden medication vector and the medication set recurrently with a reconstruction design. The medication vector is like the memory cell that encodes longitudinal information of medications. Unlike traditional methods that require the entire patient history for prediction, MICRON has a residual-based inference that allows for sequential updating based only on new patient features (e.g., new diagnoses in the recent visit) more efficiently. We evaluated MICRON on real inpatient and outpatient datasets. MICRON achieves 3.5 and 7.8 relative improvements over the best baseline in F1 score, respectively. MICRON also requires fewer parameters, which significantly reduces the training time to 38.3s per epoch with 1.5x speed-up.",Healthcare
Radiation of mixed layer near-inertial oscillations into the ocean interior,"The radiation from the mixed layer into the interior of the ocean of near-inertial oscillations excited by a passing storm in the presence of the beta effect is reconsidered as an initial-value problem. Making use of the fact that the mixed layer depth is much smaller than the total depth of the ocean, the solution is obtained in the limit of an ocean that is effectively infinitely deep. For a uniform initial condition, analytical results for the velocity, horizontal kinetic energy density and fluxes are obtained. The resulting decay of near-inertial mixed layer energy in the presence of the beta effect occurs on a timescale similar to that observed.","Radiation of mixed layer near-inertial oscillations into the ocean interior The radiation from the mixed layer into the interior of the ocean of near-inertial oscillations excited by a passing storm in the presence of the beta effect is reconsidered as an initial-value problem. Making use of the fact that the mixed layer depth is much smaller than the total depth of the ocean, the solution is obtained in the limit of an ocean that is effectively infinitely deep. For a uniform initial condition, analytical results for the velocity, horizontal kinetic energy density and fluxes are obtained. The resulting decay of near-inertial mixed layer energy in the presence of the beta effect occurs on a timescale similar to that observed.",Environment
Face Recognition using Principal Component Analysis and Log-Gabor Filters,"In this article we propose a novel face recognition method based on Principal Component Analysis (PCA) and Log-Gabor filters. The main advantages of the proposed method are its simple implementation, training, and very high recognition accuracy. For recognition experiments we used 5151 face images of 1311 persons from different sets of the FERET and AR databases that allow to analyze how recognition accuracy is affected by the change of facial expressions, illumination, and aging. Recognition experiments with the FERET database (containing photographs of 1196 persons) showed that our method can achieve maximal 97-98 first one recognition rate and 0.3-0.4 Equal Error Rate. The experiments also showed that the accuracy of our method is less affected by eye location errors and used image normalization method than of traditional PCA -based recognition method.","Face Recognition using Principal Component Analysis and Log-Gabor Filters In this article we propose a novel face recognition method based on Principal Component Analysis (PCA) and Log-Gabor filters. The main advantages of the proposed method are its simple implementation, training, and very high recognition accuracy. For recognition experiments we used 5151 face images of 1311 persons from different sets of the FERET and AR databases that allow to analyze how recognition accuracy is affected by the change of facial expressions, illumination, and aging. Recognition experiments with the FERET database (containing photographs of 1196 persons) showed that our method can achieve maximal 97-98 first one recognition rate and 0.3-0.4 Equal Error Rate. The experiments also showed that the accuracy of our method is less affected by eye location errors and used image normalization method than of traditional PCA -based recognition method.",Technology
Obtaining Membership Functions from a Neuron Fuzzy System extended by Kohonen Network,"This article presents the Neo-Fuzzy-Neuron Modified by Kohonen Network (NFN-MK), an hybrid computational model that combines fuzzy system technique and artificial neural networks. Its main task consists in the automatic generation of membership functions, in particular, triangle forms, aiming a dynamic modeling of a system. The model is tested by simulating real systems, here represented by a nonlinear mathematical function. Comparison with the results obtained by traditional neural networks, and correlated studies of neurofuzzy systems applied in system identification area, shows that the NFN-MK model has a similar performance, despite its greater simplicity.","Obtaining Membership Functions from a Neuron Fuzzy System extended by Kohonen Network This article presents the Neo-Fuzzy-Neuron Modified by Kohonen Network (NFN-MK), an hybrid computational model that combines fuzzy system technique and artificial neural networks. Its main task consists in the automatic generation of membership functions, in particular, triangle forms, aiming a dynamic modeling of a system. The model is tested by simulating real systems, here represented by a nonlinear mathematical function. Comparison with the results obtained by traditional neural networks, and correlated studies of neurofuzzy systems applied in system identification area, shows that the NFN-MK model has a similar performance, despite its greater simplicity.",Technology
Optimal systems of subalgebras for a nonlinear Black-Scholes equation,"The main object of our study is a four dimensional Lie algebra which describes the symmetry properties of a nonlinear Black-Scholes model. This model implements a feedback effect which is typical for an illiquid market. The structure of the Lie algebra depends on one parameter, i.e. we have to do with a one-parametric family of algebras. We provide a classification of these algebras using Patera--Winternitz method. Optimal systems of one-, two- and three- dimensional subalgebras are described for the family of symmetry algebras of the nonlinear Black-Scholes equation. The optimal systems give us the possibility to describe a complete set of invariant solutions to the equation.","Optimal systems of subalgebras for a nonlinear Black-Scholes equation The main object of our study is a four dimensional Lie algebra which describes the symmetry properties of a nonlinear Black-Scholes model. This model implements a feedback effect which is typical for an illiquid market. The structure of the Lie algebra depends on one parameter, i.e. we have to do with a one-parametric family of algebras. We provide a classification of these algebras using Patera--Winternitz method. Optimal systems of one-, two- and three- dimensional subalgebras are described for the family of symmetry algebras of the nonlinear Black-Scholes equation. The optimal systems give us the possibility to describe a complete set of invariant solutions to the equation.",Finance
Probabilistic Search for Object Segmentation and Recognition,"The problem of searching for a model-based scene interpretation is analyzed within a probabilistic framework. Object models are formulated as generative models for range data of the scene. A new statistical criterion, the truncated object probability, is introduced to infer an optimal sequence of object hypotheses to be evaluated for their match to the data. The truncated probability is partly determined by prior knowledge of the objects and partly learned from data. Some experiments on sequence quality and object segmentation and recognition from stereo data are presented. The article recovers classic concepts from object recognition (grouping, geometric hashing, alignment) from the probabilistic perspective and adds insight into the optimal ordering of object hypotheses for evaluation. Moreover, it introduces point-relation densities, a key component of the truncated probability, as statistical models of local surface shape.","Probabilistic Search for Object Segmentation and Recognition The problem of searching for a model-based scene interpretation is analyzed within a probabilistic framework. Object models are formulated as generative models for range data of the scene. A new statistical criterion, the truncated object probability, is introduced to infer an optimal sequence of object hypotheses to be evaluated for their match to the data. The truncated probability is partly determined by prior knowledge of the objects and partly learned from data. Some experiments on sequence quality and object segmentation and recognition from stereo data are presented. The article recovers classic concepts from object recognition (grouping, geometric hashing, alignment) from the probabilistic perspective and adds insight into the optimal ordering of object hypotheses for evaluation. Moreover, it introduces point-relation densities, a key component of the truncated probability, as statistical models of local surface shape.",Technology
Supervised learning on graphs of spatio-temporal similarity in satellite image sequences,"High resolution satellite image sequences are multidimensional signals composed of spatio-temporal patterns associated to numerous and various phenomena. Bayesian methods have been previously proposed in (Heas and Datcu, 2005) to code the information contained in satellite image sequences in a graph representation using Bayesian methods. Based on such a representation, this paper further presents a supervised learning methodology of semantics associated to spatio-temporal patterns occurring in satellite image sequences. It enables the recognition and the probabilistic retrieval of similar events. Indeed, graphs are attached to statistical models for spatio-temporal processes, which at their turn describe physical changes in the observed scene. Therefore, we adjust a parametric model evaluating similarity types between graph patterns in order to represent user-specific semantics attached to spatio-temporal phenomena. The learning step is performed by the incremental definition of similarity types via user-provided spatio-temporal pattern examples attached to positive orand negative semantics. From these examples, probabilities are inferred using a Bayesian network and a Dirichlet model. This enables to links user interest to a specific similarity model between graph patterns. According to the current state of learning, semantic posterior probabilities are updated for all possible graph patterns so that similar spatio-temporal phenomena can be recognized and retrieved from the image sequence. Few experiments performed on a multi-spectral SPOT image sequence illustrate the proposed spatio-temporal recognition method.","Supervised learning on graphs of spatio-temporal similarity in satellite image sequences High resolution satellite image sequences are multidimensional signals composed of spatio-temporal patterns associated to numerous and various phenomena. Bayesian methods have been previously proposed in (Heas and Datcu, 2005) to code the information contained in satellite image sequences in a graph representation using Bayesian methods. Based on such a representation, this paper further presents a supervised learning methodology of semantics associated to spatio-temporal patterns occurring in satellite image sequences. It enables the recognition and the probabilistic retrieval of similar events. Indeed, graphs are attached to statistical models for spatio-temporal processes, which at their turn describe physical changes in the observed scene. Therefore, we adjust a parametric model evaluating similarity types between graph patterns in order to represent user-specific semantics attached to spatio-temporal phenomena. The learning step is performed by the incremental definition of similarity types via user-provided spatio-temporal pattern examples attached to positive orand negative semantics. From these examples, probabilities are inferred using a Bayesian network and a Dirichlet model. This enables to links user interest to a specific similarity model between graph patterns. According to the current state of learning, semantic posterior probabilities are updated for all possible graph patterns so that similar spatio-temporal phenomena can be recognized and retrieved from the image sequence. Few experiments performed on a multi-spectral SPOT image sequence illustrate the proposed spatio-temporal recognition method.",Technology
Towards Low-carbon Power Networks: Optimal Integration of Renewable Energy Sources and Hydrogen Storage,"This paper proposes a new optimization model and solution method for determining optimal locations and sizing of renewable energy sources and hydrogen storage in a power network. We obtain these strategic decisions based on the multi-period alternating current optimal power (AC OPF) flow problem that considers the uncertainty of renewable output, electricity demand, and electricity prices. We develop a second-order cone programming approach within a Benders decomposition framework to provide globally optimal solutions. To the best of our knowledge, our paper is the first to propose a systematic optimization framework based on AC OPF that jointly analyzes power network, renewable, and hydrogen storage interactions in order to provide optimal locations and sizing decisions of renewables and hydrogen storage. In a test case, we show that the joint integration of renewable sources and hydrogen storage and consideration of the AC OPF model significantly reduces the operational cost of the power network. In turn, our findings can provide quantitative insights to decision-makers on how to integrate renewable sources and hydrogen storage under different settings of the hydrogen selling price, renewable curtailment costs, emission tax prices, and conversion efficiency.","Towards Low-carbon Power Networks: Optimal Integration of Renewable Energy Sources and Hydrogen Storage This paper proposes a new optimization model and solution method for determining optimal locations and sizing of renewable energy sources and hydrogen storage in a power network. We obtain these strategic decisions based on the multi-period alternating current optimal power (AC OPF) flow problem that considers the uncertainty of renewable output, electricity demand, and electricity prices. We develop a second-order cone programming approach within a Benders decomposition framework to provide globally optimal solutions. To the best of our knowledge, our paper is the first to propose a systematic optimization framework based on AC OPF that jointly analyzes power network, renewable, and hydrogen storage interactions in order to provide optimal locations and sizing decisions of renewables and hydrogen storage. In a test case, we show that the joint integration of renewable sources and hydrogen storage and consideration of the AC OPF model significantly reduces the operational cost of the power network. In turn, our findings can provide quantitative insights to decision-makers on how to integrate renewable sources and hydrogen storage under different settings of the hydrogen selling price, renewable curtailment costs, emission tax prices, and conversion efficiency.",Environment
Radiostrontium activity concentrations in milk in the Republic of Croatia for 1961 - 2001 and dose assessment,"Results of systematic measurements of Sr-90 activity concentrations in milk for the period 1961 - 2001 are summarized. An exponential decline of radioactivity followed the moratorium on atmospheric nuclear testing. The highest activity of Sr-90 deposited by fallout, being 1060 Bqm2, was recorded in 1963, while the peak Sr-90 activity concentration in milk, 1.42 -0.17 BqL, was recorded in 1964. The values in year 2001 for fallout deposition and milk were 7.7 Bqm2 and 0.07 - 0.03 BqL, respectively. The reactor accident at Chernobyl caused higher Sr-90 levels only in 1986. Sr-90 fallout activity affects milk activity, the coefficient of correlation between Sr-90 fallout activity and Sr-90 activity concentrations in milk being 0.80. The transfer coefficient from fallout deposition to milk was estimated to be 2.5 mBqyL per Bqm2. The dose incurred by milk consumption was estimated for the Croatian population, the annual collective effective dose in 2001 being approximately 2.0 man-Sv.","Radiostrontium activity concentrations in milk in the Republic of Croatia for 1961 - 2001 and dose assessment Results of systematic measurements of Sr-90 activity concentrations in milk for the period 1961 - 2001 are summarized. An exponential decline of radioactivity followed the moratorium on atmospheric nuclear testing. The highest activity of Sr-90 deposited by fallout, being 1060 Bqm2, was recorded in 1963, while the peak Sr-90 activity concentration in milk, 1.42 -0.17 BqL, was recorded in 1964. The values in year 2001 for fallout deposition and milk were 7.7 Bqm2 and 0.07 - 0.03 BqL, respectively. The reactor accident at Chernobyl caused higher Sr-90 levels only in 1986. Sr-90 fallout activity affects milk activity, the coefficient of correlation between Sr-90 fallout activity and Sr-90 activity concentrations in milk being 0.80. The transfer coefficient from fallout deposition to milk was estimated to be 2.5 mBqyL per Bqm2. The dose incurred by milk consumption was estimated for the Croatian population, the annual collective effective dose in 2001 being approximately 2.0 man-Sv.",Healthcare
Learner-Centered Analysis in Educational Metaverse Environments: Exploring Value Exchange Systems through Natural Interaction and Text Mining,"This paper explores the potential developments of self-directed learning in the metaverse in response to Education 4.0 and the Fourth Industrial Revolution. It highlights the importance of education keeping up with technological advancements and adopting learner-centered approaches. Additionally, it focuses on exploring value exchange systems through natural interaction, text mining, and analysis. The metaverse concept extends beyond extended reality (XR) technologies, encompassing digital avatars and shared ecological value. The role of educators in exploring new technologies and leveraging text-mining techniques to enhance learning efficiency is emphasized. The metaverse is presented as a platform for value exchange, necessitating meaningful and valuable content to attract users. Integrating virtual and real-world experiences within the metaverse offers practical applications and contributes to its essence. This paper sheds light on the metaverses potential to create a learner-centered educational environment and adapt to the evolving landscape of Education 4.0. Its findings, supported by text mining analysis, contribute to understanding the metaverses role in shaping education in the Fourth Industrial Revolution.","Learner-Centered Analysis in Educational Metaverse Environments: Exploring Value Exchange Systems through Natural Interaction and Text Mining This paper explores the potential developments of self-directed learning in the metaverse in response to Education 4.0 and the Fourth Industrial Revolution. It highlights the importance of education keeping up with technological advancements and adopting learner-centered approaches. Additionally, it focuses on exploring value exchange systems through natural interaction, text mining, and analysis. The metaverse concept extends beyond extended reality (XR) technologies, encompassing digital avatars and shared ecological value. The role of educators in exploring new technologies and leveraging text-mining techniques to enhance learning efficiency is emphasized. The metaverse is presented as a platform for value exchange, necessitating meaningful and valuable content to attract users. Integrating virtual and real-world experiences within the metaverse offers practical applications and contributes to its essence. This paper sheds light on the metaverses potential to create a learner-centered educational environment and adapt to the evolving landscape of Education 4.0. Its findings, supported by text mining analysis, contribute to understanding the metaverses role in shaping education in the Fourth Industrial Revolution.",Education
Retinal Vessel Segmentation Using the 2-D Morlet Wavelet and Supervised Classification,"We present a method for automated segmentation of the vasculature in retinal images. The method produces segmentations by classifying each image pixel as vessel or non-vessel, based on the pixels feature vector. Feature vectors are composed of the pixels intensity and continuous two-dimensional Morlet wavelet transform responses taken at multiple scales. The Morlet wavelet is capable of tuning to specific frequencies, thus allowing noise filtering and vessel enhancement in a single step. We use a Bayesian classifier with class-conditional probability density functions (likelihoods) described as Gaussian mixtures, yielding a fast classification, while being able to model complex decision surfaces and compare its performance with the linear minimum squared error classifier. The probability distributions are estimated based on a training set of labeled pixels obtained from manual segmentations. The methods performance is evaluated on publicly available DRIVE and STARE databases of manually labeled non-mydriatic images. On the DRIVE database, it achieves an area under the receiver operating characteristic (ROC) curve of 0.9598, being slightly superior than that presented by the method of Staal et al.","Retinal Vessel Segmentation Using the 2-D Morlet Wavelet and Supervised Classification We present a method for automated segmentation of the vasculature in retinal images. The method produces segmentations by classifying each image pixel as vessel or non-vessel, based on the pixels feature vector. Feature vectors are composed of the pixels intensity and continuous two-dimensional Morlet wavelet transform responses taken at multiple scales. The Morlet wavelet is capable of tuning to specific frequencies, thus allowing noise filtering and vessel enhancement in a single step. We use a Bayesian classifier with class-conditional probability density functions (likelihoods) described as Gaussian mixtures, yielding a fast classification, while being able to model complex decision surfaces and compare its performance with the linear minimum squared error classifier. The probability distributions are estimated based on a training set of labeled pixels obtained from manual segmentations. The methods performance is evaluated on publicly available DRIVE and STARE databases of manually labeled non-mydriatic images. On the DRIVE database, it achieves an area under the receiver operating characteristic (ROC) curve of 0.9598, being slightly superior than that presented by the method of Staal et al.",Technology
Rational Radial Distortion Models with Analytical Undistortion Formulae,"The common approach to radial distortion is by the means of polynomial approximation, which introduces distortion-specific parameters into the camera model and requires estimation of these distortion parameters. The task of estimating radial distortion is to find a radial distortion model that allows easy undistortion as well as satisfactory accuracy. This paper presents a new class of rational radial distortion models with easy analytical undistortion formulae. Experimental results are presented to show that with this class of rational radial distortion models, satisfactory and comparable accuracy is achieved.","Rational Radial Distortion Models with Analytical Undistortion Formulae The common approach to radial distortion is by the means of polynomial approximation, which introduces distortion-specific parameters into the camera model and requires estimation of these distortion parameters. The task of estimating radial distortion is to find a radial distortion model that allows easy undistortion as well as satisfactory accuracy. This paper presents a new class of rational radial distortion models with easy analytical undistortion formulae. Experimental results are presented to show that with this class of rational radial distortion models, satisfactory and comparable accuracy is achieved.",Technology
Risk aversion in flexible electricity markets,"Flexibility options, such as demand response, energy storage and interconnection, have the potential to reduce variation in electricity prices between different future scenarios, therefore reducing investment risk. Moreover, investment in flexibility options can lower the need for generation capacity. However, there are complex interactions between different flexibility options. In this paper, we investigate the interactions between flexibility and investment risk in electricity markets. We employ a large-scale stochastic transmission and generation expansion model of the European electricity system. Using this model, we first investigate the effect of risk aversion on the investment decisions. We find that the interplay of parameters leads to (i) more investment in a less emission-intensive energy system if planners are risk averse (hedging against CO2 price uncertainty) and (ii) constant total installed capacity, regardless of the level of risk aversion (planners do not hedge against demand and RES deployment uncertainties). Second, we investigate the individual effects of three flexibility elements on optimal investment levels under different levels of risk aversion: demand response, investment in additional interconnection capacity and investment in additional energy storage. We find that that flexible technologies have a higher value for risk-averse decision-makers, although the effects are nonlinear. Finally, we investigate the interactions between the flexibility elements. We find that risk-averse decision-makers show a strong preference for transmission grid expansion once flexibility is available at low cost levels.","Risk aversion in flexible electricity markets Flexibility options, such as demand response, energy storage and interconnection, have the potential to reduce variation in electricity prices between different future scenarios, therefore reducing investment risk. Moreover, investment in flexibility options can lower the need for generation capacity. However, there are complex interactions between different flexibility options. In this paper, we investigate the interactions between flexibility and investment risk in electricity markets. We employ a large-scale stochastic transmission and generation expansion model of the European electricity system. Using this model, we first investigate the effect of risk aversion on the investment decisions. We find that the interplay of parameters leads to (i) more investment in a less emission-intensive energy system if planners are risk averse (hedging against CO2 price uncertainty) and (ii) constant total installed capacity, regardless of the level of risk aversion (planners do not hedge against demand and RES deployment uncertainties). Second, we investigate the individual effects of three flexibility elements on optimal investment levels under different levels of risk aversion: demand response, investment in additional interconnection capacity and investment in additional energy storage. We find that that flexible technologies have a higher value for risk-averse decision-makers, although the effects are nonlinear. Finally, we investigate the interactions between the flexibility elements. We find that risk-averse decision-makers show a strong preference for transmission grid expansion once flexibility is available at low cost levels.",Finance
Optimal designs for the development of personalized treatment rules,"We study the design of multi-armed parallel group clinical trials to estimate personalized treatment rules that identify the best treatment for a given patient with given covariates. Assuming that the outcomes in each treatment arm are given by a homoscedastic linear model, with possibly different variances between treatment arms, and that the trial subjects form a random sample from an unselected overall population, we optimize the (possibly randomized) treatment allocation allowing the allocation rates to depend on the covariates. We find that, for the case of two treatments, the approximately optimal allocation rule does not depend on the value of the covariates but only on the variances of the responses. In contrast, for the case of three treatments or more, the optimal treatment allocation does depend on the values of the covariates as well as the true regression coefficients. The methods are illustrated with a recently published dietary clinical trial.","Optimal designs for the development of personalized treatment rules We study the design of multi-armed parallel group clinical trials to estimate personalized treatment rules that identify the best treatment for a given patient with given covariates. Assuming that the outcomes in each treatment arm are given by a homoscedastic linear model, with possibly different variances between treatment arms, and that the trial subjects form a random sample from an unselected overall population, we optimize the (possibly randomized) treatment allocation allowing the allocation rates to depend on the covariates. We find that, for the case of two treatments, the approximately optimal allocation rule does not depend on the value of the covariates but only on the variances of the responses. In contrast, for the case of three treatments or more, the optimal treatment allocation does depend on the values of the covariates as well as the true regression coefficients. The methods are illustrated with a recently published dietary clinical trial.",Healthcare
Separating Technological and Clinical Safety Assurance for Medical Devices,"The safety and clinical effectiveness of medical devices are closely associated with their specific use in clinical treatments. Assuring safety and the desired clinical effectiveness is challenging. Different people may react differently to the same treatment due to variability in their physiology and genetics. Thus, we need to consider the outputs and behaviour of the device itself as well as the effect of using the device to treat a wide variety of patients. High-intensity focused ultrasound systems and radiation therapy machines are examples of systems in which this is a primary concern. Conventional monolithic assurance cases are complex, and this complexity affects our ability to address these concerns adequately. Based on the principle of separation of concerns, we propose separating the assurance of the use of these types of systems in clinical treatments into two linked assurance cases. The first assurance case demonstrates the safety of the manufacturers device independent of the clinical treatment. The second demonstrates the safety and clinical effectiveness of the device when it is used in a specific clinical treatment. We introduce the idea of these separate assurance cases, and describe briefly how they are separated and linked.","Separating Technological and Clinical Safety Assurance for Medical Devices The safety and clinical effectiveness of medical devices are closely associated with their specific use in clinical treatments. Assuring safety and the desired clinical effectiveness is challenging. Different people may react differently to the same treatment due to variability in their physiology and genetics. Thus, we need to consider the outputs and behaviour of the device itself as well as the effect of using the device to treat a wide variety of patients. High-intensity focused ultrasound systems and radiation therapy machines are examples of systems in which this is a primary concern. Conventional monolithic assurance cases are complex, and this complexity affects our ability to address these concerns adequately. Based on the principle of separation of concerns, we propose separating the assurance of the use of these types of systems in clinical treatments into two linked assurance cases. The first assurance case demonstrates the safety of the manufacturers device independent of the clinical treatment. The second demonstrates the safety and clinical effectiveness of the device when it is used in a specific clinical treatment. We introduce the idea of these separate assurance cases, and describe briefly how they are separated and linked.",Healthcare
Advancing Sustainability via Recommender Systems: A Survey,"Human behavioral patterns and consumption paradigms have emerged as pivotal determinants in environmental degradation and climate change, with quotidian decisions pertaining to transportation, energy utilization, and resource consumption collectively precipitating substantial ecological impacts. Recommender systems, which generate personalized suggestions based on user preferences and historical interaction data, exert considerable influence on individual behavioral trajectories. However, conventional recommender systems predominantly optimize for user engagement and economic metrics, inadvertently neglecting the environmental and societal ramifications of their recommendations, potentially catalyzing over-consumption and reinforcing unsustainable behavioral patterns. Given their instrumental role in shaping user decisions, there exists an imperative need for sustainable recommender systems that incorporate sustainability principles to foster eco-conscious and socially responsible choices. This comprehensive survey addresses this critical research gap by presenting a systematic analysis of sustainable recommender systems. As these systems can simultaneously advance multiple sustainability objectives--including resource conservation, sustainable consumer behavior, and social impact enhancement--examining their implementations across distinct application domains provides a more rigorous analytical framework. Through a methodological analysis of domain-specific implementations encompassing transportation, food, buildings, and auxiliary sectors, we can better elucidate how these systems holistically advance sustainability objectives while addressing sector-specific constraints and opportunities. Moreover, we delineate future research directions for evolving recommender systems beyond sustainability advocacy toward fostering environmental resilience and social consciousness in society.","Advancing Sustainability via Recommender Systems: A Survey Human behavioral patterns and consumption paradigms have emerged as pivotal determinants in environmental degradation and climate change, with quotidian decisions pertaining to transportation, energy utilization, and resource consumption collectively precipitating substantial ecological impacts. Recommender systems, which generate personalized suggestions based on user preferences and historical interaction data, exert considerable influence on individual behavioral trajectories. However, conventional recommender systems predominantly optimize for user engagement and economic metrics, inadvertently neglecting the environmental and societal ramifications of their recommendations, potentially catalyzing over-consumption and reinforcing unsustainable behavioral patterns. Given their instrumental role in shaping user decisions, there exists an imperative need for sustainable recommender systems that incorporate sustainability principles to foster eco-conscious and socially responsible choices. This comprehensive survey addresses this critical research gap by presenting a systematic analysis of sustainable recommender systems. As these systems can simultaneously advance multiple sustainability objectives--including resource conservation, sustainable consumer behavior, and social impact enhancement--examining their implementations across distinct application domains provides a more rigorous analytical framework. Through a methodological analysis of domain-specific implementations encompassing transportation, food, buildings, and auxiliary sectors, we can better elucidate how these systems holistically advance sustainability objectives while addressing sector-specific constraints and opportunities. Moreover, we delineate future research directions for evolving recommender systems beyond sustainability advocacy toward fostering environmental resilience and social consciousness in society.",Environment
Beyond Feedforward Models Trained by Backpropagation: a Practical Training Tool for a More Efficient Universal Approximator,"Cellular Simultaneous Recurrent Neural Network (SRN) has been shown to be a function approximator more powerful than the MLP. This means that the complexity of MLP would be prohibitively large for some problems while SRN could realize the desired mapping with acceptable computational constraints. The speed of training of complex recurrent networks is crucial to their successful application. Present work improves the previous results by training the network with extended Kalman filter (EKF). We implemented a generic Cellular SRN and applied it for solving two challenging problems: 2D maze navigation and a subset of the connectedness problem. The speed of convergence has been improved by several orders of magnitude in comparison with the earlier results in the case of maze navigation, and superior generalization has been demonstrated in the case of connectedness. The implications of this improvements are discussed.","Beyond Feedforward Models Trained by Backpropagation: a Practical Training Tool for a More Efficient Universal Approximator Cellular Simultaneous Recurrent Neural Network (SRN) has been shown to be a function approximator more powerful than the MLP. This means that the complexity of MLP would be prohibitively large for some problems while SRN could realize the desired mapping with acceptable computational constraints. The speed of training of complex recurrent networks is crucial to their successful application. Present work improves the previous results by training the network with extended Kalman filter (EKF). We implemented a generic Cellular SRN and applied it for solving two challenging problems: 2D maze navigation and a subset of the connectedness problem. The speed of convergence has been improved by several orders of magnitude in comparison with the earlier results in the case of maze navigation, and superior generalization has been demonstrated in the case of connectedness. The implications of this improvements are discussed.",Technology
Household and individual economic responses to different health shocks: The role of medical innovations,"This study provides new evidence regarding the extent to which medical care mitigates the economic consequences of various health shocks for the individual and a wider family. To obtain causal effects, I focus on the role of medical scientific discoveries and leverage the longitudinal dimension of unique administrative data for Sweden. The results indicate that medical innovations strongly mitigate the negative economic consequences of a health shock for the individual and create spillovers to relatives. Such mitigating effects are highly heterogeneous across prognoses. These results suggest that medical innovation substantially reduces the burden of welfare costs yet produces income inequalities.","Household and individual economic responses to different health shocks: The role of medical innovations This study provides new evidence regarding the extent to which medical care mitigates the economic consequences of various health shocks for the individual and a wider family. To obtain causal effects, I focus on the role of medical scientific discoveries and leverage the longitudinal dimension of unique administrative data for Sweden. The results indicate that medical innovations strongly mitigate the negative economic consequences of a health shock for the individual and create spillovers to relatives. Such mitigating effects are highly heterogeneous across prognoses. These results suggest that medical innovation substantially reduces the burden of welfare costs yet produces income inequalities.",Healthcare
Green AI: Which Programming Language Consumes the Most?,"AI is demanding an evergrowing portion of environmental resources. Despite their potential impact on AI environmental sustainability, the role that programming languages play in AI (in)efficiency is to date still unknown. With this study, we aim to understand the impact that programming languages can have on AI environmental sustainability. To achieve our goal, we conduct a controlled empirical experiment by considering five programming languages (C, Java, Python, MATLAB, and R), seven AI algorithms (KNN, SVC, AdaBoost, decision tree, logistic regression, naive bayses, and random forest), three popular datasets, and the training and inference phases. The collected results show that programming languages have a considerable impact on AI environmental sustainability. Compiled and semi-compiled languages (C, Java) consistently consume less than interpreted languages (Python, MATLAB, R), which require up to 54x more energy. Some languages are cumulatively more efficient in training, while others in inference. Which programming language consumes the most highly depends on the algorithm considered. Ultimately, algorithm implementation might be the most determining factor in Green AI, regardless of the language used. As conclusion, while making AI more environmentally sustainable is paramount, a trade-off between energy efficiency and implementation ease should always be considered. Green AI can be achieved without the need of completely disrupting the development practices and technologies currently in place.","Green AI: Which Programming Language Consumes the Most? AI is demanding an evergrowing portion of environmental resources. Despite their potential impact on AI environmental sustainability, the role that programming languages play in AI (in)efficiency is to date still unknown. With this study, we aim to understand the impact that programming languages can have on AI environmental sustainability. To achieve our goal, we conduct a controlled empirical experiment by considering five programming languages (C, Java, Python, MATLAB, and R), seven AI algorithms (KNN, SVC, AdaBoost, decision tree, logistic regression, naive bayses, and random forest), three popular datasets, and the training and inference phases. The collected results show that programming languages have a considerable impact on AI environmental sustainability. Compiled and semi-compiled languages (C, Java) consistently consume less than interpreted languages (Python, MATLAB, R), which require up to 54x more energy. Some languages are cumulatively more efficient in training, while others in inference. Which programming language consumes the most highly depends on the algorithm considered. Ultimately, algorithm implementation might be the most determining factor in Green AI, regardless of the language used. As conclusion, while making AI more environmentally sustainable is paramount, a trade-off between energy efficiency and implementation ease should always be considered. Green AI can be achieved without the need of completely disrupting the development practices and technologies currently in place.",Environment
Algorithmic Complexity in Real Financial Markets,"A new approach to the understanding of complex behavior of financial markets index using tools from thermodynamics and statistical physics is developed. Physical complexity, a magnitude rooted in Kolmogorov-Chaitin theory is applied to binary sequences built up from real time series of financial markets indexes. The study is based on NASDAQ and Mexican IPC data. Different behaviors of this magnitude are shown when applied to the intervals of series placed before crashes and to intervals when no financial turbulence is observed. The connection between our results and The Efficient Market Hypothesis is discussed.","Algorithmic Complexity in Real Financial Markets A new approach to the understanding of complex behavior of financial markets index using tools from thermodynamics and statistical physics is developed. Physical complexity, a magnitude rooted in Kolmogorov-Chaitin theory is applied to binary sequences built up from real time series of financial markets indexes. The study is based on NASDAQ and Mexican IPC data. Different behaviors of this magnitude are shown when applied to the intervals of series placed before crashes and to intervals when no financial turbulence is observed. The connection between our results and The Efficient Market Hypothesis is discussed.",Finance
Real time noninvasive cancer diagnostics,"Laser illumination of tissue results in a characteristic fluorescence emission spectrum whose features depend on the type of tissue, viz., healthy, adenoma or malignant. Hence measurement of the fluorescence is potentially a rapid and reliable diagnostic method. We have applied the technique to thyroid tissues and found that judicious normalization of the fluorescence spectrum reveals a close correlation between spectral shape and tissue type. Hence the pathological state of the tissue can be unambiguously identified from the laser-induced fluorescence spectrum. The method is rapid and portable and is therefore eminently suitable for use during operations.","Real time noninvasive cancer diagnostics Laser illumination of tissue results in a characteristic fluorescence emission spectrum whose features depend on the type of tissue, viz., healthy, adenoma or malignant. Hence measurement of the fluorescence is potentially a rapid and reliable diagnostic method. We have applied the technique to thyroid tissues and found that judicious normalization of the fluorescence spectrum reveals a close correlation between spectral shape and tissue type. Hence the pathological state of the tissue can be unambiguously identified from the laser-induced fluorescence spectrum. The method is rapid and portable and is therefore eminently suitable for use during operations.",Healthcare
"Domain Analysis of Ethical, Social and Environmental Accounting Methods","Ethical, social and environmental accounting is the practice of assessing and reporting organisations performance on environmental, social and governance topics. There are ample methods that describe how to perform such sustainability assessments. This report presents a domain analysis of ethical, social and environmental accounting methods. Our analysis contains 21 methods. Each method is modelled as a process deliverable diagram. The diagrams have been validated by experts in the methods. The diagrams lay the foundation for further analysis and software development. In this report, we touch upon the ethical, social and environmental accounting method ontology that has been created based on the domain analysis.","Domain Analysis of Ethical, Social and Environmental Accounting Methods Ethical, social and environmental accounting is the practice of assessing and reporting organisations performance on environmental, social and governance topics. There are ample methods that describe how to perform such sustainability assessments. This report presents a domain analysis of ethical, social and environmental accounting methods. Our analysis contains 21 methods. Each method is modelled as a process deliverable diagram. The diagrams have been validated by experts in the methods. The diagrams lay the foundation for further analysis and software development. In this report, we touch upon the ethical, social and environmental accounting method ontology that has been created based on the domain analysis.",Environment
Prediction of tissue decompression in orbital surgery,"Objective: A method to predict the relationships between decompressed volume of orbital soft tissues, backward displacement of globe after osteotomy, and force exerted by the surgeon, was proposed to improve surgery planning in exophthalmia reduction. Design: A geometric model and a poroelastic finite element model were developed, based on Computed Tomography scan data. Background: The exophthalmia is characterised by a protrusion of the eyeball. Surgery consists in an osteotomy of the orbit walls to decompress the orbital content. A few clinical observations ruling on an almost linear relationship between globe backward displacement and tissue decompressed volume are described in the literature. Methods: Fast prediction of decompressed volume is derived from the geometric model: a sphere in interaction with a cone. Besides, a poroelastic Finite Element model involving morphology, material properties of orbital components and surgical gesture was implemented. Results: The geometric model provided a better decompression volume estimation than the Finite Element model. Besides, the Finite Element model permitted to quantify the backward displacement, the surgical gesture and the stiffness of the orbital content. Conclusions: The preliminary results obtained for one patient, in accordance with the clinical literature, were relatively satisfying. An efficient aid for location and size of osteotomies was derived and seemed to be able to help in the surgery planning. Relevance: To our knowledge, this paper concerns the first biomechanical study of exophthalmia reduction. The approach permitted to improve the treatment of orbitopathy and can be used in a clinical setting.","Prediction of tissue decompression in orbital surgery Objective: A method to predict the relationships between decompressed volume of orbital soft tissues, backward displacement of globe after osteotomy, and force exerted by the surgeon, was proposed to improve surgery planning in exophthalmia reduction. Design: A geometric model and a poroelastic finite element model were developed, based on Computed Tomography scan data. Background: The exophthalmia is characterised by a protrusion of the eyeball. Surgery consists in an osteotomy of the orbit walls to decompress the orbital content. A few clinical observations ruling on an almost linear relationship between globe backward displacement and tissue decompressed volume are described in the literature. Methods: Fast prediction of decompressed volume is derived from the geometric model: a sphere in interaction with a cone. Besides, a poroelastic Finite Element model involving morphology, material properties of orbital components and surgical gesture was implemented. Results: The geometric model provided a better decompression volume estimation than the Finite Element model. Besides, the Finite Element model permitted to quantify the backward displacement, the surgical gesture and the stiffness of the orbital content. Conclusions: The preliminary results obtained for one patient, in accordance with the clinical literature, were relatively satisfying. An efficient aid for location and size of osteotomies was derived and seemed to be able to help in the surgery planning. Relevance: To our knowledge, this paper concerns the first biomechanical study of exophthalmia reduction. The approach permitted to improve the treatment of orbitopathy and can be used in a clinical setting.",Healthcare
Portfolio selection using neural networks,In this paper we apply a heuristic method based on artificial neural networks in order to trace out the efficient frontier associated to the portfolio selection problem. We consider a generalization of the standard Markowitz mean-variance model which includes cardinality and bounding constraints. These constraints ensure the investment in a given number of different assets and limit the amount of capital to be invested in each asset. We present some experimental results obtained with the neural network heuristic and we compare them to those obtained with three previous heuristic methods.,Portfolio selection using neural networks In this paper we apply a heuristic method based on artificial neural networks in order to trace out the efficient frontier associated to the portfolio selection problem. We consider a generalization of the standard Markowitz mean-variance model which includes cardinality and bounding constraints. These constraints ensure the investment in a given number of different assets and limit the amount of capital to be invested in each asset. We present some experimental results obtained with the neural network heuristic and we compare them to those obtained with three previous heuristic methods.,Technology
Using Rigorous Ray Tracing to Incorporate Reflection into the Parabolic Approximation,"We present a parabolic approximation that incorporates reflection. With this approximation, there is no need to solve the parabolic equation for a coupled pair of solutions consisting of the incident and reflected waves. Rather, this approximation uses a synthetic wave whose spectral components manifest the incident and reflected waves.","Using Rigorous Ray Tracing to Incorporate Reflection into the Parabolic Approximation We present a parabolic approximation that incorporates reflection. With this approximation, there is no need to solve the parabolic equation for a coupled pair of solutions consisting of the incident and reflected waves. Rather, this approximation uses a synthetic wave whose spectral components manifest the incident and reflected waves.",Environment
Teaching and Learning Science with Learning Assistants,The science content classes for elementary education majors at Winona State University used Learning Assistants for the first time during the 2009 - 2010 academic year. Pre-post information was gathered about the Learning Assistants and the pre-service teachers to gauge the effect of this experience on both populations.,Teaching and Learning Science with Learning Assistants The science content classes for elementary education majors at Winona State University used Learning Assistants for the first time during the 2009 - 2010 academic year. Pre-post information was gathered about the Learning Assistants and the pre-service teachers to gauge the effect of this experience on both populations.,Education
Multilevel Thresholding for Image Segmentation through a Fast Statistical Recursive Algorithm,"A novel algorithm is proposed for segmenting an image into multiple levels using its mean and variance. Starting from the extreme pixel values at both ends of the histogram plot, the algorithm is applied recursively on sub-ranges computed from the previous step, so as to find a threshold level and a new sub-range for the next step, until no significant improvement in image quality can be achieved. The method makes use of the fact that a number of distributions tend towards Dirac delta function, peaking at the mean, in the limiting condition of vanishing variance. The procedure naturally provides for variable size segmentation with bigger blocks near the extreme pixel values and finer divisions around the mean or other chosen value for better visualization. Experiments on a variety of images show that the new algorithm effectively segments the image in computationally very less time.","Multilevel Thresholding for Image Segmentation through a Fast Statistical Recursive Algorithm A novel algorithm is proposed for segmenting an image into multiple levels using its mean and variance. Starting from the extreme pixel values at both ends of the histogram plot, the algorithm is applied recursively on sub-ranges computed from the previous step, so as to find a threshold level and a new sub-range for the next step, until no significant improvement in image quality can be achieved. The method makes use of the fact that a number of distributions tend towards Dirac delta function, peaking at the mean, in the limiting condition of vanishing variance. The procedure naturally provides for variable size segmentation with bigger blocks near the extreme pixel values and finer divisions around the mean or other chosen value for better visualization. Experiments on a variety of images show that the new algorithm effectively segments the image in computationally very less time.",Technology
Flexible Inference of Optimal Individualized Treatment Strategy in Covariate Adjusted Randomization with Multiple Covariates,"To maximize clinical benefit, clinicians routinely tailor treatment to the individual characteristics of each patient, where individualized treatment rules are needed and are of significant research interest to statisticians. In the covariate-adjusted randomization clinical trial with many covariates, we model the treatment effect with an unspecified function of a single index of the covariates and leave the baseline response completely arbitrary. We devise a class of estimators to consistently estimate the treatment effect function and its associated index while bypassing the estimation of the baseline response, which is subject to the curse of dimensionality. We further develop inference tools to identify predictive covariates and isolate effective treatment region. The usefulness of the methods is demonstrated in both simulations and a clinical data example.","Flexible Inference of Optimal Individualized Treatment Strategy in Covariate Adjusted Randomization with Multiple Covariates To maximize clinical benefit, clinicians routinely tailor treatment to the individual characteristics of each patient, where individualized treatment rules are needed and are of significant research interest to statisticians. In the covariate-adjusted randomization clinical trial with many covariates, we model the treatment effect with an unspecified function of a single index of the covariates and leave the baseline response completely arbitrary. We devise a class of estimators to consistently estimate the treatment effect function and its associated index while bypassing the estimation of the baseline response, which is subject to the curse of dimensionality. We further develop inference tools to identify predictive covariates and isolate effective treatment region. The usefulness of the methods is demonstrated in both simulations and a clinical data example.",Healthcare
Teaching Predictive Control Using Specification-based Summative Assessments,"Including Model Predictive Control (MPC) in the undergraduategraduate control curriculum is becoming vitally important due to the growing adoption of MPC in many industrial areas. In this paper, we present an overview of the predictive control course taught by the authors at Imperial College London between 2018 and 2021. We discuss how the course evolved from focusing solely on the linear MPC formulation to covering nonlinear MPC and some of its extensions. We also present a novel specification-based summative assessment framework, written in MATLAB, that was developed to assess the knowledge and understanding of the students in the course by tasking them with designing a controller for a real-world problem. The MATLAB assessment framework was designed to provide the students with the freedom to design and implement any MPC controller they wanted. The submitted controllers were then assessed against over 30 variations of the real-world problem to gauge student understanding of design robustness and the MPC topics from the course.","Teaching Predictive Control Using Specification-based Summative Assessments Including Model Predictive Control (MPC) in the undergraduategraduate control curriculum is becoming vitally important due to the growing adoption of MPC in many industrial areas. In this paper, we present an overview of the predictive control course taught by the authors at Imperial College London between 2018 and 2021. We discuss how the course evolved from focusing solely on the linear MPC formulation to covering nonlinear MPC and some of its extensions. We also present a novel specification-based summative assessment framework, written in MATLAB, that was developed to assess the knowledge and understanding of the students in the course by tasking them with designing a controller for a real-world problem. The MATLAB assessment framework was designed to provide the students with the freedom to design and implement any MPC controller they wanted. The submitted controllers were then assessed against over 30 variations of the real-world problem to gauge student understanding of design robustness and the MPC topics from the course.",Education
Face Recognition Based on Polar Frequency Features,"A novel biologically motivated face recognition algorithm based on polar frequency is presented. Polar frequency descriptors are extracted from face images by Fourier-Bessel transform (FBT). Next, the Euclidean distance between all images is computed and each image is now represented by its dissimilarity to the other images. A Pseudo-Fisher Linear Discriminant was built on this dissimilarity space. The performance of Discrete Fourier transform (DFT) descriptors, and a combination of both feature types was also evaluated. The algorithms were tested on a 40- and 1196-subjects face database (ORL and FERET, respectively). With 5 images per subject in the training and test datasets, error rate on the ORL database was 3.8, 1.25 and 0.2 for the FBT, DFT, and the combined classifier, respectively, as compared to 2.6 achieved by the best previous algorithm. The most informative polar frequency features were concentrated at low-to-medium angular frequencies coupled to low radial frequencies. On the FERET database, where an affine normalization pre-processing was applied, the FBT algorithm outperformed only the PCA in a rank recognition test. However, it achieved performance comparable to state-of-the-art methods when evaluated by verification tests. These results indicate the high informative value of the polar frequency content of face images in relation to recognition and verification tasks, and that the Cartesian frequency content can complement information about the subjects identity, but possibly only when the images are not pre-normalized. Possible implications for human face recognition are discussed.","Face Recognition Based on Polar Frequency Features A novel biologically motivated face recognition algorithm based on polar frequency is presented. Polar frequency descriptors are extracted from face images by Fourier-Bessel transform (FBT). Next, the Euclidean distance between all images is computed and each image is now represented by its dissimilarity to the other images. A Pseudo-Fisher Linear Discriminant was built on this dissimilarity space. The performance of Discrete Fourier transform (DFT) descriptors, and a combination of both feature types was also evaluated. The algorithms were tested on a 40- and 1196-subjects face database (ORL and FERET, respectively). With 5 images per subject in the training and test datasets, error rate on the ORL database was 3.8, 1.25 and 0.2 for the FBT, DFT, and the combined classifier, respectively, as compared to 2.6 achieved by the best previous algorithm. The most informative polar frequency features were concentrated at low-to-medium angular frequencies coupled to low radial frequencies. On the FERET database, where an affine normalization pre-processing was applied, the FBT algorithm outperformed only the PCA in a rank recognition test. However, it achieved performance comparable to state-of-the-art methods when evaluated by verification tests. These results indicate the high informative value of the polar frequency content of face images in relation to recognition and verification tasks, and that the Cartesian frequency content can complement information about the subjects identity, but possibly only when the images are not pre-normalized. Possible implications for human face recognition are discussed.",Technology
Algorithm and performance of a clinical IMRT beam-angle optimization system,This paper describes the algorithm and examines the performance of an IMRT beam-angle optimization (BAO) system. In this algorithm successive sets of beam angles are selected from a set of predefined directions using a fast simulated annealing (FSA) algorithm. An IMRT beam-profile optimization is performed on each generated set of beams. The IMRT optimization is accelerated by using a fast dose calculation method that utilizes a precomputed dose kernel. A compact kernel is constructed for each of the predefined beams prior to starting the FSA algorithm. The IMRT optimizations during the BAO are then performed using these kernels in a fast dose calculation engine. This technique allows the IMRT optimization to be performed more than two orders of magnitude faster than a similar optimization that uses a convolution dose calculation engine.,Algorithm and performance of a clinical IMRT beam-angle optimization system This paper describes the algorithm and examines the performance of an IMRT beam-angle optimization (BAO) system. In this algorithm successive sets of beam angles are selected from a set of predefined directions using a fast simulated annealing (FSA) algorithm. An IMRT beam-profile optimization is performed on each generated set of beams. The IMRT optimization is accelerated by using a fast dose calculation method that utilizes a precomputed dose kernel. A compact kernel is constructed for each of the predefined beams prior to starting the FSA algorithm. The IMRT optimizations during the BAO are then performed using these kernels in a fast dose calculation engine. This technique allows the IMRT optimization to be performed more than two orders of magnitude faster than a similar optimization that uses a convolution dose calculation engine.,Healthcare
Fully 3D Monte Carlo image reconstruction in SPECT using functional regions,"Image reconstruction in Single Photon Emission Computed Tomography (SPECT) is affected by physical effects such as photon attenuation, Compton scatter and detector response. These effects can be compensated for by modeling the corresponding spread of photons in 3D within the system matrix used for tomographic reconstruction. The fully 3D Monte Carlo (F3DMC) reconstruction technique consists in calculating this system matrix using Monte Carlo simulations. The inverse problem of tomographic reconstruction is then solved using conventional iterative algorithms such as maximum likelihood expectation maximization (MLEM). Although F3DMC has already shown promising results, its use is currently limited by two major issues: huge size of the fully 3D system matrix and long computation time required for calculating a robust and accurate system matrix. To address these two issues, we propose to calculate the F3DMC system matrix using a spatial sampling matching the functional regions to be reconstructed. In this approach, different regions of interest can be reconstructed with different spatial sampling. For instance, a single value is reconstructed for a functional region assumed to contain uniform activity. To assess the value of this approach, Monte Carlo simulations have been performed using GATE. Results suggest that F3DMC reconstruction using functional regions improves quantitative accuracy compared to the F3DMC reconstruction method proposed so far. In addition, it considerably reduces disk space requirement and duration of the simulations needed to estimate the system matrix. The concept of functional regions might therefore make F3DMC reconstruction practically feasible.","Fully 3D Monte Carlo image reconstruction in SPECT using functional regions Image reconstruction in Single Photon Emission Computed Tomography (SPECT) is affected by physical effects such as photon attenuation, Compton scatter and detector response. These effects can be compensated for by modeling the corresponding spread of photons in 3D within the system matrix used for tomographic reconstruction. The fully 3D Monte Carlo (F3DMC) reconstruction technique consists in calculating this system matrix using Monte Carlo simulations. The inverse problem of tomographic reconstruction is then solved using conventional iterative algorithms such as maximum likelihood expectation maximization (MLEM). Although F3DMC has already shown promising results, its use is currently limited by two major issues: huge size of the fully 3D system matrix and long computation time required for calculating a robust and accurate system matrix. To address these two issues, we propose to calculate the F3DMC system matrix using a spatial sampling matching the functional regions to be reconstructed. In this approach, different regions of interest can be reconstructed with different spatial sampling. For instance, a single value is reconstructed for a functional region assumed to contain uniform activity. To assess the value of this approach, Monte Carlo simulations have been performed using GATE. Results suggest that F3DMC reconstruction using functional regions improves quantitative accuracy compared to the F3DMC reconstruction method proposed so far. In addition, it considerably reduces disk space requirement and duration of the simulations needed to estimate the system matrix. The concept of functional regions might therefore make F3DMC reconstruction practically feasible.",Healthcare
Framework for Hopfield Network based Adaptive routing - A design level approach for adaptive routing phenomena with Artificial Neural Network,"Routing, as a basic phenomena, by itself, has got umpteen scopes to analyse, discuss and arrive at an optimal solution for the technocrats over years. Routing is analysed based on many factors; few key constraints that decide the factors are communication medium, time dependency, information source nature. Parametric routing has become the requirement of the day, with some kind of adaptation to the underlying network environment. Satellite constellations, particularly LEO satellite constellations have become a reality in operational to have a non-breaking voicedata communication around the world.Routing in these constellations has to be treated in a non conventional way, taking their network geometry into consideration. One of the efficient methods of optimization is putting Neural Networks to use. Few Artificial Neural Network models are very much suitable for the adaptive control mechanism, by their nature of network arrangement. One such efficient model is Hopfield Network model. This paper is an attempt to design a framework for the Hopfield Network based adaptive routing phenomena in satellite constellations.","Framework for Hopfield Network based Adaptive routing - A design level approach for adaptive routing phenomena with Artificial Neural Network Routing, as a basic phenomena, by itself, has got umpteen scopes to analyse, discuss and arrive at an optimal solution for the technocrats over years. Routing is analysed based on many factors; few key constraints that decide the factors are communication medium, time dependency, information source nature. Parametric routing has become the requirement of the day, with some kind of adaptation to the underlying network environment. Satellite constellations, particularly LEO satellite constellations have become a reality in operational to have a non-breaking voicedata communication around the world.Routing in these constellations has to be treated in a non conventional way, taking their network geometry into consideration. One of the efficient methods of optimization is putting Neural Networks to use. Few Artificial Neural Network models are very much suitable for the adaptive control mechanism, by their nature of network arrangement. One such efficient model is Hopfield Network model. This paper is an attempt to design a framework for the Hopfield Network based adaptive routing phenomena in satellite constellations.",Technology
Two novel evolutionary formulations of the graph coloring problem,"We introduce two novel evolutionary formulations of the problem of coloring the nodes of a graph. The first formulation is based on the relationship that exists between a graphs chromatic number and its acyclic orientations. It views such orientations as individuals and evolves them with the aid of evolutionary operators that are very heavily based on the structure of the graph and its acyclic orientations. The second formulation, unlike the first one, does not tackle one graph at a time, but rather aims at evolving a program to color all graphs belonging to a class whose members all have the same number of nodes and other common attributes. The heuristics that result from these formulations have been tested on some of the Second DIMACS Implementation Challenge benchmark graphs, and have been found to be competitive when compared to the several other heuristics that have also been tested on those graphs.","Two novel evolutionary formulations of the graph coloring problem We introduce two novel evolutionary formulations of the problem of coloring the nodes of a graph. The first formulation is based on the relationship that exists between a graphs chromatic number and its acyclic orientations. It views such orientations as individuals and evolves them with the aid of evolutionary operators that are very heavily based on the structure of the graph and its acyclic orientations. The second formulation, unlike the first one, does not tackle one graph at a time, but rather aims at evolving a program to color all graphs belonging to a class whose members all have the same number of nodes and other common attributes. The heuristics that result from these formulations have been tested on some of the Second DIMACS Implementation Challenge benchmark graphs, and have been found to be competitive when compared to the several other heuristics that have also been tested on those graphs.",Technology
TiZero: Mastering Multi-Agent Football with Curriculum Learning and Self-Play,"Multi-agent football poses an unsolved challenge in AI research. Existing work has focused on tackling simplified scenarios of the game, or else leveraging expert demonstrations. In this paper, we develop a multi-agent system to play the full 11 vs. 11 game mode, without demonstrations. This game mode contains aspects that present major challenges to modern reinforcement learning algorithms; multi-agent coordination, long-term planning, and non-transitivity. To address these challenges, we present TiZero; a self-evolving, multi-agent system that learns from scratch. TiZero introduces several innovations, including adaptive curriculum learning, a novel self-play strategy, and an objective that optimizes the policies of multiple agents jointly. Experimentally, it outperforms previous systems by a large margin on the Google Research Football environment, increasing win rates by over 30. To demonstrate the generality of TiZeros innovations, they are assessed on several environments beyond football; Overcooked, Multi-agent Particle-Environment, Tic-Tac-Toe and Connect-Four.","TiZero: Mastering Multi-Agent Football with Curriculum Learning and Self-Play Multi-agent football poses an unsolved challenge in AI research. Existing work has focused on tackling simplified scenarios of the game, or else leveraging expert demonstrations. In this paper, we develop a multi-agent system to play the full 11 vs. 11 game mode, without demonstrations. This game mode contains aspects that present major challenges to modern reinforcement learning algorithms; multi-agent coordination, long-term planning, and non-transitivity. To address these challenges, we present TiZero; a self-evolving, multi-agent system that learns from scratch. TiZero introduces several innovations, including adaptive curriculum learning, a novel self-play strategy, and an objective that optimizes the policies of multiple agents jointly. Experimentally, it outperforms previous systems by a large margin on the Google Research Football environment, increasing win rates by over 30. To demonstrate the generality of TiZeros innovations, they are assessed on several environments beyond football; Overcooked, Multi-agent Particle-Environment, Tic-Tac-Toe and Connect-Four.",Education
"Macrostate Parameter, an Econophysics Approach for the Risk Analysis of the Stock Exchange Market Transactions","In this paper we attempt to introduce an econophysics approach to evaluate some aspects of the risks in financial markets. For this purpose, the thermodynamical methods and statistical physics results about entropy and equilibrium states in the physical systems are used. Some considerations on economic value and financial information are made. Finally, on this basis, a new index for the financial risk estimation of the stock-exchange market transactions, named macrostate parameter, was introduced and discussed. Keywords: econophysics, stock-exchange markets, financial risk, informational fascicle, entropy, macrostate parameter.","Macrostate Parameter, an Econophysics Approach for the Risk Analysis of the Stock Exchange Market Transactions In this paper we attempt to introduce an econophysics approach to evaluate some aspects of the risks in financial markets. For this purpose, the thermodynamical methods and statistical physics results about entropy and equilibrium states in the physical systems are used. Some considerations on economic value and financial information are made. Finally, on this basis, a new index for the financial risk estimation of the stock-exchange market transactions, named macrostate parameter, was introduced and discussed. Keywords: econophysics, stock-exchange markets, financial risk, informational fascicle, entropy, macrostate parameter.",Finance
Dynamic behaviors in directed networks,"Motivated by the abundance of directed synaptic couplings in a real biological neuronal network, we investigate the synchronization behavior of the Hodgkin-Huxley model in a directed network. We start from the standard model of the Watts-Strogatz undirected network and then change undirected edges to directed arcs with a given probability, still preserving the connectivity of the network. A generalized clustering coefficient for directed networks is defined and used to investigate the interplay between the synchronization behavior and underlying structural properties of directed networks. We observe that the directedness of complex networks plays an important role in emerging dynamical behaviors, which is also confirmed by a numerical study of the sociological game theoretic voter model on directed networks.","Dynamic behaviors in directed networks Motivated by the abundance of directed synaptic couplings in a real biological neuronal network, we investigate the synchronization behavior of the Hodgkin-Huxley model in a directed network. We start from the standard model of the Watts-Strogatz undirected network and then change undirected edges to directed arcs with a given probability, still preserving the connectivity of the network. A generalized clustering coefficient for directed networks is defined and used to investigate the interplay between the synchronization behavior and underlying structural properties of directed networks. We observe that the directedness of complex networks plays an important role in emerging dynamical behaviors, which is also confirmed by a numerical study of the sociological game theoretic voter model on directed networks.",Healthcare
Feedforward Neural Networks with Diffused Nonlinear Weight Functions,"In this paper, feedforward neural networks are presented that have nonlinear weight functions based on look--up tables, that are specially smoothed in a regularization called the diffusion. The idea of such a type of networks is based on the hypothesis that the greater number of adaptive parameters per a weight function might reduce the total number of the weight functions needed to solve a given problem. Then, if the computational complexity of a propagation through a single such a weight function would be kept low, then the introduced neural networks might possibly be relatively fast. A number of tests is performed, showing that the presented neural networks may indeed perform better in some cases than the classic neural networks and a number of other learning machines.","Feedforward Neural Networks with Diffused Nonlinear Weight Functions In this paper, feedforward neural networks are presented that have nonlinear weight functions based on look--up tables, that are specially smoothed in a regularization called the diffusion. The idea of such a type of networks is based on the hypothesis that the greater number of adaptive parameters per a weight function might reduce the total number of the weight functions needed to solve a given problem. Then, if the computational complexity of a propagation through a single such a weight function would be kept low, then the introduced neural networks might possibly be relatively fast. A number of tests is performed, showing that the presented neural networks may indeed perform better in some cases than the classic neural networks and a number of other learning machines.",Technology
Deep ocean influence on upper ocean baroclinic instability saturation,"In this paper we extend earlier results regarding the effects of the lower layer of the ocean (below the thermocline) on the baroclinic instability within the upper layer (above the thermocline). We confront quasigeostrophic baroclinic instability properties of a 2.5-layer model with those of a 3-layer model with a very thick deep layer, which has been shown to predict spectral instability for basic state parameters for which the 2.5-layer model predicts nonlinear stability. We compute and compare maximum normal-mode perturbation growth rates, as well as rigorous upper bounds on the nonlinear growth of perturbations to unstable basic states, paying particular attention to the region of basic state parameters where the stability properties of the 2.5- and 3-layer model differ substantially. We found that normal-mode perturbation growth rates in the 3-layer model tend to maximize in this region. We also found that the size of state space available for eddy-amplitude growth tends to minimize in this same region. Moreover, we found that for a large spread of parameter values in this region the latter size reduces to only a small fraction of the total enstrophy of the system, thereby allowing us to make assessments of the significance of the instabilities.","Deep ocean influence on upper ocean baroclinic instability saturation In this paper we extend earlier results regarding the effects of the lower layer of the ocean (below the thermocline) on the baroclinic instability within the upper layer (above the thermocline). We confront quasigeostrophic baroclinic instability properties of a 2.5-layer model with those of a 3-layer model with a very thick deep layer, which has been shown to predict spectral instability for basic state parameters for which the 2.5-layer model predicts nonlinear stability. We compute and compare maximum normal-mode perturbation growth rates, as well as rigorous upper bounds on the nonlinear growth of perturbations to unstable basic states, paying particular attention to the region of basic state parameters where the stability properties of the 2.5- and 3-layer model differ substantially. We found that normal-mode perturbation growth rates in the 3-layer model tend to maximize in this region. We also found that the size of state space available for eddy-amplitude growth tends to minimize in this same region. Moreover, we found that for a large spread of parameter values in this region the latter size reduces to only a small fraction of the total enstrophy of the system, thereby allowing us to make assessments of the significance of the instabilities.",Environment
Adaptive Learning Systems: Personalized Curriculum Design Using LLM-Powered Analytics,"Large language models (LLMs) are revolutionizing the field of education by enabling personalized learning experiences tailored to individual student needs. In this paper, we introduce a framework for Adaptive Learning Systems that leverages LLM-powered analytics for personalized curriculum design. This innovative approach uses advanced machine learning to analyze real-time data, allowing the system to adapt learning pathways and recommend resources that align with each learners progress. By continuously assessing students, our framework enhances instructional strategies, ensuring that the materials presented are relevant and engaging. Experimental results indicate a marked improvement in both learner engagement and knowledge retention when using a customized curriculum. Evaluations conducted across varied educational environments demonstrate the frameworks flexibility and positive influence on learning outcomes, potentially reshaping conventional educational practices into a more adaptive and student-centered model.","Adaptive Learning Systems: Personalized Curriculum Design Using LLM-Powered Analytics Large language models (LLMs) are revolutionizing the field of education by enabling personalized learning experiences tailored to individual student needs. In this paper, we introduce a framework for Adaptive Learning Systems that leverages LLM-powered analytics for personalized curriculum design. This innovative approach uses advanced machine learning to analyze real-time data, allowing the system to adapt learning pathways and recommend resources that align with each learners progress. By continuously assessing students, our framework enhances instructional strategies, ensuring that the materials presented are relevant and engaging. Experimental results indicate a marked improvement in both learner engagement and knowledge retention when using a customized curriculum. Evaluations conducted across varied educational environments demonstrate the frameworks flexibility and positive influence on learning outcomes, potentially reshaping conventional educational practices into a more adaptive and student-centered model.",Education
Shocking concerns: public perception about climate change and the macroeconomy,"Public perceptions of climate change arguably contribute to shaping private adaptation and support for policy intervention. In this paper, we propose a novel Climate Concern Index (CCI), based on disaggregated web-search volumes related to climate change topics, to gauge the intensity and dynamic evolution of collective climate perceptions, and evaluate its impacts on the business cycle. Using data from the United States over the 2004:2024 span, we capture widespread shifts in perceived climate-related risks, particularly those consistent with the postcognitive interpretation of affective responses to extreme climate events. To assess the aggregate implications of evolving public concerns about the climate, we estimate a proxy-SVAR model and find that exogenous variation in the CCI entails a statistically significant drop in both employment and private consumption and a persistent surge in stock market volatility, while core inflation remains largely unaffected. These results suggest that, even in the absence of direct physical risks, heightened concerns for climate-related phenomena can trigger behavioral adaptation with nontrivial consequences for the macroeconomy, thereby demanding attention from institutional players in the macro-financial field.","Shocking concerns: public perception about climate change and the macroeconomy Public perceptions of climate change arguably contribute to shaping private adaptation and support for policy intervention. In this paper, we propose a novel Climate Concern Index (CCI), based on disaggregated web-search volumes related to climate change topics, to gauge the intensity and dynamic evolution of collective climate perceptions, and evaluate its impacts on the business cycle. Using data from the United States over the 2004:2024 span, we capture widespread shifts in perceived climate-related risks, particularly those consistent with the postcognitive interpretation of affective responses to extreme climate events. To assess the aggregate implications of evolving public concerns about the climate, we estimate a proxy-SVAR model and find that exogenous variation in the CCI entails a statistically significant drop in both employment and private consumption and a persistent surge in stock market volatility, while core inflation remains largely unaffected. These results suggest that, even in the absence of direct physical risks, heightened concerns for climate-related phenomena can trigger behavioral adaptation with nontrivial consequences for the macroeconomy, thereby demanding attention from institutional players in the macro-financial field.",Environment
Improving probabilistic weather forecasts using seasonally varying calibration parameters,We show that probabilistic weather forecasts of site specific temperatures can be dramatically improved by using seasonally varying rather than constant calibration parameters.,Improving probabilistic weather forecasts using seasonally varying calibration parameters We show that probabilistic weather forecasts of site specific temperatures can be dramatically improved by using seasonally varying rather than constant calibration parameters.,Environment
Inflation and unemployment in Japan: from 1980 to 2050,"The evolution of inflation, p(t), and unemployment, UE(t), in Japan has been modeled. Both variables were represented as linear functions of the change rate of labor force, dLFLF. These models provide an accurate description of disinflation in the 1990s and a deflationary period in the 2000s. In Japan, there exists a statistically reliable (R20.68) Phillips curve, which is characterized by a negative relation between inflation and unemployment and their synchronous evolution: UE(t)  -0.94p(t)  0.045. Effectively, growing unemployment has resulted in decreasing inflation since 1982. A linear and lagged generalized relationship between inflation, unemployment and labor force has been also obtained for Japan: p(t)  2.8dLF(t)LF(t)  0.9UE(t) - 0.0392. Labor force projections allow a prediction of inflation and unemployment in Japan: CPI inflation will be negative (between -0.5 and -1 per year) during the next 40 years. Unemployment will increase from 4.0 in 2010 to 5.3 in 2050.","Inflation and unemployment in Japan: from 1980 to 2050 The evolution of inflation, p(t), and unemployment, UE(t), in Japan has been modeled. Both variables were represented as linear functions of the change rate of labor force, dLFLF. These models provide an accurate description of disinflation in the 1990s and a deflationary period in the 2000s. In Japan, there exists a statistically reliable (R20.68) Phillips curve, which is characterized by a negative relation between inflation and unemployment and their synchronous evolution: UE(t)  -0.94p(t)  0.045. Effectively, growing unemployment has resulted in decreasing inflation since 1982. A linear and lagged generalized relationship between inflation, unemployment and labor force has been also obtained for Japan: p(t)  2.8dLF(t)LF(t)  0.9UE(t) - 0.0392. Labor force projections allow a prediction of inflation and unemployment in Japan: CPI inflation will be negative (between -0.5 and -1 per year) during the next 40 years. Unemployment will increase from 4.0 in 2010 to 5.3 in 2050.",Finance
A Simple Solution of the Lotka-Volterra Equations,"In this work we consider a simple, approximate, tending toward exact, solution of the system of two usual Lotka-Volterra differential equations. Given solution is obtained by an iterative method. In any finite approximation order of this solution, exponents of the corresponding Lotka-Volterra variables have simple, time polynomial form. When approximation order tends to infinity obtained approximate solution converges toward exact solution in some finite time interval.","A Simple Solution of the Lotka-Volterra Equations In this work we consider a simple, approximate, tending toward exact, solution of the system of two usual Lotka-Volterra differential equations. Given solution is obtained by an iterative method. In any finite approximation order of this solution, exponents of the corresponding Lotka-Volterra variables have simple, time polynomial form. When approximation order tends to infinity obtained approximate solution converges toward exact solution in some finite time interval.",Healthcare
Estimating Individualized Treatment Rules for Ordinal Treatments,"Precision medicine is an emerging scientific topic for disease treatment and prevention that takes into account individual patient characteristics. It is an important direction for clinical research, and many statistical methods have been recently proposed. One of the primary goals of precision medicine is to obtain an optimal individual treatment rule (ITR), which can help make decisions on treatment selection according to each patients specific characteristics. Recently, outcome weighted learning (OWL) has been proposed to estimate such an optimal ITR in a binary treatment setting by maximizing the expected clinical outcome. However, for ordinal treatment settings, such as individualized dose finding, it is unclear how to use OWL. In this paper, we propose a new technique for estimating ITR with ordinal treatments. In particular, we propose a data duplication technique with a piecewise convex loss function. We establish Fisher consistency for the resulting estimated ITR under certain conditions, and obtain the convergence and risk bound properties. Simulated examples and two applications to datasets from an irritable bowel problem and a type 2 diabetes mellitus observational study demonstrate the highly competitive performance of the proposed method compared to existing alternatives.","Estimating Individualized Treatment Rules for Ordinal Treatments Precision medicine is an emerging scientific topic for disease treatment and prevention that takes into account individual patient characteristics. It is an important direction for clinical research, and many statistical methods have been recently proposed. One of the primary goals of precision medicine is to obtain an optimal individual treatment rule (ITR), which can help make decisions on treatment selection according to each patients specific characteristics. Recently, outcome weighted learning (OWL) has been proposed to estimate such an optimal ITR in a binary treatment setting by maximizing the expected clinical outcome. However, for ordinal treatment settings, such as individualized dose finding, it is unclear how to use OWL. In this paper, we propose a new technique for estimating ITR with ordinal treatments. In particular, we propose a data duplication technique with a piecewise convex loss function. We establish Fisher consistency for the resulting estimated ITR under certain conditions, and obtain the convergence and risk bound properties. Simulated examples and two applications to datasets from an irritable bowel problem and a type 2 diabetes mellitus observational study demonstrate the highly competitive performance of the proposed method compared to existing alternatives.",Healthcare
Monte Carlo study of the scattering error of a quartz reflective absorption tube,"A Monte Carlo model was used to study the scattering error of an absorption meter with a divergent light beam and a limited acceptance angle of the receiver. Reflections at both ends of the tube were taken into account. Calculations of the effect of varying optical properties of water, as well as the receiver geometry, were performed. A weighting function showing the scattering error quantitatively as a function of angle was introduced. Some cases of the practical interests are discussed.","Monte Carlo study of the scattering error of a quartz reflective absorption tube A Monte Carlo model was used to study the scattering error of an absorption meter with a divergent light beam and a limited acceptance angle of the receiver. Reflections at both ends of the tube were taken into account. Calculations of the effect of varying optical properties of water, as well as the receiver geometry, were performed. A weighting function showing the scattering error quantitatively as a function of angle was introduced. Some cases of the practical interests are discussed.",Environment
Swing Options Valuation: a BSDE with Constrained Jumps Approach,"We introduce a new probabilistic method for solving a class of impulse control problems based on their representations as Backward Stochastic Differential Equations (BSDEs for short) with constrained jumps. As an example, our method is used for pricing Swing options. We deal with the jump constraint by a penalization procedure and apply a discrete-time backward scheme to the resulting penalized BSDE with jumps. We study the convergence of this numerical method, with respect to the main approximation parameters: the jump intensity lambda, the penalization parameter p  0 and the time step. In particular, we obtain a convergence rate of the error due to penalization of order (lambda p)alpha - frac12, forall alpha in left(0, frac12right). Combining this approach with Monte Carlo techniques, we then work out the valuation problem of (normalized) Swing options in the Black and Scholes framework. We present numerical tests and compare our results with a classical iteration method.","Swing Options Valuation: a BSDE with Constrained Jumps Approach We introduce a new probabilistic method for solving a class of impulse control problems based on their representations as Backward Stochastic Differential Equations (BSDEs for short) with constrained jumps. As an example, our method is used for pricing Swing options. We deal with the jump constraint by a penalization procedure and apply a discrete-time backward scheme to the resulting penalized BSDE with jumps. We study the convergence of this numerical method, with respect to the main approximation parameters: the jump intensity lambda, the penalization parameter p  0 and the time step. In particular, we obtain a convergence rate of the error due to penalization of order (lambda p)alpha - frac12, forall alpha in left(0, frac12right). Combining this approach with Monte Carlo techniques, we then work out the valuation problem of (normalized) Swing options in the Black and Scholes framework. We present numerical tests and compare our results with a classical iteration method.",Finance
Bridging the AI Adoption Gap: Designing an Interactive Pedagogical Agent for Higher Education Instructors,"Instructors play a pivotal role in integrating AI into education, yet their adoption of AI-powered tools remains inconsistent. Despite this, limited research explores how to design AI tools that support broader instructor adoption. This study applies a human-centered design approach, incorporating qualitative methods, to investigate the design of interactive pedagogical agents that provide instructional suggestions in response to instructors questions. We conducted a formative study involving interviews with five pedagogy experts to examine existing strategies for supporting instructors pedagogical needs. Building on these insights, we facilitated a participatory design session with ten pedagogy experts, where participants reviewed a storyboard depicting a chatbot designed for instructors with varying levels of AI literacy and differing attitudes toward AI. Experts also evaluated the quality of LLM-generated suggestions based on common teaching challenges. Our findings highlight the need for chatbot interactions that foster trust, especially for AI-conservative instructors. Experts emphasized the importance of social transparency (for example, showing how peers use the tool) and allowing instructors to flexibly control how much or how little they engage with the system. We also propose design recommendations to enhance the quality of AI-generated teaching suggestions, such as adapting them to reflect instructors prior teaching experience. This work underscores the urgent need to support AI-conservative instructors, as AI literacy and attitudes are closely intertwined. Without thoughtful design, there is a risk of widening pedagogical divides and reducing students learning opportunities.","Bridging the AI Adoption Gap: Designing an Interactive Pedagogical Agent for Higher Education Instructors Instructors play a pivotal role in integrating AI into education, yet their adoption of AI-powered tools remains inconsistent. Despite this, limited research explores how to design AI tools that support broader instructor adoption. This study applies a human-centered design approach, incorporating qualitative methods, to investigate the design of interactive pedagogical agents that provide instructional suggestions in response to instructors questions. We conducted a formative study involving interviews with five pedagogy experts to examine existing strategies for supporting instructors pedagogical needs. Building on these insights, we facilitated a participatory design session with ten pedagogy experts, where participants reviewed a storyboard depicting a chatbot designed for instructors with varying levels of AI literacy and differing attitudes toward AI. Experts also evaluated the quality of LLM-generated suggestions based on common teaching challenges. Our findings highlight the need for chatbot interactions that foster trust, especially for AI-conservative instructors. Experts emphasized the importance of social transparency (for example, showing how peers use the tool) and allowing instructors to flexibly control how much or how little they engage with the system. We also propose design recommendations to enhance the quality of AI-generated teaching suggestions, such as adapting them to reflect instructors prior teaching experience. This work underscores the urgent need to support AI-conservative instructors, as AI literacy and attitudes are closely intertwined. Without thoughtful design, there is a risk of widening pedagogical divides and reducing students learning opportunities.",Education
Credit derivatives: instruments of hedging and factors of instability. The example of ?Credit Default Swaps? on French reference entities,"Through a long-period analysis of the inter-temporal relations between the French markets for credit default swaps (CDS), shares and bonds between 2001 and 2008, this article shows how a financial innovation like CDS could heighten financial instability. After describing the operating principles of credit derivatives in general and CDS in particular, we construct two difference VAR models on the series: the share return rates, the variation in bond spreads and the variation in CDS spreads for thirteen French companies, with the aim of bringing to light the relations between these three markets. According to these models, there is indeed an interdependence between the French share, CDS and bond markets, with a strong influence of the share market on the other two. This interdependence increases during periods of tension on the markets (2001-2002, and since the summer of 2007).","Credit derivatives: instruments of hedging and factors of instability. The example of ?Credit Default Swaps? on French reference entities Through a long-period analysis of the inter-temporal relations between the French markets for credit default swaps (CDS), shares and bonds between 2001 and 2008, this article shows how a financial innovation like CDS could heighten financial instability. After describing the operating principles of credit derivatives in general and CDS in particular, we construct two difference VAR models on the series: the share return rates, the variation in bond spreads and the variation in CDS spreads for thirteen French companies, with the aim of bringing to light the relations between these three markets. According to these models, there is indeed an interdependence between the French share, CDS and bond markets, with a strong influence of the share market on the other two. This interdependence increases during periods of tension on the markets (2001-2002, and since the summer of 2007).",Finance
Learning rational stochastic languages,"Given a finite set of words w1,...,wn independently drawn according to a fixed unknown distribution law P called a stochastic language, an usual goal in Grammatical Inference is to infer an estimate of P in some class of probabilistic models, such as Probabilistic Automata (PA). Here, we study the class of rational stochastic languages, which consists in stochastic languages that can be generated by Multiplicity Automata (MA) and which strictly includes the class of stochastic languages generated by PA. Rational stochastic languages have minimal normal representation which may be very concise, and whose parameters can be efficiently estimated from stochastic samples. We design an efficient inference algorithm DEES which aims at building a minimal normal representation of the target. Despite the fact that no recursively enumerable class of MA computes exactly the set of rational stochastic languages over Q, we show that DEES strongly identifies tis set in the limit. We study the intermediary MA output by DEES and show that they compute rational series which converge absolutely to one and which can be used to provide stochastic languages which closely estimate the target.","Learning rational stochastic languages Given a finite set of words w1,...,wn independently drawn according to a fixed unknown distribution law P called a stochastic language, an usual goal in Grammatical Inference is to infer an estimate of P in some class of probabilistic models, such as Probabilistic Automata (PA). Here, we study the class of rational stochastic languages, which consists in stochastic languages that can be generated by Multiplicity Automata (MA) and which strictly includes the class of stochastic languages generated by PA. Rational stochastic languages have minimal normal representation which may be very concise, and whose parameters can be efficiently estimated from stochastic samples. We design an efficient inference algorithm DEES which aims at building a minimal normal representation of the target. Despite the fact that no recursively enumerable class of MA computes exactly the set of rational stochastic languages over Q, we show that DEES strongly identifies tis set in the limit. We study the intermediary MA output by DEES and show that they compute rational series which converge absolutely to one and which can be used to provide stochastic languages which closely estimate the target.",Technology
Adaptive treatment allocation and selection in multi-arm clinical trials: a Bayesian perspective,"Clinical trials are an instrument for making informed decisions based on evidence from well-designed experiments. Here we consider adaptive designs mainly from the perspective of multi-arm Phase II clinical trials, in which one or more experimental treatments are compared to a control. Treatment allocation of individual trial participants is assumed to take place according to a fixed block randomization, albeit with an important twist: The performance of each treatment arm is assessed after every measured outcome, in terms of the posterior distribution of a corresponding model parameter. Different treatments arms are then compared to each other, according to pre-defined criteria and using the joint posterior as the basis for such assessment. If a treatment is found to be sufficiently clearly inferior to the currently best candidate, it can be closed off either temporarily or permanently from further participant accrual. The latter possibility provides a method for adaptive treatment selection, including early stopping of the trial. The main development in the paper is in terms of binary outcomes, but some extensions, notably for handling time-to-event data, are discussed as well. The presentation is to a large extent comparative and expository.","Adaptive treatment allocation and selection in multi-arm clinical trials: a Bayesian perspective Clinical trials are an instrument for making informed decisions based on evidence from well-designed experiments. Here we consider adaptive designs mainly from the perspective of multi-arm Phase II clinical trials, in which one or more experimental treatments are compared to a control. Treatment allocation of individual trial participants is assumed to take place according to a fixed block randomization, albeit with an important twist: The performance of each treatment arm is assessed after every measured outcome, in terms of the posterior distribution of a corresponding model parameter. Different treatments arms are then compared to each other, according to pre-defined criteria and using the joint posterior as the basis for such assessment. If a treatment is found to be sufficiently clearly inferior to the currently best candidate, it can be closed off either temporarily or permanently from further participant accrual. The latter possibility provides a method for adaptive treatment selection, including early stopping of the trial. The main development in the paper is in terms of binary outcomes, but some extensions, notably for handling time-to-event data, are discussed as well. The presentation is to a large extent comparative and expository.",Healthcare
To accept or not to accept? An IRT-TOE Framework to Understand Educators Resistance to Generative AI in Higher Education,"Since the public release of Chat Generative Pre-Trained Transformer (ChatGPT), extensive discourse has emerged concerning the potential advantages and challenges of integrating Generative Artificial Intelligence (GenAI) into education. In the realm of information systems, research on technology adoption is crucial for understanding the diverse factors influencing the uptake of specific technologies. Theoretical frameworks, refined and validated over decades, serve as guiding tools to elucidate the individual and organizational dynamics, obstacles, and perceptions surrounding technology adoption. However, while several models have been proposed, they often prioritize elucidating the factors that facilitate acceptance over those that impede it, typically focusing on the student perspective and leaving a gap in empirical evidence regarding educators viewpoints. Given the pivotal role educators play in higher education, this study aims to develop a theoretical model to empirically predict the barriers preventing educators from adopting GenAI in their classrooms. Acknowledging the lack of theoretical models tailored to identifying such barriers, our approach is grounded in the Innovation Resistance Theory (IRT) framework and augmented with constructs from the Technology-Organization-Environment (TOE) framework. This model is transformed into a measurement instrument employing a quantitative approach, complemented by a qualitative approach to enrich the analysis and uncover concerns related to GenAI adoption in the higher education domain.","To accept or not to accept? An IRT-TOE Framework to Understand Educators Resistance to Generative AI in Higher Education Since the public release of Chat Generative Pre-Trained Transformer (ChatGPT), extensive discourse has emerged concerning the potential advantages and challenges of integrating Generative Artificial Intelligence (GenAI) into education. In the realm of information systems, research on technology adoption is crucial for understanding the diverse factors influencing the uptake of specific technologies. Theoretical frameworks, refined and validated over decades, serve as guiding tools to elucidate the individual and organizational dynamics, obstacles, and perceptions surrounding technology adoption. However, while several models have been proposed, they often prioritize elucidating the factors that facilitate acceptance over those that impede it, typically focusing on the student perspective and leaving a gap in empirical evidence regarding educators viewpoints. Given the pivotal role educators play in higher education, this study aims to develop a theoretical model to empirically predict the barriers preventing educators from adopting GenAI in their classrooms. Acknowledging the lack of theoretical models tailored to identifying such barriers, our approach is grounded in the Innovation Resistance Theory (IRT) framework and augmented with constructs from the Technology-Organization-Environment (TOE) framework. This model is transformed into a measurement instrument employing a quantitative approach, complemented by a qualitative approach to enrich the analysis and uncover concerns related to GenAI adoption in the higher education domain.",Education
Contains and Inside relationships within combinatorial Pyramids,Irregular pyramids are made of a stack of successively reduced graphs embedded in the plane. Such pyramids are used within the segmentation framework to encode a hierarchy of partitions. The different graph models used within the irregular pyramid framework encode different types of relationships between regions. This paper compares different graph models used within the irregular pyramid framework according to a set of relationships between regions. We also define a new algorithm based on a pyramid of combinatorial maps which allows to determine if one region contains the other using only local calculus.,Contains and Inside relationships within combinatorial Pyramids Irregular pyramids are made of a stack of successively reduced graphs embedded in the plane. Such pyramids are used within the segmentation framework to encode a hierarchy of partitions. The different graph models used within the irregular pyramid framework encode different types of relationships between regions. This paper compares different graph models used within the irregular pyramid framework according to a set of relationships between regions. We also define a new algorithm based on a pyramid of combinatorial maps which allows to determine if one region contains the other using only local calculus.,Technology
Induction of First-Order Decision Lists: Results on Learning the Past Tense of English Verbs,"This paper presents a method for inducing logic programs from examples that learns a new class of concepts called first-order decision lists, defined as ordered lists of clauses each ending in a cut. The method, called FOIDL, is based on FOIL (Quinlan, 1990) but employs intensional background knowledge and avoids the need for explicit negative examples. It is particularly useful for problems that involve rules with specific exceptions, such as learning the past-tense of English verbs, a task widely studied in the context of the symbolicconnectionist debate. FOIDL is able to learn concise, accurate programs for this problem from significantly fewer examples than previous methods (both connectionist and symbolic).","Induction of First-Order Decision Lists: Results on Learning the Past Tense of English Verbs This paper presents a method for inducing logic programs from examples that learns a new class of concepts called first-order decision lists, defined as ordered lists of clauses each ending in a cut. The method, called FOIDL, is based on FOIL (Quinlan, 1990) but employs intensional background knowledge and avoids the need for explicit negative examples. It is particularly useful for problems that involve rules with specific exceptions, such as learning the past-tense of English verbs, a task widely studied in the context of the symbolicconnectionist debate. FOIDL is able to learn concise, accurate programs for this problem from significantly fewer examples than previous methods (both connectionist and symbolic).",Technology
Predictive Directions for Individualized Treatment Selection in Clinical Trials,"In many clinical trials, individuals in different subgroups have experience differential treatment effects. This leads to individualized differences in treatment benefit. In this article, we introduce the general concept of predictive directions, which are risk scores motivated by potential outcomes considerations. These techniques borrow heavily from sufficient dimension reduction (SDR) and causal inference methodology. Under some conditions, one can use existing methods from the SDR literature to estimate the directions assuming an idealized complete data structure, which subsequently yields an obvious extension to clinical trial datasets. In addition, we generalize the direction idea to a nonlinear setting that exploits support vector machines. The methodology is illustrated with application to a series of colorectal cancer clinical trials.","Predictive Directions for Individualized Treatment Selection in Clinical Trials In many clinical trials, individuals in different subgroups have experience differential treatment effects. This leads to individualized differences in treatment benefit. In this article, we introduce the general concept of predictive directions, which are risk scores motivated by potential outcomes considerations. These techniques borrow heavily from sufficient dimension reduction (SDR) and causal inference methodology. Under some conditions, one can use existing methods from the SDR literature to estimate the directions assuming an idealized complete data structure, which subsequently yields an obvious extension to clinical trial datasets. In addition, we generalize the direction idea to a nonlinear setting that exploits support vector machines. The methodology is illustrated with application to a series of colorectal cancer clinical trials.",Healthcare
Energy Management for Renewable-Colocated Artificial Intelligence Data Centers,"We develop an energy management system (EMS) for artificial intelligence (AI) data centers with colocated renewable generation. Under a profit-maximizing framework, the EMS of renewable-colocated data center (RCDC) co-optimizes AI workload scheduling, on-site renewable utilization, and electricity market participation. Within both wholesale and retail market participation models, the economic benefit of the RCDC operation is maximized. Empirical evaluations using real-world traces of electricity prices, data center power consumption, and renewable generation demonstrate significant profit gains from renewable and AI data center colocations.","Energy Management for Renewable-Colocated Artificial Intelligence Data Centers We develop an energy management system (EMS) for artificial intelligence (AI) data centers with colocated renewable generation. Under a profit-maximizing framework, the EMS of renewable-colocated data center (RCDC) co-optimizes AI workload scheduling, on-site renewable utilization, and electricity market participation. Within both wholesale and retail market participation models, the economic benefit of the RCDC operation is maximized. Empirical evaluations using real-world traces of electricity prices, data center power consumption, and renewable generation demonstrate significant profit gains from renewable and AI data center colocations.",Environment
Presentation Du Nouvel Accord De Bale Sur Les Fonds Propres,"In order to adapt to the liberalization of the financial sphere started in the Eighties, marked in particular by the end of the framing of credit, the disappearance of the various forms of protection of the State whose profited the banks, and the privatization of the near total of the establishments in Europe, the banking regulation evolved to a prudential approach, perceived like the only mode of regulation not entering in contradiction with the rules of the market. The current banking regulation is pressed on the supervision, the discipline of the market and the ratios prudential; in particular the ratios of the minimal own capital stocks. The object of this article is the presentation of the architecture of the new agreement of Basle (1999) which is based on three pillars consolidating it self mutually.","Presentation Du Nouvel Accord De Bale Sur Les Fonds Propres In order to adapt to the liberalization of the financial sphere started in the Eighties, marked in particular by the end of the framing of credit, the disappearance of the various forms of protection of the State whose profited the banks, and the privatization of the near total of the establishments in Europe, the banking regulation evolved to a prudential approach, perceived like the only mode of regulation not entering in contradiction with the rules of the market. The current banking regulation is pressed on the supervision, the discipline of the market and the ratios prudential; in particular the ratios of the minimal own capital stocks. The object of this article is the presentation of the architecture of the new agreement of Basle (1999) which is based on three pillars consolidating it self mutually.",Finance
Big Data and Education: using big data analytics in language learning,"Working with big data using data mining tools is rapidly becoming a trend in education industry. The combination of the current capacity to collect, store, manage and process data in a timely manner, and data from online educational platforms represents an unprecedented opportunity for educational institutes, learners, educators, and researchers. In this position paper, we consider some basic concepts as well as most popular tools, methods and techniques regarding Educational Data Mining and Learning Analytics, and discuss big data applications in language learning, in particular.","Big Data and Education: using big data analytics in language learning Working with big data using data mining tools is rapidly becoming a trend in education industry. The combination of the current capacity to collect, store, manage and process data in a timely manner, and data from online educational platforms represents an unprecedented opportunity for educational institutes, learners, educators, and researchers. In this position paper, we consider some basic concepts as well as most popular tools, methods and techniques regarding Educational Data Mining and Learning Analytics, and discuss big data applications in language learning, in particular.",Education
Representation of Functional Data in Neural Networks,"Functional Data Analysis (FDA) is an extension of traditional data analysis to functional data, for example spectra, temporal series, spatio-temporal images, gesture recognition data, etc. Functional data are rarely known in practice; usually a regular or irregular sampling is known. For this reason, some processing is needed in order to benefit from the smooth character of functional data in the analysis methods. This paper shows how to extend the Radial-Basis Function Networks (RBFN) and Multi-Layer Perceptron (MLP) models to functional data inputs, in particular when the latter are known through lists of input-output pairs. Various possibilities for functional processing are discussed, including the projection on smooth bases, Functional Principal Component Analysis, functional centering and reduction, and the use of differential operators. It is shown how to incorporate these functional processing into the RBFN and MLP models. The functional approach is illustrated on a benchmark of spectrometric data analysis.","Representation of Functional Data in Neural Networks Functional Data Analysis (FDA) is an extension of traditional data analysis to functional data, for example spectra, temporal series, spatio-temporal images, gesture recognition data, etc. Functional data are rarely known in practice; usually a regular or irregular sampling is known. For this reason, some processing is needed in order to benefit from the smooth character of functional data in the analysis methods. This paper shows how to extend the Radial-Basis Function Networks (RBFN) and Multi-Layer Perceptron (MLP) models to functional data inputs, in particular when the latter are known through lists of input-output pairs. Various possibilities for functional processing are discussed, including the projection on smooth bases, Functional Principal Component Analysis, functional centering and reduction, and the use of differential operators. It is shown how to incorporate these functional processing into the RBFN and MLP models. The functional approach is illustrated on a benchmark of spectrometric data analysis.",Technology
A simplified model of the source channel of the Leksell GammaKnife tested with PENELOPE,"Monte Carlo simulations using the code PENELOPE have been performed to test a simplified model of the source channel geometry of the Leksell GammaKnifecircledR. The characteristics of the radiation passing through the treatment helmets are analysed in detail. We have found that only primary particles emitted from the source with polar angles smaller than 3rm o with respect to the beam axis are relevant for the dosimetry of the Gamma Knife. The photons trajectories reaching the output helmet collimators at (x,y,z236 rm mm), show strong correlations between rho(x2y2)12 and their polar angle theta, on one side, and between tan-1(yx) and their azimuthal angle phi, on the other. This enables us to propose a simplified model which treats the full source channel as a mathematical collimator. This simplified model produces doses in excellent agreement with those found for the full geometry. In the region of maximal dose, the relative differences between both calculations are within 3, for the 18 and 14 mm helmets, and 10, for the 8 and 4 mm ones. Besides, the simplified model permits a strong reduction (larger than a factor 15) in the computational time.","A simplified model of the source channel of the Leksell GammaKnife tested with PENELOPE Monte Carlo simulations using the code PENELOPE have been performed to test a simplified model of the source channel geometry of the Leksell GammaKnifecircledR. The characteristics of the radiation passing through the treatment helmets are analysed in detail. We have found that only primary particles emitted from the source with polar angles smaller than 3rm o with respect to the beam axis are relevant for the dosimetry of the Gamma Knife. The photons trajectories reaching the output helmet collimators at (x,y,z236 rm mm), show strong correlations between rho(x2y2)12 and their polar angle theta, on one side, and between tan-1(yx) and their azimuthal angle phi, on the other. This enables us to propose a simplified model which treats the full source channel as a mathematical collimator. This simplified model produces doses in excellent agreement with those found for the full geometry. In the region of maximal dose, the relative differences between both calculations are within 3, for the 18 and 14 mm helmets, and 10, for the 8 and 4 mm ones. Besides, the simplified model permits a strong reduction (larger than a factor 15) in the computational time.",Healthcare
Financial heat machine,"We consider dynamics of financial markets as dynamics of expectations and discuss such a dynamics from the point of view of phenomenological thermodynamics. We describe a financial Carnot cycle and the financial analogue of a heat machine. We see, that while in physics a perpetuum mobile is absolutely impossible, in economics such mobile may exist under some conditions. Our thermodynamical model for the financial market induces a rather unusual interpretation of the role of financial crises. In contrast to the common point of view, in our model financial crises play a crucial role in functioning of the modern financial market. This is an important (concluding) stage of any financial cycle that is analogous to the stage of cooling in the ordinary Carnot cycle. A financial cycle could not be completed without such a stage as well as the ordinary Carnot cycle. Thus, in spite its destructive (at the first sight) consequences the stage or financial crises is as well important as the stage of boiling of the financial market (heating of expectations)","Financial heat machine We consider dynamics of financial markets as dynamics of expectations and discuss such a dynamics from the point of view of phenomenological thermodynamics. We describe a financial Carnot cycle and the financial analogue of a heat machine. We see, that while in physics a perpetuum mobile is absolutely impossible, in economics such mobile may exist under some conditions. Our thermodynamical model for the financial market induces a rather unusual interpretation of the role of financial crises. In contrast to the common point of view, in our model financial crises play a crucial role in functioning of the modern financial market. This is an important (concluding) stage of any financial cycle that is analogous to the stage of cooling in the ordinary Carnot cycle. A financial cycle could not be completed without such a stage as well as the ordinary Carnot cycle. Thus, in spite its destructive (at the first sight) consequences the stage or financial crises is as well important as the stage of boiling of the financial market (heating of expectations)",Finance
Dosimetry for radiocolloid therapy of cystic craniopharyngiomas,"The dosimetry for radiocolloid therapy of cystic craniopharyngiomas is investigated. Analytical calculations based on the Loevinger and the Berger formulae for electrons and photons, respectively, are compared with Monte Carlo simulations. The role of the material of which the colloid introduced inside the craniopharyngioma is made of as well as that forming the cyst wall is analyzed. It is found that the analytical approaches provide a very good description of the simulated data in the conditions where they can be applied (i.e., in the case of a uniform and infinite homogeneous medium). However, the consideration of the different materials and interfaces produces a strong reduction of the dose delivered to the cyst wall in relation to that predicted by the Loevinger and the Berger formulae.","Dosimetry for radiocolloid therapy of cystic craniopharyngiomas The dosimetry for radiocolloid therapy of cystic craniopharyngiomas is investigated. Analytical calculations based on the Loevinger and the Berger formulae for electrons and photons, respectively, are compared with Monte Carlo simulations. The role of the material of which the colloid introduced inside the craniopharyngioma is made of as well as that forming the cyst wall is analyzed. It is found that the analytical approaches provide a very good description of the simulated data in the conditions where they can be applied (i.e., in the case of a uniform and infinite homogeneous medium). However, the consideration of the different materials and interfaces produces a strong reduction of the dose delivered to the cyst wall in relation to that predicted by the Loevinger and the Berger formulae.",Healthcare
Learning Optimal Augmented Bayes Networks,"Naive Bayes is a simple Bayesian classifier with strong independence assumptions among the attributes. This classifier, desipte its strong independence assumptions, often performs well in practice. It is believed that relaxing the independence assumptions of a naive Bayes classifier may improve the classification accuracy of the resulting structure. While finding an optimal unconstrained Bayesian Network (for most any reasonable scoring measure) is an NP-hard problem, it is possible to learn in polynomial time optimal networks obeying various structural restrictions. Several authors have examined the possibilities of adding augmenting arcs between attributes of a Naive Bayes classifier. Friedman, Geiger and Goldszmidt define the TAN structure in which the augmenting arcs form a tree on the attributes, and present a polynomial time algorithm that learns an optimal TAN with respect to MDL score. Keogh and Pazzani define Augmented Bayes Networks in which the augmenting arcs form a forest on the attributes (a collection of trees, hence a relaxation of the stuctural restriction of TAN), and present heuristic search methods for learning good, though not optimal, augmenting arc sets. The authors, however, evaluate the learned structure only in terms of observed misclassification error and not against a scoring metric, such as MDL. In this paper, we present a simple, polynomial time greedy algorithm for learning an optimal Augmented Bayes Network with respect to MDL score.","Learning Optimal Augmented Bayes Networks Naive Bayes is a simple Bayesian classifier with strong independence assumptions among the attributes. This classifier, desipte its strong independence assumptions, often performs well in practice. It is believed that relaxing the independence assumptions of a naive Bayes classifier may improve the classification accuracy of the resulting structure. While finding an optimal unconstrained Bayesian Network (for most any reasonable scoring measure) is an NP-hard problem, it is possible to learn in polynomial time optimal networks obeying various structural restrictions. Several authors have examined the possibilities of adding augmenting arcs between attributes of a Naive Bayes classifier. Friedman, Geiger and Goldszmidt define the TAN structure in which the augmenting arcs form a tree on the attributes, and present a polynomial time algorithm that learns an optimal TAN with respect to MDL score. Keogh and Pazzani define Augmented Bayes Networks in which the augmenting arcs form a forest on the attributes (a collection of trees, hence a relaxation of the stuctural restriction of TAN), and present heuristic search methods for learning good, though not optimal, augmenting arc sets. The authors, however, evaluate the learned structure only in terms of observed misclassification error and not against a scoring metric, such as MDL. In this paper, we present a simple, polynomial time greedy algorithm for learning an optimal Augmented Bayes Network with respect to MDL score.",Technology
Renewable levelized cost of energy available for export: An indicator for exploring global renewable energy trade potential,"Renewable energy resources are widely available, yet they are unevenly distributed globally. In a renewable future, countries lacking high-quality renewable resources may choose to import energy from other countries. To assess the resource-dependent and techno-economic basis for global renewable energy trade and identify potential importers and exporters, this study introduces two new metrics: Renewable Levelized Cost of Energy available for Export (RLCOE_Ex) and Potential Energy Export Volume (PEEV). These metrics are computed based on regional resource potential, domestic energy demand and varying financial costs across countries, without the need for any energy system modeling. By applying these two metrics to 165 countriesregions, we identify countries with significant potential for exporting renewable energy (e.g., the US, China) and those that lack the domestic resources to satisfy demand (e.g., South Korea, Japan). The RLCOE_Ex and PEEV metrics are validated through a separate analysis, employing a comprehensive energy system model for each countryregion.","Renewable levelized cost of energy available for export: An indicator for exploring global renewable energy trade potential Renewable energy resources are widely available, yet they are unevenly distributed globally. In a renewable future, countries lacking high-quality renewable resources may choose to import energy from other countries. To assess the resource-dependent and techno-economic basis for global renewable energy trade and identify potential importers and exporters, this study introduces two new metrics: Renewable Levelized Cost of Energy available for Export (RLCOE_Ex) and Potential Energy Export Volume (PEEV). These metrics are computed based on regional resource potential, domestic energy demand and varying financial costs across countries, without the need for any energy system modeling. By applying these two metrics to 165 countriesregions, we identify countries with significant potential for exporting renewable energy (e.g., the US, China) and those that lack the domestic resources to satisfy demand (e.g., South Korea, Japan). The RLCOE_Ex and PEEV metrics are validated through a separate analysis, employing a comprehensive energy system model for each countryregion.",Environment
CausalMed: Causality-Based Personalized Medication Recommendation Centered on Patient health state,"Medication recommendation systems are developed to recommend suitable medications tailored to specific patient. Previous researches primarily focus on learning medication representations, which have yielded notable advances. However, these methods are limited to capturing personalized patient representations due to the following primary limitations: (i) unable to capture the differences in the impact of diseasesprocedures on patients across various patient health states; (ii) fail to model the direct causal relationships between medications and specific health state of patients, resulting in an inability to determine which specific disease each medication is treating. To address these limitations, we propose CausalMed, a patient health state-centric model capable of enhancing the personalization of patient representations. Specifically, CausalMed first captures the causal relationship between diseasesprocedures and medications through causal discovery and evaluates their causal effects. Building upon this, CausalMed focuses on analyzing the health state of patients, capturing the dynamic differences of diseasesprocedures in different health states of patients, and transforming diseasesprocedures into medications on direct causal relationships. Ultimately, CausalMed integrates information from longitudinal visits to recommend medication combinations. Extensive experiments on real-world datasets show that our method learns more personalized patient representation and outperforms state-of-the-art models in accuracy and safety.","CausalMed: Causality-Based Personalized Medication Recommendation Centered on Patient health state Medication recommendation systems are developed to recommend suitable medications tailored to specific patient. Previous researches primarily focus on learning medication representations, which have yielded notable advances. However, these methods are limited to capturing personalized patient representations due to the following primary limitations: (i) unable to capture the differences in the impact of diseasesprocedures on patients across various patient health states; (ii) fail to model the direct causal relationships between medications and specific health state of patients, resulting in an inability to determine which specific disease each medication is treating. To address these limitations, we propose CausalMed, a patient health state-centric model capable of enhancing the personalization of patient representations. Specifically, CausalMed first captures the causal relationship between diseasesprocedures and medications through causal discovery and evaluates their causal effects. Building upon this, CausalMed focuses on analyzing the health state of patients, capturing the dynamic differences of diseasesprocedures in different health states of patients, and transforming diseasesprocedures into medications on direct causal relationships. Ultimately, CausalMed integrates information from longitudinal visits to recommend medication combinations. Extensive experiments on real-world datasets show that our method learns more personalized patient representation and outperforms state-of-the-art models in accuracy and safety.",Healthcare
A 3D dynamical biomechanical tongue model to study speech motor control,"A 3D biomechanical dynamical model of human tongue is presented, that is elaborated in the aim to test hypotheses about speech motor control. Tissue elastic properties are accounted for in Finite Element Modeling (FEM). The FEM mesh was designed in order to facilitate the implementation of muscle arrangement within the tongue. Therefore, its structure was determined on the basis of accurate anatomical data about the tongue. Mechanically, the hypothesis of hyperelasticity was adopted with the Mooney-Rivlin formulation of the strain energy function. Muscles are modeled as general force generators that act on anatomically specified sets of nodes of the FEM structure. The 8 muscles that are known to be largely involved in the production of basic speech movements are modeled. The model and the solving of the Lagrangian equations of movement are implemented using the ANSYSTM software. Simulations of the influence of muscle activations onto the tongue shape are presented and analyzed.","A 3D dynamical biomechanical tongue model to study speech motor control A 3D biomechanical dynamical model of human tongue is presented, that is elaborated in the aim to test hypotheses about speech motor control. Tissue elastic properties are accounted for in Finite Element Modeling (FEM). The FEM mesh was designed in order to facilitate the implementation of muscle arrangement within the tongue. Therefore, its structure was determined on the basis of accurate anatomical data about the tongue. Mechanically, the hypothesis of hyperelasticity was adopted with the Mooney-Rivlin formulation of the strain energy function. Muscles are modeled as general force generators that act on anatomically specified sets of nodes of the FEM structure. The 8 muscles that are known to be largely involved in the production of basic speech movements are modeled. The model and the solving of the Lagrangian equations of movement are implemented using the ANSYSTM software. Simulations of the influence of muscle activations onto the tongue shape are presented and analyzed.",Healthcare
Optimal Treatment Allocations Accounting for Population Differences,"The treatment allocation mechanism in a randomized clinical trial can be optimized by maximizing the nonparametric efficiency bound for a specific measure of treatment effect. Optimal treatment allocations which may or may not depend on baseline covariates have been derived for a variety of effect measures focusing on the trial population, the patient population represented by the trial participants. Frequently, clinical trial data are used to estimate treatment effects in a target population that is related to but different from the trial population. This article provides optimal treatment allocations that account for the impact of such population differences. We consider three cases with different data configurations: transportation, generalization, and post-stratification. Our results indicate that, for general effect measures, optimal treatment allocations may depend on the covariate distribution in the target population but not on the configuration of data or information that describes the target covariate distribution. For estimating average treatment effects, there is a unique covariate-dependent allocation that achieves maximal efficiency regardless of the target covariate distribution and the associated data configuration.","Optimal Treatment Allocations Accounting for Population Differences The treatment allocation mechanism in a randomized clinical trial can be optimized by maximizing the nonparametric efficiency bound for a specific measure of treatment effect. Optimal treatment allocations which may or may not depend on baseline covariates have been derived for a variety of effect measures focusing on the trial population, the patient population represented by the trial participants. Frequently, clinical trial data are used to estimate treatment effects in a target population that is related to but different from the trial population. This article provides optimal treatment allocations that account for the impact of such population differences. We consider three cases with different data configurations: transportation, generalization, and post-stratification. Our results indicate that, for general effect measures, optimal treatment allocations may depend on the covariate distribution in the target population but not on the configuration of data or information that describes the target covariate distribution. For estimating average treatment effects, there is a unique covariate-dependent allocation that achieves maximal efficiency regardless of the target covariate distribution and the associated data configuration.",Healthcare
Climate Monitoring using Internet of X-Things,Global climate change is significantly affecting the life on planet Earth. Predicting timely changes in the climate is a big challenge and requires great attention from the scientific community. Research now suggests that using the internet of X-things (X-IoT) helps in monitoring global climate change.,Climate Monitoring using Internet of X-Things Global climate change is significantly affecting the life on planet Earth. Predicting timely changes in the climate is a big challenge and requires great attention from the scientific community. Research now suggests that using the internet of X-things (X-IoT) helps in monitoring global climate change.,Environment
Comparing large lecture mechanics curricula using the Force Concept Inventory: A five thousand student study,"The performance of over 5000 students in introductory calculus-based mechanics courses at the Georgia Institute of Technology was assessed using the Force Concept Inventory (FCI). Results from two different curricula were compared: a traditional mechanics curriculum and the Matter  Interactions (MI) curriculum. Post-instruction FCI averages were significantly higher for the traditional curriculum than for the MI curriculum; the differences between curricula persist after accounting for factors such as pre-instruction FCI scores, grade point averages, and SAT scores. FCI performance on categories of items organized by concepts was also compared; traditional averages were significantly higher in each concept. We examined differences in student preparation between the curricula and found that the relative fraction of homework and lecture topics devoted to FCI force and motion concepts correlated with the observed performance differences. Limitations of concept inventories as instruments for evaluating curricular reforms are discussed.","Comparing large lecture mechanics curricula using the Force Concept Inventory: A five thousand student study The performance of over 5000 students in introductory calculus-based mechanics courses at the Georgia Institute of Technology was assessed using the Force Concept Inventory (FCI). Results from two different curricula were compared: a traditional mechanics curriculum and the Matter  Interactions (MI) curriculum. Post-instruction FCI averages were significantly higher for the traditional curriculum than for the MI curriculum; the differences between curricula persist after accounting for factors such as pre-instruction FCI scores, grade point averages, and SAT scores. FCI performance on categories of items organized by concepts was also compared; traditional averages were significantly higher in each concept. We examined differences in student preparation between the curricula and found that the relative fraction of homework and lecture topics devoted to FCI force and motion concepts correlated with the observed performance differences. Limitations of concept inventories as instruments for evaluating curricular reforms are discussed.",Education
Blockchain-Enabled EHR Framework for Internet of Medical Things,"The Internet of Medical Things (IoMT) offers an infrastructure made of smart medical equipment and software applications for health services. Through the internet, the IoMT is capable of providing remote medical diagnosis and timely health services. The patients can use their smart devices to create, store and share their electronic health records (EHR) with a variety of medical personnel including medical doctors and nurses. However, unless the underlying combination within IoMT is secured, malicious users can intercept, modify and even delete the sensitive EHR data of patients. Patients also lose full control of their EHR since most health services within IoMT are constructed under a centralized platform outsourced in the cloud. Therefore, it is appealing to design a decentralized, auditable and secure EHR system that guarantees absolute access control for the patients while ensuring privacy and security. Using the features of blockchain including decentralization, auditability and immutability, we propose a secure EHR framework which is mainly maintained by the medical centers. In this framework, the patients EHR data are encrypted and stored in the servers of medical institutions while the corresponding hash values are kept on the blockchain. We make use of security primitives to offer authentication, integrity and confidentiality of EHR data while access control and immutability is guaranteed by the blockchain technology. The security analysis and performance evaluation of the proposed framework confirms its efficiency.","Blockchain-Enabled EHR Framework for Internet of Medical Things The Internet of Medical Things (IoMT) offers an infrastructure made of smart medical equipment and software applications for health services. Through the internet, the IoMT is capable of providing remote medical diagnosis and timely health services. The patients can use their smart devices to create, store and share their electronic health records (EHR) with a variety of medical personnel including medical doctors and nurses. However, unless the underlying combination within IoMT is secured, malicious users can intercept, modify and even delete the sensitive EHR data of patients. Patients also lose full control of their EHR since most health services within IoMT are constructed under a centralized platform outsourced in the cloud. Therefore, it is appealing to design a decentralized, auditable and secure EHR system that guarantees absolute access control for the patients while ensuring privacy and security. Using the features of blockchain including decentralization, auditability and immutability, we propose a secure EHR framework which is mainly maintained by the medical centers. In this framework, the patients EHR data are encrypted and stored in the servers of medical institutions while the corresponding hash values are kept on the blockchain. We make use of security primitives to offer authentication, integrity and confidentiality of EHR data while access control and immutability is guaranteed by the blockchain technology. The security analysis and performance evaluation of the proposed framework confirms its efficiency.",Healthcare
Complete and competitive financial markets in a complex world,We investigate the possibility of completing financial markets in a model with no exogenous probability measure and market imperfections. A necessary and sufficient condition is obtained for such extension to be possible.,Complete and competitive financial markets in a complex world We investigate the possibility of completing financial markets in a model with no exogenous probability measure and market imperfections. A necessary and sufficient condition is obtained for such extension to be possible.,Finance
Adjoints and Automatic (Algorithmic) Differentiation in Computational Finance,"Two of the most important areas in computational finance: Greeks and, respectively, calibration, are based on efficient and accurate computation of a large number of sensitivities. This paper gives an overview of adjoint and automatic differentiation (AD), also known as algorithmic differentiation, techniques to calculate these sensitivities. When compared to finite difference approximation, this approach can potentially reduce the computational cost by several orders of magnitude, with sensitivities accurate up to machine precision. Examples and a literature survey are also provided.","Adjoints and Automatic (Algorithmic) Differentiation in Computational Finance Two of the most important areas in computational finance: Greeks and, respectively, calibration, are based on efficient and accurate computation of a large number of sensitivities. This paper gives an overview of adjoint and automatic differentiation (AD), also known as algorithmic differentiation, techniques to calculate these sensitivities. When compared to finite difference approximation, this approach can potentially reduce the computational cost by several orders of magnitude, with sensitivities accurate up to machine precision. Examples and a literature survey are also provided.",Finance
Computers in Secondary Schools: Educational Games,"This entry introduces educational games in secondary schools. Educational games include three main types of educational activities with a playful learning intention supported by digital technologies: educational serious games, educational gamification, and learning through game creation. Educational serious games are digital games that support learning objectives. Gamification is defined as the use of game design elements and game thinking in a non-gaming context (Deterding et al. 2011, p. 13). Educational gamification is not developed through a digital game but includes game elements for supporting the learning objectives. Learning through game creation is focused on the process of designing and creating a prototype of a game to support a learning process related to the game creation process or the knowledge mobilized through the game creation process. Four modalities of educational games in secondary education are introduced in this entry to describe educational games in secondary education: educational purpose of entertainment games, serious games, gamification, and game design.","Computers in Secondary Schools: Educational Games This entry introduces educational games in secondary schools. Educational games include three main types of educational activities with a playful learning intention supported by digital technologies: educational serious games, educational gamification, and learning through game creation. Educational serious games are digital games that support learning objectives. Gamification is defined as the use of game design elements and game thinking in a non-gaming context (Deterding et al. 2011, p. 13). Educational gamification is not developed through a digital game but includes game elements for supporting the learning objectives. Learning through game creation is focused on the process of designing and creating a prototype of a game to support a learning process related to the game creation process or the knowledge mobilized through the game creation process. Four modalities of educational games in secondary education are introduced in this entry to describe educational games in secondary education: educational purpose of entertainment games, serious games, gamification, and game design.",Education
Clinical use and future requirements of relative biological effectiveness: survey among all european proton therapy centres,"Background and purpose: The relative biological effectiveness (RBE) varies along the treatment field. However, in clinical practice, a constant RBE of 1.1 is assumed, which can result in undesirable side effects. This study provides an accurate overview of current clinical practice for considering proton RBE in Europe. Materials and Methods: A survey was devised and sent to all proton therapy centres in Europe that treat patients. The online questionnaire consisted of 39 questions addressing various aspects of RBE consideration in clinical practice, including treatment planning, patient follow-up and future demands. Results: All 25 proton therapy centres responded. All centres prescribed a constant RBE of 1.1, but also applied measures (except for one eye treatment centre) to counteract variable RBE effects such as avoiding beams stopping inside or in front of an organ at risk and putting restrictions on the minimum number and opening angle of incident beams for certain treatment sites. For the future, most centres (16) asked for more retrospective or prospective outcome studies investigating the potential effect of the effect of a variable RBE. To perform such studies, 18 centres asked for LET and RBE calculation and visualisation tools developed by treatment planning system vendors. Conclusion: All European proton centres are aware of RBE variability but comply with current guidelines of prescribing a constant RBE. However, they actively mitigate uncertainty and risk of side effects resulting from increased RBE by applying measures and restrictions during treatment planning. To change RBE-related clinical guidelines in the future more clinical data on RBE are explicitly demanded.","Clinical use and future requirements of relative biological effectiveness: survey among all european proton therapy centres Background and purpose: The relative biological effectiveness (RBE) varies along the treatment field. However, in clinical practice, a constant RBE of 1.1 is assumed, which can result in undesirable side effects. This study provides an accurate overview of current clinical practice for considering proton RBE in Europe. Materials and Methods: A survey was devised and sent to all proton therapy centres in Europe that treat patients. The online questionnaire consisted of 39 questions addressing various aspects of RBE consideration in clinical practice, including treatment planning, patient follow-up and future demands. Results: All 25 proton therapy centres responded. All centres prescribed a constant RBE of 1.1, but also applied measures (except for one eye treatment centre) to counteract variable RBE effects such as avoiding beams stopping inside or in front of an organ at risk and putting restrictions on the minimum number and opening angle of incident beams for certain treatment sites. For the future, most centres (16) asked for more retrospective or prospective outcome studies investigating the potential effect of the effect of a variable RBE. To perform such studies, 18 centres asked for LET and RBE calculation and visualisation tools developed by treatment planning system vendors. Conclusion: All European proton centres are aware of RBE variability but comply with current guidelines of prescribing a constant RBE. However, they actively mitigate uncertainty and risk of side effects resulting from increased RBE by applying measures and restrictions during treatment planning. To change RBE-related clinical guidelines in the future more clinical data on RBE are explicitly demanded.",Healthcare
Incomplete Continuous-time Securities Markets with Stochastic Income Volatility,"In an incomplete continuous-time securities market with uncertainty generated by Brownian motions, we derive closed-form solutions for the equilibrium interest rate and market price of risk processes. The economy has a finite number of heterogeneous exponential utility investors, who receive partially unspanned income and can trade continuously on a finite time-interval in a money market account and a single risky security. Besides establishing the existence of an equilibrium, our main result shows that if the investors unspanned income has stochastic countercyclical volatility, the resulting equilibrium can display both lower interest rates and higher risk premia compared to the Pareto efficient equilibrium in an otherwise identical complete market.","Incomplete Continuous-time Securities Markets with Stochastic Income Volatility In an incomplete continuous-time securities market with uncertainty generated by Brownian motions, we derive closed-form solutions for the equilibrium interest rate and market price of risk processes. The economy has a finite number of heterogeneous exponential utility investors, who receive partially unspanned income and can trade continuously on a finite time-interval in a money market account and a single risky security. Besides establishing the existence of an equilibrium, our main result shows that if the investors unspanned income has stochastic countercyclical volatility, the resulting equilibrium can display both lower interest rates and higher risk premia compared to the Pareto efficient equilibrium in an otherwise identical complete market.",Finance
Biomechanical models to simulate consequences of maxillofacial surgery,"This paper presents the biomechanical finite element models that have been developed in the framework of the computer-assisted maxillofacial surgery. After a brief overview of the continuous elastic modelling method, two models are introduced and their use for computer-assisted applications discussed. The first model deals with orthognathic surgery and aims at predicting the facial consequences of maxillary and mandibular osteotomies. For this, a generic three-dimensional model of the face is automatically adapted to the morphology of the patient by the mean of elastic registration. Qualitative simulations of the consequences of an osteotomy of the mandible can thus be provided. The second model addresses the Sleep Apnoea Syndrome. Its aim is to develop a complete modelling of the interaction between airflow and upper airways walls during respiration. Dynamical simulations of the interaction during a respiratory cycle are computed and compared with observed phenomena.","Biomechanical models to simulate consequences of maxillofacial surgery This paper presents the biomechanical finite element models that have been developed in the framework of the computer-assisted maxillofacial surgery. After a brief overview of the continuous elastic modelling method, two models are introduced and their use for computer-assisted applications discussed. The first model deals with orthognathic surgery and aims at predicting the facial consequences of maxillary and mandibular osteotomies. For this, a generic three-dimensional model of the face is automatically adapted to the morphology of the patient by the mean of elastic registration. Qualitative simulations of the consequences of an osteotomy of the mandible can thus be provided. The second model addresses the Sleep Apnoea Syndrome. Its aim is to develop a complete modelling of the interaction between airflow and upper airways walls during respiration. Dynamical simulations of the interaction during a respiratory cycle are computed and compared with observed phenomena.",Healthcare
Simple Analytics of the Government Investment Multiplier,What are the effects of investing in public infrastructure? We answer this question with a New Keynesian model. We recast the model as a Markov chain and develop a general solution method that nests existing ones insideoutside the zero lower bound as special cases. Our framework delivers a simple expression for the contribution of public infrastructure. We show that it provides a unified framework to study the effects of public investment in three scenarios: (i) normal times (ii) short-lived liquidity trap (iii) long-lived liquidity trap. We find that calibrations commonly used lead to multipliers that diverge with the duration of the trap.,Simple Analytics of the Government Investment Multiplier What are the effects of investing in public infrastructure? We answer this question with a New Keynesian model. We recast the model as a Markov chain and develop a general solution method that nests existing ones insideoutside the zero lower bound as special cases. Our framework delivers a simple expression for the contribution of public infrastructure. We show that it provides a unified framework to study the effects of public investment in three scenarios: (i) normal times (ii) short-lived liquidity trap (iii) long-lived liquidity trap. We find that calibrations commonly used lead to multipliers that diverge with the duration of the trap.,Finance
The Problem of Modeling of Economic Dynamics (new version),"The correctness of Harrods model in the differential form is studied. The inadequacy of exponential growth of economy is shown; an alternative result is obtained. By example of Phillips model, an approach to correction of macroeconomic models (in terms of initial prerequisites) is generalized. A methodology based on balance relations for modelling of economic dynamics, including obtaining forecast estimates, is developed. The problems thus considered are reduced to the solution of Volterra and Fredholm integral equations of the second kind.","The Problem of Modeling of Economic Dynamics (new version) The correctness of Harrods model in the differential form is studied. The inadequacy of exponential growth of economy is shown; an alternative result is obtained. By example of Phillips model, an approach to correction of macroeconomic models (in terms of initial prerequisites) is generalized. A methodology based on balance relations for modelling of economic dynamics, including obtaining forecast estimates, is developed. The problems thus considered are reduced to the solution of Volterra and Fredholm integral equations of the second kind.",Finance
Exploring the Decision Forest: An Empirical Investigation of Occams Razor in Decision Tree Induction,"We report on a series of experiments in which all decision trees consistent with the training data are constructed. These experiments were run to gain an understanding of the properties of the set of consistent decision trees and the factors that affect the accuracy of individual trees. In particular, we investigated the relationship between the size of a decision tree consistent with some training data and the accuracy of the tree on test data. The experiments were performed on a massively parallel Maspar computer. The results of the experiments on several artificial and two real world problems indicate that, for many of the problems investigated, smaller consistent decision trees are on average less accurate than the average accuracy of slightly larger trees.","Exploring the Decision Forest: An Empirical Investigation of Occams Razor in Decision Tree Induction We report on a series of experiments in which all decision trees consistent with the training data are constructed. These experiments were run to gain an understanding of the properties of the set of consistent decision trees and the factors that affect the accuracy of individual trees. In particular, we investigated the relationship between the size of a decision tree consistent with some training data and the accuracy of the tree on test data. The experiments were performed on a massively parallel Maspar computer. The results of the experiments on several artificial and two real world problems indicate that, for many of the problems investigated, smaller consistent decision trees are on average less accurate than the average accuracy of slightly larger trees.",Technology
Categorization of problems to assess and improve proficiency as teachers and learners,We describe how graduate students categorize introductory mechanics problems based on the similarity of their solutions. Graduate students were asked at the end of a teaching assistant training class to categorize problems from their own perspective and from the perspective of typical introductory physics students whom they were teaching. We compare their categorizations with the categorizations by introductory physics students and physics faculty who categorized the same problems. The utility of categorization as a tool for teaching assistant training and faculty development workshops is discussed.,Categorization of problems to assess and improve proficiency as teachers and learners We describe how graduate students categorize introductory mechanics problems based on the similarity of their solutions. Graduate students were asked at the end of a teaching assistant training class to categorize problems from their own perspective and from the perspective of typical introductory physics students whom they were teaching. We compare their categorizations with the categorizations by introductory physics students and physics faculty who categorized the same problems. The utility of categorization as a tool for teaching assistant training and faculty development workshops is discussed.,Education
"MasonNLP at SemEval-2023 Task 8: Extracting Medical Questions, Experiences and Claims from Social Media using Knowledge-Augmented Pre-trained Language Models","In online forums like Reddit, users share their experiences with medical conditions and treatments, including making claims, asking questions, and discussing the effects of treatments on their health. Building systems to understand this information can effectively monitor the spread of misinformation and verify user claims. The Task-8 of the 2023 International Workshop on Semantic Evaluation focused on medical applications, specifically extracting patient experience- and medical condition-related entities from user posts on social media. The Reddit Health Online Talk (RedHot) corpus contains posts from medical condition-related subreddits with annotations characterizing the patient experience and medical conditions. In Subtask-1, patient experience is characterized by personal experience, questions, and claims. In Subtask-2, medical conditions are characterized by population, intervention, and outcome. For the automatic extraction of patient experiences and medical condition information, as a part of the challenge, we proposed language-model-based extraction systems that ranked 3rd on both subtasks leaderboards. In this work, we describe our approach and, in addition, explore the automatic extraction of this information using domain-specific language models and the inclusion of external knowledge.","MasonNLP at SemEval-2023 Task 8: Extracting Medical Questions, Experiences and Claims from Social Media using Knowledge-Augmented Pre-trained Language Models In online forums like Reddit, users share their experiences with medical conditions and treatments, including making claims, asking questions, and discussing the effects of treatments on their health. Building systems to understand this information can effectively monitor the spread of misinformation and verify user claims. The Task-8 of the 2023 International Workshop on Semantic Evaluation focused on medical applications, specifically extracting patient experience- and medical condition-related entities from user posts on social media. The Reddit Health Online Talk (RedHot) corpus contains posts from medical condition-related subreddits with annotations characterizing the patient experience and medical conditions. In Subtask-1, patient experience is characterized by personal experience, questions, and claims. In Subtask-2, medical conditions are characterized by population, intervention, and outcome. For the automatic extraction of patient experiences and medical condition information, as a part of the challenge, we proposed language-model-based extraction systems that ranked 3rd on both subtasks leaderboards. In this work, we describe our approach and, in addition, explore the automatic extraction of this information using domain-specific language models and the inclusion of external knowledge.",Healthcare
How to use the Scuba Diving metaphor to solve problem with neutrality ?,"We proposed a new search heuristic using the scuba diving metaphor. This approach is based on the concept of evolvability and tends to exploit neutrality which exists in many real-world problems. Despite the fact that natural evolution does not directly select for evolvability, the basic idea behind the scuba search heuristic is to explicitly push evolvability to increase. A comparative study of the scuba algorithm and standard local search heuristics has shown the advantage and the limitation of the scuba search. In order to tune neutrality, we use the NKq fitness landscapes and a family of travelling salesman problems (TSP) where cities are randomly placed on a lattice and where travel distance between cities is computed with the Manhattan metric. In this last problem the amount of neutrality varies with the city concentration on the grid ; assuming the concentration below one, this TSP reasonably remains a NP-hard problem.","How to use the Scuba Diving metaphor to solve problem with neutrality ? We proposed a new search heuristic using the scuba diving metaphor. This approach is based on the concept of evolvability and tends to exploit neutrality which exists in many real-world problems. Despite the fact that natural evolution does not directly select for evolvability, the basic idea behind the scuba search heuristic is to explicitly push evolvability to increase. A comparative study of the scuba algorithm and standard local search heuristics has shown the advantage and the limitation of the scuba search. In order to tune neutrality, we use the NKq fitness landscapes and a family of travelling salesman problems (TSP) where cities are randomly placed on a lattice and where travel distance between cities is computed with the Manhattan metric. In this last problem the amount of neutrality varies with the city concentration on the grid ; assuming the concentration below one, this TSP reasonably remains a NP-hard problem.",Technology
Adaptive Problem-solving for Large-scale Scheduling Problems: A Case Study,"Although most scheduling problems are NP-hard, domain specific techniques perform well in practice but are quite expensive to construct. In adaptive problem-solving solving, domain specific knowledge is acquired automatically for a general problem solver with a flexible control architecture. In this approach, a learning system explores a space of possible heuristic methods for one well-suited to the eccentricities of the given domain and problem distribution. In this article, we discuss an application of the approach to scheduling satellite communications. Using problem distributions based on actual mission requirements, our approach identifies strategies that not only decrease the amount of CPU time required to produce schedules, but also increase the percentage of problems that are solvable within computational resource limitations.","Adaptive Problem-solving for Large-scale Scheduling Problems: A Case Study Although most scheduling problems are NP-hard, domain specific techniques perform well in practice but are quite expensive to construct. In adaptive problem-solving solving, domain specific knowledge is acquired automatically for a general problem solver with a flexible control architecture. In this approach, a learning system explores a space of possible heuristic methods for one well-suited to the eccentricities of the given domain and problem distribution. In this article, we discuss an application of the approach to scheduling satellite communications. Using problem distributions based on actual mission requirements, our approach identifies strategies that not only decrease the amount of CPU time required to produce schedules, but also increase the percentage of problems that are solvable within computational resource limitations.",Technology
"The Reality of Climate Change: Evidence, Impacts and Engineering Solutions","Climate change is one of the most significant global challenges, yet misconceptions persist regarding its causes and impact. This report addresses common myths surrounding climate change and presents scientific evidence to clarify its reality. Utilising data from NASA, NOAA, and the NSW government, this study provides evidence of rising global temperatures, melting ice sheets, rising sea levels, and extreme weather patterns in regions like New South Wales. The analysis demonstrates the human-driven nature of climate change, primarily caused by increased carbon emissions. Engineering solutions, including renewable energy technologies, green buildings, and carbon capture methods, are essential to mitigating the effects of climate change. Future research should focus on improving the scalability of these technologies and addressing the broader impact on ecosystems and human societies.","The Reality of Climate Change: Evidence, Impacts and Engineering Solutions Climate change is one of the most significant global challenges, yet misconceptions persist regarding its causes and impact. This report addresses common myths surrounding climate change and presents scientific evidence to clarify its reality. Utilising data from NASA, NOAA, and the NSW government, this study provides evidence of rising global temperatures, melting ice sheets, rising sea levels, and extreme weather patterns in regions like New South Wales. The analysis demonstrates the human-driven nature of climate change, primarily caused by increased carbon emissions. Engineering solutions, including renewable energy technologies, green buildings, and carbon capture methods, are essential to mitigating the effects of climate change. Future research should focus on improving the scalability of these technologies and addressing the broader impact on ecosystems and human societies.",Environment
State Abstraction in MAXQ Hierarchical Reinforcement Learning,"Many researchers have explored methods for hierarchical reinforcement learning (RL) with temporal abstractions, in which abstract actions are defined that can perform many primitive actions before terminating. However, little is known about learning with state abstractions, in which aspects of the state space are ignored. In previous work, we developed the MAXQ method for hierarchical RL. In this paper, we define five conditions under which state abstraction can be combined with the MAXQ value function decomposition. We prove that the MAXQ-Q learning algorithm converges under these conditions and show experimentally that state abstraction is important for the successful application of MAXQ-Q learning.","State Abstraction in MAXQ Hierarchical Reinforcement Learning Many researchers have explored methods for hierarchical reinforcement learning (RL) with temporal abstractions, in which abstract actions are defined that can perform many primitive actions before terminating. However, little is known about learning with state abstractions, in which aspects of the state space are ignored. In previous work, we developed the MAXQ method for hierarchical RL. In this paper, we define five conditions under which state abstraction can be combined with the MAXQ value function decomposition. We prove that the MAXQ-Q learning algorithm converges under these conditions and show experimentally that state abstraction is important for the successful application of MAXQ-Q learning.",Technology
Managing health insurance using blockchain technology,"Health insurance plays a significant role in ensuring quality healthcare. In response to the escalating costs of the medical industry, the demand for health insurance is soaring. Additionally, those with health insurance are more likely to receive preventative care than those without health insurance. However, from granting health insurance to delivering services to insured individuals, the health insurance industry faces numerous obstacles. Fraudulent actions, false claims, a lack of transparency and data privacy, reliance on human effort and dishonesty from consumers, healthcare professionals, or even the insurer party itself, are the most common and important hurdles towards success. Given these constraints, this chapter briefly covers the most immediate concerns in the health insurance industry and provides insight into how blockchain technology integration can contribute to resolving these issues. This chapter finishes by highlighting existing limitations as well as potential future directions.","Managing health insurance using blockchain technology Health insurance plays a significant role in ensuring quality healthcare. In response to the escalating costs of the medical industry, the demand for health insurance is soaring. Additionally, those with health insurance are more likely to receive preventative care than those without health insurance. However, from granting health insurance to delivering services to insured individuals, the health insurance industry faces numerous obstacles. Fraudulent actions, false claims, a lack of transparency and data privacy, reliance on human effort and dishonesty from consumers, healthcare professionals, or even the insurer party itself, are the most common and important hurdles towards success. Given these constraints, this chapter briefly covers the most immediate concerns in the health insurance industry and provides insight into how blockchain technology integration can contribute to resolving these issues. This chapter finishes by highlighting existing limitations as well as potential future directions.",Healthcare
Co-transport-induced instability of membrane voltage in tip-growing cells,"A salient feature of stationary patterns in tip-growing cells is the key role played by the symports and antiports, membrane proteins that translocate two ionic species at the same time. It is shown that these co-transporters destabilize generically the membrane voltage if the two translocated ions diffuse differently and carry a charge of opposite (same) sign for symports (antiports). Orders of magnitude obtained for the time and lengthscale are in agreement with experiments. A weakly nonlinear analysis characterizes the bifurcation.","Co-transport-induced instability of membrane voltage in tip-growing cells A salient feature of stationary patterns in tip-growing cells is the key role played by the symports and antiports, membrane proteins that translocate two ionic species at the same time. It is shown that these co-transporters destabilize generically the membrane voltage if the two translocated ions diffuse differently and carry a charge of opposite (same) sign for symports (antiports). Orders of magnitude obtained for the time and lengthscale are in agreement with experiments. A weakly nonlinear analysis characterizes the bifurcation.",Healthcare
MOBILITY21: Strategic Investments for Transportation Infrastructure  Technology,"Americas transportation infrastructure is the backbone of our economy. A strong infrastructure means a strong America - an America that competes globally, supports local and regional economic development, and creates jobs. Strategic investments in our transportation infrastructure are vital to our national security, economic growth, transportation safety and our technology leadership. This document outlines critical needs for our transportation infrastructure, identifies new technology drivers and proposes strategic investments for safe and efficient air, ground, rail and marine mobility of people and goods.","MOBILITY21: Strategic Investments for Transportation Infrastructure  Technology Americas transportation infrastructure is the backbone of our economy. A strong infrastructure means a strong America - an America that competes globally, supports local and regional economic development, and creates jobs. Strategic investments in our transportation infrastructure are vital to our national security, economic growth, transportation safety and our technology leadership. This document outlines critical needs for our transportation infrastructure, identifies new technology drivers and proposes strategic investments for safe and efficient air, ground, rail and marine mobility of people and goods.",Finance
A Mathematical Lens for Teaching Data Science,"Using the National Academies report, em Data Science for Undergraduates: Opportunities and Options, we connect data science curricula to the more familiar pedagogy used by many mathematical scientists. We use their list of data acumen components to ground a discussion, which hopes to connect data science curricula to the more familiar pedagogy used by many mathematical scientists.","A Mathematical Lens for Teaching Data Science Using the National Academies report, em Data Science for Undergraduates: Opportunities and Options, we connect data science curricula to the more familiar pedagogy used by many mathematical scientists. We use their list of data acumen components to ground a discussion, which hopes to connect data science curricula to the more familiar pedagogy used by many mathematical scientists.",Education
Financial instability from local market measures,"We study the emergence of instabilities in a stylized model of a financial market, when different market actors calculate prices according to different (local) market measures. We derive typical properties for ensembles of large random markets using techniques borrowed from statistical mechanics of disordered systems. We show that, depending on the number of financial instruments available and on the heterogeneity of local measures, the market moves from an arbitrage-free phase to an unstable one, where the complexity of the market - as measured by the diversity of financial instruments - increases, and arbitrage opportunities arise. A sharp transition separates the two phases. Focusing on two different classes of local measures inspired by real markets strategies, we are able to analytically compute the critical lines, corroborating our findings with numerical simulations.","Financial instability from local market measures We study the emergence of instabilities in a stylized model of a financial market, when different market actors calculate prices according to different (local) market measures. We derive typical properties for ensembles of large random markets using techniques borrowed from statistical mechanics of disordered systems. We show that, depending on the number of financial instruments available and on the heterogeneity of local measures, the market moves from an arbitrage-free phase to an unstable one, where the complexity of the market - as measured by the diversity of financial instruments - increases, and arbitrage opportunities arise. A sharp transition separates the two phases. Focusing on two different classes of local measures inspired by real markets strategies, we are able to analytically compute the critical lines, corroborating our findings with numerical simulations.",Finance
An Asymptotic Expansion Formula for Up-and-Out Barrier Option Price under Stochastic Volatility Model,"This paper derives a new semi closed-form approximation formula for pricing an up-and-out barrier option under a certain type of stochastic volatility model including SABR model by applying a rigorous asymptotic expansion method developed by Kato, Takahashi and Yamada (2012). We also demonstrate the validity of our approximation method through numerical examples.","An Asymptotic Expansion Formula for Up-and-Out Barrier Option Price under Stochastic Volatility Model This paper derives a new semi closed-form approximation formula for pricing an up-and-out barrier option under a certain type of stochastic volatility model including SABR model by applying a rigorous asymptotic expansion method developed by Kato, Takahashi and Yamada (2012). We also demonstrate the validity of our approximation method through numerical examples.",Finance
Coincident Frequencies and Relative Phases Among Female-Brain Signals and Progesterone-Estrogen levels,"Fourier transform has become a basic tool for analyzing biological signals 1,2,3. Mostly a fast Fourier transform is computed for a finite sequence of data sample 4. This is the standard way apparatuses and modern computerized technology provide information, according with their frequency range, of the well known brain signals Delta, Theta, Alpha 1, Alpha 2, Beta 1 and Beta 2 furnishing experts with electroencephalographic (EEG) profile of clinical use obtained from these short periods 5,6. For long periods, an analogous novel procedure is established as follows: Assigning certain numerical value, i.e., the absolute power, to each brain signal at certain sampling times, generates data that can be interpolated and extrapolated through a long period, yielding an absolute power function of time for each signal 7. A further Fourier transform is then performed8,9, to analyze these new functions, finding typical frequencies and their corresponding periods for each one of these signals and, also, relative phases for coincident periods between two or more signals. Our procedure of analysis presented here can be applied, in principle, to any biological signal of interest.","Coincident Frequencies and Relative Phases Among Female-Brain Signals and Progesterone-Estrogen levels Fourier transform has become a basic tool for analyzing biological signals 1,2,3. Mostly a fast Fourier transform is computed for a finite sequence of data sample 4. This is the standard way apparatuses and modern computerized technology provide information, according with their frequency range, of the well known brain signals Delta, Theta, Alpha 1, Alpha 2, Beta 1 and Beta 2 furnishing experts with electroencephalographic (EEG) profile of clinical use obtained from these short periods 5,6. For long periods, an analogous novel procedure is established as follows: Assigning certain numerical value, i.e., the absolute power, to each brain signal at certain sampling times, generates data that can be interpolated and extrapolated through a long period, yielding an absolute power function of time for each signal 7. A further Fourier transform is then performed8,9, to analyze these new functions, finding typical frequencies and their corresponding periods for each one of these signals and, also, relative phases for coincident periods between two or more signals. Our procedure of analysis presented here can be applied, in principle, to any biological signal of interest.",Healthcare
"Scaling graphs of heart rate time series in athletes demonstrate the VLF, LF and HF regions","Scaling analysis of heart rate time series has emerged as an useful tool for assessment of autonomic cardiac control. We investigate the heart rate time series of ten athletes (five males and five females), by applying detrended fluctuation analysis (DFA). High resolution ECGs are recorded under standardized resting conditions over 30 minutes and subsequently heart rate time series are extracted and artefacts filtered. We find three distinct regions of scale-invariance, which correspond to the well-known VLF, LF, and HF bands in the power spectra of heart rate variability. The scaling exponents alpha are alphaHF: 1.15 0.96-1.22, alphaLF: 0.68 0.57-0.84, alphaVLF: 0.830.82-0.99; p10-5). In conclusion, DFA scaling exponents of heart rate time series should be fitted to the VLF, LF, and HF ranges, respectively.","Scaling graphs of heart rate time series in athletes demonstrate the VLF, LF and HF regions Scaling analysis of heart rate time series has emerged as an useful tool for assessment of autonomic cardiac control. We investigate the heart rate time series of ten athletes (five males and five females), by applying detrended fluctuation analysis (DFA). High resolution ECGs are recorded under standardized resting conditions over 30 minutes and subsequently heart rate time series are extracted and artefacts filtered. We find three distinct regions of scale-invariance, which correspond to the well-known VLF, LF, and HF bands in the power spectra of heart rate variability. The scaling exponents alpha are alphaHF: 1.15 0.96-1.22, alphaLF: 0.68 0.57-0.84, alphaVLF: 0.830.82-0.99; p10-5). In conclusion, DFA scaling exponents of heart rate time series should be fitted to the VLF, LF, and HF ranges, respectively.",Healthcare
Regional Centres for Space Science and Technology Education (Affiliated to the United Nations),"Education is a prerequisite to master the challenges of space science and technology. Efforts to understand and control space science and technology are necessarily intertwined with social expressions in the cultures where science and technology is carried out. The United Nations is leading an effort to establish regional Centres for Space Science and Technology Education in major regions on Earth. The status of the establishment of such institutions in Asia and the Pacific, Africa, Latin America and the Caribbean, Western Asia, and Eastern Europe is briefly described in this article.","Regional Centres for Space Science and Technology Education (Affiliated to the United Nations) Education is a prerequisite to master the challenges of space science and technology. Efforts to understand and control space science and technology are necessarily intertwined with social expressions in the cultures where science and technology is carried out. The United Nations is leading an effort to establish regional Centres for Space Science and Technology Education in major regions on Earth. The status of the establishment of such institutions in Asia and the Pacific, Africa, Latin America and the Caribbean, Western Asia, and Eastern Europe is briefly described in this article.",Education
Interlinkages and structural changes in cross-border liabilities: a network approach,"We study the international interbank market through a geometrical and a topological analysis of empirical data. The geometrical analysis of the time series of cross-country liabilities shows that the systematic information of the interbank international market is contained in a space of small dimension, from which a topological characterization could be conveniently carried out. Weighted and complete networks of financial linkages across countries are developed, for which continuous clustering, degree centrality and closeness centrality are computed. The behavior of these topological coefficients reveals an important modification acting in the financial linkages in the period 1997-2011. Here we show that, besides the generalized clustering increase, there is a persistent increment in the degree of connectivity and in the closeness centrality of some countries. These countries seem to correspond to critical locations where tax policies might provide opportunities to shift debts. Such critical locations highlight the role that specific countries play in the network structure and helps to situates the turbulent period that has been characterizing the global financial system since the Summer 2007 as the counterpart of a larger structural change going on for a more than one decade.","Interlinkages and structural changes in cross-border liabilities: a network approach We study the international interbank market through a geometrical and a topological analysis of empirical data. The geometrical analysis of the time series of cross-country liabilities shows that the systematic information of the interbank international market is contained in a space of small dimension, from which a topological characterization could be conveniently carried out. Weighted and complete networks of financial linkages across countries are developed, for which continuous clustering, degree centrality and closeness centrality are computed. The behavior of these topological coefficients reveals an important modification acting in the financial linkages in the period 1997-2011. Here we show that, besides the generalized clustering increase, there is a persistent increment in the degree of connectivity and in the closeness centrality of some countries. These countries seem to correspond to critical locations where tax policies might provide opportunities to shift debts. Such critical locations highlight the role that specific countries play in the network structure and helps to situates the turbulent period that has been characterizing the global financial system since the Summer 2007 as the counterpart of a larger structural change going on for a more than one decade.",Finance
Computer-aided hepatic tumour ablation,Surgical resection of hepatic tumours is not always possible. Alternative techniques consist in locally using chemical or physical agents to destroy the tumour and this may be performed percutaneously. It requires a precise localisation of the tumour placement during ablation. Computer-assisted surgery tools may be used in conjunction to these new ablation techniques to improve the therapeutic efficiency whilst benefiting from minimal invasiveness. This communication introduces the principles of a system for computer-assisted hepatic tumour ablation.,Computer-aided hepatic tumour ablation Surgical resection of hepatic tumours is not always possible. Alternative techniques consist in locally using chemical or physical agents to destroy the tumour and this may be performed percutaneously. It requires a precise localisation of the tumour placement during ablation. Computer-assisted surgery tools may be used in conjunction to these new ablation techniques to improve the therapeutic efficiency whilst benefiting from minimal invasiveness. This communication introduces the principles of a system for computer-assisted hepatic tumour ablation.,Healthcare
Quantitative Measure of Stability in Gene Regulatory Networks,"A quantitative measure of stability in stochastic dynamics starts to emerge in recent experiments on bioswitches. This quantity, similar to the potential function in mathematics, is deeply rooted in biology, dated back at the beginning of quantitative description of biological processes: the adaptive landscape of Wright (1932) and the development landscape of Waddington (1940). Nevertheless, its quantitative implication has been frequently challenged by biologists. Recent progresses in quantitative biology begin to meet those outstanding challenges.","Quantitative Measure of Stability in Gene Regulatory Networks A quantitative measure of stability in stochastic dynamics starts to emerge in recent experiments on bioswitches. This quantity, similar to the potential function in mathematics, is deeply rooted in biology, dated back at the beginning of quantitative description of biological processes: the adaptive landscape of Wright (1932) and the development landscape of Waddington (1940). Nevertheless, its quantitative implication has been frequently challenged by biologists. Recent progresses in quantitative biology begin to meet those outstanding challenges.",Healthcare
Detection of fraudulent users in P2P financial market,"Financial fraud detection is one of the core technological assets of Fintech companies. It saves tens of millions of money fro m Chinese Fintech companies since the bad loan rate is more than 10. HC Financial Service Group is the 3rd largest company in the Chinese P2P financial market. In this paper we illustrate how we tackle the fraud detection problem at HC Financial. We utilize two powerful workhorses in the machine learning field - random forest and gradient boosting decision tree to detect fraudulent users . We demonstrate that by carefully select features and tune model parameters , we could effectively filter out fraudulent users in the P2P market.","Detection of fraudulent users in P2P financial market Financial fraud detection is one of the core technological assets of Fintech companies. It saves tens of millions of money fro m Chinese Fintech companies since the bad loan rate is more than 10. HC Financial Service Group is the 3rd largest company in the Chinese P2P financial market. In this paper we illustrate how we tackle the fraud detection problem at HC Financial. We utilize two powerful workhorses in the machine learning field - random forest and gradient boosting decision tree to detect fraudulent users . We demonstrate that by carefully select features and tune model parameters , we could effectively filter out fraudulent users in the P2P market.",Finance
Investigating the Investment Behaviors in Cryptocurrency,"This study investigates the socio-demographic characteristics that individual cryptocurrency investors exhibit and the factors which go into their investment decisions in different Initial Coin Offerings. A web based revealed preference survey was conducted among Australian and Chinese blockchain and cryptocurrency followers, and a Multinomial Logit model was applied to inferentially analyze the characteristics of cryptocurrency investors and the determinants of the choice of investment in cryptocurrency coins versus other types of ICO tokens. The results show a difference between the determinant of these two choices among Australian and Chinese cryptocurrency folks. The significant factors of these two choices include age, gender, education, occupation, and investment experience, and they align well with the behavioural literature. Furthermore, alongside differences in how they rank the attributes of ICOs, there is further variance between how Chinese and Australian investors rank deterrence factors and investment strategies.","Investigating the Investment Behaviors in Cryptocurrency This study investigates the socio-demographic characteristics that individual cryptocurrency investors exhibit and the factors which go into their investment decisions in different Initial Coin Offerings. A web based revealed preference survey was conducted among Australian and Chinese blockchain and cryptocurrency followers, and a Multinomial Logit model was applied to inferentially analyze the characteristics of cryptocurrency investors and the determinants of the choice of investment in cryptocurrency coins versus other types of ICO tokens. The results show a difference between the determinant of these two choices among Australian and Chinese cryptocurrency folks. The significant factors of these two choices include age, gender, education, occupation, and investment experience, and they align well with the behavioural literature. Furthermore, alongside differences in how they rank the attributes of ICOs, there is further variance between how Chinese and Australian investors rank deterrence factors and investment strategies.",Finance
Multifractal analysis of normal RR heart-interbeat signals in power spectra ranges,"Power spectral density is an accepted measure of heart rate variability. Two estimators of multifractal properties: Wavelet Transform Modulus Maxima and Multifractal Detrended Fluctuation Analysis are used to investigate multifractal properties for the three strongly physiologically grounded components of power spectra: low frequency (LF), very low frequency (VLF) and ultra low frequency (ULV). Circadian rhythm changes are examined by discrimination of daily activity from nocturnal rest. Investigations consider normal sinus rhythms of healthy 39 subjects which are grouped in two sets: 5-hour wake series and 5-hour sleep series. Qualitative arguments are provided to conjecture the presence of stochastic persistence in LF range, loss of heart rate variability during night in VLF range and its increase in ULF.","Multifractal analysis of normal RR heart-interbeat signals in power spectra ranges Power spectral density is an accepted measure of heart rate variability. Two estimators of multifractal properties: Wavelet Transform Modulus Maxima and Multifractal Detrended Fluctuation Analysis are used to investigate multifractal properties for the three strongly physiologically grounded components of power spectra: low frequency (LF), very low frequency (VLF) and ultra low frequency (ULV). Circadian rhythm changes are examined by discrimination of daily activity from nocturnal rest. Investigations consider normal sinus rhythms of healthy 39 subjects which are grouped in two sets: 5-hour wake series and 5-hour sleep series. Qualitative arguments are provided to conjecture the presence of stochastic persistence in LF range, loss of heart rate variability during night in VLF range and its increase in ULF.",Healthcare
Lagrangian Approaches for a class of Matching Problems in Computational Biology,"This paper presents efficient algorithms for solving the problem of aligning a protein structure template to a query amino-acid sequence, known as protein threading problem. We consider the problem as a special case of graph matching problem. We give formal graph and integer programming models of the problem. After studying the properties of these models, we propose two kinds of Lagrangian relaxation for solving them. We present experimental results on real life instances showing the efficiency of our approaches.","Lagrangian Approaches for a class of Matching Problems in Computational Biology This paper presents efficient algorithms for solving the problem of aligning a protein structure template to a query amino-acid sequence, known as protein threading problem. We consider the problem as a special case of graph matching problem. We give formal graph and integer programming models of the problem. After studying the properties of these models, we propose two kinds of Lagrangian relaxation for solving them. We present experimental results on real life instances showing the efficiency of our approaches.",Healthcare
Optimality and sustainability of hybrid limit cycles in the pollution control problem with regime shifts,"In this paper, we consider the problem of pollution control in a system that undergoes regular regime shifts. We first show that the optimal policy of pollution abatement is periodic as well, and is described by the unique hybrid limit cycle. We next introduce the notion of an environmentally sustainable solution, and demonstrate that such a policy is the only one that yields the best possible trade-off between steadily achieving profit and ensuring environmental preservation. In contrast to that, the policy that is not environmentally sustainable eventually enters stagnation. To further illustrate our findings, we compare the optimal periodic solution with a myopic one. Interestingly enough, the myopic solution yields higher overall payoff in the short-run, but completely fails in the long-run, while the environmentally sustainable policy yields maximal payoff and preserves the environment over the infinite time interval.","Optimality and sustainability of hybrid limit cycles in the pollution control problem with regime shifts In this paper, we consider the problem of pollution control in a system that undergoes regular regime shifts. We first show that the optimal policy of pollution abatement is periodic as well, and is described by the unique hybrid limit cycle. We next introduce the notion of an environmentally sustainable solution, and demonstrate that such a policy is the only one that yields the best possible trade-off between steadily achieving profit and ensuring environmental preservation. In contrast to that, the policy that is not environmentally sustainable eventually enters stagnation. To further illustrate our findings, we compare the optimal periodic solution with a myopic one. Interestingly enough, the myopic solution yields higher overall payoff in the short-run, but completely fails in the long-run, while the environmentally sustainable policy yields maximal payoff and preserves the environment over the infinite time interval.",Environment
Benchmarking the Pedagogical Knowledge of Large Language Models,"Benchmarks like Massive Multitask Language Understanding (MMLU) have played a pivotal role in evaluating AIs knowledge and abilities across diverse domains. However, existing benchmarks predominantly focus on content knowledge, leaving a critical gap in assessing models understanding of pedagogy - the method and practice of teaching. This paper introduces The Pedagogy Benchmark, a novel dataset designed to evaluate large language models on their Cross-Domain Pedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND) pedagogical knowledge. These benchmarks are built on a carefully curated set of questions sourced from professional development exams for teachers, which cover a range of pedagogical subdomains such as teaching strategies and assessment methods. Here we outline the methodology and development of these benchmarks. We report results for 97 models, with accuracies spanning a range from 28 to 89 on the pedagogical knowledge questions. We consider the relationship between cost and accuracy and chart the progression of the Pareto value frontier over time. We provide online leaderboards at https:rebrand.lypedagogy which are updated with new models and allow interactive exploration and filtering based on various model properties, such as cost per token and open-vs-closed weights, as well as looking at performance in different subjects. LLMs and generative AI have tremendous potential to influence education and help to address the global learning crisis. Education-focused benchmarks are crucial to measure models capacities to understand pedagogical concepts, respond appropriately to learners needs, and support effective teaching practices across diverse contexts. They are needed for informing the responsible and evidence-based deployment of LLMs and LLM-based tools in educational settings, and for guiding both development and policy decisions.","Benchmarking the Pedagogical Knowledge of Large Language Models Benchmarks like Massive Multitask Language Understanding (MMLU) have played a pivotal role in evaluating AIs knowledge and abilities across diverse domains. However, existing benchmarks predominantly focus on content knowledge, leaving a critical gap in assessing models understanding of pedagogy - the method and practice of teaching. This paper introduces The Pedagogy Benchmark, a novel dataset designed to evaluate large language models on their Cross-Domain Pedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND) pedagogical knowledge. These benchmarks are built on a carefully curated set of questions sourced from professional development exams for teachers, which cover a range of pedagogical subdomains such as teaching strategies and assessment methods. Here we outline the methodology and development of these benchmarks. We report results for 97 models, with accuracies spanning a range from 28 to 89 on the pedagogical knowledge questions. We consider the relationship between cost and accuracy and chart the progression of the Pareto value frontier over time. We provide online leaderboards at https:rebrand.lypedagogy which are updated with new models and allow interactive exploration and filtering based on various model properties, such as cost per token and open-vs-closed weights, as well as looking at performance in different subjects. LLMs and generative AI have tremendous potential to influence education and help to address the global learning crisis. Education-focused benchmarks are crucial to measure models capacities to understand pedagogical concepts, respond appropriately to learners needs, and support effective teaching practices across diverse contexts. They are needed for informing the responsible and evidence-based deployment of LLMs and LLM-based tools in educational settings, and for guiding both development and policy decisions.",Education
About one 3-parameter Model of Testing,"This article offers a 3-parameter model of testing, with 1) the difference between the ability level of the examinee and item difficulty; 2) the examinee discrimination and 3) the item discrimination as model parameters.","About one 3-parameter Model of Testing This article offers a 3-parameter model of testing, with 1) the difference between the ability level of the examinee and item difficulty; 2) the examinee discrimination and 3) the item discrimination as model parameters.",Technology
Differential Invariants under Gamma Correction,This paper presents invariants under gamma correction and similarity transformations. The invariants are local features based on differentials which are implemented using derivatives of the Gaussian. The use of the proposed invariant representation is shown to yield improved correlation results in a template matching scenario.,Differential Invariants under Gamma Correction This paper presents invariants under gamma correction and similarity transformations. The invariants are local features based on differentials which are implemented using derivatives of the Gaussian. The use of the proposed invariant representation is shown to yield improved correlation results in a template matching scenario.,Technology
State-dependence of climate sensitivity: attractor constraints and palaeoclimate regimes,"Equilibrium climate sensitivity (ECS) is a key predictor of climate change. However, it is not very well constrained, either by climate models or by observational data. The reasons for this include strong internal variability and forcing on many time scales. In practise this means that the equilibrium will only be relative to fixing the slow feedback processes before comparing palaeoclimate sensitivity estimates with estimates from model simulations. In addition, information from the late Pleistocene ice age cycles indicates that the climate cycles between cold and warm regimes, and the climate sensitivity varies considerably between regime because of fast feedback processes changing relative strength and time scales over one cycle. In this paper we consider climate sensitivity for quite general climate dynamics. Using a conceptual Earth system model of Gildor and Tziperman (2001) (with Milankovich forcing and dynamical ocean biogeochemistry) we explore various ways of quantifying the state-dependence of climate sensitivity from unperturbed and perturbed model time series. Even without considering any perturbations, we suggest that climate sensitivity can be usefully thought of as a distribution that quantifies variability within the climate attractor and where there is a strong dependence on climate state and more specificially on the climate regime where fast processes are approximately in equilibrium. We also consider perturbations by instantaneous doubling of CO_2 and similarly find a strong dependence on the climate state using our approach.","State-dependence of climate sensitivity: attractor constraints and palaeoclimate regimes Equilibrium climate sensitivity (ECS) is a key predictor of climate change. However, it is not very well constrained, either by climate models or by observational data. The reasons for this include strong internal variability and forcing on many time scales. In practise this means that the equilibrium will only be relative to fixing the slow feedback processes before comparing palaeoclimate sensitivity estimates with estimates from model simulations. In addition, information from the late Pleistocene ice age cycles indicates that the climate cycles between cold and warm regimes, and the climate sensitivity varies considerably between regime because of fast feedback processes changing relative strength and time scales over one cycle. In this paper we consider climate sensitivity for quite general climate dynamics. Using a conceptual Earth system model of Gildor and Tziperman (2001) (with Milankovich forcing and dynamical ocean biogeochemistry) we explore various ways of quantifying the state-dependence of climate sensitivity from unperturbed and perturbed model time series. Even without considering any perturbations, we suggest that climate sensitivity can be usefully thought of as a distribution that quantifies variability within the climate attractor and where there is a strong dependence on climate state and more specificially on the climate regime where fast processes are approximately in equilibrium. We also consider perturbations by instantaneous doubling of CO_2 and similarly find a strong dependence on the climate state using our approach.",Environment
Are Educational Escape Rooms More Effective Than Traditional Lectures for Teaching Software Engineering? A Randomized Controlled Trial,"Contribution: This article analyzes the learning effectiveness of a virtual educational escape room for teaching software engineering and compares this activity with traditional teaching through a randomized controlled trial. Background: Educational escape rooms have been used across a wide variety of disciplines at all levels of education and they are becoming increasingly popular among teachers. Nevertheless, there is a clear general need for more robust empirical evidence on the learning effectiveness of these novel activities and, particularly, on their application in software engineering education. Research Questions: Is game-based learning using educational escape rooms more effective than traditional lectures for teaching software engineering? What are the perceptions of software engineering students toward game-based learning using educational escape rooms? Methodology: The study presented in this article is a randomized controlled trial with a pre-and post-test design that was completed by a total of 326 software engineering students. The 164 students belonging to the experimental group learned software modeling by playing an educational escape room whereas the 162 students belonging to the control group learned the same subject matter through a traditional lecture. Findings: The results of the randomized controlled trial show that the students who learned software modeling through the educational escape room had very positive perceptions toward this activity, significantly increased their knowledge, and outperformed those students who learned through a traditional lecture in terms of knowledge acquisition.","Are Educational Escape Rooms More Effective Than Traditional Lectures for Teaching Software Engineering? A Randomized Controlled Trial Contribution: This article analyzes the learning effectiveness of a virtual educational escape room for teaching software engineering and compares this activity with traditional teaching through a randomized controlled trial. Background: Educational escape rooms have been used across a wide variety of disciplines at all levels of education and they are becoming increasingly popular among teachers. Nevertheless, there is a clear general need for more robust empirical evidence on the learning effectiveness of these novel activities and, particularly, on their application in software engineering education. Research Questions: Is game-based learning using educational escape rooms more effective than traditional lectures for teaching software engineering? What are the perceptions of software engineering students toward game-based learning using educational escape rooms? Methodology: The study presented in this article is a randomized controlled trial with a pre-and post-test design that was completed by a total of 326 software engineering students. The 164 students belonging to the experimental group learned software modeling by playing an educational escape room whereas the 162 students belonging to the control group learned the same subject matter through a traditional lecture. Findings: The results of the randomized controlled trial show that the students who learned software modeling through the educational escape room had very positive perceptions toward this activity, significantly increased their knowledge, and outperformed those students who learned through a traditional lecture in terms of knowledge acquisition.",Education
Adversarial Curriculum Graph Contrastive Learning with Pair-wise Augmentation,"Graph contrastive learning (GCL) has emerged as a pivotal technique in the domain of graph representation learning. A crucial aspect of effective GCL is the caliber of generated positive and negative samples, which is intrinsically dictated by their resemblance to the original data. Nevertheless, precise control over similarity during sample generation presents a formidable challenge, often impeding the effective discovery of representative graph patterns. To address this challenge, we propose an innovative framework: Adversarial Curriculum Graph Contrastive Learning (ACGCL), which capitalizes on the merits of pair-wise augmentation to engender graph-level positive and negative samples with controllable similarity, alongside subgraph contrastive learning to discern effective graph patterns therein. Within the ACGCL framework, we have devised a novel adversarial curriculum training methodology that facilitates progressive learning by sequentially increasing the difficulty of distinguishing the generated samples. Notably, this approach transcends the prevalent sparsity issue inherent in conventional curriculum learning strategies by adaptively concentrating on more challenging training data. Finally, a comprehensive assessment of ACGCL is conducted through extensive experiments on six well-known benchmark datasets, wherein ACGCL conspicuously surpasses a set of state-of-the-art baselines.","Adversarial Curriculum Graph Contrastive Learning with Pair-wise Augmentation Graph contrastive learning (GCL) has emerged as a pivotal technique in the domain of graph representation learning. A crucial aspect of effective GCL is the caliber of generated positive and negative samples, which is intrinsically dictated by their resemblance to the original data. Nevertheless, precise control over similarity during sample generation presents a formidable challenge, often impeding the effective discovery of representative graph patterns. To address this challenge, we propose an innovative framework: Adversarial Curriculum Graph Contrastive Learning (ACGCL), which capitalizes on the merits of pair-wise augmentation to engender graph-level positive and negative samples with controllable similarity, alongside subgraph contrastive learning to discern effective graph patterns therein. Within the ACGCL framework, we have devised a novel adversarial curriculum training methodology that facilitates progressive learning by sequentially increasing the difficulty of distinguishing the generated samples. Notably, this approach transcends the prevalent sparsity issue inherent in conventional curriculum learning strategies by adaptively concentrating on more challenging training data. Finally, a comprehensive assessment of ACGCL is conducted through extensive experiments on six well-known benchmark datasets, wherein ACGCL conspicuously surpasses a set of state-of-the-art baselines.",Education
Eroding market stability by proliferation of financial instruments,"We contrast Arbitrage Pricing Theory (APT), the theoretical basis for the development of financial instruments, with a dynamical picture of an interacting market, in a simple setting. The proliferation of financial instruments apparently provides more means for risk diversification, making the market more efficient and complete. In the simple market of interacting traders discussed here, the proliferation of financial instruments erodes systemic stability and it drives the market to a critical state characterized by large susceptibility, strong fluctuations and enhanced correlations among risks. This suggests that the hypothesis of APT may not be compatible with a stable market dynamics. In this perspective, market stability acquires the properties of a common good, which suggests that appropriate measures should be introduced in derivative markets, to preserve stability.","Eroding market stability by proliferation of financial instruments We contrast Arbitrage Pricing Theory (APT), the theoretical basis for the development of financial instruments, with a dynamical picture of an interacting market, in a simple setting. The proliferation of financial instruments apparently provides more means for risk diversification, making the market more efficient and complete. In the simple market of interacting traders discussed here, the proliferation of financial instruments erodes systemic stability and it drives the market to a critical state characterized by large susceptibility, strong fluctuations and enhanced correlations among risks. This suggests that the hypothesis of APT may not be compatible with a stable market dynamics. In this perspective, market stability acquires the properties of a common good, which suggests that appropriate measures should be introduced in derivative markets, to preserve stability.",Finance
Computation of amplification for systems arising from cellular signaling pathways,"A commonly employed measure of the signal amplification properties of an inputoutput system is its induced L2 norm, sometimes also known as H infinity gain. In general, however, it is extremely difficult to compute the numerical value for this norm, or even to check that it is finite, unless the system being studied is linear. This paper describes a class of systems for which it is possible to reduce this computation to that of finding the norm of an associated linear system. In contrast to linearization approaches, a precise value, not an estimate, is obtained for the full nonlinear model. The class of systems that we study arose from the modeling of certain biological intracellular signaling cascades, but the results should be of wider applicability.","Computation of amplification for systems arising from cellular signaling pathways A commonly employed measure of the signal amplification properties of an inputoutput system is its induced L2 norm, sometimes also known as H infinity gain. In general, however, it is extremely difficult to compute the numerical value for this norm, or even to check that it is finite, unless the system being studied is linear. This paper describes a class of systems for which it is possible to reduce this computation to that of finding the norm of an associated linear system. In contrast to linearization approaches, a precise value, not an estimate, is obtained for the full nonlinear model. The class of systems that we study arose from the modeling of certain biological intracellular signaling cascades, but the results should be of wider applicability.",Healthcare
Fifteen Years of Econophysics Research,"Econophysics is a new research field, which makes an attempt to bring economics in the fold of natural sciences or specifically attempts for a physics of economics. The term Econophysics was formally born in Kolkata in 1995. The entry on Econophysics in The New Palgrave Dictionary of Economics, 2nd Ed., Vol 2, Macmillan, NY (2008), pp 729-732, begins with ... the term econophysics was neologized in 1995 at the second Statphys- Kolkata conference in Kolkata (formerly Calcutta), India .... The Econophysics research therefore formally completes fifteen years of research by the end of this year! The importance and proliferation of the interdisciplinary research of Econophysics is highlighted in the special issue of Science  Culture, which presents a collection of twenty nine papers (giving country wise perspectives, reviews of the recent developments and original research communications), written by more than forty renowned experts in physics, mathematics or economics, from all over the world. We present here the list of contents and the editorial. The manuscript files are available at http:fiquant.mas.ecp.frchakraboa for preview. This special issue will be published online at http:www.scienceandculture-isna.orgjournal.htm, at the end of October 2010.","Fifteen Years of Econophysics Research Econophysics is a new research field, which makes an attempt to bring economics in the fold of natural sciences or specifically attempts for a physics of economics. The term Econophysics was formally born in Kolkata in 1995. The entry on Econophysics in The New Palgrave Dictionary of Economics, 2nd Ed., Vol 2, Macmillan, NY (2008), pp 729-732, begins with ... the term econophysics was neologized in 1995 at the second Statphys- Kolkata conference in Kolkata (formerly Calcutta), India .... The Econophysics research therefore formally completes fifteen years of research by the end of this year! The importance and proliferation of the interdisciplinary research of Econophysics is highlighted in the special issue of Science  Culture, which presents a collection of twenty nine papers (giving country wise perspectives, reviews of the recent developments and original research communications), written by more than forty renowned experts in physics, mathematics or economics, from all over the world. We present here the list of contents and the editorial. The manuscript files are available at http:fiquant.mas.ecp.frchakraboa for preview. This special issue will be published online at http:www.scienceandculture-isna.orgjournal.htm, at the end of October 2010.",Finance
Bayesian inference with an adaptive proposal density for GARCH models,"We perform the Bayesian inference of a GARCH model by the Metropolis-Hastings algorithm with an adaptive proposal density. The adaptive proposal density is assumed to be the Students t-distribution and the distribution parameters are evaluated by using the data sampled during the simulation. We apply the method for the QGARCH model which is one of asymmetric GARCH models and make empirical studies for for Nikkei 225, DAX and Hang indexes. We find that autocorrelation times from our method are very small, thus the method is very efficient for generating uncorrelated Monte Carlo data. The results from the QGARCH model show that all the three indexes show the leverage effect, i.e. the volatility is high after negative observations.","Bayesian inference with an adaptive proposal density for GARCH models We perform the Bayesian inference of a GARCH model by the Metropolis-Hastings algorithm with an adaptive proposal density. The adaptive proposal density is assumed to be the Students t-distribution and the distribution parameters are evaluated by using the data sampled during the simulation. We apply the method for the QGARCH model which is one of asymmetric GARCH models and make empirical studies for for Nikkei 225, DAX and Hang indexes. We find that autocorrelation times from our method are very small, thus the method is very efficient for generating uncorrelated Monte Carlo data. The results from the QGARCH model show that all the three indexes show the leverage effect, i.e. the volatility is high after negative observations.",Finance
From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with Pedagogy using Reinforcement Learning,"Large language models (LLMs) can transform education, but their optimization for direct question-answering often undermines effective pedagogy which requires strategically withholding answers. To mitigate this, we propose an online reinforcement learning (RL)-based alignment framework that can quickly adapt LLMs into effective tutors using simulated student-tutor interactions by emphasizing pedagogical quality and guided problem-solving over simply giving away answers. We use our method to train a 7B parameter tutor model without human annotations which reaches similar performance to larger proprietary models like LearnLM. We introduce a controllable reward weighting to balance pedagogical support and student solving accuracy, allowing us to trace the Pareto frontier between these two objectives. Our models better preserve reasoning capabilities than single-turn SFT baselines and can optionally enhance interpretability through thinking tags that expose the models instructional planning.","From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with Pedagogy using Reinforcement Learning Large language models (LLMs) can transform education, but their optimization for direct question-answering often undermines effective pedagogy which requires strategically withholding answers. To mitigate this, we propose an online reinforcement learning (RL)-based alignment framework that can quickly adapt LLMs into effective tutors using simulated student-tutor interactions by emphasizing pedagogical quality and guided problem-solving over simply giving away answers. We use our method to train a 7B parameter tutor model without human annotations which reaches similar performance to larger proprietary models like LearnLM. We introduce a controllable reward weighting to balance pedagogical support and student solving accuracy, allowing us to trace the Pareto frontier between these two objectives. Our models better preserve reasoning capabilities than single-turn SFT baselines and can optionally enhance interpretability through thinking tags that expose the models instructional planning.",Education
When a rat race implies an intergenerational wealth trap,"Two critical questions about intergenerational outcomes are: one, whether significant barriers or traps exist between different social or economic strata; and two, the extent to which intergenerational outcomes do (or can be used to) affect individual investment and consumption decisions. We develop a model to explicitly relate these two questions, and prove the first such rat race theorem, showing that a fundamental relationship exists between high levels of individual investment and the existence of a wealth trap, which traps otherwise identical agents at a lower level of wealth. Our simple model of intergenerational wealth dynamics involves agents which balance current consumption with investment in a single descendant. Investments then determine descendant wealth via a potentially nonlinear and discontinuous competitiveness function about which we do not make concavity assumptions. From this model we demonstrate how to infer such a competitiveness function from investments, along with geometric criteria to determine individual decisions. Additionally we investigate the stability of a wealth distribution, both to local perturbations and to the introduction of new agents with no wealth.","When a rat race implies an intergenerational wealth trap Two critical questions about intergenerational outcomes are: one, whether significant barriers or traps exist between different social or economic strata; and two, the extent to which intergenerational outcomes do (or can be used to) affect individual investment and consumption decisions. We develop a model to explicitly relate these two questions, and prove the first such rat race theorem, showing that a fundamental relationship exists between high levels of individual investment and the existence of a wealth trap, which traps otherwise identical agents at a lower level of wealth. Our simple model of intergenerational wealth dynamics involves agents which balance current consumption with investment in a single descendant. Investments then determine descendant wealth via a potentially nonlinear and discontinuous competitiveness function about which we do not make concavity assumptions. From this model we demonstrate how to infer such a competitiveness function from investments, along with geometric criteria to determine individual decisions. Additionally we investigate the stability of a wealth distribution, both to local perturbations and to the introduction of new agents with no wealth.",Finance
Flexible Camera Calibration Using a New Analytical Radial Undistortion Formula with Application to Mobile Robot Localization,"Most algorithms in 3D computer vision rely on the pinhole camera model because of its simplicity, whereas virtually all imaging devices introduce certain amount of nonlinear distortion, where the radial distortion is the most severe part. Common approach to radial distortion is by the means of polynomial approximation, which introduces distortion-specific parameters into the camera model and requires estimation of these distortion parameters. The task of estimating radial distortion is to find a radial distortion model that allows easy undistortion as well as satisfactory accuracy. This paper presents a new radial distortion model with an easy analytical undistortion formula, which also belongs to the polynomial approximation category. Experimental results are presented to show that with this radial distortion model, satisfactory accuracy is achieved. An application of the new radial distortion model is non-iterative yellow line alignment with a calibrated camera on ODIS, a robot built in our CSOIS.","Flexible Camera Calibration Using a New Analytical Radial Undistortion Formula with Application to Mobile Robot Localization Most algorithms in 3D computer vision rely on the pinhole camera model because of its simplicity, whereas virtually all imaging devices introduce certain amount of nonlinear distortion, where the radial distortion is the most severe part. Common approach to radial distortion is by the means of polynomial approximation, which introduces distortion-specific parameters into the camera model and requires estimation of these distortion parameters. The task of estimating radial distortion is to find a radial distortion model that allows easy undistortion as well as satisfactory accuracy. This paper presents a new radial distortion model with an easy analytical undistortion formula, which also belongs to the polynomial approximation category. Experimental results are presented to show that with this radial distortion model, satisfactory accuracy is achieved. An application of the new radial distortion model is non-iterative yellow line alignment with a calibrated camera on ODIS, a robot built in our CSOIS.",Technology
Navigating the New Landscape: A Conceptual Model for Project-Based Assessment (PBA) in the Age of GenAI,"The rapid integration of Generative Artificial Intelligence (GenAI) into higher education presents both opportunities and challenges for assessment design, particularly within Project-Based Assessment (PBA) contexts. Traditional assessment methods often emphasise the final product in the PBA, which can now be significantly influenced or created by GenAI tools, raising concerns regarding product authenticity, academic integrity, and learning validation. This paper advocates for a reimagined assessment model for Project-Based Learning (PBL) or a capstone project that prioritises process-oriented evaluation, multi-modal and multifaceted assessment design, and ethical engagement with GenAI to enable higher-order thinking. The model also emphasises the use of (GenAI-assisted) personalised feedback by a supervisor as an observance of the learning process during the project lifecycle. A use case scenario is provided to illustrate the application of the model in a capstone project setting. The paper concludes with recommendations for educators and curriculum designers to ensure that assessment practices remain robust, learner-centric, and integrity-driven in the evolving landscape of GenAI.","Navigating the New Landscape: A Conceptual Model for Project-Based Assessment (PBA) in the Age of GenAI The rapid integration of Generative Artificial Intelligence (GenAI) into higher education presents both opportunities and challenges for assessment design, particularly within Project-Based Assessment (PBA) contexts. Traditional assessment methods often emphasise the final product in the PBA, which can now be significantly influenced or created by GenAI tools, raising concerns regarding product authenticity, academic integrity, and learning validation. This paper advocates for a reimagined assessment model for Project-Based Learning (PBL) or a capstone project that prioritises process-oriented evaluation, multi-modal and multifaceted assessment design, and ethical engagement with GenAI to enable higher-order thinking. The model also emphasises the use of (GenAI-assisted) personalised feedback by a supervisor as an observance of the learning process during the project lifecycle. A use case scenario is provided to illustrate the application of the model in a capstone project setting. The paper concludes with recommendations for educators and curriculum designers to ensure that assessment practices remain robust, learner-centric, and integrity-driven in the evolving landscape of GenAI.",Education
An Analytical Piecewise Radial Distortion Model for Precision Camera Calibration,"The common approach to radial distortion is by the means of polynomial approximation, which introduces distortion-specific parameters into the camera model and requires estimation of these distortion parameters. The task of estimating radial distortion is to find a radial distortion model that allows easy undistortion as well as satisfactory accuracy. This paper presents a new piecewise radial distortion model with easy analytical undistortion formula. The motivation for seeking a piecewise radial distortion model is that, when a camera is resulted in a low quality during manufacturing, the nonlinear radial distortion can be complex. Using low order polynomials to approximate the radial distortion might not be precise enough. On the other hand, higher order polynomials suffer from the inverse problem. With the new piecewise radial distortion function, more flexibility is obtained and the radial undistortion can be performed analytically. Experimental results are presented to show that with this new piecewise radial distortion model, better performance is achieved than that using the single function. Furthermore, a comparable performance with the conventional polynomial model using 2 coefficients can also be accomplished.","An Analytical Piecewise Radial Distortion Model for Precision Camera Calibration The common approach to radial distortion is by the means of polynomial approximation, which introduces distortion-specific parameters into the camera model and requires estimation of these distortion parameters. The task of estimating radial distortion is to find a radial distortion model that allows easy undistortion as well as satisfactory accuracy. This paper presents a new piecewise radial distortion model with easy analytical undistortion formula. The motivation for seeking a piecewise radial distortion model is that, when a camera is resulted in a low quality during manufacturing, the nonlinear radial distortion can be complex. Using low order polynomials to approximate the radial distortion might not be precise enough. On the other hand, higher order polynomials suffer from the inverse problem. With the new piecewise radial distortion function, more flexibility is obtained and the radial undistortion can be performed analytically. Experimental results are presented to show that with this new piecewise radial distortion model, better performance is achieved than that using the single function. Furthermore, a comparable performance with the conventional polynomial model using 2 coefficients can also be accomplished.",Technology
Harnessing the Complexity of Education with Information Technology,"Education at all levels is facing several challenges in most countries, such as low quality, high costs, lack of educators, and unsatisfied student demand. Traditional approaches are becoming unable to deliver the required education. Several causes for this inefficiency can be identified. I argue that beyond specific causes, the lack of effective education is related to complexity. However, information technology is helping us overcome this complexity.","Harnessing the Complexity of Education with Information Technology Education at all levels is facing several challenges in most countries, such as low quality, high costs, lack of educators, and unsatisfied student demand. Traditional approaches are becoming unable to deliver the required education. Several causes for this inefficiency can be identified. I argue that beyond specific causes, the lack of effective education is related to complexity. However, information technology is helping us overcome this complexity.",Education
You are right. I am ALARMED -- But by Climate Change Counter Movement,"The world is facing the challenge of climate crisis. Despite the consensus in scientific community about anthropogenic global warming, the web is flooded with articles spreading climate misinformation. These articles are carefully constructed by climate change counter movement (cccm) organizations to influence the narrative around climate change. We revisit the literature on climate misinformation in social sciences and repackage it to introduce in the community of NLP. Despite considerable work in detection of fake news, there is no misinformation dataset available that is specific to the domain.of climate change. We try to bridge this gap by scraping and releasing articles with known climate change misinformation.","You are right. I am ALARMED -- But by Climate Change Counter Movement The world is facing the challenge of climate crisis. Despite the consensus in scientific community about anthropogenic global warming, the web is flooded with articles spreading climate misinformation. These articles are carefully constructed by climate change counter movement (cccm) organizations to influence the narrative around climate change. We revisit the literature on climate misinformation in social sciences and repackage it to introduce in the community of NLP. Despite considerable work in detection of fake news, there is no misinformation dataset available that is specific to the domain.of climate change. We try to bridge this gap by scraping and releasing articles with known climate change misinformation.",Environment
Singapores Role for ASEANs Portfolio Investment,"We investigate the elasticity of portfolio investment of ASEAN and OECD members to geographical distance in a gravity model utilizing a bilateral panel of 86 reporting and 241 counterparty countriesterritories for 2007-2017. We find that the elasticity is more negative for ASEAN than OECD members. The difference is larger if we exclude Singapore. This indicates that Singapores behavior is distinct from other ASEAN members. While Singapore tends to invest in distant OECD countries, other ASEAN members tend to invest in nearby countries. Our study sheds light on the role of a regional financial center in global finance.","Singapores Role for ASEANs Portfolio Investment We investigate the elasticity of portfolio investment of ASEAN and OECD members to geographical distance in a gravity model utilizing a bilateral panel of 86 reporting and 241 counterparty countriesterritories for 2007-2017. We find that the elasticity is more negative for ASEAN than OECD members. The difference is larger if we exclude Singapore. This indicates that Singapores behavior is distinct from other ASEAN members. While Singapore tends to invest in distant OECD countries, other ASEAN members tend to invest in nearby countries. Our study sheds light on the role of a regional financial center in global finance.",Finance
A New Analytical Radial Distortion Model for Camera Calibration,"Common approach to radial distortion is by the means of polynomial approximation, which introduces distortion-specific parameters into the camera model and requires estimation of these distortion parameters. The task of estimating radial distortion is to find a radial distortion model that allows easy undistortion as well as satisfactory accuracy. This paper presents a new radial distortion model with an easy analytical undistortion formula, which also belongs to the polynomial approximation category. Experimental results are presented to show that with this radial distortion model, satisfactory accuracy is achieved.","A New Analytical Radial Distortion Model for Camera Calibration Common approach to radial distortion is by the means of polynomial approximation, which introduces distortion-specific parameters into the camera model and requires estimation of these distortion parameters. The task of estimating radial distortion is to find a radial distortion model that allows easy undistortion as well as satisfactory accuracy. This paper presents a new radial distortion model with an easy analytical undistortion formula, which also belongs to the polynomial approximation category. Experimental results are presented to show that with this radial distortion model, satisfactory accuracy is achieved.",Technology
CuSINeS: Curriculum-driven Structure Induced Negative Sampling for Statutory Article Retrieval,"In this paper, we introduce CuSINeS, a negative sampling approach to enhance the performance of Statutory Article Retrieval (SAR). CuSINeS offers three key contributions. Firstly, it employs a curriculum-based negative sampling strategy guiding the model to focus on easier negatives initially and progressively tackle more difficult ones. Secondly, it leverages the hierarchical and sequential information derived from the structural organization of statutes to evaluate the difficulty of samples. Lastly, it introduces a dynamic semantic difficulty assessment using the being-trained model itself, surpassing conventional static methods like BM25, adapting the negatives to the models evolving competence. Experimental results on a real-world expert-annotated SAR dataset validate the effectiveness of CuSINeS across four different baselines, demonstrating its versatility.","CuSINeS: Curriculum-driven Structure Induced Negative Sampling for Statutory Article Retrieval In this paper, we introduce CuSINeS, a negative sampling approach to enhance the performance of Statutory Article Retrieval (SAR). CuSINeS offers three key contributions. Firstly, it employs a curriculum-based negative sampling strategy guiding the model to focus on easier negatives initially and progressively tackle more difficult ones. Secondly, it leverages the hierarchical and sequential information derived from the structural organization of statutes to evaluate the difficulty of samples. Lastly, it introduces a dynamic semantic difficulty assessment using the being-trained model itself, surpassing conventional static methods like BM25, adapting the negatives to the models evolving competence. Experimental results on a real-world expert-annotated SAR dataset validate the effectiveness of CuSINeS across four different baselines, demonstrating its versatility.",Education
Worksheets for Guiding Novices through the Visualization Design Process,"For visualization pedagogy, an important but challenging notion to teach is design, from making to evaluating visualization encodings, user interactions, or data visualization systems. In our previous work, we introduced the design activity framework to codify the high-level activities of the visualization design process. This framework has helped structure experts design processes to create visualization systems, but the frameworks four activities lack a breakdown into steps with a concrete example to help novices utilizing this framework in their own real-world design process. To provide students with such concrete guidelines, we created worksheets for each design activity: understand, ideate, make, and deploy. Each worksheet presents a high-level summary of the activity with actionable, guided steps for a novice designer to follow. We validated the use of this framework and the worksheets in a graduate-level visualization course taught at our university. For this evaluation, we surveyed the class and conducted 13 student interviews to garner qualitative, open-ended feedback and suggestions on the worksheets. We conclude this work with a discussion and highlight various areas for future work on improving visualization design pedagogy.","Worksheets for Guiding Novices through the Visualization Design Process For visualization pedagogy, an important but challenging notion to teach is design, from making to evaluating visualization encodings, user interactions, or data visualization systems. In our previous work, we introduced the design activity framework to codify the high-level activities of the visualization design process. This framework has helped structure experts design processes to create visualization systems, but the frameworks four activities lack a breakdown into steps with a concrete example to help novices utilizing this framework in their own real-world design process. To provide students with such concrete guidelines, we created worksheets for each design activity: understand, ideate, make, and deploy. Each worksheet presents a high-level summary of the activity with actionable, guided steps for a novice designer to follow. We validated the use of this framework and the worksheets in a graduate-level visualization course taught at our university. For this evaluation, we surveyed the class and conducted 13 student interviews to garner qualitative, open-ended feedback and suggestions on the worksheets. We conclude this work with a discussion and highlight various areas for future work on improving visualization design pedagogy.",Education
Rseaux dAutomates de Caianiello Revisit,"We exhibit a family of neural networks of McCulloch and Pitts of size 2nk2 which can be simulated by a neural networks of Caianiello of size 2n2 and memory length k. This simulation allows us to find again one of the result of the following article: Cycles exponentiels des reseaux de Caianiello et compteurs en arithmetique redondante, Technique et Science Informatiques Vol. 19, pages 985-1008 on the existence of neural networks of Caianiello of size 2n2 and memory length k which describes a cycle of length k times 2nk.","Rseaux dAutomates de Caianiello Revisit We exhibit a family of neural networks of McCulloch and Pitts of size 2nk2 which can be simulated by a neural networks of Caianiello of size 2n2 and memory length k. This simulation allows us to find again one of the result of the following article: Cycles exponentiels des reseaux de Caianiello et compteurs en arithmetique redondante, Technique et Science Informatiques Vol. 19, pages 985-1008 on the existence of neural networks of Caianiello of size 2n2 and memory length k which describes a cycle of length k times 2nk.",Technology
Enhancing LLMs for Governance with Human Oversight: Evaluating and Aligning LLMs on Expert Classification of Climate Misinformation for Detecting False or Misleading Claims about Climate Change,"Climate misinformation is a problem that has the potential to be substantially aggravated by the development of Large Language Models (LLMs). In this study we evaluate the potential for LLMs to be part of the solution for mitigating online dismisinformation rather than the problem. Employing a public expert annotated dataset and a curated sample of social media content we evaluate the performance of proprietary vs. open source LLMs on climate misinformation classification task, comparing them to existing climate-focused computer-assisted tools and expert assessments. Results show (1) open-source models substantially under-perform in classifying climate misinformation compared to proprietary models, (2) existing climate-focused computer-assisted tools leveraging expert-annotated datasets continues to outperform many of proprietary models, including GPT-4o, and (3) demonstrate the efficacy and generalizability of fine-tuning GPT-3.5-turbo on expert annotated dataset in classifying claims about climate change at the equivalency of climate change experts with over 20 years of experience in climate communication. These findings highlight 1) the importance of incorporating human-oversight, such as incorporating expert-annotated datasets in training LLMs, for governance tasks that require subject-matter expertise like classifying climate misinformation, and 2) the potential for LLMs in facilitating civil society organizations to engage in various governance tasks such as classifying false or misleading claims in domains beyond climate change such as politics and health science.","Enhancing LLMs for Governance with Human Oversight: Evaluating and Aligning LLMs on Expert Classification of Climate Misinformation for Detecting False or Misleading Claims about Climate Change Climate misinformation is a problem that has the potential to be substantially aggravated by the development of Large Language Models (LLMs). In this study we evaluate the potential for LLMs to be part of the solution for mitigating online dismisinformation rather than the problem. Employing a public expert annotated dataset and a curated sample of social media content we evaluate the performance of proprietary vs. open source LLMs on climate misinformation classification task, comparing them to existing climate-focused computer-assisted tools and expert assessments. Results show (1) open-source models substantially under-perform in classifying climate misinformation compared to proprietary models, (2) existing climate-focused computer-assisted tools leveraging expert-annotated datasets continues to outperform many of proprietary models, including GPT-4o, and (3) demonstrate the efficacy and generalizability of fine-tuning GPT-3.5-turbo on expert annotated dataset in classifying claims about climate change at the equivalency of climate change experts with over 20 years of experience in climate communication. These findings highlight 1) the importance of incorporating human-oversight, such as incorporating expert-annotated datasets in training LLMs, for governance tasks that require subject-matter expertise like classifying climate misinformation, and 2) the potential for LLMs in facilitating civil society organizations to engage in various governance tasks such as classifying false or misleading claims in domains beyond climate change such as politics and health science.",Environment
Maximum likelihood estimation of phylogenetic tree and substitution rates via generalized neighbor-joining and the EM algorithm,"A central task in the study of molecular sequence data from present-day species is the reconstruction of the ancestral relationships. The most established approach to tree reconstruction is the maximum likelihood (ML) method. In this method, evolution is described in terms of a discrete-state continuous-time Markov process on a phylogenetic tree. The substitution rate matrix, that determines the Markov process, can be estimated using the expectation maximization (EM) algorithm. Unfortunately, an exhaustive search for the ML phylogenetic tree is computationally prohibitive for large data sets. In such situations, the neighbor-joining (NJ) method is frequently used because of its computational speed. The NJ method reconstructs trees by clustering neighboring sequences recursively, based on pairwise comparisons between the sequences. The NJ method can be generalized such that reconstruction is based on comparisons of subtrees rather than pairwise distances. In this paper, we present an algorithm for simultaneous substitution rate estimation and phylogenetic tree reconstruction. The algorithm iterates between the EM algorithm for estimating substitution rates and the generalized NJ method for tree reconstruction. Preliminary results of the approach are encouraging.","Maximum likelihood estimation of phylogenetic tree and substitution rates via generalized neighbor-joining and the EM algorithm A central task in the study of molecular sequence data from present-day species is the reconstruction of the ancestral relationships. The most established approach to tree reconstruction is the maximum likelihood (ML) method. In this method, evolution is described in terms of a discrete-state continuous-time Markov process on a phylogenetic tree. The substitution rate matrix, that determines the Markov process, can be estimated using the expectation maximization (EM) algorithm. Unfortunately, an exhaustive search for the ML phylogenetic tree is computationally prohibitive for large data sets. In such situations, the neighbor-joining (NJ) method is frequently used because of its computational speed. The NJ method reconstructs trees by clustering neighboring sequences recursively, based on pairwise comparisons between the sequences. The NJ method can be generalized such that reconstruction is based on comparisons of subtrees rather than pairwise distances. In this paper, we present an algorithm for simultaneous substitution rate estimation and phylogenetic tree reconstruction. The algorithm iterates between the EM algorithm for estimating substitution rates and the generalized NJ method for tree reconstruction. Preliminary results of the approach are encouraging.",Healthcare
Navigating the Paradox: Challenges and Strategies of University Students Managing Mental Health Medication in Real-World Practices,"Mental health has become a growing concern among university students. While medication is a common treatment, understanding how university students manage their medication for mental health symptoms in real-world practice has not been fully explored. In this study, we conducted semi-structured interviews with university students to understand the unique challenges in the mental health medication management process and their coping strategies, particularly examining the role of various technologies in this process. We discovered that due to struggles with self-acceptance and the interdependent relationship between medication, symptoms, schedules, and life changes, the medication management process for students was a highly dynamic journey involving frequent dosage changes. Thus, students adopted flexible strategies of using minimal technology to manage their medication in different situations while maintaining a high degree of autonomy. Based on our findings, we propose design implications for future technologies to seamlessly integrate into their daily lives and assist students in managing their mental health medications.","Navigating the Paradox: Challenges and Strategies of University Students Managing Mental Health Medication in Real-World Practices Mental health has become a growing concern among university students. While medication is a common treatment, understanding how university students manage their medication for mental health symptoms in real-world practice has not been fully explored. In this study, we conducted semi-structured interviews with university students to understand the unique challenges in the mental health medication management process and their coping strategies, particularly examining the role of various technologies in this process. We discovered that due to struggles with self-acceptance and the interdependent relationship between medication, symptoms, schedules, and life changes, the medication management process for students was a highly dynamic journey involving frequent dosage changes. Thus, students adopted flexible strategies of using minimal technology to manage their medication in different situations while maintaining a high degree of autonomy. Based on our findings, we propose design implications for future technologies to seamlessly integrate into their daily lives and assist students in managing their mental health medications.",Healthcare
Social Media Usage in Kuwait: A Comparison of Perspectives Between Healthcare Practitioners and Patients,"Social Media has been transforming numerous activities of everyday life, impacting also healthcare. However, few studies investigate the medical use of social media by patients and medical practitioners, especially in the Arabian Gulf region and Kuwait. To understand the behavior of patients and medical practitioners in social media toward healthcare and medical purposes, we conducted user studies. Through an online survey, we identified a decrease in patients and medical practitioners use of social media for medical purposes. Patients reported to be more aware than practitioners concerning: health education, health-related network support, and communication activities. While practitioners use social media mostly as a source of medical information, for clinician marketing and for professional development. The findings highlighted the need to design a social media platform that support healthcare online campaign, professional career identity, medical repository, and social privacy setting to increase users engagements toward medical purposes.","Social Media Usage in Kuwait: A Comparison of Perspectives Between Healthcare Practitioners and Patients Social Media has been transforming numerous activities of everyday life, impacting also healthcare. However, few studies investigate the medical use of social media by patients and medical practitioners, especially in the Arabian Gulf region and Kuwait. To understand the behavior of patients and medical practitioners in social media toward healthcare and medical purposes, we conducted user studies. Through an online survey, we identified a decrease in patients and medical practitioners use of social media for medical purposes. Patients reported to be more aware than practitioners concerning: health education, health-related network support, and communication activities. While practitioners use social media mostly as a source of medical information, for clinician marketing and for professional development. The findings highlighted the need to design a social media platform that support healthcare online campaign, professional career identity, medical repository, and social privacy setting to increase users engagements toward medical purposes.",Healthcare
The entry and exit game in the electricity markets: a mean-field game approach,"We develop a model for the industry dynamics in the electricity market, based on mean-field games of optimal stopping. In our model, there are two types of agents: the renewable producers and the conventional producers. The renewable producers choose the optimal moment to build new renewable plants, and the conventional producers choose the optimal moment to exit the market. The agents interact through the market price, determined by matching the aggregate supply of the two types of producers with an exogenous demand function. Using a relaxed formulation of optimal stopping mean-field games, we prove the existence of a Nash equilibrium and the uniqueness of the equilibrium price process. An empirical example, inspired by the UK electricity market is presented. The example shows that while renewable subsidies clearly lead to higher renewable penetration, this may entail a cost to the consumer in terms of higher peakload prices. In order to avoid rising prices, the renewable subsidies must be combined with mechanisms ensuring that sufficient conventional capacity remains in place to meet the energy demand during peak periods.","The entry and exit game in the electricity markets: a mean-field game approach We develop a model for the industry dynamics in the electricity market, based on mean-field games of optimal stopping. In our model, there are two types of agents: the renewable producers and the conventional producers. The renewable producers choose the optimal moment to build new renewable plants, and the conventional producers choose the optimal moment to exit the market. The agents interact through the market price, determined by matching the aggregate supply of the two types of producers with an exogenous demand function. Using a relaxed formulation of optimal stopping mean-field games, we prove the existence of a Nash equilibrium and the uniqueness of the equilibrium price process. An empirical example, inspired by the UK electricity market is presented. The example shows that while renewable subsidies clearly lead to higher renewable penetration, this may entail a cost to the consumer in terms of higher peakload prices. In order to avoid rising prices, the renewable subsidies must be combined with mechanisms ensuring that sufficient conventional capacity remains in place to meet the energy demand during peak periods.",Environment
Characterizing health informatics journals by subject-level dependencies: a citation network analysis,"Citation network analysis has become one of methods to study how scientific knowledge flows from one domain to another. Health informatics is a multidisciplinary field that includes social science, software engineering, behavioral science, medical science and others. In this study, we perform an analysis of citation statistics from health informatics journals using data set extracted from CrossRef. For each health informatics journal, we extract the number of citations fromto studies related to computer science, medicineclinical medicine and other fields, including the number of self-citations from the health informatics journal. With a similar number of articles used in our analysis, we show that the Journal of the American Medical Informatics Association (JAMIA) has more in-citations than the Journal of Medical Internet Research (JMIR); while JMIR has a higher number of out-citations and self-citations. We also show that JMIR cites more articles from health informatics journals and medicine related journals. In addition, the Journal of Medical Systems (JMS) cites more articles from computer science journals compared with other health informatics journals included in our analysis.","Characterizing health informatics journals by subject-level dependencies: a citation network analysis Citation network analysis has become one of methods to study how scientific knowledge flows from one domain to another. Health informatics is a multidisciplinary field that includes social science, software engineering, behavioral science, medical science and others. In this study, we perform an analysis of citation statistics from health informatics journals using data set extracted from CrossRef. For each health informatics journal, we extract the number of citations fromto studies related to computer science, medicineclinical medicine and other fields, including the number of self-citations from the health informatics journal. With a similar number of articles used in our analysis, we show that the Journal of the American Medical Informatics Association (JAMIA) has more in-citations than the Journal of Medical Internet Research (JMIR); while JMIR has a higher number of out-citations and self-citations. We also show that JMIR cites more articles from health informatics journals and medicine related journals. In addition, the Journal of Medical Systems (JMS) cites more articles from computer science journals compared with other health informatics journals included in our analysis.",Healthcare
"Multiplicative noise, fast convolution, and pricing","In this work we detail the application of a fast convolution algorithm computing high dimensional integrals to the context of multiplicative noise stochastic processes. The algorithm provides a numerical solution to the problem of characterizing conditional probability density functions at arbitrary time, and we applied it successfully to quadratic and piecewise linear diffusion processes. The ability in reproducing statistical features of financial return time series, such as thickness of the tails and scaling properties, makes this processes appealing for option pricing. Since exact analytical results are missing, we exploit the fast convolution as a numerical method alternative to the Monte Carlo simulation both in objective and risk neutral settings. In numerical sections we document how fast convolution outperforms Monte Carlo both in velocity and efficiency terms.","Multiplicative noise, fast convolution, and pricing In this work we detail the application of a fast convolution algorithm computing high dimensional integrals to the context of multiplicative noise stochastic processes. The algorithm provides a numerical solution to the problem of characterizing conditional probability density functions at arbitrary time, and we applied it successfully to quadratic and piecewise linear diffusion processes. The ability in reproducing statistical features of financial return time series, such as thickness of the tails and scaling properties, makes this processes appealing for option pricing. Since exact analytical results are missing, we exploit the fast convolution as a numerical method alternative to the Monte Carlo simulation both in objective and risk neutral settings. In numerical sections we document how fast convolution outperforms Monte Carlo both in velocity and efficiency terms.",Finance
CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims,"We introduce CLIMATE-FEVER, a new publicly available dataset for verification of climate change-related claims. By providing a dataset for the research community, we aim to facilitate and encourage work on improving algorithms for retrieving evidential support for climate-specific claims, addressing the underlying language understanding challenges, and ultimately help alleviate the impact of misinformation on climate change. We adapt the methodology of FEVER 1, the largest dataset of artificially designed claims, to real-life claims collected from the Internet. While during this process, we could rely on the expertise of renowned climate scientists, it turned out to be no easy task. We discuss the surprising, subtle complexity of modeling real-world climate-related claims within the textscfever framework, which we believe provides a valuable challenge for general natural language understanding. We hope that our work will mark the beginning of a new exciting long-term joint effort by the climate science and AI community.","CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims We introduce CLIMATE-FEVER, a new publicly available dataset for verification of climate change-related claims. By providing a dataset for the research community, we aim to facilitate and encourage work on improving algorithms for retrieving evidential support for climate-specific claims, addressing the underlying language understanding challenges, and ultimately help alleviate the impact of misinformation on climate change. We adapt the methodology of FEVER 1, the largest dataset of artificially designed claims, to real-life claims collected from the Internet. While during this process, we could rely on the expertise of renowned climate scientists, it turned out to be no easy task. We discuss the surprising, subtle complexity of modeling real-world climate-related claims within the textscfever framework, which we believe provides a valuable challenge for general natural language understanding. We hope that our work will mark the beginning of a new exciting long-term joint effort by the climate science and AI community.",Environment
Invest or Exit? Optimal Decisions in the Face of a Declining Profit Stream,"Even in the face of deteriorating and highly volatile demand, firms often invest in, rather than discard, aging technologies. In order to study this phenomenon, we model the firms profit stream as a Brownian motion with negative drift. At each point in time, the firm can continue operations, or it can stop and exit the project. In addition, there is a one-time option to make an investment which boosts the projects profit rate. Using stochastic analysis, we show that the optimal policy always exists and that it is characterized by three thresholds. There are investment and exit thresholds before investment, and there is a threshold for exit after investment. We also effect a comparative statics analysis of the thresholds with respect to the drift and the volatility of the Brownian motion. When the profit boost upon investment is sufficiently large, we find a novel result: the investment threshold decreases in volatility.","Invest or Exit? Optimal Decisions in the Face of a Declining Profit Stream Even in the face of deteriorating and highly volatile demand, firms often invest in, rather than discard, aging technologies. In order to study this phenomenon, we model the firms profit stream as a Brownian motion with negative drift. At each point in time, the firm can continue operations, or it can stop and exit the project. In addition, there is a one-time option to make an investment which boosts the projects profit rate. Using stochastic analysis, we show that the optimal policy always exists and that it is characterized by three thresholds. There are investment and exit thresholds before investment, and there is a threshold for exit after investment. We also effect a comparative statics analysis of the thresholds with respect to the drift and the volatility of the Brownian motion. When the profit boost upon investment is sufficiently large, we find a novel result: the investment threshold decreases in volatility.",Finance
Generalized simulation model of teaching and its research on PC,"One of the important problems of cyber pedagogy is the following: how, knowing the parameters of the student, his initial level of knowledge and the impact of the teacher to predict knowledge of student at subsequent times. Simulation method allows you to create a computer program that simulates the behavior of the teacher-student-system and investigate the influence of system parameters on the results of learning.","Generalized simulation model of teaching and its research on PC One of the important problems of cyber pedagogy is the following: how, knowing the parameters of the student, his initial level of knowledge and the impact of the teacher to predict knowledge of student at subsequent times. Simulation method allows you to create a computer program that simulates the behavior of the teacher-student-system and investigate the influence of system parameters on the results of learning.",Education
Modeling systemic risks in financial markets,We survey systemic risks to financial markets and present a high-level description of an algorithm that measures systemic risk in terms of coupled networks.,Modeling systemic risks in financial markets We survey systemic risks to financial markets and present a high-level description of an algorithm that measures systemic risk in terms of coupled networks.,Finance
PALM: PAnoramic Learning Map Integrating Learning Analytics and Curriculum Map for Scalable Insights Across Courses,"This study proposes and evaluates the PAnoramic Learning Map (PALM), a learning analytics (LA) dashboard designed to address the scalability challenges of LA by integrating curriculum-level information. Traditional LA research has predominantly focused on individual courses or learners and often lacks a framework that considers the relationships between courses and the long-term trajectory of learning. To bridge this gap, PALM was developed to integrate multilayered educational data into a curriculum map, enabling learners to intuitively understand their learning records and academic progression. We conducted a system evaluation to assess PALMs effectiveness in two key areas: (1) its impact on students awareness of their learning behaviors, and (2) its comparative performance against existing systems. The results indicate that PALM enhances learners awareness of study planning and reflection, particularly by improving perceived behavioral control through the visual presentation of individual learning histories and statistical trends, which clarify the links between learning actions and outcomes. Although PALM requires ongoing refinement as a system, it received significantly higher evaluations than existing systems in terms of visual appeal and usability. By serving as an information resource with previously inaccessible insights, PALM enhances self-regulated learning and engagement, representing a significant step beyond conventional LA toward a comprehensive and scalable approach.","PALM: PAnoramic Learning Map Integrating Learning Analytics and Curriculum Map for Scalable Insights Across Courses This study proposes and evaluates the PAnoramic Learning Map (PALM), a learning analytics (LA) dashboard designed to address the scalability challenges of LA by integrating curriculum-level information. Traditional LA research has predominantly focused on individual courses or learners and often lacks a framework that considers the relationships between courses and the long-term trajectory of learning. To bridge this gap, PALM was developed to integrate multilayered educational data into a curriculum map, enabling learners to intuitively understand their learning records and academic progression. We conducted a system evaluation to assess PALMs effectiveness in two key areas: (1) its impact on students awareness of their learning behaviors, and (2) its comparative performance against existing systems. The results indicate that PALM enhances learners awareness of study planning and reflection, particularly by improving perceived behavioral control through the visual presentation of individual learning histories and statistical trends, which clarify the links between learning actions and outcomes. Although PALM requires ongoing refinement as a system, it received significantly higher evaluations than existing systems in terms of visual appeal and usability. By serving as an information resource with previously inaccessible insights, PALM enhances self-regulated learning and engagement, representing a significant step beyond conventional LA toward a comprehensive and scalable approach.",Education
Symbolic Reachability Analysis of Genetic Regulatory Networks using Qualitative Abstractions,"The switch-like character of gene regulation has motivated the use of hybrid, discrete-continuous models of genetic regulatory networks. While powerful techniques for the analysis, verification, and control of hybrid systems have been developed, the specificities of the biological application domain pose a number of challenges, notably the absence of quantitative information on parameter values and the size and complexity of networks of biological interest. We introduce a method for the analysis of reachability properties of genetic regulatory networks that is based on a class of discontinuous piecewise-affine (PA) differential equations well-adapted to the above constraints. More specifically, we introduce a hyperrectangular partition of the state space that forms the basis for a discrete abstraction preserving the sign of the derivatives of the state variables. The resulting discrete transition system provides a conservative approximation of the qualitative dynamics of the network and can be efficiently computed in a symbolic manner from inequality constraints on the parameters. The method has been implemented in the computer tool Genetic Network Analyzer (GNA), which has been applied to the analysis of a regulatory system whose functioning is not well-understood by biologists, the nutritional stress response in the bacterium Escherichia coli.","Symbolic Reachability Analysis of Genetic Regulatory Networks using Qualitative Abstractions The switch-like character of gene regulation has motivated the use of hybrid, discrete-continuous models of genetic regulatory networks. While powerful techniques for the analysis, verification, and control of hybrid systems have been developed, the specificities of the biological application domain pose a number of challenges, notably the absence of quantitative information on parameter values and the size and complexity of networks of biological interest. We introduce a method for the analysis of reachability properties of genetic regulatory networks that is based on a class of discontinuous piecewise-affine (PA) differential equations well-adapted to the above constraints. More specifically, we introduce a hyperrectangular partition of the state space that forms the basis for a discrete abstraction preserving the sign of the derivatives of the state variables. The resulting discrete transition system provides a conservative approximation of the qualitative dynamics of the network and can be efficiently computed in a symbolic manner from inequality constraints on the parameters. The method has been implemented in the computer tool Genetic Network Analyzer (GNA), which has been applied to the analysis of a regulatory system whose functioning is not well-understood by biologists, the nutritional stress response in the bacterium Escherichia coli.",Healthcare
SAFER: A Calibrated Risk-Aware Multimodal Recommendation Model for Dynamic Treatment Regimes,"Dynamic treatment regimes (DTRs) are critical to precision medicine, optimizing long-term outcomes through personalized, real-time decision-making in evolving clinical contexts, but require careful supervision for unsafe treatment risks. Existing efforts rely primarily on clinician-prescribed gold standards despite the absence of a known optimal strategy, and predominantly using structured EHR data without extracting valuable insights from clinical notes, limiting their reliability for treatment recommendations. In this work, we introduce SAFER, a calibrated risk-aware tabular-language recommendation framework for DTR that integrates both structured EHR and clinical notes, enabling them to learn from each other, and addresses inherent label uncertainty by assuming ambiguous optimal treatment solution for deceased patients. Moreover, SAFER employs conformal prediction to provide statistical guarantees, ensuring safe treatment recommendations while filtering out uncertain predictions. Experiments on two publicly available sepsis datasets demonstrate that SAFER outperforms state-of-the-art baselines across multiple recommendation metrics and counterfactual mortality rate, while offering robust formal assurances. These findings underscore SAFER potential as a trustworthy and theoretically grounded solution for high-stakes DTR applications.","SAFER: A Calibrated Risk-Aware Multimodal Recommendation Model for Dynamic Treatment Regimes Dynamic treatment regimes (DTRs) are critical to precision medicine, optimizing long-term outcomes through personalized, real-time decision-making in evolving clinical contexts, but require careful supervision for unsafe treatment risks. Existing efforts rely primarily on clinician-prescribed gold standards despite the absence of a known optimal strategy, and predominantly using structured EHR data without extracting valuable insights from clinical notes, limiting their reliability for treatment recommendations. In this work, we introduce SAFER, a calibrated risk-aware tabular-language recommendation framework for DTR that integrates both structured EHR and clinical notes, enabling them to learn from each other, and addresses inherent label uncertainty by assuming ambiguous optimal treatment solution for deceased patients. Moreover, SAFER employs conformal prediction to provide statistical guarantees, ensuring safe treatment recommendations while filtering out uncertain predictions. Experiments on two publicly available sepsis datasets demonstrate that SAFER outperforms state-of-the-art baselines across multiple recommendation metrics and counterfactual mortality rate, while offering robust formal assurances. These findings underscore SAFER potential as a trustworthy and theoretically grounded solution for high-stakes DTR applications.",Healthcare
On Modeling Economic Default Time: A Reduced-Form Model Approach,"In the aftermath of the global financial crisis, much attention has been paid to investigating the appropriateness of the current practice of default risk modeling in banking, finance and insurance industries. A recent empirical study by Guo et al.(2008) shows that the time difference between the economic and recorded default dates has a significant impact on recovery rate estimates. Guo et al.(2011) develop a theoretical structural firm asset value model for a firm default process that embeds the distinction of these two default times. To be more consistent with the practice, in this paper, we assume the market participants cannot observe the firm asset value directly and developed a reduced-form model to characterize the economic and recorded default times. We derive the probability distribution of these two default times. The numerical study on the difference between these two shows that our proposed model can both capture the features and fit the empirical data.","On Modeling Economic Default Time: A Reduced-Form Model Approach In the aftermath of the global financial crisis, much attention has been paid to investigating the appropriateness of the current practice of default risk modeling in banking, finance and insurance industries. A recent empirical study by Guo et al.(2008) shows that the time difference between the economic and recorded default dates has a significant impact on recovery rate estimates. Guo et al.(2011) develop a theoretical structural firm asset value model for a firm default process that embeds the distinction of these two default times. To be more consistent with the practice, in this paper, we assume the market participants cannot observe the firm asset value directly and developed a reduced-form model to characterize the economic and recorded default times. We derive the probability distribution of these two default times. The numerical study on the difference between these two shows that our proposed model can both capture the features and fit the empirical data.",Finance
Diagnostic performance of radiologists with and without different CAD systems for mammography,"The purpose of this study is the evaluation of the variation of performance in terms of sensitivity and specificity of two radiologists with different experience in mammography, with and without the assistance of two different CAD systems. The CAD considered are SecondLookTM (CADx Medical Systems, Canada), and CALMA (Computer Assisted Library in MAmmography). The first is a commercial system, the other is the result of a a research project, supported by INFN (Istituto Nazionale di Fisica Nucleare, Italy); their characteristics have been already reported in literature. To compare the results with and without these tools, a dataset composed by 70 images of patients with cancer (biopsy proven) and 120 images of healthy breasts (with a three years follow up) has been collected. All the images have been digitized and analysed by two CAD, then two radiologists with respectively 6 and 2 years of experience in mammography indipendently made their diagnosis without and with, the support of the two CAD systems. In this work sensitivity and specificity variation, the Az area under the ROC curve, are reported. The results show that the use of a CAD allows for a substantial increment in sensitivity and a less pronounced decrement in specificity. The extent of these effects depends on the experience of the readers and is comparable for the two CAD considered.","Diagnostic performance of radiologists with and without different CAD systems for mammography The purpose of this study is the evaluation of the variation of performance in terms of sensitivity and specificity of two radiologists with different experience in mammography, with and without the assistance of two different CAD systems. The CAD considered are SecondLookTM (CADx Medical Systems, Canada), and CALMA (Computer Assisted Library in MAmmography). The first is a commercial system, the other is the result of a a research project, supported by INFN (Istituto Nazionale di Fisica Nucleare, Italy); their characteristics have been already reported in literature. To compare the results with and without these tools, a dataset composed by 70 images of patients with cancer (biopsy proven) and 120 images of healthy breasts (with a three years follow up) has been collected. All the images have been digitized and analysed by two CAD, then two radiologists with respectively 6 and 2 years of experience in mammography indipendently made their diagnosis without and with, the support of the two CAD systems. In this work sensitivity and specificity variation, the Az area under the ROC curve, are reported. The results show that the use of a CAD allows for a substantial increment in sensitivity and a less pronounced decrement in specificity. The extent of these effects depends on the experience of the readers and is comparable for the two CAD considered.",Healthcare
Of Access and Inclusivity Digital Divide in Online Education,Can online education enable all students to participate in and benefit from it equally? Massive online education without addressing the huge access gap and disparities in digital infrastructure would not only exclude a vast majority of students from learning opportunities but also exacerbate the existing socio-economic disparities in educational opportunities.,Of Access and Inclusivity Digital Divide in Online Education Can online education enable all students to participate in and benefit from it equally? Massive online education without addressing the huge access gap and disparities in digital infrastructure would not only exclude a vast majority of students from learning opportunities but also exacerbate the existing socio-economic disparities in educational opportunities.,Education
Climate Change Conspiracy Theories on Social Media,"One of the critical emerging challenges in climate change communication is the prevalence of conspiracy theories. This paper discusses some of the major conspiracy theories related to climate change found in a large Twitter corpus. We use a state-of-the-art stance detection method to find whether conspiracy theories are more popular among Disbelievers or Believers of climate change. We then analyze which conspiracy theory is more popular than the others and how popularity changes with climate change belief. We find that Disbelievers of climate change are overwhelmingly responsible for sharing messages with conspiracy theory-related keywords, and not all conspiracy theories are equally shared. Lastly, we discuss the implications of our findings for climate change communication.","Climate Change Conspiracy Theories on Social Media One of the critical emerging challenges in climate change communication is the prevalence of conspiracy theories. This paper discusses some of the major conspiracy theories related to climate change found in a large Twitter corpus. We use a state-of-the-art stance detection method to find whether conspiracy theories are more popular among Disbelievers or Believers of climate change. We then analyze which conspiracy theory is more popular than the others and how popularity changes with climate change belief. We find that Disbelievers of climate change are overwhelmingly responsible for sharing messages with conspiracy theory-related keywords, and not all conspiracy theories are equally shared. Lastly, we discuss the implications of our findings for climate change communication.",Environment
The Medical Deconfounder: Assessing Treatment Effects with Electronic Health Records,"The treatment effects of medications play a key role in guiding medical prescriptions. They are usually assessed with randomized controlled trials (RCTs), which are expensive. Recently, large-scale electronic health records (EHRs) have become available, opening up new opportunities for more cost-effective assessments. However, assessing a treatment effect from EHRs is challenging: it is biased by unobserved confounders, unmeasured variables that affect both patients medical prescription and their outcome, e.g. the patients social economic status. To adjust for unobserved confounders, we develop the medical deconfounder, a machine learning algorithm that unbiasedly estimates treatment effects from EHRs. The medical deconfounder first constructs a substitute confounder by modeling which medications were prescribed to each patient; this substitute confounder is guaranteed to capture all multi-medication confounders, observed or unobserved (arXiv:1805.06826). It then uses this substitute confounder to adjust for the confounding bias in the analysis. We validate the medical deconfounder on two simulated and two real medical data sets. Compared to classical approaches, the medical deconfounder produces closer-to-truth treatment effect estimates; it also identifies effective medications that are more consistent with the findings in the medical literature.","The Medical Deconfounder: Assessing Treatment Effects with Electronic Health Records The treatment effects of medications play a key role in guiding medical prescriptions. They are usually assessed with randomized controlled trials (RCTs), which are expensive. Recently, large-scale electronic health records (EHRs) have become available, opening up new opportunities for more cost-effective assessments. However, assessing a treatment effect from EHRs is challenging: it is biased by unobserved confounders, unmeasured variables that affect both patients medical prescription and their outcome, e.g. the patients social economic status. To adjust for unobserved confounders, we develop the medical deconfounder, a machine learning algorithm that unbiasedly estimates treatment effects from EHRs. The medical deconfounder first constructs a substitute confounder by modeling which medications were prescribed to each patient; this substitute confounder is guaranteed to capture all multi-medication confounders, observed or unobserved (arXiv:1805.06826). It then uses this substitute confounder to adjust for the confounding bias in the analysis. We validate the medical deconfounder on two simulated and two real medical data sets. Compared to classical approaches, the medical deconfounder produces closer-to-truth treatment effect estimates; it also identifies effective medications that are more consistent with the findings in the medical literature.",Healthcare
OPUS: An Efficient Admissible Algorithm for Unordered Search,"OPUS is a branch and bound search algorithm that enables efficient admissible search through spaces for which the order of search operator application is not significant. The algorithms search efficiency is demonstrated with respect to very large machine learning search spaces. The use of admissible search is of potential value to the machine learning community as it means that the exact learning biases to be employed for complex learning tasks can be precisely specified and manipulated. OPUS also has potential for application in other areas of artificial intelligence, notably, truth maintenance.","OPUS: An Efficient Admissible Algorithm for Unordered Search OPUS is a branch and bound search algorithm that enables efficient admissible search through spaces for which the order of search operator application is not significant. The algorithms search efficiency is demonstrated with respect to very large machine learning search spaces. The use of admissible search is of potential value to the machine learning community as it means that the exact learning biases to be employed for complex learning tasks can be precisely specified and manipulated. OPUS also has potential for application in other areas of artificial intelligence, notably, truth maintenance.",Technology
FLECS: Planning with a Flexible Commitment Strategy,"There has been evidence that least-commitment planners can efficiently handle planning problems that involve difficult goal interactions. This evidence has led to the common belief that delayed-commitment is the best possible planning strategy. However, we recently found evidence that eager-commitment planners can handle a variety of planning problems more efficiently, in particular those with difficult operator choices. Resigned to the futility of trying to find a universally successful planning strategy, we devised a planner that can be used to study which domains and problems are best for which planning strategies. In this article we introduce this new planning algorithm, FLECS, which uses a FLExible Commitment Strategy with respect to plan-step orderings. It is able to use any strategy from delayed-commitment to eager-commitment. The combination of delayed and eager operator-ordering commitments allows FLECS to take advantage of the benefits of explicitly using a simulated execution state and reasoning about planning constraints. FLECS can vary its commitment strategy across different problems and domains, and also during the course of a single planning problem. FLECS represents a novel contribution to planning in that it explicitly provides the choice of which commitment strategy to use while planning. FLECS provides a framework to investigate the mapping from planning domains and problems to efficient planning strategies.","FLECS: Planning with a Flexible Commitment Strategy There has been evidence that least-commitment planners can efficiently handle planning problems that involve difficult goal interactions. This evidence has led to the common belief that delayed-commitment is the best possible planning strategy. However, we recently found evidence that eager-commitment planners can handle a variety of planning problems more efficiently, in particular those with difficult operator choices. Resigned to the futility of trying to find a universally successful planning strategy, we devised a planner that can be used to study which domains and problems are best for which planning strategies. In this article we introduce this new planning algorithm, FLECS, which uses a FLExible Commitment Strategy with respect to plan-step orderings. It is able to use any strategy from delayed-commitment to eager-commitment. The combination of delayed and eager operator-ordering commitments allows FLECS to take advantage of the benefits of explicitly using a simulated execution state and reasoning about planning constraints. FLECS can vary its commitment strategy across different problems and domains, and also during the course of a single planning problem. FLECS represents a novel contribution to planning in that it explicitly provides the choice of which commitment strategy to use while planning. FLECS provides a framework to investigate the mapping from planning domains and problems to efficient planning strategies.",Technology
Scuba Search : when selection meets innovation,"We proposed a new search heuristic using the scuba diving metaphor. This approach is based on the concept of evolvability and tends to exploit neutrality in fitness landscape. Despite the fact that natural evolution does not directly select for evolvability, the basic idea behind the scuba search heuristic is to explicitly push the evolvability to increase. The search process switches between two phases: Conquest-of-the-Waters and Invasion-of-the-Land. A comparative study of the new algorithm and standard local search heuristics on the NKq-landscapes has shown advantage and limit of the scuba search. To enlighten qualitative differences between neutral search processes, the space is changed into a connected graph to visualize the pathways that the search is likely to follow.","Scuba Search : when selection meets innovation We proposed a new search heuristic using the scuba diving metaphor. This approach is based on the concept of evolvability and tends to exploit neutrality in fitness landscape. Despite the fact that natural evolution does not directly select for evolvability, the basic idea behind the scuba search heuristic is to explicitly push the evolvability to increase. The search process switches between two phases: Conquest-of-the-Waters and Invasion-of-the-Land. A comparative study of the new algorithm and standard local search heuristics on the NKq-landscapes has shown advantage and limit of the scuba search. To enlighten qualitative differences between neutral search processes, the space is changed into a connected graph to visualize the pathways that the search is likely to follow.",Technology
The survival-incorporated median versus the median in the survivors or in the always-survivors: What are we measuring? And why?,"Many clinical studies evaluate the benefit of a treatment based on both survival and other continuousordinal clinical outcomes, such as Quality of Life scores. In these studies, when subjects die before the follow-up assessment, the clinical outcomes become undefined and are truncated by death. Treating outcomes as missing or censored due to death can be misleading for treatment effect evaluation. We show that if we use the median in the survivors or in the always-survivors as estimands to summarize clinical outcomes, we may conclude that a trade-off exists between the probability of survival and good clinical outcomes, even in settings where both the probability of survival and the probability of any good clinical outcome are better for one treatment. Therefore, we advocate not always treating death as a mechanism through which clinical outcomes are missing, but rather as part of the outcome measure. To account for the survival status, we describe the survival-incorporated median as an alternative summary measure for outcomes in the presence of death. The survival-incorporated median is the threshold such that 50 of the population is alive with an outcome above that threshold. Through conceptual examples and an application to a prostate cancer treatment study, we show that the survival-incorporated median provides a simple and useful summary measure to inform clinical practice.","The survival-incorporated median versus the median in the survivors or in the always-survivors: What are we measuring? And why? Many clinical studies evaluate the benefit of a treatment based on both survival and other continuousordinal clinical outcomes, such as Quality of Life scores. In these studies, when subjects die before the follow-up assessment, the clinical outcomes become undefined and are truncated by death. Treating outcomes as missing or censored due to death can be misleading for treatment effect evaluation. We show that if we use the median in the survivors or in the always-survivors as estimands to summarize clinical outcomes, we may conclude that a trade-off exists between the probability of survival and good clinical outcomes, even in settings where both the probability of survival and the probability of any good clinical outcome are better for one treatment. Therefore, we advocate not always treating death as a mechanism through which clinical outcomes are missing, but rather as part of the outcome measure. To account for the survival status, we describe the survival-incorporated median as an alternative summary measure for outcomes in the presence of death. The survival-incorporated median is the threshold such that 50 of the population is alive with an outcome above that threshold. Through conceptual examples and an application to a prostate cancer treatment study, we show that the survival-incorporated median provides a simple and useful summary measure to inform clinical practice.",Healthcare
Least squares fitting of circles and lines,We study theoretical and computational aspects of the least squares fit (LSF) of circles and circular arcs. First we discuss the existence and uniqueness of LSF and various parametrization schemes. Then we evaluate several popular circle fitting algorithms and propose a new one that surpasses the existing methods in reliability. We also discuss and compare direct (algebraic) circle fits.,Least squares fitting of circles and lines We study theoretical and computational aspects of the least squares fit (LSF) of circles and circular arcs. First we discuss the existence and uniqueness of LSF and various parametrization schemes. Then we evaluate several popular circle fitting algorithms and propose a new one that surpasses the existing methods in reliability. We also discuss and compare direct (algebraic) circle fits.,Technology
Development of an elementary climate model: two-layer cellular case,"A qualitative understanding of the greenhouse effect has long been available through models based on globally- and time-averaged quantities. We examine here a simple 864-cell climatological model that emphasizes vertical radiative energy transport within each cell. It reproduces yearly average temperatures obtained earlier from one of these global models and predicts a locally distributed non-radiative flux when observed temperatures are employed as input data. Vertical and lateral transport of latent heat do not appear explicitly in this model. They are apparently handled well by one non-radiative flux variable, SNR, which shows a strong latitude dependence. Only the Sahara desert and Saudi Arabian regions appear to be complex. For those interested in climatology and construction of climate models, our model provides constraints upon specifying averaged non-radiative energy transport. The model is a useful simplification for learning about radiative energy transfer into and out of Earth?s atmosphere and for representing the results of more sophisticated models.","Development of an elementary climate model: two-layer cellular case A qualitative understanding of the greenhouse effect has long been available through models based on globally- and time-averaged quantities. We examine here a simple 864-cell climatological model that emphasizes vertical radiative energy transport within each cell. It reproduces yearly average temperatures obtained earlier from one of these global models and predicts a locally distributed non-radiative flux when observed temperatures are employed as input data. Vertical and lateral transport of latent heat do not appear explicitly in this model. They are apparently handled well by one non-radiative flux variable, SNR, which shows a strong latitude dependence. Only the Sahara desert and Saudi Arabian regions appear to be complex. For those interested in climatology and construction of climate models, our model provides constraints upon specifying averaged non-radiative energy transport. The model is a useful simplification for learning about radiative energy transfer into and out of Earth?s atmosphere and for representing the results of more sophisticated models.",Environment
MentalHealthAI: Utilizing Personal Health Device Data to Optimize Psychiatry Treatment,"Mental health disorders remain a significant challenge in modern healthcare, with diagnosis and treatment often relying on subjective patient descriptions and past medical history. To address this issue, we propose a personalized mental health tracking and mood prediction system that utilizes patient physiological data collected through personal health devices. Our system leverages a decentralized learning mechanism that combines transfer and federated machine learning concepts using smart contracts, allowing data to remain on users devices and enabling effective tracking of mental health conditions for psychiatric treatment and management in a privacy-aware and accountable manner. We evaluate our model using a popular mental health dataset that demonstrates promising results. By utilizing connected health systems and machine learning models, our approach offers a novel solution to the challenge of providing psychiatrists with further insight into their patients mental health outside of traditional office visits.","MentalHealthAI: Utilizing Personal Health Device Data to Optimize Psychiatry Treatment Mental health disorders remain a significant challenge in modern healthcare, with diagnosis and treatment often relying on subjective patient descriptions and past medical history. To address this issue, we propose a personalized mental health tracking and mood prediction system that utilizes patient physiological data collected through personal health devices. Our system leverages a decentralized learning mechanism that combines transfer and federated machine learning concepts using smart contracts, allowing data to remain on users devices and enabling effective tracking of mental health conditions for psychiatric treatment and management in a privacy-aware and accountable manner. We evaluate our model using a popular mental health dataset that demonstrates promising results. By utilizing connected health systems and machine learning models, our approach offers a novel solution to the challenge of providing psychiatrists with further insight into their patients mental health outside of traditional office visits.",Healthcare
The Difficulties of Learning Logic Programs with Cut,"As real logic programmers normally use cut (!), an effective learning procedure for logic programs should be able to deal with it. Because the cut predicate has only a procedural meaning, clauses containing cut cannot be learned using an extensional evaluation method, as is done in most learning systems. On the other hand, searching a space of possible programs (instead of a space of independent clauses) is unfeasible. An alternative solution is to generate first a candidate base program which covers the positive examples, and then make it consistent by inserting cut where appropriate. The problem of learning programs with cut has not been investigated before and this seems to be a natural and reasonable approach. We generalize this scheme and investigate the difficulties that arise. Some of the major shortcomings are actually caused, in general, by the need for intensional evaluation. As a conclusion, the analysis of this paper suggests, on precise and technical grounds, that learning cut is difficult, and current induction techniques should probably be restricted to purely declarative logic languages.","The Difficulties of Learning Logic Programs with Cut As real logic programmers normally use cut (!), an effective learning procedure for logic programs should be able to deal with it. Because the cut predicate has only a procedural meaning, clauses containing cut cannot be learned using an extensional evaluation method, as is done in most learning systems. On the other hand, searching a space of possible programs (instead of a space of independent clauses) is unfeasible. An alternative solution is to generate first a candidate base program which covers the positive examples, and then make it consistent by inserting cut where appropriate. The problem of learning programs with cut has not been investigated before and this seems to be a natural and reasonable approach. We generalize this scheme and investigate the difficulties that arise. Some of the major shortcomings are actually caused, in general, by the need for intensional evaluation. As a conclusion, the analysis of this paper suggests, on precise and technical grounds, that learning cut is difficult, and current induction techniques should probably be restricted to purely declarative logic languages.",Technology
Dual Pricing to Prioritize Renewable Energy and Consumer Preferences in Electricity Markets,"Electricity markets currently fail to incorporate preferences of buyers, treating polluting and renewable energy sources as having equal social benefit under a system of uniform clearing prices. Meanwhile, renewable energy is prone to curtailment due to transmission constraints, forcing grid operators to reduce or shut down renewable energy production despite its availability and need. This paper proposes a dual pricing mechanism which allows buyers to bid both their willingness to pay for electricity, and additionally, their preference for green energy. Designed for use in deregulated electricity markets, this mechanism prioritizes the dispatch of more renewable energy sources according to consumer preferences. Traditional uniform clearing prices, which treat all energy sources equally, do not reflect the growing share of green energy in the power grid and the environmental values of consumers. By allowing load-serving entities to bid their willingness to pay for renewable energy directly into the clearing market, our proposed framework generates distinct pricing signals for green and black electricity.","Dual Pricing to Prioritize Renewable Energy and Consumer Preferences in Electricity Markets Electricity markets currently fail to incorporate preferences of buyers, treating polluting and renewable energy sources as having equal social benefit under a system of uniform clearing prices. Meanwhile, renewable energy is prone to curtailment due to transmission constraints, forcing grid operators to reduce or shut down renewable energy production despite its availability and need. This paper proposes a dual pricing mechanism which allows buyers to bid both their willingness to pay for electricity, and additionally, their preference for green energy. Designed for use in deregulated electricity markets, this mechanism prioritizes the dispatch of more renewable energy sources according to consumer preferences. Traditional uniform clearing prices, which treat all energy sources equally, do not reflect the growing share of green energy in the power grid and the environmental values of consumers. By allowing load-serving entities to bid their willingness to pay for renewable energy directly into the clearing market, our proposed framework generates distinct pricing signals for green and black electricity.",Environment
Understanding Financial Market States Using Artificial Double Auction Market,"The ultimate value of theories of the fundamental mechanisms comprising the asset price in financial systems will be reflected in the capacity of such theories to understand these systems. Although the models that explain the various states of financial markets offer substantial evidences from the fields of finance, mathematics, and even physics to explain states observed in the real financial markets, previous theories that attempt to fully explain the complexities of financial markets have been inadequate. In this study, we propose an artificial double auction market as an agent-based model approach to study the origin of complex states in the financial markets, characterizing important parameters with an investment strategy that can cover the dynamics of the financial market. The investment strategy of chartist traders after market information arrives should reduce market stability originating in the price fluctuations of risky assets. However, fundamentalist traders strategically submit orders with a fundamental value and, thereby stabilize the market. We construct a continuous double auction market and find that the market is controlled by a fraction of chartists, P_c. We show that mimicking real financial markets state, which emerges in real financial systems, is given between approximately P_c  0.40 and P_c  0.85, but that mimicking the efficient market hypothesis state can be generated in a range of less than P_c  0.40. In particular, we observe that the mimicking market collapse state created in a value greater than P_c  0.85, in which a liquidity shortage occurs, and the phase transition behavior is P_c  0.85.","Understanding Financial Market States Using Artificial Double Auction Market The ultimate value of theories of the fundamental mechanisms comprising the asset price in financial systems will be reflected in the capacity of such theories to understand these systems. Although the models that explain the various states of financial markets offer substantial evidences from the fields of finance, mathematics, and even physics to explain states observed in the real financial markets, previous theories that attempt to fully explain the complexities of financial markets have been inadequate. In this study, we propose an artificial double auction market as an agent-based model approach to study the origin of complex states in the financial markets, characterizing important parameters with an investment strategy that can cover the dynamics of the financial market. The investment strategy of chartist traders after market information arrives should reduce market stability originating in the price fluctuations of risky assets. However, fundamentalist traders strategically submit orders with a fundamental value and, thereby stabilize the market. We construct a continuous double auction market and find that the market is controlled by a fraction of chartists, P_c. We show that mimicking real financial markets state, which emerges in real financial systems, is given between approximately P_c  0.40 and P_c  0.85, but that mimicking the efficient market hypothesis state can be generated in a range of less than P_c  0.40. In particular, we observe that the mimicking market collapse state created in a value greater than P_c  0.85, in which a liquidity shortage occurs, and the phase transition behavior is P_c  0.85.",Finance
NLP for Climate Policy: Creating a Knowledge Platform for Holistic and Effective Climate Action,"Climate change is a burning issue of our time, with the Sustainable Development Goal (SDG) 13 of the United Nations demanding global climate action. Realizing the urgency, in 2015 in Paris, world leaders signed an agreement committing to taking voluntary action to reduce carbon emissions. However, the scale, magnitude, and climate action processes vary globally, especially between developed and developing countries. Therefore, from parliament to social media, the debates and discussions on climate change gather data from wide-ranging sources essential to the policy design and implementation. The downside is that we do not currently have the mechanisms to pool the worldwide dispersed knowledge emerging from the structured and unstructured data sources. The paper thematically discusses how NLP techniques could be employed in climate policy research and contribute to societys good at large. In particular, we exemplify symbiosis of NLP and Climate Policy Research via four methodologies. The first one deals with the major topics related to climate policy using automated content analysis. We investigate the opinions (sentiments) of major actors narratives towards climate policy in the second methodology. The third technique explores the climate actors beliefs towards pro or anti-climate orientation. Finally, we discuss developing a Climate Knowledge Graph. The present theme paper further argues that creating a knowledge platform would help in the formulation of a holistic climate policy and effective climate action. Such a knowledge platform would integrate the policy actors varied opinions from different social sectors like government, business, civil society, and the scientific community. The research outcome will add value to effective climate action because policymakers can make informed decisions by looking at the diverse public opinion on a comprehensive platform.","NLP for Climate Policy: Creating a Knowledge Platform for Holistic and Effective Climate Action Climate change is a burning issue of our time, with the Sustainable Development Goal (SDG) 13 of the United Nations demanding global climate action. Realizing the urgency, in 2015 in Paris, world leaders signed an agreement committing to taking voluntary action to reduce carbon emissions. However, the scale, magnitude, and climate action processes vary globally, especially between developed and developing countries. Therefore, from parliament to social media, the debates and discussions on climate change gather data from wide-ranging sources essential to the policy design and implementation. The downside is that we do not currently have the mechanisms to pool the worldwide dispersed knowledge emerging from the structured and unstructured data sources. The paper thematically discusses how NLP techniques could be employed in climate policy research and contribute to societys good at large. In particular, we exemplify symbiosis of NLP and Climate Policy Research via four methodologies. The first one deals with the major topics related to climate policy using automated content analysis. We investigate the opinions (sentiments) of major actors narratives towards climate policy in the second methodology. The third technique explores the climate actors beliefs towards pro or anti-climate orientation. Finally, we discuss developing a Climate Knowledge Graph. The present theme paper further argues that creating a knowledge platform would help in the formulation of a holistic climate policy and effective climate action. Such a knowledge platform would integrate the policy actors varied opinions from different social sectors like government, business, civil society, and the scientific community. The research outcome will add value to effective climate action because policymakers can make informed decisions by looking at the diverse public opinion on a comprehensive platform.",Environment
The Antarctic climate anomaly and galactic cosmic rays,"It has been proposed that galactic cosmic rays may influence the Earths climate by affecting cloud formation. If changes in cloudiness play a part in climate change, their effect changes sign in Antarctica. Satellite data from the Earth Radiation Budget Experiment (ERBE) are here used to calculate the changes in surface temperatures at all latitudes, due to small percentage changes in cloudiness. The results match the observed contrasts in temperature changes, globally and in Antarctica. Evidently clouds do not just respond passively to climate changes but take an active part in the forcing, in accordance with changes in the solar magnetic field that vary the cosmic-ray flux.","The Antarctic climate anomaly and galactic cosmic rays It has been proposed that galactic cosmic rays may influence the Earths climate by affecting cloud formation. If changes in cloudiness play a part in climate change, their effect changes sign in Antarctica. Satellite data from the Earth Radiation Budget Experiment (ERBE) are here used to calculate the changes in surface temperatures at all latitudes, due to small percentage changes in cloudiness. The results match the observed contrasts in temperature changes, globally and in Antarctica. Evidently clouds do not just respond passively to climate changes but take an active part in the forcing, in accordance with changes in the solar magnetic field that vary the cosmic-ray flux.",Environment
AI in Design Education at College Level-Educators Perspectives and Challenges,"Artificial intelligence has deeply permeated numerous fields, especially the design area which relies on technology as a tool for innovation. This change naturally extends to the field of design education, which is closest to design practice. This has led to further exploration of the impact of AI on college-level education in the design discipline. This study aims to examine how current design educators perceive the role of AI in college-level design education, their perspectives on integrating AI into teaching and research, and their concerns regarding its potential challenges in design education and research. Through qualitative, semi-structured, in-depth interviews with seven faculties in U.S. design colleges, the findings reveal that AI, as a tool and source of information, has become an integral part of design education. AI- derived functionalities are increasingly utilized in design software, and educators are actively incorporating AI as a theoretical framework in their teaching. Educators can guide students in using AI tools, but only if they first acquire a strong foundation in basic design principles and skills. This study also indicates the importance of promoting a cooperative relationship between design educators and AI. At the same time, educators express anticipation for advancements in ethical standards, authenticity, and the resolution of copyright issues related to AI.","AI in Design Education at College Level-Educators Perspectives and Challenges Artificial intelligence has deeply permeated numerous fields, especially the design area which relies on technology as a tool for innovation. This change naturally extends to the field of design education, which is closest to design practice. This has led to further exploration of the impact of AI on college-level education in the design discipline. This study aims to examine how current design educators perceive the role of AI in college-level design education, their perspectives on integrating AI into teaching and research, and their concerns regarding its potential challenges in design education and research. Through qualitative, semi-structured, in-depth interviews with seven faculties in U.S. design colleges, the findings reveal that AI, as a tool and source of information, has become an integral part of design education. AI- derived functionalities are increasingly utilized in design software, and educators are actively incorporating AI as a theoretical framework in their teaching. Educators can guide students in using AI tools, but only if they first acquire a strong foundation in basic design principles and skills. This study also indicates the importance of promoting a cooperative relationship between design educators and AI. At the same time, educators express anticipation for advancements in ethical standards, authenticity, and the resolution of copyright issues related to AI.",Education
Mapping the Climate Change Landscape on TikTok,"Social media platforms shape climate action discourse. Mapping these online conversations is essential for effective communication strategies. TikToks climate discussions are particularly relevant given its young, climate-concerned audience. In this work, we collect the first TikTok dataset on climate topics. We collected 590K videos from 14K creators along with their follower networks. By applying topic modeling to the video descriptions, we map the topics discussed on the platform on a climate taxonomy that we construct by consolidating existing categorizations. Results show TikTok creators primarily approach climate through the angle of lifestyle and dietary choices. By examining semantic connections between topics, we identified non-climate gateway topics that could draw new audiences into climate discussions.","Mapping the Climate Change Landscape on TikTok Social media platforms shape climate action discourse. Mapping these online conversations is essential for effective communication strategies. TikToks climate discussions are particularly relevant given its young, climate-concerned audience. In this work, we collect the first TikTok dataset on climate topics. We collected 590K videos from 14K creators along with their follower networks. By applying topic modeling to the video descriptions, we map the topics discussed on the platform on a climate taxonomy that we construct by consolidating existing categorizations. Results show TikTok creators primarily approach climate through the angle of lifestyle and dietary choices. By examining semantic connections between topics, we identified non-climate gateway topics that could draw new audiences into climate discussions.",Environment
Continuous State-Space Models for Optimal Sepsis Treatment - a Deep Reinforcement Learning Approach,"Sepsis is a leading cause of mortality in intensive care units (ICUs) and costs hospitals billions annually. Treating a septic patient is highly challenging, because individual patients respond very differently to medical interventions and there is no universally agreed-upon treatment for sepsis. Understanding more about a patients physiological state at a given time could hold the key to effective treatment policies. In this work, we propose a new approach to deduce optimal treatment policies for septic patients by using continuous state-space models and deep reinforcement learning. Learning treatment policies over continuous spaces is important, because we retain more of the patients physiological information. Our model is able to learn clinically interpretable treatment policies, similar in important aspects to the treatment policies of physicians. Evaluating our algorithm on past ICU patient data, we find that our model could reduce patient mortality in the hospital by up to 3.6 over observed clinical policies, from a baseline mortality of 13.7. The learned treatment policies could be used to aid intensive care clinicians in medical decision making and improve the likelihood of patient survival.","Continuous State-Space Models for Optimal Sepsis Treatment - a Deep Reinforcement Learning Approach Sepsis is a leading cause of mortality in intensive care units (ICUs) and costs hospitals billions annually. Treating a septic patient is highly challenging, because individual patients respond very differently to medical interventions and there is no universally agreed-upon treatment for sepsis. Understanding more about a patients physiological state at a given time could hold the key to effective treatment policies. In this work, we propose a new approach to deduce optimal treatment policies for septic patients by using continuous state-space models and deep reinforcement learning. Learning treatment policies over continuous spaces is important, because we retain more of the patients physiological information. Our model is able to learn clinically interpretable treatment policies, similar in important aspects to the treatment policies of physicians. Evaluating our algorithm on past ICU patient data, we find that our model could reduce patient mortality in the hospital by up to 3.6 over observed clinical policies, from a baseline mortality of 13.7. The learned treatment policies could be used to aid intensive care clinicians in medical decision making and improve the likelihood of patient survival.",Healthcare
Military use of depleted uranium: assessment of prolonged population exposure,"This work is an exposure assessment for a population living in an area contaminated by use of depleted uranium (DU) weapons. RESRAD 5.91 code is used to evaluate the average effective dose delivered from 1, 10, 20 cm depths of contaminated soil, in a residential farmer scenario. Critical pathway and group are identified in soil inhalation or ingestion and children playing with the soil, respectively. From available information on DU released on targeted sites, both critical and average exposure can leave to toxicological hazards; annual dose limit for population can be exceeded on short-term period (years) for soil inhalation. As a consequence, in targeted sites cleaning up must be planned on the basis of measured concentration, when available, while special cautions have to be adopted altogether to reduce unaware exposures, taking into account the amount of the avertable dose.","Military use of depleted uranium: assessment of prolonged population exposure This work is an exposure assessment for a population living in an area contaminated by use of depleted uranium (DU) weapons. RESRAD 5.91 code is used to evaluate the average effective dose delivered from 1, 10, 20 cm depths of contaminated soil, in a residential farmer scenario. Critical pathway and group are identified in soil inhalation or ingestion and children playing with the soil, respectively. From available information on DU released on targeted sites, both critical and average exposure can leave to toxicological hazards; annual dose limit for population can be exceeded on short-term period (years) for soil inhalation. As a consequence, in targeted sites cleaning up must be planned on the basis of measured concentration, when available, while special cautions have to be adopted altogether to reduce unaware exposures, taking into account the amount of the avertable dose.",Healthcare
The origin of the spacetime metric: Bells Lorentzian pedagogy and its significance in general relativity,"The purpose of this paper is to evaluate the Lorentzian pedagogy defended by J.S. Bell in his essay How to teach special relativity, and to explore its consistency with Einsteins thinking from 1905 to 1952. Some remarks are also made in this context on Weyls philosophy of relativity and his 1918 gauge theory. Finally, it is argued that the Lorentzian pedagogy - which stresses the important connection between kinematics and dynamics - clarifies the role of rods and clocks in general relativity.","The origin of the spacetime metric: Bells Lorentzian pedagogy and its significance in general relativity The purpose of this paper is to evaluate the Lorentzian pedagogy defended by J.S. Bell in his essay How to teach special relativity, and to explore its consistency with Einsteins thinking from 1905 to 1952. Some remarks are also made in this context on Weyls philosophy of relativity and his 1918 gauge theory. Finally, it is argued that the Lorentzian pedagogy - which stresses the important connection between kinematics and dynamics - clarifies the role of rods and clocks in general relativity.",Education
International Financial Markets Through 150 Years: Evaluating Stylized Facts,"In the theory of financial markets, a stylized fact is a qualitative summary of a pattern in financial market data that is observed across multiple assets, asset classes and time horizons. In this article, we test a set of eleven stylized facts for financial market data. Our main contribution is to consider a broad range of geographical regions across Asia, continental Europe, and the US over a time period of 150 years, as well as two of the most traded cryptocurrencies, thus providing insights into the robustness and generalizability of commonly known stylized facts.","International Financial Markets Through 150 Years: Evaluating Stylized Facts In the theory of financial markets, a stylized fact is a qualitative summary of a pattern in financial market data that is observed across multiple assets, asset classes and time horizons. In this article, we test a set of eleven stylized facts for financial market data. Our main contribution is to consider a broad range of geographical regions across Asia, continental Europe, and the US over a time period of 150 years, as well as two of the most traded cryptocurrencies, thus providing insights into the robustness and generalizability of commonly known stylized facts.",Finance
ICT and Employment in India: A Sectoral Level Analysis,"How technology affects growth or employment has long been debated. With a hiatus, the debate revived once again in the form of how Information and Communications Technology, as a form of new technology, exerts on productivity and employment. Information and Communications Technology perceived as General Purpose Technology like steam engine or electricity in the past, ushered the world into a new techno-economic paradigm, given its deep social, economic and cultural implications. For instance, within economic implication, it is hard to imagine an economic activity that does not it, directly or indirectly. Eventually, Information and Communications Technology intensity, measure as the ratio of Information and Communications Technology investment to total investment, increased phenomenally in industries across sectors.","ICT and Employment in India: A Sectoral Level Analysis How technology affects growth or employment has long been debated. With a hiatus, the debate revived once again in the form of how Information and Communications Technology, as a form of new technology, exerts on productivity and employment. Information and Communications Technology perceived as General Purpose Technology like steam engine or electricity in the past, ushered the world into a new techno-economic paradigm, given its deep social, economic and cultural implications. For instance, within economic implication, it is hard to imagine an economic activity that does not it, directly or indirectly. Eventually, Information and Communications Technology intensity, measure as the ratio of Information and Communications Technology investment to total investment, increased phenomenally in industries across sectors.",Finance
Decidable Reasoning in Terminological Knowledge Representation Systems,"Terminological knowledge representation systems (TKRSs) are tools for designing and using knowledge bases that make use of terminological languages (or concept languages). We analyze from a theoretical point of view a TKRS whose capabilities go beyond the ones of presently available TKRSs. The new features studied, often required in practical applications, can be summarized in three main points. First, we consider a highly expressive terminological language, called ALCNR, including general complements of concepts, number restrictions and role conjunction. Second, we allow to express inclusion statements between general concepts, and terminological cycles as a particular case. Third, we prove the decidability of a number of desirable TKRS-deduction services (like satisfiability, subsumption and instance checking) through a sound, complete and terminating calculus for reasoning in ALCNR-knowledge bases. Our calculus extends the general technique of constraint systems. As a byproduct of the proof, we get also the result that inclusion statements in ALCNR can be simulated by terminological cycles, if descriptive semantics is adopted.","Decidable Reasoning in Terminological Knowledge Representation Systems Terminological knowledge representation systems (TKRSs) are tools for designing and using knowledge bases that make use of terminological languages (or concept languages). We analyze from a theoretical point of view a TKRS whose capabilities go beyond the ones of presently available TKRSs. The new features studied, often required in practical applications, can be summarized in three main points. First, we consider a highly expressive terminological language, called ALCNR, including general complements of concepts, number restrictions and role conjunction. Second, we allow to express inclusion statements between general concepts, and terminological cycles as a particular case. Third, we prove the decidability of a number of desirable TKRS-deduction services (like satisfiability, subsumption and instance checking) through a sound, complete and terminating calculus for reasoning in ALCNR-knowledge bases. Our calculus extends the general technique of constraint systems. As a byproduct of the proof, we get also the result that inclusion statements in ALCNR can be simulated by terminological cycles, if descriptive semantics is adopted.",Technology
PAC Classification based on PAC Estimates of Label Class Distributions,"A standard approach in pattern classification is to estimate the distributions of the label classes, and then to apply the Bayes classifier to the estimates of the distributions in order to classify unlabeled examples. As one might expect, the better our estimates of the label class distributions, the better the resulting classifier will be. In this paper we make this observation precise by identifying risk bounds of a classifier in terms of the quality of the estimates of the label class distributions. We show how PAC learnability relates to estimates of the distributions that have a PAC guarantee on their L_1 distance from the true distribution, and we bound the increase in negative log likelihood risk in terms of PAC bounds on the KL-divergence. We give an inefficient but general-purpose smoothing method for converting an estimated distribution that is good under the L_1 metric into a distribution that is good under the KL-divergence.","PAC Classification based on PAC Estimates of Label Class Distributions A standard approach in pattern classification is to estimate the distributions of the label classes, and then to apply the Bayes classifier to the estimates of the distributions in order to classify unlabeled examples. As one might expect, the better our estimates of the label class distributions, the better the resulting classifier will be. In this paper we make this observation precise by identifying risk bounds of a classifier in terms of the quality of the estimates of the label class distributions. We show how PAC learnability relates to estimates of the distributions that have a PAC guarantee on their L_1 distance from the true distribution, and we bound the increase in negative log likelihood risk in terms of PAC bounds on the KL-divergence. We give an inefficient but general-purpose smoothing method for converting an estimated distribution that is good under the L_1 metric into a distribution that is good under the KL-divergence.",Technology
"Time Series Analysis for Education: Methods, Applications, and Future Directions","Recent advancements in the collection and analysis of sequential educational data have brought time series analysis to a pivotal position in educational research, highlighting its essential role in facilitating data-driven decision-making. However, there is a lack of comprehensive summaries that consolidate these advancements. To the best of our knowledge, this paper is the first to provide a comprehensive review of time series analysis techniques specifically within the educational context. We begin by exploring the landscape of educational data analytics, categorizing various data sources and types relevant to education. We then review four prominent time series methods-forecasting, classification, clustering, and anomaly detection-illustrating their specific application points in educational settings. Subsequently, we present a range of educational scenarios and applications, focusing on how these methods are employed to address diverse educational tasks, which highlights the practical integration of multiple time series methods to solve complex educational problems. Finally, we conclude with a discussion on future directions, including personalized learning analytics, multimodal data fusion, and the role of large language models (LLMs) in educational time series. The contributions of this paper include a detailed taxonomy of educational data, a synthesis of time series techniques with specific educational applications, and a forward-looking perspective on emerging trends and future research opportunities in educational analysis. The related papers and resources are available and regularly updated at the project page.","Time Series Analysis for Education: Methods, Applications, and Future Directions Recent advancements in the collection and analysis of sequential educational data have brought time series analysis to a pivotal position in educational research, highlighting its essential role in facilitating data-driven decision-making. However, there is a lack of comprehensive summaries that consolidate these advancements. To the best of our knowledge, this paper is the first to provide a comprehensive review of time series analysis techniques specifically within the educational context. We begin by exploring the landscape of educational data analytics, categorizing various data sources and types relevant to education. We then review four prominent time series methods-forecasting, classification, clustering, and anomaly detection-illustrating their specific application points in educational settings. Subsequently, we present a range of educational scenarios and applications, focusing on how these methods are employed to address diverse educational tasks, which highlights the practical integration of multiple time series methods to solve complex educational problems. Finally, we conclude with a discussion on future directions, including personalized learning analytics, multimodal data fusion, and the role of large language models (LLMs) in educational time series. The contributions of this paper include a detailed taxonomy of educational data, a synthesis of time series techniques with specific educational applications, and a forward-looking perspective on emerging trends and future research opportunities in educational analysis. The related papers and resources are available and regularly updated at the project page.",Education
"Why Not Borrow, Invest, and Escape Poverty?","Take up of microcredit by the poor for investment in businesses or human capital turned out to be very low. We show that this could be explained by risk aversion, without relying on fixed costs or other forms of non-convexity in the technology, if the investment is aimed at increasing the probability of success. Under this framework, rational risk-averse agents choose corner solutions, unlike in the case of a risky investment with an exogenous probability of success. Our online experiment confirms our theoretical predictions about how agents choices differ when facing the two types of investments.","Why Not Borrow, Invest, and Escape Poverty? Take up of microcredit by the poor for investment in businesses or human capital turned out to be very low. We show that this could be explained by risk aversion, without relying on fixed costs or other forms of non-convexity in the technology, if the investment is aimed at increasing the probability of success. Under this framework, rational risk-averse agents choose corner solutions, unlike in the case of a risky investment with an exogenous probability of success. Our online experiment confirms our theoretical predictions about how agents choices differ when facing the two types of investments.",Finance
"Housing Investment, Stock Market Participation and Household Portfolio choice: Evidence from Chinas Urban Areas","This paper employs the survey data of CHFS (2013) to investigate the impact of housing investment on household stock market participation and portfolio choice. The results show that larger housing investment encourages the household participation in the stock market, but reduces the proportion of their stockholding. The above conclusion remains true even when the endogeneity problem is controlled with risk attitude classification, Heckman model test and subsample regression. This study shows that the growth in the housing market will not lead to stock market development because of lack of household financial literacy and the low expected yield on stock market.","Housing Investment, Stock Market Participation and Household Portfolio choice: Evidence from Chinas Urban Areas This paper employs the survey data of CHFS (2013) to investigate the impact of housing investment on household stock market participation and portfolio choice. The results show that larger housing investment encourages the household participation in the stock market, but reduces the proportion of their stockholding. The above conclusion remains true even when the endogeneity problem is controlled with risk attitude classification, Heckman model test and subsample regression. This study shows that the growth in the housing market will not lead to stock market development because of lack of household financial literacy and the low expected yield on stock market.",Finance
Renewable Energy Transition in South America: Predictive Analysis of Generation Capacity by 2050,"In this research, renewable energy expansion in South America up to 2050 is predicted based on machine learning models that are trained on past energy data. The research employs gradient boosting regression and Prophet time series forecasting to make predictions of future generation capacities for solar, wind, hydroelectric, geothermal, biomass, and other renewable sources in South American nations. Model output analysis indicates staggering future expansion in the generation of renewable energy, with solar and wind energy registering the highest expansion rates. Geospatial visualization methods were applied to illustrate regional disparities in the utilization of renewable energy. The results forecast South America to record nearly 3-fold growth in the generation of renewable energy by the year 2050, with Brazil and Chile spearheading regional development. Such projections help design energy policy, investment strategy, and climate change mitigation throughout the region, in helping the developing economies to transition to sustainable energy.","Renewable Energy Transition in South America: Predictive Analysis of Generation Capacity by 2050 In this research, renewable energy expansion in South America up to 2050 is predicted based on machine learning models that are trained on past energy data. The research employs gradient boosting regression and Prophet time series forecasting to make predictions of future generation capacities for solar, wind, hydroelectric, geothermal, biomass, and other renewable sources in South American nations. Model output analysis indicates staggering future expansion in the generation of renewable energy, with solar and wind energy registering the highest expansion rates. Geospatial visualization methods were applied to illustrate regional disparities in the utilization of renewable energy. The results forecast South America to record nearly 3-fold growth in the generation of renewable energy by the year 2050, with Brazil and Chile spearheading regional development. Such projections help design energy policy, investment strategy, and climate change mitigation throughout the region, in helping the developing economies to transition to sustainable energy.",Environment
Assessing contribution of treatment phases through tipping point analyses via counterfactual elicitation using rank preserving structural failure time models,"This article provides a novel approach to assess the importance of specific treatment phases within a treatment regimen through tipping point analyses (TPA) of a time-to-event endpoint using rank-preserving-structural-failure-time (RPSFT) modelling. In oncology clinical research, an experimental treatment is often added to the standard of care therapy in multiple treatment phases to improve patient outcomes. When the resulting new regimen provides a meaningful benefit over standard of care, gaining insights into the contribution of each treatment phase becomes important to properly guide clinical practice. New statistical approaches are needed since traditional methods are inadequate in answering such questions. RPSFT modelling is an approach for causal inference, typically used to adjust for treatment switching in randomized clinical trials with time-to-event endpoints. A tipping-point analysis is commonly used in situations where a statistically significant treatment effect is suspected to be an artifact of missing or unobserved data rather than a real treatment difference. The methodology proposed in this article is an amalgamation of these two ideas to investigate the contribution of a specific component of a regimen comprising multiple treatment phases. We provide different variants of the method and construct indices of contribution of a treatment phase to the overall benefit of a regimen that facilitates interpretation of results. The proposed approaches are illustrated with findings from a recently concluded, real-life phase 3 cancer clinical trial. We conclude with several considerations and recommendations for practical implementation of this new methodology.","Assessing contribution of treatment phases through tipping point analyses via counterfactual elicitation using rank preserving structural failure time models This article provides a novel approach to assess the importance of specific treatment phases within a treatment regimen through tipping point analyses (TPA) of a time-to-event endpoint using rank-preserving-structural-failure-time (RPSFT) modelling. In oncology clinical research, an experimental treatment is often added to the standard of care therapy in multiple treatment phases to improve patient outcomes. When the resulting new regimen provides a meaningful benefit over standard of care, gaining insights into the contribution of each treatment phase becomes important to properly guide clinical practice. New statistical approaches are needed since traditional methods are inadequate in answering such questions. RPSFT modelling is an approach for causal inference, typically used to adjust for treatment switching in randomized clinical trials with time-to-event endpoints. A tipping-point analysis is commonly used in situations where a statistically significant treatment effect is suspected to be an artifact of missing or unobserved data rather than a real treatment difference. The methodology proposed in this article is an amalgamation of these two ideas to investigate the contribution of a specific component of a regimen comprising multiple treatment phases. We provide different variants of the method and construct indices of contribution of a treatment phase to the overall benefit of a regimen that facilitates interpretation of results. The proposed approaches are illustrated with findings from a recently concluded, real-life phase 3 cancer clinical trial. We conclude with several considerations and recommendations for practical implementation of this new methodology.",Healthcare
Data Science Education in Undergraduate Physics: Lessons Learned from a Community of Practice,"It is becoming increasingly important that physics educators equip their students with the skills to work with data effectively. However, many educators may lack the necessary training and expertise in data science to teach these skills. To address this gap, we created the Data Science Education Community of Practice (DSECOP), bringing together graduate students and physics educators from different institutions and backgrounds to share best practices and lessons learned from integrating data science into undergraduate physics education. In this article we present insights and experiences from this community of practice, highlighting key strategies and challenges in incorporating data science into the introductory physics curriculum. Our goal is to provide guidance and inspiration to educators who seek to integrate data science into their teaching, helping to prepare the next generation of physicists for a data-driven world.","Data Science Education in Undergraduate Physics: Lessons Learned from a Community of Practice It is becoming increasingly important that physics educators equip their students with the skills to work with data effectively. However, many educators may lack the necessary training and expertise in data science to teach these skills. To address this gap, we created the Data Science Education Community of Practice (DSECOP), bringing together graduate students and physics educators from different institutions and backgrounds to share best practices and lessons learned from integrating data science into undergraduate physics education. In this article we present insights and experiences from this community of practice, highlighting key strategies and challenges in incorporating data science into the introductory physics curriculum. Our goal is to provide guidance and inspiration to educators who seek to integrate data science into their teaching, helping to prepare the next generation of physicists for a data-driven world.",Education
The impact of field shape optimization: A feasibility study,"The impact of field shape optimization is studied for prostate type geometry. For this study, 76 and 81 Gy plans were generated. Dose distributions for wedged plans and Intensity Modulated (IM) plans for three and seven fields were compared for a quadratic cost function. For wedged plans, a Simulated Annealing Algorithm (SAA) was used to optimize gantry angles, wedge angles, beam weights and field shapes. Two kinds of wedged plans were generated: 1) field sizes were determined by the requirement of full target coverage in the beams-eye-view (fixed fields) and 2) the field shape, in particular at the critical organ target overlap region was also among the variables optimized. For IM plans the SAA was used to optimize gantry angles and a conjugate gradient algorithm was used to optimize the IM beam fluences. Both the field shape optimized wedged plans and IM plans had significantly superior dose area histograms of the target, rectum and the bladder and cost function values compared to the fixed field optimized wedged plans.","The impact of field shape optimization: A feasibility study The impact of field shape optimization is studied for prostate type geometry. For this study, 76 and 81 Gy plans were generated. Dose distributions for wedged plans and Intensity Modulated (IM) plans for three and seven fields were compared for a quadratic cost function. For wedged plans, a Simulated Annealing Algorithm (SAA) was used to optimize gantry angles, wedge angles, beam weights and field shapes. Two kinds of wedged plans were generated: 1) field sizes were determined by the requirement of full target coverage in the beams-eye-view (fixed fields) and 2) the field shape, in particular at the critical organ target overlap region was also among the variables optimized. For IM plans the SAA was used to optimize gantry angles and a conjugate gradient algorithm was used to optimize the IM beam fluences. Both the field shape optimized wedged plans and IM plans had significantly superior dose area histograms of the target, rectum and the bladder and cost function values compared to the fixed field optimized wedged plans.",Healthcare
Lamarckian Evolution and the Baldwin Effect in Evolutionary Neural Networks,"Hybrid neuro-evolutionary algorithms may be inspired on Darwinian or Lamarckian evolu- tion. In the case of Darwinian evolution, the Baldwin effect, that is, the progressive incorporation of learned characteristics to the genotypes, can be observed and leveraged to improve the search. The purpose of this paper is to carry out an exper- imental study into how learning can improve G-Prop genetic search. Two ways of combining learning and genetic search are explored: one exploits the Baldwin effect, while the other uses a Lamarckian strategy. Our experiments show that using a Lamarckian op- erator makes the algorithm find networks with a low error rate, and the smallest size, while using the Bald- win effect obtains MLPs with the smallest error rate, and a larger size, taking longer to reach a solution. Both approaches obtain a lower average error than other BP-based algorithms like RPROP, other evolu- tionary methods and fuzzy logic based methods","Lamarckian Evolution and the Baldwin Effect in Evolutionary Neural Networks Hybrid neuro-evolutionary algorithms may be inspired on Darwinian or Lamarckian evolu- tion. In the case of Darwinian evolution, the Baldwin effect, that is, the progressive incorporation of learned characteristics to the genotypes, can be observed and leveraged to improve the search. The purpose of this paper is to carry out an exper- imental study into how learning can improve G-Prop genetic search. Two ways of combining learning and genetic search are explored: one exploits the Baldwin effect, while the other uses a Lamarckian strategy. Our experiments show that using a Lamarckian op- erator makes the algorithm find networks with a low error rate, and the smallest size, while using the Bald- win effect obtains MLPs with the smallest error rate, and a larger size, taking longer to reach a solution. Both approaches obtain a lower average error than other BP-based algorithms like RPROP, other evolu- tionary methods and fuzzy logic based methods",Technology
Indoor-Atmospheric Radon-Related Radioactivity Affected by a Change of Ventilation Strategy,"The present author has kept observation for concentrations of atmospheric radon, radon progeny and thoron progeny for several years at the campus of Fukushima Medical University. Accidentally, in the midst of an observation term, i.e., February 2005, the facility management group of the university changed a strategy for the manner of ventilation, probably because of a recession: (I) tidy everyday ventilation of 7:30-24:00 into (II) shortened weekday ventilation of 8:00-21:00 with weekend halts. This change of ventilation manner brought a clear alteration for the concentrations of radon-related natural radioactivity in indoor air. The present paper concerns an investigation of the effect of the ventilation strategy on the indoor-atmospheric radon-related radioactivity.","Indoor-Atmospheric Radon-Related Radioactivity Affected by a Change of Ventilation Strategy The present author has kept observation for concentrations of atmospheric radon, radon progeny and thoron progeny for several years at the campus of Fukushima Medical University. Accidentally, in the midst of an observation term, i.e., February 2005, the facility management group of the university changed a strategy for the manner of ventilation, probably because of a recession: (I) tidy everyday ventilation of 7:30-24:00 into (II) shortened weekday ventilation of 8:00-21:00 with weekend halts. This change of ventilation manner brought a clear alteration for the concentrations of radon-related natural radioactivity in indoor air. The present paper concerns an investigation of the effect of the ventilation strategy on the indoor-atmospheric radon-related radioactivity.",Healthcare
GDP Trend Deviations and the Yield Spread: the Case of Five E.U. Countries,"Several studies have established the predictive power of the yield curve in terms of real economic activity. In this paper we use data for a variety of E.U. countries: both EMU (Germany, France, Italy) and non-EMU members (Sweden and the U.K.). The data used range from 1991:Q1 to 2009:Q1. For each country, we extract the long run trend and the cyclical component of real economic activity, while the corresponding interbank interest rates of long and short term maturities are used for the calculation of the country specific yield spreads. We also augment the models tested with non monetary policy variables: the countries unemployment rates and stock indices. The methodology employed in the effort to forecast real output, is a probit model of the inverse cumulative distribution function of the standard distribution, using several formal forecasting and goodness of fit evaluation tests. The results show that the yield curve augmented with the non-monetary variables has significant forecasting power in terms of real economic activity but the results differ qualitatively between the individual economies examined raising non-trivial policy implications.","GDP Trend Deviations and the Yield Spread: the Case of Five E.U. Countries Several studies have established the predictive power of the yield curve in terms of real economic activity. In this paper we use data for a variety of E.U. countries: both EMU (Germany, France, Italy) and non-EMU members (Sweden and the U.K.). The data used range from 1991:Q1 to 2009:Q1. For each country, we extract the long run trend and the cyclical component of real economic activity, while the corresponding interbank interest rates of long and short term maturities are used for the calculation of the country specific yield spreads. We also augment the models tested with non monetary policy variables: the countries unemployment rates and stock indices. The methodology employed in the effort to forecast real output, is a probit model of the inverse cumulative distribution function of the standard distribution, using several formal forecasting and goodness of fit evaluation tests. The results show that the yield curve augmented with the non-monetary variables has significant forecasting power in terms of real economic activity but the results differ qualitatively between the individual economies examined raising non-trivial policy implications.",Finance
What shapes climate change perceptions in Africa? A random forest approach,"Climate change perceptions are fundamental for adaptation and environmental policy support. Although Africa is one of the most vulnerable regions to climate change, little research has focused on how climate change is perceived in the continent. Using random forest methodology, we analyse Afrobarometer data (N  45,732), joint with climatic data, to explore what shapes climate change perceptions in Africa. We include 5 different dimensions of climate change perceptions: awareness, belief in its human cause, risk perception, need to stop it and self-efficacy. Results indicate that perceived agriculture conditions are crucial for perceiving climate change. Country-level factors and long-term changes in local weather conditions are among the most important predictors. Moreover, education level, access to information, poverty, authoritarian values, and trust in institutions shape individual climate change perceptions. Demographic effects -- including religion -- seem negligible. These findings suggest policymakers and environmental communicators how to frame climate change in Africa to raise awareness, gather public support and induce adaptation.","What shapes climate change perceptions in Africa? A random forest approach Climate change perceptions are fundamental for adaptation and environmental policy support. Although Africa is one of the most vulnerable regions to climate change, little research has focused on how climate change is perceived in the continent. Using random forest methodology, we analyse Afrobarometer data (N  45,732), joint with climatic data, to explore what shapes climate change perceptions in Africa. We include 5 different dimensions of climate change perceptions: awareness, belief in its human cause, risk perception, need to stop it and self-efficacy. Results indicate that perceived agriculture conditions are crucial for perceiving climate change. Country-level factors and long-term changes in local weather conditions are among the most important predictors. Moreover, education level, access to information, poverty, authoritarian values, and trust in institutions shape individual climate change perceptions. Demographic effects -- including religion -- seem negligible. These findings suggest policymakers and environmental communicators how to frame climate change in Africa to raise awareness, gather public support and induce adaptation.",Environment
A New Kind of Finance,"Finance has benefited from the Wolframs NKS approach but it can and will benefit even more in the future, and the gains from the influence may actually be concentrated among practitioners who unintentionally employ those principles as a group.","A New Kind of Finance Finance has benefited from the Wolframs NKS approach but it can and will benefit even more in the future, and the gains from the influence may actually be concentrated among practitioners who unintentionally employ those principles as a group.",Finance
Complexity analysis for algorithmically simple strings,"Given a reference computer, Kolmogorov complexity is a well defined function on all binary strings. In the standard approach, however, only the asymptotic properties of such functions are considered because they do not depend on the reference computer. We argue that this approach can be more useful if it is refined to include an important practical case of simple binary strings. Kolmogorov complexity calculus may be developed for this case if we restrict the class of available reference computers. The interesting problem is to define a class of computers which is restricted in a it natural way modeling the real-life situation where only a limited class of computers is physically available to us. We give an example of what such a natural restriction might look like mathematically, and show that under such restrictions some error terms, even logarithmic in complexity, can disappear from the standard complexity calculus. Keywords: Kolmogorov complexity; Algorithmic information theory.","Complexity analysis for algorithmically simple strings Given a reference computer, Kolmogorov complexity is a well defined function on all binary strings. In the standard approach, however, only the asymptotic properties of such functions are considered because they do not depend on the reference computer. We argue that this approach can be more useful if it is refined to include an important practical case of simple binary strings. Kolmogorov complexity calculus may be developed for this case if we restrict the class of available reference computers. The interesting problem is to define a class of computers which is restricted in a it natural way modeling the real-life situation where only a limited class of computers is physically available to us. We give an example of what such a natural restriction might look like mathematically, and show that under such restrictions some error terms, even logarithmic in complexity, can disappear from the standard complexity calculus. Keywords: Kolmogorov complexity; Algorithmic information theory.",Technology
Robust Classification for Imprecise Environments,"In real-world environments it usually is difficult to specify target operating conditions precisely, for example, target misclassification costs. This uncertainty makes building robust classification systems problematic. We show that it is possible to build a hybrid classifier that will perform at least as well as the best available classifier for any target conditions. In some cases, the performance of the hybrid actually can surpass that of the best known classifier. This robust performance extends across a wide variety of comparison frameworks, including the optimization of metrics such as accuracy, expected cost, lift, precision, recall, and workforce utilization. The hybrid also is efficient to build, to store, and to update. The hybrid is based on a method for the comparison of classifier performance that is robust to imprecise class distributions and misclassification costs. The ROC convex hull (ROCCH) method combines techniques from ROC analysis, decision analysis and computational geometry, and adapts them to the particulars of analyzing learned classifiers. The method is efficient and incremental, minimizes the management of classifier performance data, and allows for clear visual comparisons and sensitivity analyses. Finally, we point to empirical evidence that a robust hybrid classifier indeed is needed for many real-world problems.","Robust Classification for Imprecise Environments In real-world environments it usually is difficult to specify target operating conditions precisely, for example, target misclassification costs. This uncertainty makes building robust classification systems problematic. We show that it is possible to build a hybrid classifier that will perform at least as well as the best available classifier for any target conditions. In some cases, the performance of the hybrid actually can surpass that of the best known classifier. This robust performance extends across a wide variety of comparison frameworks, including the optimization of metrics such as accuracy, expected cost, lift, precision, recall, and workforce utilization. The hybrid also is efficient to build, to store, and to update. The hybrid is based on a method for the comparison of classifier performance that is robust to imprecise class distributions and misclassification costs. The ROC convex hull (ROCCH) method combines techniques from ROC analysis, decision analysis and computational geometry, and adapts them to the particulars of analyzing learned classifiers. The method is efficient and incremental, minimizes the management of classifier performance data, and allows for clear visual comparisons and sensitivity analyses. Finally, we point to empirical evidence that a robust hybrid classifier indeed is needed for many real-world problems.",Technology
Multiple-bubble testing in the cryptocurrency market: a case study of bitcoin,Economic periods and financial crises have highlighted the importance of evaluating financial markets to investors and researchers in recent decades.,Multiple-bubble testing in the cryptocurrency market: a case study of bitcoin Economic periods and financial crises have highlighted the importance of evaluating financial markets to investors and researchers in recent decades.,Finance
A New Kind of Hopfield Networks for Finding Global Optimum,"The Hopfield network has been applied to solve optimization problems over decades. However, it still has many limitations in accomplishing this task. Most of them are inherited from the optimization algorithms it implements. The computation of a Hopfield network, defined by a set of difference equations, can easily be trapped into one local optimum or another, sensitive to initial conditions, perturbations, and neuron update orders. It doesnt know how long it will take to converge, as well as if the final solution is a global optimum, or not. In this paper, we present a Hopfield network with a new set of difference equations to fix those problems. The difference equations directly implement a new powerful optimization algorithm.","A New Kind of Hopfield Networks for Finding Global Optimum The Hopfield network has been applied to solve optimization problems over decades. However, it still has many limitations in accomplishing this task. Most of them are inherited from the optimization algorithms it implements. The computation of a Hopfield network, defined by a set of difference equations, can easily be trapped into one local optimum or another, sensitive to initial conditions, perturbations, and neuron update orders. It doesnt know how long it will take to converge, as well as if the final solution is a global optimum, or not. In this paper, we present a Hopfield network with a new set of difference equations to fix those problems. The difference equations directly implement a new powerful optimization algorithm.",Technology
Applying GSAT to Non-Clausal Formulas,"In this paper we describe how to modify GSAT so that it can be applied to non-clausal formulas. The idea is to use a particular score function which gives the number of clauses of the CNF conversion of a formula which are false under a given truth assignment. Its value is computed in linear time, without constructing the CNF conversion itself. The proposed methodology applies to most of the variants of GSAT proposed so far.","Applying GSAT to Non-Clausal Formulas In this paper we describe how to modify GSAT so that it can be applied to non-clausal formulas. The idea is to use a particular score function which gives the number of clauses of the CNF conversion of a formula which are false under a given truth assignment. Its value is computed in linear time, without constructing the CNF conversion itself. The proposed methodology applies to most of the variants of GSAT proposed so far.",Technology
Optimal intervention in the foreign exchange market when interventions affect market dynamics,"We address the problem of optimal Central Bank intervention in the exchange rate market when interventions create feedback in the rate dynamics. In particular, we extend the work done on optimal impulse control by Cadenillas and Zapatero to incorporate temporary market reactions, of random duration and level, to Bank interventions, and to establish results for more general rate processes. We obtain new explicit optimal impulse control strategies that account for these market reactions, and show that they cannot be obtained simply by adjusting the intervention cost in a model without market reactions.","Optimal intervention in the foreign exchange market when interventions affect market dynamics We address the problem of optimal Central Bank intervention in the exchange rate market when interventions create feedback in the rate dynamics. In particular, we extend the work done on optimal impulse control by Cadenillas and Zapatero to incorporate temporary market reactions, of random duration and level, to Bank interventions, and to establish results for more general rate processes. We obtain new explicit optimal impulse control strategies that account for these market reactions, and show that they cannot be obtained simply by adjusting the intervention cost in a model without market reactions.",Finance
Realized Volatility Analysis in A Spin Model of Financial Markets,We calculate the realized volatility in the spin model of financial markets and examine the returns standardized by the realized volatility. We find that moments of the standardized returns agree with the theoretical values of standard normal variables. This is the first evidence that the return dynamics of the spin financial market is consistent with the view of the mixture-of-distribution hypothesis that also holds in the real financial markets.,Realized Volatility Analysis in A Spin Model of Financial Markets We calculate the realized volatility in the spin model of financial markets and examine the returns standardized by the realized volatility. We find that moments of the standardized returns agree with the theoretical values of standard normal variables. This is the first evidence that the return dynamics of the spin financial market is consistent with the view of the mixture-of-distribution hypothesis that also holds in the real financial markets.,Finance
Integrated Cell Manipulation Systems,"A new type of microfluidic system for biological cell manipulation, a CMOSmicrofluidic hybrid, is demonstrated. The hybrid system starts with a custom-designed CMOS (complementary metal-oxide semiconductor) chip fabricated in a semiconductor foundry using standard integration circuit technology. A microfluidic channel is post-fabricated on top of the CMOS chip to provide biocompatible environment. The motion of individual biological cells that are tagged with magnetic beads is directly controlled by the CMOS chip that generates localized magnetic filed patterns using an on-chip array of micro-electromagnets. The speed and the programmability of the CMOS chip further allow for the dynamic reconfiguration of the magnetic fields, substantially increasing the manipulation capability of the hybrid system. The concept of a hybrid system is verified by simultaneously manipulating individual biological cells with microscopic resolution. A new operation protocol that exploits the fast speed of electronics to trap and move a large number of cells with less power consumption is also demonstrated. Combining the advantages of microelectronics, the CMOSmicrofluidic hybrid approach presents a new model for a multifunctional lab-on-a chip for biological and medical applications.","Integrated Cell Manipulation Systems A new type of microfluidic system for biological cell manipulation, a CMOSmicrofluidic hybrid, is demonstrated. The hybrid system starts with a custom-designed CMOS (complementary metal-oxide semiconductor) chip fabricated in a semiconductor foundry using standard integration circuit technology. A microfluidic channel is post-fabricated on top of the CMOS chip to provide biocompatible environment. The motion of individual biological cells that are tagged with magnetic beads is directly controlled by the CMOS chip that generates localized magnetic filed patterns using an on-chip array of micro-electromagnets. The speed and the programmability of the CMOS chip further allow for the dynamic reconfiguration of the magnetic fields, substantially increasing the manipulation capability of the hybrid system. The concept of a hybrid system is verified by simultaneously manipulating individual biological cells with microscopic resolution. A new operation protocol that exploits the fast speed of electronics to trap and move a large number of cells with less power consumption is also demonstrated. Combining the advantages of microelectronics, the CMOSmicrofluidic hybrid approach presents a new model for a multifunctional lab-on-a chip for biological and medical applications.",Healthcare
Relationship between sunspot number and total annual precipitation at Izana (Tenerife): Maximum precipitation prediction with three year lagged sunspots?,"A possible relationship between sunspot number and total annual precipitation from the Izana Observatory has been found. The annual precipitation period ranges from 1916 to 1998, thus including nearly eight 11-year solar cycles. When points of total precipitation for a given year at Izana are plotted on the ordinate axis versus the yearly sunspot number on the abcisa axis three years back from the precipitation one, nearly all of them lie in the lower left hand corner of the diagram. This seems to indicate a relationship between the above mentioned variables. If this relationship is confirmed it would permit the prediction of a maximum annual precipitation at Izana three years in advance.","Relationship between sunspot number and total annual precipitation at Izana (Tenerife): Maximum precipitation prediction with three year lagged sunspots? A possible relationship between sunspot number and total annual precipitation from the Izana Observatory has been found. The annual precipitation period ranges from 1916 to 1998, thus including nearly eight 11-year solar cycles. When points of total precipitation for a given year at Izana are plotted on the ordinate axis versus the yearly sunspot number on the abcisa axis three years back from the precipitation one, nearly all of them lie in the lower left hand corner of the diagram. This seems to indicate a relationship between the above mentioned variables. If this relationship is confirmed it would permit the prediction of a maximum annual precipitation at Izana three years in advance.",Environment
Nurses as agents for achieving Environmentally Sustainable Health Systems: A bibliometric analysis,"Objective: To analyze the current scientific knowledge and research lines focused on environmentally sustainable health systems, including the role of nurses. Background: There seem to be differences between creating interventions focused on environmentally sustainable health systems, including nurses, and the scarcity of research on this topic, framed on the Sustainable Development Goals. Methods: A bibliometric analysis was carried out, via three databases (Web of Science, Scopus, and Pubmed), and the guideline recommendations were followed to select bibliometric data. Results: The search resulted in 159 publications, significantly increasing the trends from 2017 to 2021 (p0.028). The most relevant countries in this area were the United States of America, the United Kingdom, and Sweden. Also, the top articles were from relevant journals, indexed in Journal Citation Report, and the first and the second quartile linked to the nursing field and citations (p0.001). Conclusion: Education is key to achieving environmentally sustainable health systems via institutions and policies. Implications for nursing management: There is a lack of experimental data and policies on achieving or maintaining environmentally sustainable health care systems, indicating that nurses have an important role and should be consulted and included in decision-making policies regarding sustainability in the healthcare systems.","Nurses as agents for achieving Environmentally Sustainable Health Systems: A bibliometric analysis Objective: To analyze the current scientific knowledge and research lines focused on environmentally sustainable health systems, including the role of nurses. Background: There seem to be differences between creating interventions focused on environmentally sustainable health systems, including nurses, and the scarcity of research on this topic, framed on the Sustainable Development Goals. Methods: A bibliometric analysis was carried out, via three databases (Web of Science, Scopus, and Pubmed), and the guideline recommendations were followed to select bibliometric data. Results: The search resulted in 159 publications, significantly increasing the trends from 2017 to 2021 (p0.028). The most relevant countries in this area were the United States of America, the United Kingdom, and Sweden. Also, the top articles were from relevant journals, indexed in Journal Citation Report, and the first and the second quartile linked to the nursing field and citations (p0.001). Conclusion: Education is key to achieving environmentally sustainable health systems via institutions and policies. Implications for nursing management: There is a lack of experimental data and policies on achieving or maintaining environmentally sustainable health care systems, indicating that nurses have an important role and should be consulted and included in decision-making policies regarding sustainability in the healthcare systems.",Environment
Dynamic Grouping for Climate Change Negotiation: Facilitating Cooperation and Balancing Interests through Effective Strategies,"The current framework for climate change negotiation models presents several limitations that warrant further research and development. In this track, we discuss mainly two key areas for improvement, focusing on the geographical impacts and utility framework. In the aspects of geographical impacts, We explore five critical aspects: (1) the shift from local to global impact, (2) variability in climate change effects across regions, (3) heterogeneity in geographical location and political structures, and (4) collaborations between adjacent nations, (5) the importance of including historical and cultural factors influencing climate negotiations. Furthermore, we emphasize the need to refine the utility and rewards framework to reduce the homogeneity and the level of overestimating the climate mitigation by integrating the positive effects of saving rates into the reward function and heterogeneity among all regions. By addressing these limitations, we hope to enhance the accuracy and effectiveness of climate change negotiation models, enabling policymakers and stakeholders to devise targeted and appropriate strategies to tackle climate change at both regional and global levels.","Dynamic Grouping for Climate Change Negotiation: Facilitating Cooperation and Balancing Interests through Effective Strategies The current framework for climate change negotiation models presents several limitations that warrant further research and development. In this track, we discuss mainly two key areas for improvement, focusing on the geographical impacts and utility framework. In the aspects of geographical impacts, We explore five critical aspects: (1) the shift from local to global impact, (2) variability in climate change effects across regions, (3) heterogeneity in geographical location and political structures, and (4) collaborations between adjacent nations, (5) the importance of including historical and cultural factors influencing climate negotiations. Furthermore, we emphasize the need to refine the utility and rewards framework to reduce the homogeneity and the level of overestimating the climate mitigation by integrating the positive effects of saving rates into the reward function and heterogeneity among all regions. By addressing these limitations, we hope to enhance the accuracy and effectiveness of climate change negotiation models, enabling policymakers and stakeholders to devise targeted and appropriate strategies to tackle climate change at both regional and global levels.",Environment
A Stochastic Geometry Based Techno-Economic Analysis of RIS-Assisted Cellular Networks,"Reconfigurable intelligent surfaces (RISs) are a promising technology for enhancing cellular network performance and yielding additional value to network operators. This paper proposes a techno-economic analysis of RIS-assisted cellular networks to guide operators in deciding between deploying additional RISs or base stations (BS). We assume a relative cost model that considers the total cost of ownership (TCO) of deploying additional nodes, either BSs or RISs. We assume a return on investment (RoI) that is proportional to the systems spectral efficiency. The latter is evaluated based on a stochastic geometry model that gives an integral formula for the ergodic rate in cellular networks equipped with RISs. The marginal RoI for any investment strategy is determined by the partial derivative of this integral expression with respect to node densities. We investigate two case studies: throughput enhancement and coverage hole mitigation. These examples demonstrate how operators could determine the optimal investment strategy in scenarios defined by the current densities of BSs and RISs, and their relative costs. Numerical results illustrate the evolution of ergodic rates based on the proposed investment strategy, demonstrating the investment decision-making process while considering technological and economic factors. This work quantitatively demonstrates that strategically investing in RISs can offer better system-level benefits than solely investing in BS densification.","A Stochastic Geometry Based Techno-Economic Analysis of RIS-Assisted Cellular Networks Reconfigurable intelligent surfaces (RISs) are a promising technology for enhancing cellular network performance and yielding additional value to network operators. This paper proposes a techno-economic analysis of RIS-assisted cellular networks to guide operators in deciding between deploying additional RISs or base stations (BS). We assume a relative cost model that considers the total cost of ownership (TCO) of deploying additional nodes, either BSs or RISs. We assume a return on investment (RoI) that is proportional to the systems spectral efficiency. The latter is evaluated based on a stochastic geometry model that gives an integral formula for the ergodic rate in cellular networks equipped with RISs. The marginal RoI for any investment strategy is determined by the partial derivative of this integral expression with respect to node densities. We investigate two case studies: throughput enhancement and coverage hole mitigation. These examples demonstrate how operators could determine the optimal investment strategy in scenarios defined by the current densities of BSs and RISs, and their relative costs. Numerical results illustrate the evolution of ergodic rates based on the proposed investment strategy, demonstrating the investment decision-making process while considering technological and economic factors. This work quantitatively demonstrates that strategically investing in RISs can offer better system-level benefits than solely investing in BS densification.",Finance
Arbitrage strategy,"An arbitrage strategy allows a financial agent to make certain profit out of nothing, i.e., out of zero initial investment. This has to be disallowed on economic basis if the market is in equilibrium state, as opportunities for riskless profit would result in an instantaneous movement of prices of certain financial instruments. The principle of not allowing for arbitrage opportunities in financial markets has far-reaching consequences, most notably the option-pricing and hedging formulas in complete markets.","Arbitrage strategy An arbitrage strategy allows a financial agent to make certain profit out of nothing, i.e., out of zero initial investment. This has to be disallowed on economic basis if the market is in equilibrium state, as opportunities for riskless profit would result in an instantaneous movement of prices of certain financial instruments. The principle of not allowing for arbitrage opportunities in financial markets has far-reaching consequences, most notably the option-pricing and hedging formulas in complete markets.",Finance
Does Environmental Attention by Governments Promote Carbon Reductions,"The carbon-reducing effect of attention is scarcer than that of material resources, and when the government focuses its attention on the environment, resources will be allocated in a direction that is conducive to reducing carbon. Using panel data from 30 Chinese provinces from 2007 to 2019, this study revealed the impact of governments environmental attention on carbon emissions and the synergistic mechanism between governments environmental attention and informatization level. The findings suggested that (1)the environmental attention index of local governments in China showed an overall fluctuating upward trend; (2)governments environmental atten-tion had the effect of reducing carbon emissions; (3)the emission-reducing effect of governments environmental attention is more significant in the western region but not in the central and eastern regions; (4)informatization level plays a positive moderating role in the relationship between governments environmental attention and carbon emissions; (5)there is a significant threshold effect on the carbon reduction effect of governments environmental attention. Based on the findings, this study proposed policy implications from the perspectives of promoting the sustainable enhancement of environmental attention, bringing institutional functions into play, emphasizing the ecological benefits and strengthening the disclosure of information.","Does Environmental Attention by Governments Promote Carbon Reductions The carbon-reducing effect of attention is scarcer than that of material resources, and when the government focuses its attention on the environment, resources will be allocated in a direction that is conducive to reducing carbon. Using panel data from 30 Chinese provinces from 2007 to 2019, this study revealed the impact of governments environmental attention on carbon emissions and the synergistic mechanism between governments environmental attention and informatization level. The findings suggested that (1)the environmental attention index of local governments in China showed an overall fluctuating upward trend; (2)governments environmental atten-tion had the effect of reducing carbon emissions; (3)the emission-reducing effect of governments environmental attention is more significant in the western region but not in the central and eastern regions; (4)informatization level plays a positive moderating role in the relationship between governments environmental attention and carbon emissions; (5)there is a significant threshold effect on the carbon reduction effect of governments environmental attention. Based on the findings, this study proposed policy implications from the perspectives of promoting the sustainable enhancement of environmental attention, bringing institutional functions into play, emphasizing the ecological benefits and strengthening the disclosure of information.",Environment
Multilevel Monte Carlo methods for applications in finance,"Since Giles introduced the multilevel Monte Carlo path simulation method 18, there has been rapid development of the technique for a variety of applications in computational finance. This paper surveys the progress so far, highlights the key features in achieving a high rate of multilevel variance convergence, and suggests directions for future research.","Multilevel Monte Carlo methods for applications in finance Since Giles introduced the multilevel Monte Carlo path simulation method 18, there has been rapid development of the technique for a variety of applications in computational finance. This paper surveys the progress so far, highlights the key features in achieving a high rate of multilevel variance convergence, and suggests directions for future research.",Finance
Business fluctuations in a credit-network economy,"We model a network economy with three sectors: downstream firms, upstream firms, and banks. Agents are linked by productive and credit relationships so that the behavior of one agent influences the behavior of the others through network connections. Credit interlinkages among agents are a source of bankruptcy diffusion: in fact, failure of fulfilling debt commitments would lead to bankruptcy chains. All in all, the bankruptcy in one sector can diffuse to other sectors through linkages creating a vicious cycle and bankruptcy avalanches in the network economy. Our analysis show how the choices of credit supply by both banks and firms are interrelated. While the initial impact of monetary policy is on bank behaviour, we show the interactive play between the choices made by banks, the choices made by firms in their role as providers of credit, and the choices made by firms in their role as producers.","Business fluctuations in a credit-network economy We model a network economy with three sectors: downstream firms, upstream firms, and banks. Agents are linked by productive and credit relationships so that the behavior of one agent influences the behavior of the others through network connections. Credit interlinkages among agents are a source of bankruptcy diffusion: in fact, failure of fulfilling debt commitments would lead to bankruptcy chains. All in all, the bankruptcy in one sector can diffuse to other sectors through linkages creating a vicious cycle and bankruptcy avalanches in the network economy. Our analysis show how the choices of credit supply by both banks and firms are interrelated. While the initial impact of monetary policy is on bank behaviour, we show the interactive play between the choices made by banks, the choices made by firms in their role as providers of credit, and the choices made by firms in their role as producers.",Finance
Mathematical Model of Colorectal Cancer with Monoclonal Antibody Treatments,"We present a new mathematical model of colorectal cancer growth and its response to monoclonal-antibody (mAb) therapy. Although promising, most mAb drugs are still in trial phases, and the possible variations in the dosing schedules of those currently approved for use have not yet been thoroughly explored. To investigate the effectiveness of current mAb treatment schedules, and to test hypothetical treatment strategies, we have created a system of nonlinear ordinary differential equations (ODE) to model colorectal cancer growth and treatment. The model includes tumor cells, elements of the hosts immune response, and treatments. Model treatments include the chemotherapy agent irinotecan and one of two monoclonal antibodies - cetuximab, which is FDA-approved for colorectal cancer, and panitumumab, which is still being evaluated in clinical trials. The model incorporates patient-specific parameters to account for individual variations in immune system strength and in medication efficacy against the tumor. We have simulated outcomes for groups of virtual patients on treatment protocols for which clinical trial data are available, using a range of biologically reasonable patient-specific parameter values. Our results closely match clinical trial results for these protocols. We also simulated experimental dosing schedules, and have found new schedules which, in our simulations, reduce tumor size more effectively than current treatment schedules. Additionally, we examined the systems equilibria and sensitivity to parameter values. In the absence of treatment, tumor evolution is most affected by the intrinsic tumor growth rate and carrying capacity. When treatment is introduced, tumor growth is most affected by drug-specific PKPD parameters.","Mathematical Model of Colorectal Cancer with Monoclonal Antibody Treatments We present a new mathematical model of colorectal cancer growth and its response to monoclonal-antibody (mAb) therapy. Although promising, most mAb drugs are still in trial phases, and the possible variations in the dosing schedules of those currently approved for use have not yet been thoroughly explored. To investigate the effectiveness of current mAb treatment schedules, and to test hypothetical treatment strategies, we have created a system of nonlinear ordinary differential equations (ODE) to model colorectal cancer growth and treatment. The model includes tumor cells, elements of the hosts immune response, and treatments. Model treatments include the chemotherapy agent irinotecan and one of two monoclonal antibodies - cetuximab, which is FDA-approved for colorectal cancer, and panitumumab, which is still being evaluated in clinical trials. The model incorporates patient-specific parameters to account for individual variations in immune system strength and in medication efficacy against the tumor. We have simulated outcomes for groups of virtual patients on treatment protocols for which clinical trial data are available, using a range of biologically reasonable patient-specific parameter values. Our results closely match clinical trial results for these protocols. We also simulated experimental dosing schedules, and have found new schedules which, in our simulations, reduce tumor size more effectively than current treatment schedules. Additionally, we examined the systems equilibria and sensitivity to parameter values. In the absence of treatment, tumor evolution is most affected by the intrinsic tumor growth rate and carrying capacity. When treatment is introduced, tumor growth is most affected by drug-specific PKPD parameters.",Healthcare
Sustainable Multi-Modal Transportation and Routing focusing on Costs and Carbon Emissions Reduction,"Transportation plays a critical role in supply chain networks, directly impacting cost efficiency, delivery reliability, and environmental sustainability. This study provides an enhanced optimization model for transportation planning, emphasizing environmental sustainability and cost-efficiency. An Integer Linear Programming (ILP) model was developed to minimize the total transportation costs by considering organizational and third-party vehicles operational and rental costs while incorporating constraints on carbon emissions. The model incorporates multi-modal transportation routing and emission caps to select the optimized number of organizational and rental vehicles of different modes in each route to ensure adherence to sustainability goals. Key innovations include adding carbon emission constraints and optimizing route selection to reduce overall emissions. The model was implemented using the Gurobi solver, and numerical analysis reveals a trade-off between cost minimization and carbon footprint reduction. The results indicate that adopting tight environmental policies increases the costs by around 8 on average while more than 95 of the vehicles utilized will be rented. These insights provide actionable guidance for industries aiming to enhance both economic performance and environmental responsibility.","Sustainable Multi-Modal Transportation and Routing focusing on Costs and Carbon Emissions Reduction Transportation plays a critical role in supply chain networks, directly impacting cost efficiency, delivery reliability, and environmental sustainability. This study provides an enhanced optimization model for transportation planning, emphasizing environmental sustainability and cost-efficiency. An Integer Linear Programming (ILP) model was developed to minimize the total transportation costs by considering organizational and third-party vehicles operational and rental costs while incorporating constraints on carbon emissions. The model incorporates multi-modal transportation routing and emission caps to select the optimized number of organizational and rental vehicles of different modes in each route to ensure adherence to sustainability goals. Key innovations include adding carbon emission constraints and optimizing route selection to reduce overall emissions. The model was implemented using the Gurobi solver, and numerical analysis reveals a trade-off between cost minimization and carbon footprint reduction. The results indicate that adopting tight environmental policies increases the costs by around 8 on average while more than 95 of the vehicles utilized will be rented. These insights provide actionable guidance for industries aiming to enhance both economic performance and environmental responsibility.",Environment
"Characteristics of Aerosol Spectral Optical Depths over Manora Peak, Nainital - A High Altitude Station in the Central Himalayas","We present, for the first time, spectral behaviour of aerosol optical depths (AODs) over Manora Peak, Nainital located at an altitude of sim 2 km in the Shivalik ranges of central Himalayas. The observations were carried out using a Multi-Wavelength solar Radiometer during January to December 2002. The main results of the study are extremely low AODs during winter, a remarkable increase to high values in summer and a distinct change in the spectral dependencies of AODs from a relatively steeper spectra during winter to a shallower one in summer. During transparent days, the AOD values lie usually below 0.08 while during dusty (turbid) days, it lies between 0.08 to 0.69 at 0.5 mum. The average AOD value at 0.5 mum during winters, particularly in January and February, is sim 0.03pm0.01. The mean aerosol extinction law at Manora Peak during 2002 is best represented by 0.10 lambda-0.61. However during transparent days, which almost covers 40 of the time, it is represented by 0.02 lambda-0.97. This value of wavelength exponent, representing reduced coarse concentration and presence of fine aerosols, indicates that the station measures aerosol in the free troposphere at least during part of the year.","Characteristics of Aerosol Spectral Optical Depths over Manora Peak, Nainital - A High Altitude Station in the Central Himalayas We present, for the first time, spectral behaviour of aerosol optical depths (AODs) over Manora Peak, Nainital located at an altitude of sim 2 km in the Shivalik ranges of central Himalayas. The observations were carried out using a Multi-Wavelength solar Radiometer during January to December 2002. The main results of the study are extremely low AODs during winter, a remarkable increase to high values in summer and a distinct change in the spectral dependencies of AODs from a relatively steeper spectra during winter to a shallower one in summer. During transparent days, the AOD values lie usually below 0.08 while during dusty (turbid) days, it lies between 0.08 to 0.69 at 0.5 mum. The average AOD value at 0.5 mum during winters, particularly in January and February, is sim 0.03pm0.01. The mean aerosol extinction law at Manora Peak during 2002 is best represented by 0.10 lambda-0.61. However during transparent days, which almost covers 40 of the time, it is represented by 0.02 lambda-0.97. This value of wavelength exponent, representing reduced coarse concentration and presence of fine aerosols, indicates that the station measures aerosol in the free troposphere at least during part of the year.",Environment
Modelling and predicting labor force productivity,"Labor productivity in Turkey, Spain, Belgium, Austria, Switzerland, and New Zealand has been analyzed and modeled. These counties extend the previously analyzed set of the US, UK, Japan, France, Italy, and Canada. Modelling is based on the link between the rate of labor participation and real GDP per capita. New results validate the link and allow predicting a drop in productivity by 2010 in almost all studied countries.","Modelling and predicting labor force productivity Labor productivity in Turkey, Spain, Belgium, Austria, Switzerland, and New Zealand has been analyzed and modeled. These counties extend the previously analyzed set of the US, UK, Japan, France, Italy, and Canada. Modelling is based on the link between the rate of labor participation and real GDP per capita. New results validate the link and allow predicting a drop in productivity by 2010 in almost all studied countries.",Finance
Kernel methods in genomics and computational biology,"Support vector machines and kernel methods are increasingly popular in genomics and computational biology, due to their good performance in real-world applications and strong modularity that makes them suitable to a wide range of problems, from the classification of tumors to the automatic annotation of proteins. Their ability to work in high dimension, to process non-vectorial data, and the natural framework they provide to integrate heterogeneous data are particularly relevant to various problems arising in computational biology. In this chapter we survey some of the most prominent applications published so far, highlighting the particular developments in kernel methods triggered by problems in biology, and mention a few promising research directions likely to expand in the future.","Kernel methods in genomics and computational biology Support vector machines and kernel methods are increasingly popular in genomics and computational biology, due to their good performance in real-world applications and strong modularity that makes them suitable to a wide range of problems, from the classification of tumors to the automatic annotation of proteins. Their ability to work in high dimension, to process non-vectorial data, and the natural framework they provide to integrate heterogeneous data are particularly relevant to various problems arising in computational biology. In this chapter we survey some of the most prominent applications published so far, highlighting the particular developments in kernel methods triggered by problems in biology, and mention a few promising research directions likely to expand in the future.",Healthcare
Electronic Health Records and Cloud based Generic Medical Equipment Interface,"Now-a-days Health Care industry is well equipped with Medical Equipments to provide accurate and timely reports of investigation and examination results. Medical Equipments available in market are made for specific tests suited for a particular laboratory leading to a wide variety of devices. The result viewing experience on console of these devices is not only cumborsome for medical staff but inefficient. Therefore, Medical Equipment Interfaces act as backbone of any Hospital Management Information System assisting in better management and delivery of test results. It also acts as a mode to collect data for further research and analysis. These equipments communicate via a fixed data format but compatibility among these formats is a major issue being faced in modern and legacy medical equipments. In this paper, we present a case study of designing and implementing a cloud based Generic Medical Equipment Interface(GMEI) along with the state of the art in such systems. This solution removes the burden of reentry of patient details into the Electronic Health Record(EHR) and thrives for accelerating EMR initiative in the country","Electronic Health Records and Cloud based Generic Medical Equipment Interface Now-a-days Health Care industry is well equipped with Medical Equipments to provide accurate and timely reports of investigation and examination results. Medical Equipments available in market are made for specific tests suited for a particular laboratory leading to a wide variety of devices. The result viewing experience on console of these devices is not only cumborsome for medical staff but inefficient. Therefore, Medical Equipment Interfaces act as backbone of any Hospital Management Information System assisting in better management and delivery of test results. It also acts as a mode to collect data for further research and analysis. These equipments communicate via a fixed data format but compatibility among these formats is a major issue being faced in modern and legacy medical equipments. In this paper, we present a case study of designing and implementing a cloud based Generic Medical Equipment Interface(GMEI) along with the state of the art in such systems. This solution removes the burden of reentry of patient details into the Electronic Health Record(EHR) and thrives for accelerating EMR initiative in the country",Healthcare
Exploiting Heavy Tails in Training Times of Multilayer Perceptrons: A Case Study with the UCI Thyroid Disease Database,"The random initialization of weights of a multilayer perceptron makes it possible to model its training process as a Las Vegas algorithm, i.e. a randomized algorithm which stops when some required training error is obtained, and whose execution time is a random variable. This modeling is used to perform a case study on a well-known pattern recognition benchmark: the UCI Thyroid Disease Database. Empirical evidence is presented of the training time probability distribution exhibiting a heavy tail behavior, meaning a big probability mass of long executions. This fact is exploited to reduce the training time cost by applying two simple restart strategies. The first assumes full knowledge of the distribution yielding a 40 cut down in expected time with respect to the training without restarts. The second, assumes null knowledge, yielding a reduction ranging from 9 to 23.","Exploiting Heavy Tails in Training Times of Multilayer Perceptrons: A Case Study with the UCI Thyroid Disease Database The random initialization of weights of a multilayer perceptron makes it possible to model its training process as a Las Vegas algorithm, i.e. a randomized algorithm which stops when some required training error is obtained, and whose execution time is a random variable. This modeling is used to perform a case study on a well-known pattern recognition benchmark: the UCI Thyroid Disease Database. Empirical evidence is presented of the training time probability distribution exhibiting a heavy tail behavior, meaning a big probability mass of long executions. This fact is exploited to reduce the training time cost by applying two simple restart strategies. The first assumes full knowledge of the distribution yielding a 40 cut down in expected time with respect to the training without restarts. The second, assumes null knowledge, yielding a reduction ranging from 9 to 23.",Technology
Evaluation of clinical utility in emulated clinical trials,"Dynamic treatment regimes have been proposed to personalize treatment decisions by utilizing historical patient data, but optimization can only be done over information available in the database. In contrast, the standard of care or physicians decisions may be complex algorithms based on information that is not available to researchers. It is thus meaningful to integrate the standard of care into the evaluation of treatment strategies, and previous works have suggested doing so through the concept of clinical utility. Here we will focus on the comparative component of clinical utility as the average outcome had the full population received treatment based on the proposed dynamic treatment regime in comparison to the full population receiving the standard treatment assignment mechanism, such as a physicians choice. Clinical trials to evaluate clinical utility are rarely conducted, and thus, previous works have proposed an emulated clinical trial framework using observational data. However, only one simple estimator was previously suggested, and the practical details of how one would conduct this emulated trial were not detailed. Here, we illuminate these details and propose several estimators of clinical utility based on estimators proposed in the dynamic treatment regime literature. We illustrate the considerations and the estimators in a real data example investigating treatment rules for rheumatoid arthritis, where we highlight that in addition to the standard of care, the current medical guidelines should also be compared to any optimal decision rule.","Evaluation of clinical utility in emulated clinical trials Dynamic treatment regimes have been proposed to personalize treatment decisions by utilizing historical patient data, but optimization can only be done over information available in the database. In contrast, the standard of care or physicians decisions may be complex algorithms based on information that is not available to researchers. It is thus meaningful to integrate the standard of care into the evaluation of treatment strategies, and previous works have suggested doing so through the concept of clinical utility. Here we will focus on the comparative component of clinical utility as the average outcome had the full population received treatment based on the proposed dynamic treatment regime in comparison to the full population receiving the standard treatment assignment mechanism, such as a physicians choice. Clinical trials to evaluate clinical utility are rarely conducted, and thus, previous works have proposed an emulated clinical trial framework using observational data. However, only one simple estimator was previously suggested, and the practical details of how one would conduct this emulated trial were not detailed. Here, we illuminate these details and propose several estimators of clinical utility based on estimators proposed in the dynamic treatment regime literature. We illustrate the considerations and the estimators in a real data example investigating treatment rules for rheumatoid arthritis, where we highlight that in addition to the standard of care, the current medical guidelines should also be compared to any optimal decision rule.",Healthcare
An Empirical Analysis of Search in GSAT,"We describe an extensive study of search in GSAT, an approximation procedure for propositional satisfiability. GSAT performs greedy hill-climbing on the number of satisfied clauses in a truth assignment. Our experiments provide a more complete picture of GSATs search than previous accounts. We describe in detail the two phases of search: rapid hill-climbing followed by a long plateau search. We demonstrate that when applied to randomly generated 3SAT problems, there is a very simple scaling with problem size for both the mean number of satisfied clauses and the mean branching rate. Our results allow us to make detailed numerical conjectures about the length of the hill-climbing phase, the average gradient of this phase, and to conjecture that both the average score and average branching rate decay exponentially during plateau search. We end by showing how these results can be used to direct future theoretical analysis. This work provides a case study of how computer experiments can be used to improve understanding of the theoretical properties of algorithms.","An Empirical Analysis of Search in GSAT We describe an extensive study of search in GSAT, an approximation procedure for propositional satisfiability. GSAT performs greedy hill-climbing on the number of satisfied clauses in a truth assignment. Our experiments provide a more complete picture of GSATs search than previous accounts. We describe in detail the two phases of search: rapid hill-climbing followed by a long plateau search. We demonstrate that when applied to randomly generated 3SAT problems, there is a very simple scaling with problem size for both the mean number of satisfied clauses and the mean branching rate. Our results allow us to make detailed numerical conjectures about the length of the hill-climbing phase, the average gradient of this phase, and to conjecture that both the average score and average branching rate decay exponentially during plateau search. We end by showing how these results can be used to direct future theoretical analysis. This work provides a case study of how computer experiments can be used to improve understanding of the theoretical properties of algorithms.",Technology
Teleo-Reactive Programs for Agent Control,"A formalism is presented for computing and organizing actions for autonomous agents in dynamic environments. We introduce the notion of teleo-reactive (T-R) programs whose execution entails the construction of circuitry for the continuous computation of the parameters and conditions on which agent action is based. In addition to continuous feedback, T-R programs support parameter binding and recursion. A primary difference between T-R programs and many other circuit-based systems is that the circuitry of T-R programs is more compact; it is constructed at run time and thus does not have to anticipate all the contingencies that might arise over all possible runs. In addition, T-R programs are intuitive and easy to write and are written in a form that is compatible with automatic planning and learning methods. We briefly describe some experimental applications of T-R programs in the control of simulated and actual mobile robots.","Teleo-Reactive Programs for Agent Control A formalism is presented for computing and organizing actions for autonomous agents in dynamic environments. We introduce the notion of teleo-reactive (T-R) programs whose execution entails the construction of circuitry for the continuous computation of the parameters and conditions on which agent action is based. In addition to continuous feedback, T-R programs support parameter binding and recursion. A primary difference between T-R programs and many other circuit-based systems is that the circuitry of T-R programs is more compact; it is constructed at run time and thus does not have to anticipate all the contingencies that might arise over all possible runs. In addition, T-R programs are intuitive and easy to write and are written in a form that is compatible with automatic planning and learning methods. We briefly describe some experimental applications of T-R programs in the control of simulated and actual mobile robots.",Technology
Assessing the risk from the depleted uranium weapons used in Operation Allied Force,"The conflict in Yugoslavia has been a source of great concern due to the radiological and toxic hazard posed by the alleged presence of depleted uranium in NATO weapons. In the present study some worst-case scenaria are assumed in order to assess the risk for Yugoslavia and its neighboring countries . The risk is proved to be negligible for the neighboring countries while for Yugoslavia itself evidence is given that any increase in total long-term cancer mortality will be so low that it will remain undetected. Local radioactive hotspots such as DU weapons fragments and abandoned battle tanks, fortified or contaminated with DU, constitute a post-war hazard which is not studied in this article.","Assessing the risk from the depleted uranium weapons used in Operation Allied Force The conflict in Yugoslavia has been a source of great concern due to the radiological and toxic hazard posed by the alleged presence of depleted uranium in NATO weapons. In the present study some worst-case scenaria are assumed in order to assess the risk for Yugoslavia and its neighboring countries . The risk is proved to be negligible for the neighboring countries while for Yugoslavia itself evidence is given that any increase in total long-term cancer mortality will be so low that it will remain undetected. Local radioactive hotspots such as DU weapons fragments and abandoned battle tanks, fortified or contaminated with DU, constitute a post-war hazard which is not studied in this article.",Healthcare
A Multiperiod OPF Model Under Renewable Generation Uncertainty and Demand Side Flexibility,"Renewable energy sources such as wind and solar have received much attention in recent years and large amount of renewable generation is being integrated to the electricity networks. A fundamental challenge in power system operation is to handle the intermittent nature of the renewable generation. In this paper we present a stochastic programming approach to solve a multiperiod optimal power flow problem under renewable generation uncertainty. The proposed approach consists of two stages. In the first stage operating points for conventional power plants are determined. Second stage realizes the generation from renewable resources and optimally accommodates it by relying on demand-side flexibility. The benefits from its application are demonstrated and discussed on a 4-bus and a 39-bus systems. Numerical results show that with limited flexibility on the demand-side substantial benefits in terms of potential additional re-dispatch costs can be achieved. The scaling properties of the approach are finally analysed based on standard IEEE test cases upto 300 buses, allowing to underlined its computational efficiency.","A Multiperiod OPF Model Under Renewable Generation Uncertainty and Demand Side Flexibility Renewable energy sources such as wind and solar have received much attention in recent years and large amount of renewable generation is being integrated to the electricity networks. A fundamental challenge in power system operation is to handle the intermittent nature of the renewable generation. In this paper we present a stochastic programming approach to solve a multiperiod optimal power flow problem under renewable generation uncertainty. The proposed approach consists of two stages. In the first stage operating points for conventional power plants are determined. Second stage realizes the generation from renewable resources and optimally accommodates it by relying on demand-side flexibility. The benefits from its application are demonstrated and discussed on a 4-bus and a 39-bus systems. Numerical results show that with limited flexibility on the demand-side substantial benefits in terms of potential additional re-dispatch costs can be achieved. The scaling properties of the approach are finally analysed based on standard IEEE test cases upto 300 buses, allowing to underlined its computational efficiency.",Environment
The Gap between Higher Education and the Software Industry -- A Case Study on Technology Differences,"We see an explosive global labour demand in the Software Industry, and higher education institutions play a crucial role in supplying the industry with professionals with relevant education. Existing literature identifies a gap between what software engineering education teaches students and what the software industry demands. Using our open-sourced Job Market AnalyseR (JMAR) text-analysis tool, we compared keywords from higher education course syllabi and job posts to investigate the knowledge gap from a technology-focused departure point. We present a trend analysis of technology in job posts over the past six years in Sweden. We found that demand for cloud and automation technology such as Kubernetes and Docker is rising in job ads but not that much in higher education syllabi. The language used in higher education syllabi and job ads differs where the former emphasizes concepts and the latter technologies more heavily. We discuss possible remedies to bridge this mismatch to draw further conclusions in future work, including calibrating JMAR to other industry-relevant aspects, including soft skills, software concepts, or new demographics.","The Gap between Higher Education and the Software Industry -- A Case Study on Technology Differences We see an explosive global labour demand in the Software Industry, and higher education institutions play a crucial role in supplying the industry with professionals with relevant education. Existing literature identifies a gap between what software engineering education teaches students and what the software industry demands. Using our open-sourced Job Market AnalyseR (JMAR) text-analysis tool, we compared keywords from higher education course syllabi and job posts to investigate the knowledge gap from a technology-focused departure point. We present a trend analysis of technology in job posts over the past six years in Sweden. We found that demand for cloud and automation technology such as Kubernetes and Docker is rising in job ads but not that much in higher education syllabi. The language used in higher education syllabi and job ads differs where the former emphasizes concepts and the latter technologies more heavily. We discuss possible remedies to bridge this mismatch to draw further conclusions in future work, including calibrating JMAR to other industry-relevant aspects, including soft skills, software concepts, or new demographics.",Education
PARIS Interferometric Processor Analysis and Experimental Results: Theoretical Feasibility Analysis,"Several experimental results show that it is possible to extract useful phase information from reflected GPS signals over the oceans. In this work we begin the development of the theoretical background to account for these results and fully understand the phenomena involved. This information will then be used to define and carry out new experiments to evaluate the feasibility of using the phase from reflected GPS signals for altimetric purposes and the advantages of using interferometric combinations of the signals at different frequencies---the PIP concept. We focus on the coherence properties of the signals, including the PIP interferometric combination of phases in the different frequencies. In this work we will concentrate on a static, 8 m high receiver (at least in regards to the simulations), and an infinitely removed static source. As the ocean moves, the received field will pick up a random phase. We want to understand the behavior of this phase, as the goal is to carry out altimetric measurements using phase ranging. We will also show that this random phase carries geophysical information (intuitively, the bigger the significant wave height, the larger the phase excursions).","PARIS Interferometric Processor Analysis and Experimental Results: Theoretical Feasibility Analysis Several experimental results show that it is possible to extract useful phase information from reflected GPS signals over the oceans. In this work we begin the development of the theoretical background to account for these results and fully understand the phenomena involved. This information will then be used to define and carry out new experiments to evaluate the feasibility of using the phase from reflected GPS signals for altimetric purposes and the advantages of using interferometric combinations of the signals at different frequencies---the PIP concept. We focus on the coherence properties of the signals, including the PIP interferometric combination of phases in the different frequencies. In this work we will concentrate on a static, 8 m high receiver (at least in regards to the simulations), and an infinitely removed static source. As the ocean moves, the received field will pick up a random phase. We want to understand the behavior of this phase, as the goal is to carry out altimetric measurements using phase ranging. We will also show that this random phase carries geophysical information (intuitively, the bigger the significant wave height, the larger the phase excursions).",Environment
A K-means Algorithm for Financial Market Risk Forecasting,"Financial market risk forecasting involves applying mathematical models, historical data analysis and statistical methods to estimate the impact of future market movements on investments. This process is crucial for investors to develop strategies, financial institutions to manage assets and regulators to formulate policy. In todays society, there are problems of high error rate and low precision in financial market risk prediction, which greatly affect the accuracy of financial market risk prediction. K-means algorithm in machine learning is an effective risk prediction technique for financial market. This study uses K-means algorithm to develop a financial market risk prediction system, which significantly improves the accuracy and efficiency of financial market risk prediction. Ultimately, the outcomes of the experiments confirm that the K-means algorithm operates with user-friendly simplicity and achieves a 94.61 accuracy rate","A K-means Algorithm for Financial Market Risk Forecasting Financial market risk forecasting involves applying mathematical models, historical data analysis and statistical methods to estimate the impact of future market movements on investments. This process is crucial for investors to develop strategies, financial institutions to manage assets and regulators to formulate policy. In todays society, there are problems of high error rate and low precision in financial market risk prediction, which greatly affect the accuracy of financial market risk prediction. K-means algorithm in machine learning is an effective risk prediction technique for financial market. This study uses K-means algorithm to develop a financial market risk prediction system, which significantly improves the accuracy and efficiency of financial market risk prediction. Ultimately, the outcomes of the experiments confirm that the K-means algorithm operates with user-friendly simplicity and achieves a 94.61 accuracy rate",Finance
A dissipative particle swarm optimization,A dissipative particle swarm optimization is developed according to the self-organization of dissipative structure. The negative entropy is introduced to construct an opening dissipative system that is far-from-equilibrium so as to driving the irreversible evolution process with better fitness. The testing of two multimodal functions indicates it improves the performance effectively,A dissipative particle swarm optimization A dissipative particle swarm optimization is developed according to the self-organization of dissipative structure. The negative entropy is introduced to construct an opening dissipative system that is far-from-equilibrium so as to driving the irreversible evolution process with better fitness. The testing of two multimodal functions indicates it improves the performance effectively,Technology
Discovering Knowledge from Multi-modal Lecture Recordings,"Educational media mining is the process of converting raw media data from educational systems to useful information that can be used to design learning systems, answer research questions and allow personalized learning experiences. Knowledge discovery encompasses a wide range of techniques ranging from database queries to more recent developments in machine learning and language technology. Educational media mining techniques are now being used in IT Services research worldwide. Multi-modal Lecture Recordings is one of the important types of educational media and this paper explores the research challenges for mining lecture recordings for the efficient personalized learning experiences. Keywords: Educational Media Mining; Lecture Recordings, Multimodal Information System, Personalized Learning; Online Course Ware; Skills and Competences;","Discovering Knowledge from Multi-modal Lecture Recordings Educational media mining is the process of converting raw media data from educational systems to useful information that can be used to design learning systems, answer research questions and allow personalized learning experiences. Knowledge discovery encompasses a wide range of techniques ranging from database queries to more recent developments in machine learning and language technology. Educational media mining techniques are now being used in IT Services research worldwide. Multi-modal Lecture Recordings is one of the important types of educational media and this paper explores the research challenges for mining lecture recordings for the efficient personalized learning experiences. Keywords: Educational Media Mining; Lecture Recordings, Multimodal Information System, Personalized Learning; Online Course Ware; Skills and Competences;",Education
Provably Bounded-Optimal Agents,"Since its inception, artificial intelligence has relied upon a theoretical foundation centered around perfect rationality as the desired property of intelligent systems. We argue, as others have done, that this foundation is inadequate because it imposes fundamentally unsatisfiable requirements. As a result, there has arisen a wide gap between theory and practice in AI, hindering progress in the field. We propose instead a property called bounded optimality. Roughly speaking, an agent is bounded-optimal if its program is a solution to the constrained optimization problem presented by its architecture and the task environment. We show how to construct agents with this property for a simple class of machine architectures in a broad class of real-time environments. We illustrate these results using a simple model of an automated mail sorting facility. We also define a weaker property, asymptotic bounded optimality (ABO), that generalizes the notion of optimality in classical complexity theory. We then construct universal ABO programs, i.e., programs that are ABO no matter what real-time constraints are applied. Universal ABO programs can be used as building blocks for more complex systems. We conclude with a discussion of the prospects for bounded optimality as a theoretical basis for AI, and relate it to similar trends in philosophy, economics, and game theory.","Provably Bounded-Optimal Agents Since its inception, artificial intelligence has relied upon a theoretical foundation centered around perfect rationality as the desired property of intelligent systems. We argue, as others have done, that this foundation is inadequate because it imposes fundamentally unsatisfiable requirements. As a result, there has arisen a wide gap between theory and practice in AI, hindering progress in the field. We propose instead a property called bounded optimality. Roughly speaking, an agent is bounded-optimal if its program is a solution to the constrained optimization problem presented by its architecture and the task environment. We show how to construct agents with this property for a simple class of machine architectures in a broad class of real-time environments. We illustrate these results using a simple model of an automated mail sorting facility. We also define a weaker property, asymptotic bounded optimality (ABO), that generalizes the notion of optimality in classical complexity theory. We then construct universal ABO programs, i.e., programs that are ABO no matter what real-time constraints are applied. Universal ABO programs can be used as building blocks for more complex systems. We conclude with a discussion of the prospects for bounded optimality as a theoretical basis for AI, and relate it to similar trends in philosophy, economics, and game theory.",Technology
Optimizing hydrogen and e-methanol production through Power-to-X integration in biogas plants,"The European Union strategy for net zero emissions relies on developing hydrogen and electro fuels infrastructure. These fuels will be crucial as energy carriers and balancing agents for renewable energy variability. Large scale production requires more renewable capacity, and various Power to X (PtX) concepts are emerging in renewable rich countries. However, sourcing renewable carbon to scale carbon based electro fuels is a significant challenge. This study explores a PtX hub that sources renewable CO2 from biogas plants, integrating renewable energy, hydrogen production, and methanol synthesis on site. This concept creates an internal market for energy and materials, interfacing with the external energy system. The size and operation of the PtX hub were optimized, considering integration with local energy systems and a potential hydrogen grid. The levelized costs of hydrogen and methanol were estimated for a 2030 start, considering new legislation on renewable fuels of non biological origin (RFNBOs). Our results show the PtX hub can rely mainly on on site renewable energy, selling excess electricity to the grid. A local hydrogen grid connection improves operations, and the behind the meter market lowers energy prices, buffering against market variability. We found methanol costs could be below 650 euros per ton and hydrogen production costs below 3 euros per kg, with standalone methanol plants costing 23 per cent more. The CO2 recovery to methanol production ratio is crucial, with over 90 per cent recovery requiring significant investment in CO2 and H2 storage. Overall, our findings support planning PtX infrastructures integrated with the agricultural sector as a cost effective way to access renewable carbon.","Optimizing hydrogen and e-methanol production through Power-to-X integration in biogas plants The European Union strategy for net zero emissions relies on developing hydrogen and electro fuels infrastructure. These fuels will be crucial as energy carriers and balancing agents for renewable energy variability. Large scale production requires more renewable capacity, and various Power to X (PtX) concepts are emerging in renewable rich countries. However, sourcing renewable carbon to scale carbon based electro fuels is a significant challenge. This study explores a PtX hub that sources renewable CO2 from biogas plants, integrating renewable energy, hydrogen production, and methanol synthesis on site. This concept creates an internal market for energy and materials, interfacing with the external energy system. The size and operation of the PtX hub were optimized, considering integration with local energy systems and a potential hydrogen grid. The levelized costs of hydrogen and methanol were estimated for a 2030 start, considering new legislation on renewable fuels of non biological origin (RFNBOs). Our results show the PtX hub can rely mainly on on site renewable energy, selling excess electricity to the grid. A local hydrogen grid connection improves operations, and the behind the meter market lowers energy prices, buffering against market variability. We found methanol costs could be below 650 euros per ton and hydrogen production costs below 3 euros per kg, with standalone methanol plants costing 23 per cent more. The CO2 recovery to methanol production ratio is crucial, with over 90 per cent recovery requiring significant investment in CO2 and H2 storage. Overall, our findings support planning PtX infrastructures integrated with the agricultural sector as a cost effective way to access renewable carbon.",Environment
Lets Learn Step by Step: Enhancing In-Context Learning Ability with Curriculum Learning,"Demonstration ordering, which is an important strategy for in-context learning (ICL), can significantly affects the performance of large language models (LLMs). However, most of the current approaches of ordering require high computational costs to introduce the priori knowledge. In this paper, inspired by the human learning process, we propose a simple but effective demonstration ordering method for ICL, named the few-shot In-Context Curriculum Learning (ICCL). The ICCL implies gradually increasing the complexity of prompt demonstrations during the inference process. The difficulty can be assessed by human experts or LLMs-driven metrics, such as perplexity. Then we design extensive experiments to discuss the effectiveness of the ICCL at both corpus-level and instance-level. Moreover, we also investigate the formation mechanism of LLMs ICCL capability. Experimental results demonstrate that ICCL, developed during the instruction-tuning stage, is effective for representative open-source LLMs. To facilitate further research and applications by other scholars, we make the code publicly available.","Lets Learn Step by Step: Enhancing In-Context Learning Ability with Curriculum Learning Demonstration ordering, which is an important strategy for in-context learning (ICL), can significantly affects the performance of large language models (LLMs). However, most of the current approaches of ordering require high computational costs to introduce the priori knowledge. In this paper, inspired by the human learning process, we propose a simple but effective demonstration ordering method for ICL, named the few-shot In-Context Curriculum Learning (ICCL). The ICCL implies gradually increasing the complexity of prompt demonstrations during the inference process. The difficulty can be assessed by human experts or LLMs-driven metrics, such as perplexity. Then we design extensive experiments to discuss the effectiveness of the ICCL at both corpus-level and instance-level. Moreover, we also investigate the formation mechanism of LLMs ICCL capability. Experimental results demonstrate that ICCL, developed during the instruction-tuning stage, is effective for representative open-source LLMs. To facilitate further research and applications by other scholars, we make the code publicly available.",Education
A note comprising a negative resolution of the Efficient Market Hypothesis,This note comprises a negative resolution of the Efficient Market Hypothesis.,A note comprising a negative resolution of the Efficient Market Hypothesis This note comprises a negative resolution of the Efficient Market Hypothesis.,Finance
Towards responsible AI for education: Hybrid human-AI to confront the Elephant in the room,"Despite significant advancements in AI-driven educational systems and ongoing calls for responsible AI for education, several critical issues remain unresolved -- acting as the elephant in the room within AI in education, learning analytics, educational data mining, learning sciences, and educational psychology communities. This critical analysis identifies and examines nine persistent challenges that continue to undermine the fairness, transparency, and effectiveness of current AI methods and applications in education. These include: (1) the lack of clarity around what AI for education truly means -- often ignoring the distinct purposes, strengths, and limitations of different AI families -- and the trend of equating it with domain-agnostic, company-driven large language models; (2) the widespread neglect of essential learning processes such as motivation, emotion, and (meta)cognition in AI-driven learner modelling and their contextual nature; (3) limited integration of domain knowledge and lack of stakeholder involvement in AI design and development; (4) continued use of non-sequential machine learning models on temporal educational data; (5) misuse of non-sequential metrics to evaluate sequential models; (6) use of unreliable explainable AI methods to provide explanations for black-box models; (7) ignoring ethical guidelines in addressing data inconsistencies during model training; (8) use of mainstream AI methods for pattern discovery and learning analytics without systematic benchmarking; and (9) overemphasis on global prescriptions while overlooking localised, student-specific recommendations. Supported by theoretical and empirical research, we demonstrate how hybrid AI methods -- specifically neural-symbolic AI -- can address the elephant in the room and serve as the foundation for responsible, trustworthy AI systems in education.","Towards responsible AI for education: Hybrid human-AI to confront the Elephant in the room Despite significant advancements in AI-driven educational systems and ongoing calls for responsible AI for education, several critical issues remain unresolved -- acting as the elephant in the room within AI in education, learning analytics, educational data mining, learning sciences, and educational psychology communities. This critical analysis identifies and examines nine persistent challenges that continue to undermine the fairness, transparency, and effectiveness of current AI methods and applications in education. These include: (1) the lack of clarity around what AI for education truly means -- often ignoring the distinct purposes, strengths, and limitations of different AI families -- and the trend of equating it with domain-agnostic, company-driven large language models; (2) the widespread neglect of essential learning processes such as motivation, emotion, and (meta)cognition in AI-driven learner modelling and their contextual nature; (3) limited integration of domain knowledge and lack of stakeholder involvement in AI design and development; (4) continued use of non-sequential machine learning models on temporal educational data; (5) misuse of non-sequential metrics to evaluate sequential models; (6) use of unreliable explainable AI methods to provide explanations for black-box models; (7) ignoring ethical guidelines in addressing data inconsistencies during model training; (8) use of mainstream AI methods for pattern discovery and learning analytics without systematic benchmarking; and (9) overemphasis on global prescriptions while overlooking localised, student-specific recommendations. Supported by theoretical and empirical research, we demonstrate how hybrid AI methods -- specifically neural-symbolic AI -- can address the elephant in the room and serve as the foundation for responsible, trustworthy AI systems in education.",Education
ClimateSet: A Large-Scale Climate Model Dataset for Machine Learning,"Climate models have been key for assessing the impact of climate change and simulating future climate scenarios. The machine learning (ML) community has taken an increased interest in supporting climate scientists efforts on various tasks such as climate model emulation, downscaling, and prediction tasks. Many of those tasks have been addressed on datasets created with single climate models. However, both the climate science and ML communities have suggested that to address those tasks at scale, we need large, consistent, and ML-ready climate model datasets. Here, we introduce ClimateSet, a dataset containing the inputs and outputs of 36 climate models from the Input4MIPs and CMIP6 archives. In addition, we provide a modular dataset pipeline for retrieving and preprocessing additional climate models and scenarios. We showcase the potential of our dataset by using it as a benchmark for ML-based climate model emulation. We gain new insights about the performance and generalization capabilities of the different ML models by analyzing their performance across different climate models. Furthermore, the dataset can be used to train an ML emulator on several climate models instead of just one. Such a super emulator can quickly project new climate change scenarios, complementing existing scenarios already provided to policymakers. We believe ClimateSet will create the basis needed for the ML community to tackle climate-related tasks at scale.","ClimateSet: A Large-Scale Climate Model Dataset for Machine Learning Climate models have been key for assessing the impact of climate change and simulating future climate scenarios. The machine learning (ML) community has taken an increased interest in supporting climate scientists efforts on various tasks such as climate model emulation, downscaling, and prediction tasks. Many of those tasks have been addressed on datasets created with single climate models. However, both the climate science and ML communities have suggested that to address those tasks at scale, we need large, consistent, and ML-ready climate model datasets. Here, we introduce ClimateSet, a dataset containing the inputs and outputs of 36 climate models from the Input4MIPs and CMIP6 archives. In addition, we provide a modular dataset pipeline for retrieving and preprocessing additional climate models and scenarios. We showcase the potential of our dataset by using it as a benchmark for ML-based climate model emulation. We gain new insights about the performance and generalization capabilities of the different ML models by analyzing their performance across different climate models. Furthermore, the dataset can be used to train an ML emulator on several climate models instead of just one. Such a super emulator can quickly project new climate change scenarios, complementing existing scenarios already provided to policymakers. We believe ClimateSet will create the basis needed for the ML community to tackle climate-related tasks at scale.",Environment
How to Teach a Teacher: Challenges and Opportunities in Physics Teacher Education in Germany and the USA,"Preparing future physics teachers for the demanding nature of their profession is an important and complex endeavor. Teacher education systems must provide a structure for the coherent professional development of prospective teachers. Worldwide, physics teacher education is organized in different ways, but have to face similar challenges, like the relation between academic studies and practical preparation. To meet these challenges, it is worth taking look at different teacher education systems. In this chapter, we compare physics teacher education in two countries, representing two different educational traditions: Germany and the USA. Comparing different aspects of physics teacher education (standards, organization and institutionalization, content of teacher education, quality assurance), we describe both systems in their current state and why they are organized in the way they are. In doing so, we identify surprising commonalities but also different opportunities for both systems to learn from each other.","How to Teach a Teacher: Challenges and Opportunities in Physics Teacher Education in Germany and the USA Preparing future physics teachers for the demanding nature of their profession is an important and complex endeavor. Teacher education systems must provide a structure for the coherent professional development of prospective teachers. Worldwide, physics teacher education is organized in different ways, but have to face similar challenges, like the relation between academic studies and practical preparation. To meet these challenges, it is worth taking look at different teacher education systems. In this chapter, we compare physics teacher education in two countries, representing two different educational traditions: Germany and the USA. Comparing different aspects of physics teacher education (standards, organization and institutionalization, content of teacher education, quality assurance), we describe both systems in their current state and why they are organized in the way they are. In doing so, we identify surprising commonalities but also different opportunities for both systems to learn from each other.",Education
Cloud Computing-based Higher Education Platforms during the COVID-19 Pandemic,"Cloud computing has become the infrastructure that supports peoples daily activities, business operations, and education delivery around the world. Cloud computing-based education platforms have been widely applied to assist online teaching during the COVID-19 pandemic. This paper examines the impact and importance of cloud computing in remote learning and education. This study conducted multiple-case analyses of 22 online platforms of higher education in Chinese universities during the epidemic. A comparative analysis of the 22 platforms revealed that they applied different cloud computing models and tools based on their unique requirements and needs. The study results provide strategic insights to higher education institutions regarding effective approaches to applying cloud computing-based platforms for remote education, especially during crisis situations.","Cloud Computing-based Higher Education Platforms during the COVID-19 Pandemic Cloud computing has become the infrastructure that supports peoples daily activities, business operations, and education delivery around the world. Cloud computing-based education platforms have been widely applied to assist online teaching during the COVID-19 pandemic. This paper examines the impact and importance of cloud computing in remote learning and education. This study conducted multiple-case analyses of 22 online platforms of higher education in Chinese universities during the epidemic. A comparative analysis of the 22 platforms revealed that they applied different cloud computing models and tools based on their unique requirements and needs. The study results provide strategic insights to higher education institutions regarding effective approaches to applying cloud computing-based platforms for remote education, especially during crisis situations.",Education
Leverage Bubble,"Leverage is strongly related to liquidity in a market and lack of liquidity is considered a cause andor consequence of the recent financial crisis. A repurchase agreement is a financial instrument where a security is sold simultaneously with an agreement to buy it back at a later date. Repurchase agreements (repos) market size is a very important element in calculating the overall leverage in a financial market. Therefore, studying the behavior of repos market size can help to understand a process that can contribute to the birth of a financial crisis. We hypothesize that herding behavior among large investors led to massive over-leveraging through the use of repos, resulting in a bubble (built up over the previous years) and subsequent crash in this market in early 2008. We use the Johansen-Ledoit-Sornette (JLS) model of rational expectation bubbles and behavioral finance to study the dynamics of the repo market that led to the crash. The JLS model qualifies a bubble by the presence of characteristic patterns in the price dynamics, called log-periodic power law (LPPL) behavior. We show that there was significant LPPL behavior in the market before that crash and that the predicted range of times predicted by the model for the end of the bubble is consistent with the observations.","Leverage Bubble Leverage is strongly related to liquidity in a market and lack of liquidity is considered a cause andor consequence of the recent financial crisis. A repurchase agreement is a financial instrument where a security is sold simultaneously with an agreement to buy it back at a later date. Repurchase agreements (repos) market size is a very important element in calculating the overall leverage in a financial market. Therefore, studying the behavior of repos market size can help to understand a process that can contribute to the birth of a financial crisis. We hypothesize that herding behavior among large investors led to massive over-leveraging through the use of repos, resulting in a bubble (built up over the previous years) and subsequent crash in this market in early 2008. We use the Johansen-Ledoit-Sornette (JLS) model of rational expectation bubbles and behavioral finance to study the dynamics of the repo market that led to the crash. The JLS model qualifies a bubble by the presence of characteristic patterns in the price dynamics, called log-periodic power law (LPPL) behavior. We show that there was significant LPPL behavior in the market before that crash and that the predicted range of times predicted by the model for the end of the bubble is consistent with the observations.",Finance
An Inexpensive Arterial Pressure Wave Sensor and its application in different physiological condition,"Arterial Blood Pressure wave monitoring is considered to be important in assessment of cardiovascular system. We developed a novel pulse wave detection system using low frequency specific piezoelectric material as pressure wave sensor. The transducer detects the periodic change in the arterial wall diameter produced by pressure wave and the amplified signal after integration represents the pressure wave. The signal before integration is proportional to the rate of change of pressure wave and it not only reproduces the pressure waveform faithfully, but also its sharper nature helps to reliably detect the heart period variability (HPV). We have studied the position-specific (e.g. over carotid or radial artery) nature of change of this pulse wave signal (shape and amplitude) and also the changes at different physiological states.","An Inexpensive Arterial Pressure Wave Sensor and its application in different physiological condition Arterial Blood Pressure wave monitoring is considered to be important in assessment of cardiovascular system. We developed a novel pulse wave detection system using low frequency specific piezoelectric material as pressure wave sensor. The transducer detects the periodic change in the arterial wall diameter produced by pressure wave and the amplified signal after integration represents the pressure wave. The signal before integration is proportional to the rate of change of pressure wave and it not only reproduces the pressure waveform faithfully, but also its sharper nature helps to reliably detect the heart period variability (HPV). We have studied the position-specific (e.g. over carotid or radial artery) nature of change of this pulse wave signal (shape and amplitude) and also the changes at different physiological states.",Healthcare
How relevant is climate change research for climate change policy? An empirical analysis based on Overton data,"Climate change is an ongoing topic in nearly all areas of society since many years. A discussion of climate change without referring to scientific results is not imaginable. This is especially the case for policies since action on the macro scale is required to avoid costly consequences for society. In this study, we deal with the question of how research on climate change and policy are connected. In 2019, the new Overton database of policy documents was released including links to research papers that are cited by policy documents. The use of results and recommendations from research on climate change might be reflected in citations of scientific papers in policy documents. Although we suspect a lot of uncertainty related to the coverage of policy documents in Overton, there seems to be an impact of international climate policy cycles on policy document publication. We observe local peaks in climate policy documents around major decisions in international climate diplomacy. Our results point out that IGOs and think tanks -- with a focus on climate change -- have published more climate change policy documents than expected. We found that climate change papers that are cited in climate change policy documents received significantly more citations on average than climate change papers that are not cited in these documents. Both areas of society (science and policy) focus on similar climate change research fields: biology, earth sciences, engineering, and disease sciences. Based on these and other empirical results in this study, we propose a simple model of policy impact considering a chain of different document types: the chain starts with scientific assessment reports (systematic reviews) that lead via science communication documents (policy briefs, policy reports or plain language summaries) and government reports to legislative documents.","How relevant is climate change research for climate change policy? An empirical analysis based on Overton data Climate change is an ongoing topic in nearly all areas of society since many years. A discussion of climate change without referring to scientific results is not imaginable. This is especially the case for policies since action on the macro scale is required to avoid costly consequences for society. In this study, we deal with the question of how research on climate change and policy are connected. In 2019, the new Overton database of policy documents was released including links to research papers that are cited by policy documents. The use of results and recommendations from research on climate change might be reflected in citations of scientific papers in policy documents. Although we suspect a lot of uncertainty related to the coverage of policy documents in Overton, there seems to be an impact of international climate policy cycles on policy document publication. We observe local peaks in climate policy documents around major decisions in international climate diplomacy. Our results point out that IGOs and think tanks -- with a focus on climate change -- have published more climate change policy documents than expected. We found that climate change papers that are cited in climate change policy documents received significantly more citations on average than climate change papers that are not cited in these documents. Both areas of society (science and policy) focus on similar climate change research fields: biology, earth sciences, engineering, and disease sciences. Based on these and other empirical results in this study, we propose a simple model of policy impact considering a chain of different document types: the chain starts with scientific assessment reports (systematic reviews) that lead via science communication documents (policy briefs, policy reports or plain language summaries) and government reports to legislative documents.",Environment
Robustly Extracting Medical Knowledge from EHRs: A Case Study of Learning a Health Knowledge Graph,"Increasingly large electronic health records (EHRs) provide an opportunity to algorithmically learn medical knowledge. In one prominent example, a causal health knowledge graph could learn relationships between diseases and symptoms and then serve as a diagnostic tool to be refined with additional clinical input. Prior research has demonstrated the ability to construct such a graph from over 270,000 emergency department patient visits. In this work, we describe methods to evaluate a health knowledge graph for robustness. Moving beyond precision and recall, we analyze for which diseases and for which patients the graph is most accurate. We identify sample size and unmeasured confounders as major sources of error in the health knowledge graph. We introduce a method to leverage non-linear functions in building the causal graph to better understand existing model assumptions. Finally, to assess model generalizability, we extend to a larger set of complete patient visits within a hospital system. We conclude with a discussion on how to robustly extract medical knowledge from EHRs.","Robustly Extracting Medical Knowledge from EHRs: A Case Study of Learning a Health Knowledge Graph Increasingly large electronic health records (EHRs) provide an opportunity to algorithmically learn medical knowledge. In one prominent example, a causal health knowledge graph could learn relationships between diseases and symptoms and then serve as a diagnostic tool to be refined with additional clinical input. Prior research has demonstrated the ability to construct such a graph from over 270,000 emergency department patient visits. In this work, we describe methods to evaluate a health knowledge graph for robustness. Moving beyond precision and recall, we analyze for which diseases and for which patients the graph is most accurate. We identify sample size and unmeasured confounders as major sources of error in the health knowledge graph. We introduce a method to leverage non-linear functions in building the causal graph to better understand existing model assumptions. Finally, to assess model generalizability, we extend to a larger set of complete patient visits within a hospital system. We conclude with a discussion on how to robustly extract medical knowledge from EHRs.",Healthcare
Open Mass Spectrometry Search Algorithm,"Large numbers of MSMS peptide spectra generated in proteomics experiments require efficient, sensitive and specific algorithms for peptide identification. In the Open Mass Spectrometry Search Algorithm OMSSA, specificity is calculated by a classic probability score using an explicit model for matching experimental spectra to sequences. At default thresholds, OMSSA matches more spectra from a standard protein cocktail than a comparable algorithm. OMSSA is designed to be faster than published algorithms in searching large MSMS datasets.","Open Mass Spectrometry Search Algorithm Large numbers of MSMS peptide spectra generated in proteomics experiments require efficient, sensitive and specific algorithms for peptide identification. In the Open Mass Spectrometry Search Algorithm OMSSA, specificity is calculated by a classic probability score using an explicit model for matching experimental spectra to sequences. At default thresholds, OMSSA matches more spectra from a standard protein cocktail than a comparable algorithm. OMSSA is designed to be faster than published algorithms in searching large MSMS datasets.",Healthcare
Granger Causality: Basic Theory and Application to Neuroscience,Multi-electrode neurophysiological recordings produce massive quantities of data. Multivariate time series analysis provides the basic framework for analyzing the patterns of neural interactions in these data. It has long been recognized that neural interactions are directional. Being able to assess the directionality of neuronal interactions is thus a highly desired capability for understanding the cooperative nature of neural computation. Research over the last few years has shown that Granger causality is a key technique to furnish this capability. The main goal of this article is to provide an expository introduction to the concept of Granger causality. Mathematical frameworks for both bivariate Granger causality and conditional Granger causality are developed in detail with particular emphasis on their spectral representations. The technique is demonstrated in numerical examples where the exact answers of causal influences are known. It is then applied to analyze multichannel local field potentials recorded from monkeys performing a visuomotor task. Our results are shown to be physiologically interpretable and yield new insights into the dynamical organization of large-scale oscillatory cortical networks.,Granger Causality: Basic Theory and Application to Neuroscience Multi-electrode neurophysiological recordings produce massive quantities of data. Multivariate time series analysis provides the basic framework for analyzing the patterns of neural interactions in these data. It has long been recognized that neural interactions are directional. Being able to assess the directionality of neuronal interactions is thus a highly desired capability for understanding the cooperative nature of neural computation. Research over the last few years has shown that Granger causality is a key technique to furnish this capability. The main goal of this article is to provide an expository introduction to the concept of Granger causality. Mathematical frameworks for both bivariate Granger causality and conditional Granger causality are developed in detail with particular emphasis on their spectral representations. The technique is demonstrated in numerical examples where the exact answers of causal influences are known. It is then applied to analyze multichannel local field potentials recorded from monkeys performing a visuomotor task. Our results are shown to be physiologically interpretable and yield new insights into the dynamical organization of large-scale oscillatory cortical networks.,Healthcare
Using Pivot Consistency to Decompose and Solve Functional CSPs,"Many studies have been carried out in order to increase the search efficiency of constraint satisfaction problems; among them, some make use of structural properties of the constraint network; others take into account semantic properties of the constraints, generally assuming that all the constraints possess the given property. In this paper, we propose a new decomposition method benefiting from both semantic properties of functional constraints (not bijective constraints) and structural properties of the network; furthermore, not all the constraints need to be functional. We show that under some conditions, the existence of solutions can be guaranteed. We first characterize a particular subset of the variables, which we name a root set. We then introduce pivot consistency, a new local consistency which is a weak form of path consistency and can be achieved in O(n2d2) complexity (instead of O(n3d3) for path consistency), and we present associated properties; in particular, we show that any consistent instantiation of the root set can be linearly extended to a solution, which leads to the presentation of the aforementioned new method for solving by decomposing functional CSPs.","Using Pivot Consistency to Decompose and Solve Functional CSPs Many studies have been carried out in order to increase the search efficiency of constraint satisfaction problems; among them, some make use of structural properties of the constraint network; others take into account semantic properties of the constraints, generally assuming that all the constraints possess the given property. In this paper, we propose a new decomposition method benefiting from both semantic properties of functional constraints (not bijective constraints) and structural properties of the network; furthermore, not all the constraints need to be functional. We show that under some conditions, the existence of solutions can be guaranteed. We first characterize a particular subset of the variables, which we name a root set. We then introduce pivot consistency, a new local consistency which is a weak form of path consistency and can be achieved in O(n2d2) complexity (instead of O(n3d3) for path consistency), and we present associated properties; in particular, we show that any consistent instantiation of the root set can be linearly extended to a solution, which leads to the presentation of the aforementioned new method for solving by decomposing functional CSPs.",Technology
Optimization and performance of an optical cardio-magnetometer,Cardiomagnetometry is a growing field of noninvasive medical diagnostics that has triggered a need for affordable high-sensitivity magnetometers. Optical pumping magnetometers are promising candidates satisfying that need since it was demonstrated that they can map the heart magnetic field. For the optimization of such devices theoretical limits on the performance as well as an experimental approach is presented. The promising result is a intrinsic magnetometric sensitivity of 63 fT  Hz12 a measurement bandwidth of 140 Hz and a spatial resolution of 28 mm.,Optimization and performance of an optical cardio-magnetometer Cardiomagnetometry is a growing field of noninvasive medical diagnostics that has triggered a need for affordable high-sensitivity magnetometers. Optical pumping magnetometers are promising candidates satisfying that need since it was demonstrated that they can map the heart magnetic field. For the optimization of such devices theoretical limits on the performance as well as an experimental approach is presented. The promising result is a intrinsic magnetometric sensitivity of 63 fT  Hz12 a measurement bandwidth of 140 Hz and a spatial resolution of 28 mm.,Healthcare
"A Techno-Economic Analysis of the Interconnectedness between Energy Resources, Climate Change, and Sustainable Development","Abstract: The rising global temperatures caused by climate change significantly impact energy consumption and electricity generation. Fluctuating temperatures and frequent extreme weather events disrupt energy production and consumption patterns. Addressing these challenges has become a priority, prompting governments, industries, and societies to pursue sustainable development and embrace eco-friendly economies. This strategy aims to decouple economic growth from environmental harm, ensuring a sustainable future for generations. Understanding the link between climate change, energy resources, and sustainable development is crucial. Techno-economic analysis provides a framework for evaluating energy-related projects and policies, guiding decision-makers toward sustainable solutions. A case study highlights the interaction between hydroponic unit energy needs, electricity pricing from wind farms, and product sales prices. Findings suggest that smaller 2-megawatt investments are more efficient and adaptable than larger 18-megawatt projects, proving economically viable and technologically flexible. However, such investments must also consider their social and environmental impacts on local communities. Sustainable development seeks to ensure that progress benefits all stakeholders while protecting the environment. Achieving this requires collaboration among governments, businesses, researchers, and individuals. By fostering innovation, adopting eco-friendly practices, and creating supportive policies, society can transition to a green economy, mitigating climate change and promoting a sustainable, resilient future.","A Techno-Economic Analysis of the Interconnectedness between Energy Resources, Climate Change, and Sustainable Development Abstract: The rising global temperatures caused by climate change significantly impact energy consumption and electricity generation. Fluctuating temperatures and frequent extreme weather events disrupt energy production and consumption patterns. Addressing these challenges has become a priority, prompting governments, industries, and societies to pursue sustainable development and embrace eco-friendly economies. This strategy aims to decouple economic growth from environmental harm, ensuring a sustainable future for generations. Understanding the link between climate change, energy resources, and sustainable development is crucial. Techno-economic analysis provides a framework for evaluating energy-related projects and policies, guiding decision-makers toward sustainable solutions. A case study highlights the interaction between hydroponic unit energy needs, electricity pricing from wind farms, and product sales prices. Findings suggest that smaller 2-megawatt investments are more efficient and adaptable than larger 18-megawatt projects, proving economically viable and technologically flexible. However, such investments must also consider their social and environmental impacts on local communities. Sustainable development seeks to ensure that progress benefits all stakeholders while protecting the environment. Achieving this requires collaboration among governments, businesses, researchers, and individuals. By fostering innovation, adopting eco-friendly practices, and creating supportive policies, society can transition to a green economy, mitigating climate change and promoting a sustainable, resilient future.",Environment
Well-Founded Semantics for Extended Logic Programs with Dynamic Preferences,The paper describes an extension of well-founded semantics for logic programs with two types of negation. In this extension information about preferences between rules can be expressed in the logical language and derived dynamically. This is achieved by using a reserved predicate symbol and a naming technique. Conflicts among rules are resolved whenever possible on the basis of derived preference information. The well-founded conclusions of prioritized logic programs can be computed in polynomial time. A legal reasoning example illustrates the usefulness of the approach.,Well-Founded Semantics for Extended Logic Programs with Dynamic Preferences The paper describes an extension of well-founded semantics for logic programs with two types of negation. In this extension information about preferences between rules can be expressed in the logical language and derived dynamically. This is achieved by using a reserved predicate symbol and a naming technique. Conflicts among rules are resolved whenever possible on the basis of derived preference information. The well-founded conclusions of prioritized logic programs can be computed in polynomial time. A legal reasoning example illustrates the usefulness of the approach.,Technology
Electron capture decay of indium-111 human carbonic anhydrase I: A time differential K X ray coincidence perturbed angular correlation study,"The relaxation effects in the perturbed angular correlation spectra of indium-111 human carbonic anhydrase I (HCA I) are the result of chemical transmutation andor the complex Auger cascades that follow the electron capture decay of indium-111. Time differential K X ray coincidence perturbed angular correlation (PAC) spectroscopy shows that these relaxation effects are independent of the Auger cascade intensity. This suggests that chemical transmutation is responsible for the relaxation effects, and that bond breaking and damage product formation around the decay site resulting from localized energy deposition by Auger and Coster-Kronig electrons probably occur in the microsecond time regime. Numerical simulations of chemical transmutation relaxation effects in the time differential PAC spectrum of indium-111 HCA I are also presented.","Electron capture decay of indium-111 human carbonic anhydrase I: A time differential K X ray coincidence perturbed angular correlation study The relaxation effects in the perturbed angular correlation spectra of indium-111 human carbonic anhydrase I (HCA I) are the result of chemical transmutation andor the complex Auger cascades that follow the electron capture decay of indium-111. Time differential K X ray coincidence perturbed angular correlation (PAC) spectroscopy shows that these relaxation effects are independent of the Auger cascade intensity. This suggests that chemical transmutation is responsible for the relaxation effects, and that bond breaking and damage product formation around the decay site resulting from localized energy deposition by Auger and Coster-Kronig electrons probably occur in the microsecond time regime. Numerical simulations of chemical transmutation relaxation effects in the time differential PAC spectrum of indium-111 HCA I are also presented.",Healthcare
Competing with Markov prediction strategies,"Assuming that the loss function is convex in the prediction, we construct a prediction strategy universal for the class of Markov prediction strategies, not necessarily continuous. Allowing randomization, we remove the requirement of convexity.","Competing with Markov prediction strategies Assuming that the loss function is convex in the prediction, we construct a prediction strategy universal for the class of Markov prediction strategies, not necessarily continuous. Allowing randomization, we remove the requirement of convexity.",Technology
Industrial Strength Software in Computer Based Engineering Education (CBEE): a Case Study,Challenging problems of modern engineering education and a role of information technology are reviewed. An importance of simulation of real world physical problems in both graduate and lifelongcorporate education is discussed. It is proposed to integrate the hypermedia theory courseware with industrial strength simulation software. As a proof of the concept an educational environment Heat and Mass Transfer in Advanced Semiconductor Technology has been developed.,Industrial Strength Software in Computer Based Engineering Education (CBEE): a Case Study Challenging problems of modern engineering education and a role of information technology are reviewed. An importance of simulation of real world physical problems in both graduate and lifelongcorporate education is discussed. It is proposed to integrate the hypermedia theory courseware with industrial strength simulation software. As a proof of the concept an educational environment Heat and Mass Transfer in Advanced Semiconductor Technology has been developed.,Education
Iterative Optimization and Simplification of Hierarchical Clusterings,"Clustering is often used for discovering structure in data. Clustering systems differ in the objective function used to evaluate clustering quality and the control strategy used to search the space of clusterings. Ideally, the search strategy should consistently construct clusterings of high quality, but be computationally inexpensive as well. In general, we cannot have it both ways, but we can partition the search so that a system inexpensively constructs a tentative clustering for initial examination, followed by iterative optimization, which continues to search in background for improved clusterings. Given this motivation, we evaluate an inexpensive strategy for creating initial clusterings, coupled with several control strategies for iterative optimization, each of which repeatedly modifies an initial clustering in search of a better one. One of these methods appears novel as an iterative optimization strategy in clustering contexts. Once a clustering has been constructed it is judged by analysts -- often according to task-specific criteria. Several authors have abstracted these criteria and posited a generic performance task akin to pattern completion, where the error rate over completed patterns is used to externally judge clustering utility. Given this performance task, we adapt resampling-based pruning strategies used by supervised learning systems to the task of simplifying hierarchical clusterings, thus promising to ease post-clustering analysis. Finally, we propose a number of objective functions, based on attribute-selection measures for decision-tree induction, that might perform well on the error rate and simplicity dimensions.","Iterative Optimization and Simplification of Hierarchical Clusterings Clustering is often used for discovering structure in data. Clustering systems differ in the objective function used to evaluate clustering quality and the control strategy used to search the space of clusterings. Ideally, the search strategy should consistently construct clusterings of high quality, but be computationally inexpensive as well. In general, we cannot have it both ways, but we can partition the search so that a system inexpensively constructs a tentative clustering for initial examination, followed by iterative optimization, which continues to search in background for improved clusterings. Given this motivation, we evaluate an inexpensive strategy for creating initial clusterings, coupled with several control strategies for iterative optimization, each of which repeatedly modifies an initial clustering in search of a better one. One of these methods appears novel as an iterative optimization strategy in clustering contexts. Once a clustering has been constructed it is judged by analysts -- often according to task-specific criteria. Several authors have abstracted these criteria and posited a generic performance task akin to pattern completion, where the error rate over completed patterns is used to externally judge clustering utility. Given this performance task, we adapt resampling-based pruning strategies used by supervised learning systems to the task of simplifying hierarchical clusterings, thus promising to ease post-clustering analysis. Finally, we propose a number of objective functions, based on attribute-selection measures for decision-tree induction, that might perform well on the error rate and simplicity dimensions.",Technology
Dimensional analysis identifies contrasting dynamics of past climate states and critical transitions,"While one can unequivocally identify past climate transitions, we lack comprehensive knowledge about their underlying mechanisms and timescales. Our study employs a dimensional analysis of benthic stable isotope records to uncover, across different timescales, how the climatic fluctuation of the Cenozoic are associated with changes in the number of effective degrees of freedom. Precession timescales dominate the Hothouse and Warmhouse states, while the Icehouse climate is primarily influenced by obliquity and eccentricity timescales. Notably, the Coolhouse state lacks dominant timescales. Our analysis proves effective in objectively identifying abrupt climate shifts and extremes. This is also demonstrated using high-resolution data from the last glacial cycle, revealing abrupt climate shifts within a single climate state. These findings significantly impact our understanding of the inherent stability of each climate state and the evaluation of (paleo-)climate models ability to replicate key features of pastfuture climate states and transitions.","Dimensional analysis identifies contrasting dynamics of past climate states and critical transitions While one can unequivocally identify past climate transitions, we lack comprehensive knowledge about their underlying mechanisms and timescales. Our study employs a dimensional analysis of benthic stable isotope records to uncover, across different timescales, how the climatic fluctuation of the Cenozoic are associated with changes in the number of effective degrees of freedom. Precession timescales dominate the Hothouse and Warmhouse states, while the Icehouse climate is primarily influenced by obliquity and eccentricity timescales. Notably, the Coolhouse state lacks dominant timescales. Our analysis proves effective in objectively identifying abrupt climate shifts and extremes. This is also demonstrated using high-resolution data from the last glacial cycle, revealing abrupt climate shifts within a single climate state. These findings significantly impact our understanding of the inherent stability of each climate state and the evaluation of (paleo-)climate models ability to replicate key features of pastfuture climate states and transitions.",Environment
Artificial Neural Networks and their Applications,The Artificial Neural network is a functional imitation of simplified model of the biological neurons and their goal is to construct useful computers for real world problems. The ANN applications have increased dramatically in the last few years fired by both theoretical and practical applications in a wide variety of applications. A brief theory of ANN is presented and potential areas are identified and future trends are discussed.,Artificial Neural Networks and their Applications The Artificial Neural network is a functional imitation of simplified model of the biological neurons and their goal is to construct useful computers for real world problems. The ANN applications have increased dramatically in the last few years fired by both theoretical and practical applications in a wide variety of applications. A brief theory of ANN is presented and potential areas are identified and future trends are discussed.,Technology
Prediction with expert advice for the Brier game,"We show that the Brier game of prediction is mixable and find the optimal learning rate and substitution function for it. The resulting prediction algorithm is applied to predict results of football and tennis matches. The theoretical performance guarantee turns out to be rather tight on these data sets, especially in the case of the more extensive tennis data.","Prediction with expert advice for the Brier game We show that the Brier game of prediction is mixable and find the optimal learning rate and substitution function for it. The resulting prediction algorithm is applied to predict results of football and tennis matches. The theoretical performance guarantee turns out to be rather tight on these data sets, especially in the case of the more extensive tennis data.",Technology
Exact Simulation of the 32 Model,"This paper discusses the exact simulation of the stock price process underlying the 32 model. Using a result derived by Craddock and Lennox using Lie Symmetry Analysis, we adapt the Broadie-Kaya algorithm for the simulation of affine processes to the 32 model. We also discuss variance reduction techniques and find that conditional Monte Carlo techniques combined with quasi-Monte Carlo point sets result in significant variance reductions.","Exact Simulation of the 32 Model This paper discusses the exact simulation of the stock price process underlying the 32 model. Using a result derived by Craddock and Lennox using Lie Symmetry Analysis, we adapt the Broadie-Kaya algorithm for the simulation of affine processes to the 32 model. We also discuss variance reduction techniques and find that conditional Monte Carlo techniques combined with quasi-Monte Carlo point sets result in significant variance reductions.",Finance
The Transfer Pricing Problem with Non-Linearities,"A number of approaches to solving the well-known transfer pricing problem are known. However, few models satisfactorily resolve the core problem of allowing both the source and receiving divisions to earn a profit on transfers during a period in such a way that sub-optimal output levels are avoided. In 1969, Samuel proposed to use a transfer price schedule instead of just a single transfer price. An essential improvement of Samuels model was given by Tomkins (1990) in his pragmatic-analytical transfer pricing approach, which is a combination of a single cost-plus transfer price and the pragmatic process of negotiation. This fundamental approach was developed under the assumption that the net average revenue curve for the final product is linear. In this paper, Tomkins pragmatic-analytical model is further developed for non-linear net average revenue curves. In particular, typical quadratic functions are considered and corresponding transfer price schedules are determined. A similar technique can be used for the transfer pricing problem with any net average revenue curve.","The Transfer Pricing Problem with Non-Linearities A number of approaches to solving the well-known transfer pricing problem are known. However, few models satisfactorily resolve the core problem of allowing both the source and receiving divisions to earn a profit on transfers during a period in such a way that sub-optimal output levels are avoided. In 1969, Samuel proposed to use a transfer price schedule instead of just a single transfer price. An essential improvement of Samuels model was given by Tomkins (1990) in his pragmatic-analytical transfer pricing approach, which is a combination of a single cost-plus transfer price and the pragmatic process of negotiation. This fundamental approach was developed under the assumption that the net average revenue curve for the final product is linear. In this paper, Tomkins pragmatic-analytical model is further developed for non-linear net average revenue curves. In particular, typical quadratic functions are considered and corresponding transfer price schedules are determined. A similar technique can be used for the transfer pricing problem with any net average revenue curve.",Finance
Quantum Portfolios of Observables and the Risk Neutral Valuation Model,"Quantum Portfolios of quantum algorithms encoded on qbits have recently been reported. In this paper a discussion of the continuous variables version of quantum portfolios is presented. A risk neutral valuation model for options dependent on the measured values of the observables, analogous to the traditional Black-Scholes valuation model, is obtained from the underlying stochastic equations. The quantum algorithms are here encoded on simple harmonic oscillator (SHO) states, and a Fokker-Planck equation for the Glauber P-representation is obtained as a starting point for the analysis. A discussion of the observation of the polarization of a portfolio of qbits is also obtained and the resultant Fokker-Planck equation is used to obtain the risk neutral valuation of the qbit polarization portfolio.","Quantum Portfolios of Observables and the Risk Neutral Valuation Model Quantum Portfolios of quantum algorithms encoded on qbits have recently been reported. In this paper a discussion of the continuous variables version of quantum portfolios is presented. A risk neutral valuation model for options dependent on the measured values of the observables, analogous to the traditional Black-Scholes valuation model, is obtained from the underlying stochastic equations. The quantum algorithms are here encoded on simple harmonic oscillator (SHO) states, and a Fokker-Planck equation for the Glauber P-representation is obtained as a starting point for the analysis. A discussion of the observation of the polarization of a portfolio of qbits is also obtained and the resultant Fokker-Planck equation is used to obtain the risk neutral valuation of the qbit polarization portfolio.",Finance
Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm,"The Markov Blanket Bayesian Classifier is a recently-proposed algorithm for construction of probabilistic classifiers. This paper presents an empirical comparison of the MBBC algorithm with three other Bayesian classifiers: Naive Bayes, Tree-Augmented Naive Bayes and a general Bayesian network. All of these are implemented using the K2 framework of Cooper and Herskovits. The classifiers are compared in terms of their performance (using simple accuracy measures and ROC curves) and speed, on a range of standard benchmark data sets. It is concluded that MBBC is competitive in terms of speed and accuracy with the other algorithms considered.","Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm The Markov Blanket Bayesian Classifier is a recently-proposed algorithm for construction of probabilistic classifiers. This paper presents an empirical comparison of the MBBC algorithm with three other Bayesian classifiers: Naive Bayes, Tree-Augmented Naive Bayes and a general Bayesian network. All of these are implemented using the K2 framework of Cooper and Herskovits. The classifiers are compared in terms of their performance (using simple accuracy measures and ROC curves) and speed, on a range of standard benchmark data sets. It is concluded that MBBC is competitive in terms of speed and accuracy with the other algorithms considered.",Technology
Stability of Semi-Implicit and Iterative Centred-Implicit Time Discretizations for Various Equation Systems Used in NWP,"The stability of classical semi-implicit scheme, and some more advanced iterative schemes recently proposed for Numerical Weather Prediction (NWP) purpose is examined. In all these schemes, the solution of the centred-implicit non-linear equation is approached by an iterative fixed-point algorithm, preconditioned by a simple, constant in time, linear operator. A general methodology for assessing analytically the stability of these schemes on canonical problems for a vertically unbounded atmosphere is presented. The proposed method is valid for all the equation systems usually employed in NWP. However, as in earlier studies, the method can be applied only in simplified meteorological contexts, thus overestimating the actual stability that would occur in more realistic meteorological contexts. The analysis is performed in the spatially-continuous framework, hence allowing to eliminate the spatial-discretisation or the boundary conditions as possible causes of the fundamental instabilities linked to the time-scheme itself. The general method is then shown concretely to apply to various time-discretisation schemes and equation-systems (namely shallow-water, and fully compressible Euler equations). Analytical results found in the literature are recovered from the proposed method, and some original results are presented.","Stability of Semi-Implicit and Iterative Centred-Implicit Time Discretizations for Various Equation Systems Used in NWP The stability of classical semi-implicit scheme, and some more advanced iterative schemes recently proposed for Numerical Weather Prediction (NWP) purpose is examined. In all these schemes, the solution of the centred-implicit non-linear equation is approached by an iterative fixed-point algorithm, preconditioned by a simple, constant in time, linear operator. A general methodology for assessing analytically the stability of these schemes on canonical problems for a vertically unbounded atmosphere is presented. The proposed method is valid for all the equation systems usually employed in NWP. However, as in earlier studies, the method can be applied only in simplified meteorological contexts, thus overestimating the actual stability that would occur in more realistic meteorological contexts. The analysis is performed in the spatially-continuous framework, hence allowing to eliminate the spatial-discretisation or the boundary conditions as possible causes of the fundamental instabilities linked to the time-scheme itself. The general method is then shown concretely to apply to various time-discretisation schemes and equation-systems (namely shallow-water, and fully compressible Euler equations). Analytical results found in the literature are recovered from the proposed method, and some original results are presented.",Environment
RARE: Renewable Energy Aware Resource Management in Datacenters,"The exponential growth in demand for digital services drives massive datacenter energy consumption and negative environmental impacts. Promoting sustainable solutions to pressing energy and digital infrastructure challenges is crucial. Several hyperscale cloud providers have announced plans to power their datacenters using renewable energy. However, integrating renewables to power the datacenters is challenging because the power generation is intermittent, necessitating approaches to tackle power supply variability. Hand engineering domain-specific heuristics-based schedulers to meet specific objective functions in such complex dynamic green datacenter environments is time-consuming, expensive, and requires extensive tuning by domain experts. The green datacenters need smart systems and system software to employ multiple renewable energy sources (wind and solar) by intelligently adapting computing to renewable energy generation. We present RARE (Renewable energy Aware REsource management), a Deep Reinforcement Learning (DRL) job scheduler that automatically learns effective job scheduling policies while continually adapting to datacenters complex dynamic environment. The resulting DRL scheduler performs better than heuristic scheduling policies with different workloads and adapts to the intermittent power supply from renewables. We demonstrate DRL scheduler system design parameters that, when tuned correctly, produce better performance. Finally, we demonstrate that the DRL scheduler can learn from and improve upon existing heuristic policies using Offline Learning.","RARE: Renewable Energy Aware Resource Management in Datacenters The exponential growth in demand for digital services drives massive datacenter energy consumption and negative environmental impacts. Promoting sustainable solutions to pressing energy and digital infrastructure challenges is crucial. Several hyperscale cloud providers have announced plans to power their datacenters using renewable energy. However, integrating renewables to power the datacenters is challenging because the power generation is intermittent, necessitating approaches to tackle power supply variability. Hand engineering domain-specific heuristics-based schedulers to meet specific objective functions in such complex dynamic green datacenter environments is time-consuming, expensive, and requires extensive tuning by domain experts. The green datacenters need smart systems and system software to employ multiple renewable energy sources (wind and solar) by intelligently adapting computing to renewable energy generation. We present RARE (Renewable energy Aware REsource management), a Deep Reinforcement Learning (DRL) job scheduler that automatically learns effective job scheduling policies while continually adapting to datacenters complex dynamic environment. The resulting DRL scheduler performs better than heuristic scheduling policies with different workloads and adapts to the intermittent power supply from renewables. We demonstrate DRL scheduler system design parameters that, when tuned correctly, produce better performance. Finally, we demonstrate that the DRL scheduler can learn from and improve upon existing heuristic policies using Offline Learning.",Environment
Teaching and learning mathematics with Prolog,"Procedural computer languages have long been used in many aspects of mathematics pedagogy. In this work, we examine the use of Prolog, a declarative language for the same purpose. We find the factsrules aspect of Prolog to be a novel platform for developing coding lessons to supplement the learning of mathematics. Specific examples are presented.","Teaching and learning mathematics with Prolog Procedural computer languages have long been used in many aspects of mathematics pedagogy. In this work, we examine the use of Prolog, a declarative language for the same purpose. We find the factsrules aspect of Prolog to be a novel platform for developing coding lessons to supplement the learning of mathematics. Specific examples are presented.",Education
Mathematical Modeling of BCG-based Bladder Cancer Treatment Using Socio-Demographics,"Cancer is one of the most widespread diseases around the world with millions of new patients each year. Bladder cancer is one of the most prevalent types of cancer affecting all individuals alike with no obvious prototypical patient. The current standard treatment for BC follows a routine weekly Bacillus Calmette-Guerin (BCG) immunotherapy-based therapy protocol which is applied to all patients alike. The clinical outcomes associated with BCG treatment vary significantly among patients due to the biological and clinical complexity of the interaction between the immune system, treatments, and cancer cells. In this study, we take advantage of the patients socio-demographics to offer a personalized mathematical model that describes the clinical dynamics associated with BCG-based treatment. To this end, we adopt a well-established BCG treatment model and integrate a machine learning component to temporally adjust and reconfigure key parameters within the model thus promoting its personalization. Using real clinical data, we show that our personalized model favorably compares with the original one in predicting the number of cancer cells at the end of the treatment, with 14.8 improvement, on average.","Mathematical Modeling of BCG-based Bladder Cancer Treatment Using Socio-Demographics Cancer is one of the most widespread diseases around the world with millions of new patients each year. Bladder cancer is one of the most prevalent types of cancer affecting all individuals alike with no obvious prototypical patient. The current standard treatment for BC follows a routine weekly Bacillus Calmette-Guerin (BCG) immunotherapy-based therapy protocol which is applied to all patients alike. The clinical outcomes associated with BCG treatment vary significantly among patients due to the biological and clinical complexity of the interaction between the immune system, treatments, and cancer cells. In this study, we take advantage of the patients socio-demographics to offer a personalized mathematical model that describes the clinical dynamics associated with BCG-based treatment. To this end, we adopt a well-established BCG treatment model and integrate a machine learning component to temporally adjust and reconfigure key parameters within the model thus promoting its personalization. Using real clinical data, we show that our personalized model favorably compares with the original one in predicting the number of cancer cells at the end of the treatment, with 14.8 improvement, on average.",Healthcare
A Tale of Two Curricula: The performance of two thousand students in introductory electromagnetism,"The performance of over 2000 students in introductory calculus-based electromagnetism (EM) courses at four large research universities was measured using the Brief Electricity and Magnetism Assessment (BEMA). Two different curricula were used at these universities: a traditional EM curriculum and the Matter  Interactions (MI) curriculum. At each university, post-instruction BEMA test averages were significantly higher for the MI curriculum than for the traditional curriculum. The differences in post-test averages cannot be explained by differences in variables such as pre-instruction BEMA scores, grade point average, or SAT scores. BEMA performance on categories of items organized by subtopic was also compared at one of the universities; MI averages were significantly higher in each topic. The results suggest that the MI curriculum is more effective than the traditional curriculum at teaching EM concepts to students, possibly because the learning progression in MI reorganizes and augments the traditional sequence of topics, for example, by increasing early emphasis on the vector field concept and by emphasizing the effects of fields on matter at the microscopic level.","A Tale of Two Curricula: The performance of two thousand students in introductory electromagnetism The performance of over 2000 students in introductory calculus-based electromagnetism (EM) courses at four large research universities was measured using the Brief Electricity and Magnetism Assessment (BEMA). Two different curricula were used at these universities: a traditional EM curriculum and the Matter  Interactions (MI) curriculum. At each university, post-instruction BEMA test averages were significantly higher for the MI curriculum than for the traditional curriculum. The differences in post-test averages cannot be explained by differences in variables such as pre-instruction BEMA scores, grade point average, or SAT scores. BEMA performance on categories of items organized by subtopic was also compared at one of the universities; MI averages were significantly higher in each topic. The results suggest that the MI curriculum is more effective than the traditional curriculum at teaching EM concepts to students, possibly because the learning progression in MI reorganizes and augments the traditional sequence of topics, for example, by increasing early emphasis on the vector field concept and by emphasizing the effects of fields on matter at the microscopic level.",Education
Methodical Aspects of Informatization of Physical Education at a Higher Education Institution,"The article considers the methodical aspects of informatization of physical education at a higher education institution. Besides, the article discloses the conception of the information educational environment and information technologies oriented at realizing psychological and pedagogical objectives of teaching and educating at a higher education institution. Information technologies in a higher education institution are oriented at realizing psychological and pedagogical objectives of teaching and educating. The authors of the article show the importance and possibilities of applying information technology media to the system of higher education training of specialists in the sphere of Physics. The authors consider developing the content of methodical training of future Physics teachers to be one of ways of solving the problem of informatization of education.","Methodical Aspects of Informatization of Physical Education at a Higher Education Institution The article considers the methodical aspects of informatization of physical education at a higher education institution. Besides, the article discloses the conception of the information educational environment and information technologies oriented at realizing psychological and pedagogical objectives of teaching and educating at a higher education institution. Information technologies in a higher education institution are oriented at realizing psychological and pedagogical objectives of teaching and educating. The authors of the article show the importance and possibilities of applying information technology media to the system of higher education training of specialists in the sphere of Physics. The authors consider developing the content of methodical training of future Physics teachers to be one of ways of solving the problem of informatization of education.",Education
"Integrating Captive Portal Technology into Computer Science Education: A Modular, Hands-On Approach to Infrastructure","In this paper, we present an educational project aimed to introduce students to the technology behind Captive Portals infrastructures. For doing this, we developed a series of modules to emphasize each of the different aspects and features of this technology. The project is based on an open source implementation which is widely used in many computer network courses, making it well-suited and very appealing for instructors and practitioners in this field.","Integrating Captive Portal Technology into Computer Science Education: A Modular, Hands-On Approach to Infrastructure In this paper, we present an educational project aimed to introduce students to the technology behind Captive Portals infrastructures. For doing this, we developed a series of modules to emphasize each of the different aspects and features of this technology. The project is based on an open source implementation which is widely used in many computer network courses, making it well-suited and very appealing for instructors and practitioners in this field.",Education
Defaultable Bonds via HKA,"To construct a no-arbitrage defaultable bond market, we work on the state price density framework. Using the heat kernel approach (HKA for short) with the killing of a Markov process, we construct a single defaultable bond market that enables an explicit expression of a defaultable bond and credit spread under quadratic Gaussian settings. Some simulation results show that the model is not only tractable but realistic.","Defaultable Bonds via HKA To construct a no-arbitrage defaultable bond market, we work on the state price density framework. Using the heat kernel approach (HKA for short) with the killing of a Markov process, we construct a single defaultable bond market that enables an explicit expression of a defaultable bond and credit spread under quadratic Gaussian settings. Some simulation results show that the model is not only tractable but realistic.",Finance
ClimateNLP: Analyzing Public Sentiment Towards Climate Change Using Natural Language Processing,"Climate changes impact on human health poses unprecedented and diverse challenges. Unless proactive measures based on solid evidence are implemented, these threats will likely escalate and continue to endanger human well-being. The escalating advancements in information and communication technologies have facilitated the widespread availability and utilization of social media platforms. Individuals utilize platforms such as Twitter and Facebook to express their opinions, thoughts, and critiques on diverse subjects, encompassing the pressing issue of climate change. The proliferation of climate change-related content on social media necessitates comprehensive analysis to glean meaningful insights. This paper employs natural language processing (NLP) techniques to analyze climate change discourse and quantify the sentiment of climate change-related tweets. We use ClimateBERT, a pretrained model fine-tuned specifically for the climate change domain. The objective is to discern the sentiment individuals express and uncover patterns in public opinion concerning climate change. Analyzing tweet sentiments allows a deeper comprehension of public perceptions, concerns, and emotions about this critical global challenge. The findings from this experiment unearth valuable insights into public sentiment and the entities associated with climate change discourse. Policymakers, researchers, and organizations can leverage such analyses to understand public perceptions, identify influential actors, and devise informed strategies to address climate change challenges.","ClimateNLP: Analyzing Public Sentiment Towards Climate Change Using Natural Language Processing Climate changes impact on human health poses unprecedented and diverse challenges. Unless proactive measures based on solid evidence are implemented, these threats will likely escalate and continue to endanger human well-being. The escalating advancements in information and communication technologies have facilitated the widespread availability and utilization of social media platforms. Individuals utilize platforms such as Twitter and Facebook to express their opinions, thoughts, and critiques on diverse subjects, encompassing the pressing issue of climate change. The proliferation of climate change-related content on social media necessitates comprehensive analysis to glean meaningful insights. This paper employs natural language processing (NLP) techniques to analyze climate change discourse and quantify the sentiment of climate change-related tweets. We use ClimateBERT, a pretrained model fine-tuned specifically for the climate change domain. The objective is to discern the sentiment individuals express and uncover patterns in public opinion concerning climate change. Analyzing tweet sentiments allows a deeper comprehension of public perceptions, concerns, and emotions about this critical global challenge. The findings from this experiment unearth valuable insights into public sentiment and the entities associated with climate change discourse. Policymakers, researchers, and organizations can leverage such analyses to understand public perceptions, identify influential actors, and devise informed strategies to address climate change challenges.",Environment
A quantum statistical approach to simplified stock markets,We use standard perturbation techniques originally formulated in quantum (statistical) mechanics in the analysis of a toy model of a stock market which is given in terms of bosonic operators. In particular we discuss the probability of transition from a given value of the em portfolio of a certain trader to a different one. This computation can also be carried out using some kind of em Feynman graphs adapted to the present context.,A quantum statistical approach to simplified stock markets We use standard perturbation techniques originally formulated in quantum (statistical) mechanics in the analysis of a toy model of a stock market which is given in terms of bosonic operators. In particular we discuss the probability of transition from a given value of the em portfolio of a certain trader to a different one. This computation can also be carried out using some kind of em Feynman graphs adapted to the present context.,Finance
Dosimetry characterization of 32P intravascular brachytherapy source wires using Monte Carlo codes PENELOPE and GEANT4,"Monte Carlo calculations using the codes PENELOPE and GEANT4 have been performed to characterize the dosimetric parameters of the new 20 mm long catheter based 32P beta source manufactured by Guidant Corporation. The dose distribution along the transverse axis and the two dimensional dose rate table have been calculated. Also, the dose rate at the reference point, the radial dose function and the anisotropy function were evaluated according to the adapted TG-60 formalism for cylindrical sources. PENELOPE and GEANT4 codes were first verified against previous results corresponding to the old 27 mm Guidant 32P beta source. The dose rate at the reference point for the unsheathed 27 mm source in water was calculated to be 0.215 pm 0.001 cGy s-1 mCi-1, for PENELOPE, and 0.2312 pm 0.0008 cGy s-1 mCi-1, for GEANT4. For the unsheathed 20 mm source these values were 0.2908 pm 0.0009 cGy s-1 mCi-1 and 0.311 pm 0.001 cGy s-1 mCi-1, respectively. Also, a comparison with the limited data available on this new source is shown. We found non negligible differences between the results obtained with PENELOPE and GEANT4.","Dosimetry characterization of 32P intravascular brachytherapy source wires using Monte Carlo codes PENELOPE and GEANT4 Monte Carlo calculations using the codes PENELOPE and GEANT4 have been performed to characterize the dosimetric parameters of the new 20 mm long catheter based 32P beta source manufactured by Guidant Corporation. The dose distribution along the transverse axis and the two dimensional dose rate table have been calculated. Also, the dose rate at the reference point, the radial dose function and the anisotropy function were evaluated according to the adapted TG-60 formalism for cylindrical sources. PENELOPE and GEANT4 codes were first verified against previous results corresponding to the old 27 mm Guidant 32P beta source. The dose rate at the reference point for the unsheathed 27 mm source in water was calculated to be 0.215 pm 0.001 cGy s-1 mCi-1, for PENELOPE, and 0.2312 pm 0.0008 cGy s-1 mCi-1, for GEANT4. For the unsheathed 20 mm source these values were 0.2908 pm 0.0009 cGy s-1 mCi-1 and 0.311 pm 0.001 cGy s-1 mCi-1, respectively. Also, a comparison with the limited data available on this new source is shown. We found non negligible differences between the results obtained with PENELOPE and GEANT4.",Healthcare
Fast Convergence of Regress-Later Estimates in Least Squares Monte Carlo,"Many problems in financial engineering involve the estimation of unknown conditional expectations across a time interval. Often Least Squares Monte Carlo techniques are used for the estimation. One method that can be combined with Least Squares Monte Carlo is the Regress-Later method. Unlike conventional methods where the value function is regressed on a set of basis functions valued at the beginning of the interval, the Regress-Later method regresses the value function on a set of basis functions valued at the end of the interval. The conditional expectation across the interval is then computed exactly for each basis function. We provide sufficient conditions under which we derive the convergence rate of Regress-Later estimators. Importantly, our results hold on non-compact sets. We show that the Regress-Later method is capable of converging significantly faster than conventional methods and provide an explicit example. Achieving faster convergence speed provides a strong motivation for using Regress-Later methods in estimating conditional expectations across time.","Fast Convergence of Regress-Later Estimates in Least Squares Monte Carlo Many problems in financial engineering involve the estimation of unknown conditional expectations across a time interval. Often Least Squares Monte Carlo techniques are used for the estimation. One method that can be combined with Least Squares Monte Carlo is the Regress-Later method. Unlike conventional methods where the value function is regressed on a set of basis functions valued at the beginning of the interval, the Regress-Later method regresses the value function on a set of basis functions valued at the end of the interval. The conditional expectation across the interval is then computed exactly for each basis function. We provide sufficient conditions under which we derive the convergence rate of Regress-Later estimators. Importantly, our results hold on non-compact sets. We show that the Regress-Later method is capable of converging significantly faster than conventional methods and provide an explicit example. Achieving faster convergence speed provides a strong motivation for using Regress-Later methods in estimating conditional expectations across time.",Finance
"Form-Substance Discrimination: Concept, Cognition, and Pedagogy","The skill to separate form from substance in writing has gained new prominence in the age of AI-generated content. The challenge - discriminating between fluent expression and substantive thought - constitutes a critical literacy skill for modern education. This paper examines form-substance discrimination (FSD) as an essential learning outcome for curriculum development in higher education. We analyze its cognitive foundations in fluency bias and inhibitory control, trace its evolution from composition theory concepts like higher-order concerns, and explore how readers progress from novice acceptance of polished text to expert critical assessment. Drawing on research in cognitive psychology, composition studies, and emerging AI pedagogy, we propose practical strategies for fostering this ability through curriculum design, assessment practices, and explicit instruction. By prioritizing substance over surface in writing education, institutions can prepare students to navigate an information landscape where AI-generated content amplifies the ancient tension between style and meaning, ultimately safeguarding the value of authentic human thought in knowledge construction and communication.","Form-Substance Discrimination: Concept, Cognition, and Pedagogy The skill to separate form from substance in writing has gained new prominence in the age of AI-generated content. The challenge - discriminating between fluent expression and substantive thought - constitutes a critical literacy skill for modern education. This paper examines form-substance discrimination (FSD) as an essential learning outcome for curriculum development in higher education. We analyze its cognitive foundations in fluency bias and inhibitory control, trace its evolution from composition theory concepts like higher-order concerns, and explore how readers progress from novice acceptance of polished text to expert critical assessment. Drawing on research in cognitive psychology, composition studies, and emerging AI pedagogy, we propose practical strategies for fostering this ability through curriculum design, assessment practices, and explicit instruction. By prioritizing substance over surface in writing education, institutions can prepare students to navigate an information landscape where AI-generated content amplifies the ancient tension between style and meaning, ultimately safeguarding the value of authentic human thought in knowledge construction and communication.",Education
A comparison of delayed radiobiological effects of depleted-uranium munitions versus fourth-generation nuclear weapons,"It is shown that the radiological burden due to the battlefield use of circa 400 tons of depleted-uranium munitions in Iraq (and of about 40 tons in Yugoslavia) is comparable to that arising from the hypothetical battle-field use of more than 600 kt (respectively 60 kt) of high-explosive equivalent pure-fusion fourth-generation nuclear weapons. Despite the limited knowledge openly available on existing and future nuclear weapons, there is sufficient published information on their physical principles and radiological effects to make such a comparison. In fact, it is shown that this comparison can be made with very simple and convincing arguments so that the main technical conclusions of the paper are undisputable -- although it would be worthwhile to supplement the hand calculations presented in the paper by more detailed computer simulations in order to consolidate the conclusions and refute any possible objections.","A comparison of delayed radiobiological effects of depleted-uranium munitions versus fourth-generation nuclear weapons It is shown that the radiological burden due to the battlefield use of circa 400 tons of depleted-uranium munitions in Iraq (and of about 40 tons in Yugoslavia) is comparable to that arising from the hypothetical battle-field use of more than 600 kt (respectively 60 kt) of high-explosive equivalent pure-fusion fourth-generation nuclear weapons. Despite the limited knowledge openly available on existing and future nuclear weapons, there is sufficient published information on their physical principles and radiological effects to make such a comparison. In fact, it is shown that this comparison can be made with very simple and convincing arguments so that the main technical conclusions of the paper are undisputable -- although it would be worthwhile to supplement the hand calculations presented in the paper by more detailed computer simulations in order to consolidate the conclusions and refute any possible objections.",Healthcare
Foundation Models for Education: Promises and Prospects,"With the advent of foundation models like ChatGPT, educators are excited about the transformative role that AI might play in propelling the next education revolution. The developing speed and the profound impact of foundation models in various industries force us to think deeply about the changes they will make to education, a domain that is critically important for the future of humans. In this paper, we discuss the strengths of foundation models, such as personalized learning, education inequality, and reasoning capabilities, as well as the development of agent architecture tailored for education, which integrates AI agents with pedagogical frameworks to create adaptive learning environments. Furthermore, we highlight the risks and opportunities of AI overreliance and creativity. Lastly, we envision a future where foundation models in education harmonize human and AI capabilities, fostering a dynamic, inclusive, and adaptive educational ecosystem.","Foundation Models for Education: Promises and Prospects With the advent of foundation models like ChatGPT, educators are excited about the transformative role that AI might play in propelling the next education revolution. The developing speed and the profound impact of foundation models in various industries force us to think deeply about the changes they will make to education, a domain that is critically important for the future of humans. In this paper, we discuss the strengths of foundation models, such as personalized learning, education inequality, and reasoning capabilities, as well as the development of agent architecture tailored for education, which integrates AI agents with pedagogical frameworks to create adaptive learning environments. Furthermore, we highlight the risks and opportunities of AI overreliance and creativity. Lastly, we envision a future where foundation models in education harmonize human and AI capabilities, fostering a dynamic, inclusive, and adaptive educational ecosystem.",Education
"Bridging MOOCs, Smart Teaching, and AI: A Decade of Evolution Toward a Unified Pedagogy","Over the past decade, higher education has evolved through three distinct paradigms: the emergence of Massive Open Online Courses (MOOCs), the integration of Smart Teaching technologies into classrooms, and the rise of AI-enhanced learning. Each paradigm is intended to address specific challenges in traditional education: MOOCs enable ubiquitous access to learning resources; Smart Teaching supports real-time interaction with data-driven insights; and generative AI offers personalized feedback and on-demand content generation. However, these paradigms are often implemented in isolation due to their disparate technological origins and policy-driven adoption. This paper examines the origins, strengths, and limitations of each paradigm, and advocates a unified pedagogical perspective that synthesizes their complementary affordances. We propose a three-layer instructional framework that combines the scalability of MOOCs, the responsiveness of Smart Teaching, and the adaptivity of AI. To demonstrate its feasibility, we present a curriculum design for a project-based course. The findings highlight the frameworks potential to enhance learner engagement, support instructors, and enable personalized yet scalable learning.","Bridging MOOCs, Smart Teaching, and AI: A Decade of Evolution Toward a Unified Pedagogy Over the past decade, higher education has evolved through three distinct paradigms: the emergence of Massive Open Online Courses (MOOCs), the integration of Smart Teaching technologies into classrooms, and the rise of AI-enhanced learning. Each paradigm is intended to address specific challenges in traditional education: MOOCs enable ubiquitous access to learning resources; Smart Teaching supports real-time interaction with data-driven insights; and generative AI offers personalized feedback and on-demand content generation. However, these paradigms are often implemented in isolation due to their disparate technological origins and policy-driven adoption. This paper examines the origins, strengths, and limitations of each paradigm, and advocates a unified pedagogical perspective that synthesizes their complementary affordances. We propose a three-layer instructional framework that combines the scalability of MOOCs, the responsiveness of Smart Teaching, and the adaptivity of AI. To demonstrate its feasibility, we present a curriculum design for a project-based course. The findings highlight the frameworks potential to enhance learner engagement, support instructors, and enable personalized yet scalable learning.",Education
Inferring Fundamental Value and Crash Nonlinearity from Bubble Calibration,"Identifying unambiguously the presence of a bubble in an asset price remains an unsolved problem in standard econometric and financial economic approaches. A large part of the problem is that the fundamental value of an asset is, in general, not directly observable and it is poorly constrained to calculate. Further, it is not possible to distinguish between an exponentially growing fundamental price and an exponentially growing bubble price. We present a series of new models based on the Johansen-Ledoit-Sornette (JLS) model, which is a flexible tool to detect bubbles and predict changes of regime in financial markets. Our new models identify the fundamental value of an asset price and crash nonlinearity from a bubble calibration. In addition to forecasting the time of the end of a bubble, the new models can also estimate the fundamental value and the crash nonlinearity. Besides, the crash nonlinearity obtained in the new models presents a new approach to possibly identify the dynamics of a crash after a bubble. We test the models using data from three historical bubbles ending in crashes from different markets. They are: the Hong Kong Hang Seng index 1997 crash, the SP 500 index 1987 crash and the Shanghai Composite index 2009 crash. All results suggest that the new models perform very well in describing bubbles, forecasting their ending times and estimating fundamental value and the crash nonlinearity. The performance of the new models is tested under both the Gaussian and non-Gaussian residual assumption. Under the Gaussian residual assumption, nested hypotheses with the Wilks statistics are used and the p-values suggest that models with more parameters are necessary. Under non-Gaussian residual assumption, we use a bootstrap method to get type I and II errors of the hypotheses. All tests confirm that the generalized JLS models provide useful improvements over the standard JLS model.","Inferring Fundamental Value and Crash Nonlinearity from Bubble Calibration Identifying unambiguously the presence of a bubble in an asset price remains an unsolved problem in standard econometric and financial economic approaches. A large part of the problem is that the fundamental value of an asset is, in general, not directly observable and it is poorly constrained to calculate. Further, it is not possible to distinguish between an exponentially growing fundamental price and an exponentially growing bubble price. We present a series of new models based on the Johansen-Ledoit-Sornette (JLS) model, which is a flexible tool to detect bubbles and predict changes of regime in financial markets. Our new models identify the fundamental value of an asset price and crash nonlinearity from a bubble calibration. In addition to forecasting the time of the end of a bubble, the new models can also estimate the fundamental value and the crash nonlinearity. Besides, the crash nonlinearity obtained in the new models presents a new approach to possibly identify the dynamics of a crash after a bubble. We test the models using data from three historical bubbles ending in crashes from different markets. They are: the Hong Kong Hang Seng index 1997 crash, the SP 500 index 1987 crash and the Shanghai Composite index 2009 crash. All results suggest that the new models perform very well in describing bubbles, forecasting their ending times and estimating fundamental value and the crash nonlinearity. The performance of the new models is tested under both the Gaussian and non-Gaussian residual assumption. Under the Gaussian residual assumption, nested hypotheses with the Wilks statistics are used and the p-values suggest that models with more parameters are necessary. Under non-Gaussian residual assumption, we use a bootstrap method to get type I and II errors of the hypotheses. All tests confirm that the generalized JLS models provide useful improvements over the standard JLS model.",Finance
Crosswashing in Sustainable Investing: Unveiling Strategic Practices Impacting ESG Scores,"This paper introduces and defines a novel concept in sustainable investing, termed crosswashing, and explore its impact on ESG (Environmental, Social, and Governance) ratings through quantitative analysis using a Multi-Criteria Decision Making (MCDM) model. The study emphasises that this specific form of greenwashing is not currently considered in existing ESG assessments, potentially leading to an inflated perception of corporate ethical practices. Unlike traditional greenwashing, crosswashing involves companies strategically investing in sustainable activities to boost Environmental, Social, and Governance (ESG) scores while preserving nonsustainable core operations. By unveiling the nuances of crosswashing, the research contributes to a more nuanced understanding of sustainable investing, offering insights for improved evaluation and regulation of corporate environmental and ethical responsibilities.","Crosswashing in Sustainable Investing: Unveiling Strategic Practices Impacting ESG Scores This paper introduces and defines a novel concept in sustainable investing, termed crosswashing, and explore its impact on ESG (Environmental, Social, and Governance) ratings through quantitative analysis using a Multi-Criteria Decision Making (MCDM) model. The study emphasises that this specific form of greenwashing is not currently considered in existing ESG assessments, potentially leading to an inflated perception of corporate ethical practices. Unlike traditional greenwashing, crosswashing involves companies strategically investing in sustainable activities to boost Environmental, Social, and Governance (ESG) scores while preserving nonsustainable core operations. By unveiling the nuances of crosswashing, the research contributes to a more nuanced understanding of sustainable investing, offering insights for improved evaluation and regulation of corporate environmental and ethical responsibilities.",Finance
Trend and Thoughts: Understanding Climate Change Concern using Machine Learning and Social Media Data,"Nowadays social media platforms such as Twitter provide a great opportunity to understand public opinion of climate change compared to traditional survey methods. In this paper, we constructed a massive climate change Twitter dataset and conducted comprehensive analysis using machine learning. By conducting topic modeling and natural language processing, we show the relationship between the number of tweets about climate change and major climate events; the common topics people discuss climate change; and the trend of sentiment. Our dataset was published on Kaggle (urlhttps:www.kaggle.comleonshangguanclimate-change-tweets-ids-until-aug-2021) and can be used in further research.","Trend and Thoughts: Understanding Climate Change Concern using Machine Learning and Social Media Data Nowadays social media platforms such as Twitter provide a great opportunity to understand public opinion of climate change compared to traditional survey methods. In this paper, we constructed a massive climate change Twitter dataset and conducted comprehensive analysis using machine learning. By conducting topic modeling and natural language processing, we show the relationship between the number of tweets about climate change and major climate events; the common topics people discuss climate change; and the trend of sentiment. Our dataset was published on Kaggle (urlhttps:www.kaggle.comleonshangguanclimate-change-tweets-ids-until-aug-2021) and can be used in further research.",Environment
Remote Renewable Energy Hubs: a Taxonomy,"Serving the energy demand with renewable energy is hindered by its limited availability near load centres (i.e. places where the energy demand is high). To address this challenge, the concept of Remote Renewable Energy Hubs (RREH) emerges as a promising solution. RREHs are energy hubs located in areas with abundant renewable energy sources, such as sun in the Sahara Desert or wind in Greenland. In these hubs, renewable energy sources are used to synthetise energy molecules. To produce specific energy molecules, a tailored hub configuration must be designed, which means choosing a set of technologies that are interacting with each other as well as defining how they are integrated in their local environment. The plurality of technologies that may be employed in RREHs results in a large diversity of hubs. In order to characterize this diversity, we propose in this paper a taxonomy for accurately defining these hubs. This taxonomy allows to better describe and compare designs of hubs as well as to identify new ones. Thus, it may guide policymakers and engineers in hub design, contributing to cost efficiency andor improving local integration.","Remote Renewable Energy Hubs: a Taxonomy Serving the energy demand with renewable energy is hindered by its limited availability near load centres (i.e. places where the energy demand is high). To address this challenge, the concept of Remote Renewable Energy Hubs (RREH) emerges as a promising solution. RREHs are energy hubs located in areas with abundant renewable energy sources, such as sun in the Sahara Desert or wind in Greenland. In these hubs, renewable energy sources are used to synthetise energy molecules. To produce specific energy molecules, a tailored hub configuration must be designed, which means choosing a set of technologies that are interacting with each other as well as defining how they are integrated in their local environment. The plurality of technologies that may be employed in RREHs results in a large diversity of hubs. In order to characterize this diversity, we propose in this paper a taxonomy for accurately defining these hubs. This taxonomy allows to better describe and compare designs of hubs as well as to identify new ones. Thus, it may guide policymakers and engineers in hub design, contributing to cost efficiency andor improving local integration.",Environment
"Artificial Intelligence Technologies in Education: Benefits, Challenges and Strategies of Implementation","Since the education sector is associated with highly dynamic business environments which are controlled and maintained by information systems, recent technological advancements and the increasing pace of adopting artificial intelligence (AI) technologies constitute a need to identify and analyze the issues regarding their implementation in education sector. However, a study of the contemporary literature reveled that relatively little research has been undertaken in this area. To fill this void, we have identified the benefits and challenges of implementing artificial intelligence in the education sector, preceded by a short discussion on the concepts of AI and its evolution over time. Moreover, we have also reviewed modern AI technologies for learners and educators, currently available on the software market, evaluating their usefulness. Last but not least, we have developed a strategy implementation model, described by a five-stage, generic process, along with the corresponding configuration guide. To verify and validate their design, we separately developed three implementation strategies for three different higher education organizations. We believe that the obtained results will contribute to better understanding the specificities of AI systems, services and tools, and afterwards pave a smooth way in their implementation.","Artificial Intelligence Technologies in Education: Benefits, Challenges and Strategies of Implementation Since the education sector is associated with highly dynamic business environments which are controlled and maintained by information systems, recent technological advancements and the increasing pace of adopting artificial intelligence (AI) technologies constitute a need to identify and analyze the issues regarding their implementation in education sector. However, a study of the contemporary literature reveled that relatively little research has been undertaken in this area. To fill this void, we have identified the benefits and challenges of implementing artificial intelligence in the education sector, preceded by a short discussion on the concepts of AI and its evolution over time. Moreover, we have also reviewed modern AI technologies for learners and educators, currently available on the software market, evaluating their usefulness. Last but not least, we have developed a strategy implementation model, described by a five-stage, generic process, along with the corresponding configuration guide. To verify and validate their design, we separately developed three implementation strategies for three different higher education organizations. We believe that the obtained results will contribute to better understanding the specificities of AI systems, services and tools, and afterwards pave a smooth way in their implementation.",Education
A Formal Framework for Speedup Learning from Problems and Solutions,"Speedup learning seeks to improve the computational efficiency of problem solving with experience. In this paper, we develop a formal framework for learning efficient problem solving from random problems and their solutions. We apply this framework to two different representations of learned knowledge, namely control rules and macro-operators, and prove theorems that identify sufficient conditions for learning in each representation. Our proofs are constructive in that they are accompanied with learning algorithms. Our framework captures both empirical and explanation-based speedup learning in a unified fashion. We illustrate our framework with implementations in two domains: symbolic integration and Eight Puzzle. This work integrates many strands of experimental and theoretical work in machine learning, including empirical learning of control rules, macro-operator learning, Explanation-Based Learning (EBL), and Probably Approximately Correct (PAC) Learning.","A Formal Framework for Speedup Learning from Problems and Solutions Speedup learning seeks to improve the computational efficiency of problem solving with experience. In this paper, we develop a formal framework for learning efficient problem solving from random problems and their solutions. We apply this framework to two different representations of learned knowledge, namely control rules and macro-operators, and prove theorems that identify sufficient conditions for learning in each representation. Our proofs are constructive in that they are accompanied with learning algorithms. Our framework captures both empirical and explanation-based speedup learning in a unified fashion. We illustrate our framework with implementations in two domains: symbolic integration and Eight Puzzle. This work integrates many strands of experimental and theoretical work in machine learning, including empirical learning of control rules, macro-operator learning, Explanation-Based Learning (EBL), and Probably Approximately Correct (PAC) Learning.",Technology
GPM Draft Science Implementation Plan Ground Validation Chapter,"The validation of NASA Global Precipitation Mission (GPM) satellite precipitation products is important for their credibility and utility within the larger community. This document defines GPM ground validation scientific objectives and several programmatic components for meeting those objectives. Multi-year, multi-sensor ground-based observation programs in a few locations are proposed to generate local observation products and global error covariance products. Focused measurement programs utilizing aircraft, ships, and ground-based measurements would fill in geographic and scientific gaps not addressed by the multi-year observing programs.","GPM Draft Science Implementation Plan Ground Validation Chapter The validation of NASA Global Precipitation Mission (GPM) satellite precipitation products is important for their credibility and utility within the larger community. This document defines GPM ground validation scientific objectives and several programmatic components for meeting those objectives. Multi-year, multi-sensor ground-based observation programs in a few locations are proposed to generate local observation products and global error covariance products. Focused measurement programs utilizing aircraft, ships, and ground-based measurements would fill in geographic and scientific gaps not addressed by the multi-year observing programs.",Environment
Preference Learning in Terminology Extraction: A ROC-based approach,"A key data preparation step in Text Mining, Term Extraction selects the terms, or collocation of words, attached to specific concepts. In this paper, the task of extracting relevant collocations is achieved through a supervised learning algorithm, exploiting a few collocations manually labelled as relevantirrelevant. The candidate terms are described along 13 standard statistical criteria measures. From these examples, an evolutionary learning algorithm termed Roger, based on the optimization of the Area under the ROC curve criterion, extracts an order on the candidate terms. The robustness of the approach is demonstrated on two real-world domain applications, considering different domains (biology and human resources) and different languages (English and French).","Preference Learning in Terminology Extraction: A ROC-based approach A key data preparation step in Text Mining, Term Extraction selects the terms, or collocation of words, attached to specific concepts. In this paper, the task of extracting relevant collocations is achieved through a supervised learning algorithm, exploiting a few collocations manually labelled as relevantirrelevant. The candidate terms are described along 13 standard statistical criteria measures. From these examples, an evolutionary learning algorithm termed Roger, based on the optimization of the Area under the ROC curve criterion, extracts an order on the candidate terms. The robustness of the approach is demonstrated on two real-world domain applications, considering different domains (biology and human resources) and different languages (English and French).",Technology
Dialogues between Astrobiology and Earth Pedagogy: a proposal to the continued training of science teachers,"Introduction and Objective. Assuming the need to rethink Education and its function as an element of social transformation and generator of a profound ethical change and promoter of other ways of being on planet Earth, this study seeks to understand the possible dialogues between Earth Pedagogy and Astrobiology in the construction and proposition of continuing education processes for science teachers. Methodology. A mini-course was promoted aimed at teachers and an attempt was made to report the construction process of the training process and, using Content Analysis, to analyze the contributions pointed out by the participants to their training and teaching practice. Results and Conclusion. The dialogue between knowledge in the continuing education of teachers contributes with fundamental elements to teaching practice, covering cognitive, emotional, individual and collective aspects, as well as highlighting the role of the university in promoting initiatives that bring Education and Complexity together.","Dialogues between Astrobiology and Earth Pedagogy: a proposal to the continued training of science teachers Introduction and Objective. Assuming the need to rethink Education and its function as an element of social transformation and generator of a profound ethical change and promoter of other ways of being on planet Earth, this study seeks to understand the possible dialogues between Earth Pedagogy and Astrobiology in the construction and proposition of continuing education processes for science teachers. Methodology. A mini-course was promoted aimed at teachers and an attempt was made to report the construction process of the training process and, using Content Analysis, to analyze the contributions pointed out by the participants to their training and teaching practice. Results and Conclusion. The dialogue between knowledge in the continuing education of teachers contributes with fundamental elements to teaching practice, covering cognitive, emotional, individual and collective aspects, as well as highlighting the role of the university in promoting initiatives that bring Education and Complexity together.",Education
Assessment of Amazon Comprehend Medical: Medication Information Extraction,"In November 27, 2018, Amazon Web Services (AWS) released Amazon Comprehend Medical (ACM), a deep learning based system that automatically extracts clinical concepts (which include anatomy, medical conditions, protected health information (PH)I, test names, treatment names, and medical procedures, and medications) from clinical text notes. Uptake and trust in any new data product relies on independent validation across benchmark datasets and tools to establish and confirm expected quality of results. This work focuses on the medication extraction task, and particularly, ACM was evaluated using the official test sets from the 2009 i2b2 Medication Extraction Challenge and 2018 n2c2 Track 2: Adverse Drug Events and Medication Extraction in EHRs. Overall, ACM achieved F-scores of 0.768 and 0.828. These scores ranked the lowest when compared to the three best systems in the respective challenges. To further establish the generalizability of its medication extraction performance, a set of random internal clinical text notes from NYU Langone Medical Center were also included in this work. And in this corpus, ACM garnered an F-score of 0.753.","Assessment of Amazon Comprehend Medical: Medication Information Extraction In November 27, 2018, Amazon Web Services (AWS) released Amazon Comprehend Medical (ACM), a deep learning based system that automatically extracts clinical concepts (which include anatomy, medical conditions, protected health information (PH)I, test names, treatment names, and medical procedures, and medications) from clinical text notes. Uptake and trust in any new data product relies on independent validation across benchmark datasets and tools to establish and confirm expected quality of results. This work focuses on the medication extraction task, and particularly, ACM was evaluated using the official test sets from the 2009 i2b2 Medication Extraction Challenge and 2018 n2c2 Track 2: Adverse Drug Events and Medication Extraction in EHRs. Overall, ACM achieved F-scores of 0.768 and 0.828. These scores ranked the lowest when compared to the three best systems in the respective challenges. To further establish the generalizability of its medication extraction performance, a set of random internal clinical text notes from NYU Langone Medical Center were also included in this work. And in this corpus, ACM garnered an F-score of 0.753.",Healthcare
Effects of bone- and air-tissue inhomogeneities on the dose distributions of the Leksell Gamma KnifecircledR calculated with PENELOPE,"Monte Carlo simulation with PENELOPE (v.2003) is applied to calculate Leksell Gamma KnifecircledR dose distributions for heterogeneous phantoms. The usual spherical water phantom is modified with a spherical bone shell simulating the skull and an air-filled cube simulating the frontal or maxillary sinuses. Different simulations of the 201 source configuration of the Gamma Knife have been carried out with a simplified model of the geometry of the source channel of the Gamma Knife recently tested for both single source and multisource configurations. The dose distributions determined for heterogeneous phantoms including the bone- andor air-tissue interfaces show non negligible differences with respect to those calculated for a homogeneous one, mainly when the Gamma Knife isocenter approaches the separation surfaces. Our findings confirm an important underdosage (sim10) nearby the air-tissue interface, in accordance with previous results obtained with PENELOPE code with a procedure different to ours. On the other hand, the presence of the spherical shell simulating the skull produces a few percent underdosage at the isocenter wherever it is situated.","Effects of bone- and air-tissue inhomogeneities on the dose distributions of the Leksell Gamma KnifecircledR calculated with PENELOPE Monte Carlo simulation with PENELOPE (v.2003) is applied to calculate Leksell Gamma KnifecircledR dose distributions for heterogeneous phantoms. The usual spherical water phantom is modified with a spherical bone shell simulating the skull and an air-filled cube simulating the frontal or maxillary sinuses. Different simulations of the 201 source configuration of the Gamma Knife have been carried out with a simplified model of the geometry of the source channel of the Gamma Knife recently tested for both single source and multisource configurations. The dose distributions determined for heterogeneous phantoms including the bone- andor air-tissue interfaces show non negligible differences with respect to those calculated for a homogeneous one, mainly when the Gamma Knife isocenter approaches the separation surfaces. Our findings confirm an important underdosage (sim10) nearby the air-tissue interface, in accordance with previous results obtained with PENELOPE code with a procedure different to ours. On the other hand, the presence of the spherical shell simulating the skull produces a few percent underdosage at the isocenter wherever it is situated.",Healthcare
Improvement or selection? A longitudinal analysis of students views about experimental physics in their lab courses,"Laboratory courses represent a unique and potentially important component of the undergraduate physics curriculum, which can be designed to allow students to authentically engage with the process of experimental physics. Among other possible benefits, participation in these courses throughout the undergraduate physics curriculum presents an opportunity to develop students understanding of the nature and importance of experimental physics within the discipline as a whole. Here, we present and compare both a longitudinal and pseudo-longitudinal analysis of students responses to a research-based assessment targeting students views about experimental physics -- the Colorado Learning Attitudes about Science Survey for Experimental Physics (E-CLASS) -- across multiple, required lab courses at a single institution. We find that, while pseudo-longitudinal averages showed increases in students E-CLASS scores in each consecutive course, analysis of longitudinal data indicates that this increase was not driven by a cumulative impact of laboratory instruction. Rather, the increase was driven by a selection effect in which students who persisted into higher-level lab courses already had more expert-like beliefs, attitudes, and expectations than their peers when they started the lower-level courses.","Improvement or selection? A longitudinal analysis of students views about experimental physics in their lab courses Laboratory courses represent a unique and potentially important component of the undergraduate physics curriculum, which can be designed to allow students to authentically engage with the process of experimental physics. Among other possible benefits, participation in these courses throughout the undergraduate physics curriculum presents an opportunity to develop students understanding of the nature and importance of experimental physics within the discipline as a whole. Here, we present and compare both a longitudinal and pseudo-longitudinal analysis of students responses to a research-based assessment targeting students views about experimental physics -- the Colorado Learning Attitudes about Science Survey for Experimental Physics (E-CLASS) -- across multiple, required lab courses at a single institution. We find that, while pseudo-longitudinal averages showed increases in students E-CLASS scores in each consecutive course, analysis of longitudinal data indicates that this increase was not driven by a cumulative impact of laboratory instruction. Rather, the increase was driven by a selection effect in which students who persisted into higher-level lab courses already had more expert-like beliefs, attitudes, and expectations than their peers when they started the lower-level courses.",Education
New Computational Approaches to Analysis of Interbeat Intervals in Human Subjects,"We investigate the Markov nature, Cascade of information from large time scale to small scale and extended self similarity properties of the beat to beat fluctuations of healthy subjects as well as those with congestive heart failure. To check the Markov nature, we use a novel inverse method that utilizes a set of data to construct a simple equation that governs the stochastic process for which the data have been measured, hence enabling us to reconstruct the stochastic process. The inverse method provides a novel technique for distinguishing the two classes of subjects in terms of a drift and a diffusion coefficients which behave completely differently for the two classes of subjects.To investigate the cascade of information from large to small time scales we also analyze the statistical properties of interbeat intervals cascade by considering the joint probability distribution for two interbeat increments. As a result, the joint probability distributions of the increments in the interbeat intervals obey a Fokker-Planck equation. Finally we analyze the extended self-similarity (ESS) in the beat-to-beat fluctuations in the heart rates of healthy and congestive heart failure subjects.The proposed methods provide the novel techniques for distinguishing the two classes of subjects in terms of the drift and diffusion coefficients, intermittency exponents which behave differently for two classes of the subjects, namely, healthy subjects and those with congestive heart failure.","New Computational Approaches to Analysis of Interbeat Intervals in Human Subjects We investigate the Markov nature, Cascade of information from large time scale to small scale and extended self similarity properties of the beat to beat fluctuations of healthy subjects as well as those with congestive heart failure. To check the Markov nature, we use a novel inverse method that utilizes a set of data to construct a simple equation that governs the stochastic process for which the data have been measured, hence enabling us to reconstruct the stochastic process. The inverse method provides a novel technique for distinguishing the two classes of subjects in terms of a drift and a diffusion coefficients which behave completely differently for the two classes of subjects.To investigate the cascade of information from large to small time scales we also analyze the statistical properties of interbeat intervals cascade by considering the joint probability distribution for two interbeat increments. As a result, the joint probability distributions of the increments in the interbeat intervals obey a Fokker-Planck equation. Finally we analyze the extended self-similarity (ESS) in the beat-to-beat fluctuations in the heart rates of healthy and congestive heart failure subjects.The proposed methods provide the novel techniques for distinguishing the two classes of subjects in terms of the drift and diffusion coefficients, intermittency exponents which behave differently for two classes of the subjects, namely, healthy subjects and those with congestive heart failure.",Healthcare
"Medical Multimodal Foundation Models in Clinical Diagnosis and Treatment: Applications, Challenges, and Future Directions","Recent advancements in deep learning have significantly revolutionized the field of clinical diagnosis and treatment, offering novel approaches to improve diagnostic precision and treatment efficacy across diverse clinical domains, thus driving the pursuit of precision medicine. The growing availability of multi-organ and multimodal datasets has accelerated the development of large-scale Medical Multimodal Foundation Models (MMFMs). These models, known for their strong generalization capabilities and rich representational power, are increasingly being adapted to address a wide range of clinical tasks, from early diagnosis to personalized treatment strategies. This review offers a comprehensive analysis of recent developments in MMFMs, focusing on three key aspects: datasets, model architectures, and clinical applications. We also explore the challenges and opportunities in optimizing multimodal representations and discuss how these advancements are shaping the future of healthcare by enabling improved patient outcomes and more efficient clinical workflows.","Medical Multimodal Foundation Models in Clinical Diagnosis and Treatment: Applications, Challenges, and Future Directions Recent advancements in deep learning have significantly revolutionized the field of clinical diagnosis and treatment, offering novel approaches to improve diagnostic precision and treatment efficacy across diverse clinical domains, thus driving the pursuit of precision medicine. The growing availability of multi-organ and multimodal datasets has accelerated the development of large-scale Medical Multimodal Foundation Models (MMFMs). These models, known for their strong generalization capabilities and rich representational power, are increasingly being adapted to address a wide range of clinical tasks, from early diagnosis to personalized treatment strategies. This review offers a comprehensive analysis of recent developments in MMFMs, focusing on three key aspects: datasets, model architectures, and clinical applications. We also explore the challenges and opportunities in optimizing multimodal representations and discuss how these advancements are shaping the future of healthcare by enabling improved patient outcomes and more efficient clinical workflows.",Healthcare
Interactions between Health Searchers and Search Engines,"The Web is an important resource for understanding and diagnosing medical conditions. Based on exposure to online content, people may develop undue health concerns, believing that common and benign symptoms are explained by serious illnesses. In this paper, we investigate potential strategies to mine queries and searcher histories for clues that could help search engines choose the most appropriate information to present in response to exploratory medical queries. To do this, we performed a longitudinal study of health search behavior using the logs of a popular search engine. We found that query variations which might appear innocuous (e.g. bad headache vs severe headache) may hold valuable information about the searcher which could be used by search engines to improve performance. Furthermore, we investigated how medically concerned users respond differently to search engine result pages (SERPs) and find that their disposition for clicking on concerning pages is pronounced, potentially leading to a self-reinforcement of concern. Finally, we studied to which degree variations in the SERP impact future search and real-world health-seeking behavior and obtained some surprising results (e.g., viewing concerning pages may lead to a short-term reduction of real-world health seeking).","Interactions between Health Searchers and Search Engines The Web is an important resource for understanding and diagnosing medical conditions. Based on exposure to online content, people may develop undue health concerns, believing that common and benign symptoms are explained by serious illnesses. In this paper, we investigate potential strategies to mine queries and searcher histories for clues that could help search engines choose the most appropriate information to present in response to exploratory medical queries. To do this, we performed a longitudinal study of health search behavior using the logs of a popular search engine. We found that query variations which might appear innocuous (e.g. bad headache vs severe headache) may hold valuable information about the searcher which could be used by search engines to improve performance. Furthermore, we investigated how medically concerned users respond differently to search engine result pages (SERPs) and find that their disposition for clicking on concerning pages is pronounced, potentially leading to a self-reinforcement of concern. Finally, we studied to which degree variations in the SERP impact future search and real-world health-seeking behavior and obtained some surprising results (e.g., viewing concerning pages may lead to a short-term reduction of real-world health seeking).",Healthcare
Computer assisted planning and orbital surgery: patient-related prediction of osteotomy size in proptosis reduction,"BACKGROUND: Proptosis is characterized by a protrusion of the eyeball due to an increase of the orbital tissue volume. To recover a normal eyeball positioning, the most frequent surgical technique consists in the osteotomy of orbital walls combined with the manual loading on the eyeball. Only a rough clinical rule is currently available for the surgeons but it is useless for this technique. The first biomechanical model dealing with proptosis reduction, validated in one patient, has been previously proposed by the authors. METHODS: This paper proposes a rule improving the pre-operative planning of the osteotomy size in proptosis reduction. Patient-related poroelastic FE models combined with sensitivity studies were used to propose two clinical rules to improve the pre-operative planning of proptosis reduction. This poroelastic model was run on 12 patients. Sensitivity studies permitted to establish relationships between the osteotoemy size, the patient-related orbital volume, the decompressed tissue volume and the eyeball backward displacement. FINDINGS: The eyeball displacement and the osteotomy size were non-linearly related: an exponential rule has been proposed. The patient-related orbital volume showed a significant influence: a bi-quadratic analytical equation liking the osteotomy size, the orbital volume and the targeted eyeball protrusion has been established. INTERPRETATION: Two process rules derived from patient-related biomechanical FE models have been proposed for the proptosis reduction planning. The implementation of the process rules into a clinical setting is easy since only a sagittal radiography is required. The osteotomy size can be monitored using optical guided instruments.","Computer assisted planning and orbital surgery: patient-related prediction of osteotomy size in proptosis reduction BACKGROUND: Proptosis is characterized by a protrusion of the eyeball due to an increase of the orbital tissue volume. To recover a normal eyeball positioning, the most frequent surgical technique consists in the osteotomy of orbital walls combined with the manual loading on the eyeball. Only a rough clinical rule is currently available for the surgeons but it is useless for this technique. The first biomechanical model dealing with proptosis reduction, validated in one patient, has been previously proposed by the authors. METHODS: This paper proposes a rule improving the pre-operative planning of the osteotomy size in proptosis reduction. Patient-related poroelastic FE models combined with sensitivity studies were used to propose two clinical rules to improve the pre-operative planning of proptosis reduction. This poroelastic model was run on 12 patients. Sensitivity studies permitted to establish relationships between the osteotoemy size, the patient-related orbital volume, the decompressed tissue volume and the eyeball backward displacement. FINDINGS: The eyeball displacement and the osteotomy size were non-linearly related: an exponential rule has been proposed. The patient-related orbital volume showed a significant influence: a bi-quadratic analytical equation liking the osteotomy size, the orbital volume and the targeted eyeball protrusion has been established. INTERPRETATION: Two process rules derived from patient-related biomechanical FE models have been proposed for the proptosis reduction planning. The implementation of the process rules into a clinical setting is easy since only a sagittal radiography is required. The osteotomy size can be monitored using optical guided instruments.",Healthcare
A large-scale bibliometric analysis of global climate change research between 2001 and 2018,"Global climate change is attracting widespread scientific, political, and public attention owing to the involvement of international initiatives such as the Paris Agreement and the Intergovernmental Panel on Climate Change. We present a large-scale bibliometric analysis based on approximately 120,000 climate change publications between 2001 and 2018 to examine how climate change is studied in scientific research. Our analysis provides an overview of scientific knowledge, shifts of research hotspots, global geographical distribution of research, and focus of individual countries. In our analysis, we identify five key fields in climate change research: physical sciences, paleoclimatology, climate-change ecology, climate technology, and climate policy. We draw the following key conclusions: (1) Over the investigated time period, the focus of climate change research has shifted from understanding the climate system toward climate technologies and policies, such as efficient energy use and legislation. (2) There is an imbalance in scientific production between developed and developing countries. (3) Geography, national demands, and national strategies have been important drivers that influence the research interests and concerns of researchers in different countries. Our study can be used by researchers and policy makers to reflect on the directions in which climate change research is developing and discuss priorities for future research.","A large-scale bibliometric analysis of global climate change research between 2001 and 2018 Global climate change is attracting widespread scientific, political, and public attention owing to the involvement of international initiatives such as the Paris Agreement and the Intergovernmental Panel on Climate Change. We present a large-scale bibliometric analysis based on approximately 120,000 climate change publications between 2001 and 2018 to examine how climate change is studied in scientific research. Our analysis provides an overview of scientific knowledge, shifts of research hotspots, global geographical distribution of research, and focus of individual countries. In our analysis, we identify five key fields in climate change research: physical sciences, paleoclimatology, climate-change ecology, climate technology, and climate policy. We draw the following key conclusions: (1) Over the investigated time period, the focus of climate change research has shifted from understanding the climate system toward climate technologies and policies, such as efficient energy use and legislation. (2) There is an imbalance in scientific production between developed and developing countries. (3) Geography, national demands, and national strategies have been important drivers that influence the research interests and concerns of researchers in different countries. Our study can be used by researchers and policy makers to reflect on the directions in which climate change research is developing and discuss priorities for future research.",Environment
Simulation de trajectoires de processus continus,"Continuous time stochastic processes are useful models especially for financial and insurance purposes. The numerical simulation of such models is dependant of the time discrete discretization, of the parametric estimation and of the choice of a random number generator. The aim of this paper is to provide the tools for the practical implementation of diffusion processes simulation, particularly for insurance contexts.","Simulation de trajectoires de processus continus Continuous time stochastic processes are useful models especially for financial and insurance purposes. The numerical simulation of such models is dependant of the time discrete discretization, of the parametric estimation and of the choice of a random number generator. The aim of this paper is to provide the tools for the practical implementation of diffusion processes simulation, particularly for insurance contexts.",Finance
Teaching Logics through Their Philosophical Commitments: Logical Worldviews,I have developed a pedagogy and textbook for teaching logic centered on what I call logical worldviews. A logical worldview examines the close connection between philosophical commitments and the logical principles and method for a particular historial logical system. The class examines multiple historical logical worldviews to show how philosophical positions have over time yielded corresponding changes in the development of logic. Such an approach has great benefits to teaching logic to undergraduates.,Teaching Logics through Their Philosophical Commitments: Logical Worldviews I have developed a pedagogy and textbook for teaching logic centered on what I call logical worldviews. A logical worldview examines the close connection between philosophical commitments and the logical principles and method for a particular historial logical system. The class examines multiple historical logical worldviews to show how philosophical positions have over time yielded corresponding changes in the development of logic. Such an approach has great benefits to teaching logic to undergraduates.,Education
Cultural Investment and Urban Socio-Economic Development: A Geo-Social Network Approach,"Being able to assess the impact of government-led investment onto socio-economic indicators in cities has long been an important target of urban planning. However, due to the lack of large-scale data with a fine spatio-temporal resolution, there have been limitations in terms of how planners can track the impact and measure the effectiveness of cultural investment in small urban areas. Taking advantage of nearly 4 million transition records for three years in London from a popular location-based social network service, Foursquare, we study how the socio-economic impact of government cultural expenditure can be detected and predicted. Our analysis shows that network indicators such as average clustering coefficient or centrality can be exploited to estimate the likelihood of local growth in response to cultural investment. We subsequently integrate these features in supervised learning models to infer socio-economic deprivation changes for Londons neighbourhoods. This research presents how geo-social and mobile services can be used as a proxy to track and predict socio-economic deprivation changes as government financial effort is put in developing urban areas and thus gives evidence and suggestions for further policy-making and investment optimisation.","Cultural Investment and Urban Socio-Economic Development: A Geo-Social Network Approach Being able to assess the impact of government-led investment onto socio-economic indicators in cities has long been an important target of urban planning. However, due to the lack of large-scale data with a fine spatio-temporal resolution, there have been limitations in terms of how planners can track the impact and measure the effectiveness of cultural investment in small urban areas. Taking advantage of nearly 4 million transition records for three years in London from a popular location-based social network service, Foursquare, we study how the socio-economic impact of government cultural expenditure can be detected and predicted. Our analysis shows that network indicators such as average clustering coefficient or centrality can be exploited to estimate the likelihood of local growth in response to cultural investment. We subsequently integrate these features in supervised learning models to infer socio-economic deprivation changes for Londons neighbourhoods. This research presents how geo-social and mobile services can be used as a proxy to track and predict socio-economic deprivation changes as government financial effort is put in developing urban areas and thus gives evidence and suggestions for further policy-making and investment optimisation.",Finance
The impact of extracurricular education on socioeconomic mobility in Japan: an application of causal machine learning,"This paper explores the socioeconomic impacts of extracurricular education, specifically private tutoring, on social mobility in Japan. Using data from the 2015 National Survey on Social Stratification and Social Mobility (SSM), we employed a causal machine learning approach to evaluate this educational intervention on income, educational attainment, and occupational prestige. Our research suggests that while shadow education holds the potential for positive socioeconomic impacts, its benefits are undermined by the economic disparities among households, resulting in minimal overall improvement. This highlights the complex mechanisms between individual demographics and educational interventions, revealing promising machine learning applications in this field.","The impact of extracurricular education on socioeconomic mobility in Japan: an application of causal machine learning This paper explores the socioeconomic impacts of extracurricular education, specifically private tutoring, on social mobility in Japan. Using data from the 2015 National Survey on Social Stratification and Social Mobility (SSM), we employed a causal machine learning approach to evaluate this educational intervention on income, educational attainment, and occupational prestige. Our research suggests that while shadow education holds the potential for positive socioeconomic impacts, its benefits are undermined by the economic disparities among households, resulting in minimal overall improvement. This highlights the complex mechanisms between individual demographics and educational interventions, revealing promising machine learning applications in this field.",Education
Distributionally Robust Co-Optimization of Power Dispatch and Do-Not-Exceed Limits,"To address the challenge of the renewable energy uncertainty, the ISO New England (ISO-NE) has proposed to apply do-not-exceed (DNE) limits, which represent the maximum nodal injection of renewable energy the grid can accommodate. Unfortunately, it appears challenging to compute DNE limits that simultaneously maintain the system flexibility and incorporate a large portion of the available renewable energy at the minimum cost. In addition, it is often challenging to accurately estimate the joint probability distribution of the renewable energy. In this paper, we propose a two-stage distributionally robust optimization model that co-optimizes the power dispatch and the DNE limits, by adopting an affinely adjustable power re-dispatch and an adjustable joint chance constraint that measures the renewable utilization. Notably, this model admits a second-order conic reformulation that can be efficiently solved by the commercial solvers (e.g., MOSEK). We conduct case studies based on modified IEEE test instances to demonstrate the effectiveness of the proposed approach and analyze the trade-off among the system flexibility, the renewable utilization, and the dispatch cost.","Distributionally Robust Co-Optimization of Power Dispatch and Do-Not-Exceed Limits To address the challenge of the renewable energy uncertainty, the ISO New England (ISO-NE) has proposed to apply do-not-exceed (DNE) limits, which represent the maximum nodal injection of renewable energy the grid can accommodate. Unfortunately, it appears challenging to compute DNE limits that simultaneously maintain the system flexibility and incorporate a large portion of the available renewable energy at the minimum cost. In addition, it is often challenging to accurately estimate the joint probability distribution of the renewable energy. In this paper, we propose a two-stage distributionally robust optimization model that co-optimizes the power dispatch and the DNE limits, by adopting an affinely adjustable power re-dispatch and an adjustable joint chance constraint that measures the renewable utilization. Notably, this model admits a second-order conic reformulation that can be efficiently solved by the commercial solvers (e.g., MOSEK). We conduct case studies based on modified IEEE test instances to demonstrate the effectiveness of the proposed approach and analyze the trade-off among the system flexibility, the renewable utilization, and the dispatch cost.",Environment
Variational local structure estimation for image super-resolution,"Super-resolution is an important but difficult problem in imagevideo processing. If a video sequence or some training set other than the given low-resolution image is available, this kind of extra information can greatly aid in the reconstruction of the high-resolution image. The problem is substantially more difficult with only a single low-resolution image on hand. The image reconstruction methods designed primarily for denoising is insufficient for super-resolution problem in the sense that it tends to oversmooth images with essentially no noise. We propose a new adaptive linear interpolation method based on variational method and inspired by local linear embedding (LLE). The experimental result shows that our method avoids the problem of oversmoothing and preserves image structures well.","Variational local structure estimation for image super-resolution Super-resolution is an important but difficult problem in imagevideo processing. If a video sequence or some training set other than the given low-resolution image is available, this kind of extra information can greatly aid in the reconstruction of the high-resolution image. The problem is substantially more difficult with only a single low-resolution image on hand. The image reconstruction methods designed primarily for denoising is insufficient for super-resolution problem in the sense that it tends to oversmooth images with essentially no noise. We propose a new adaptive linear interpolation method based on variational method and inspired by local linear embedding (LLE). The experimental result shows that our method avoids the problem of oversmoothing and preserves image structures well.",Technology
Exploring Indoor Localization for Smart Education,"This comprehensive study delves into the realm of indoor positioning technologies within the domain of Smart Education (SE). Focusing on typical techniques and technologies in educational settings, the research emphasizes the importance and potential services of localization in SE. Moreover, this work explores the feasibility and limitations of these technologies, providing a detailed account of their role in educational settings. The paper also contains in an innovative Proof of Concept (PoC), demonstrating an automatic attendance control (AAC) system that integrates 5G and WiFi technologies. This PoC effectively showcases the possibilities and effectiveness of location-based services in educational surroundings even with a limited budget, setting the stage for optimizing teaching time, enhancing the quality of education.","Exploring Indoor Localization for Smart Education This comprehensive study delves into the realm of indoor positioning technologies within the domain of Smart Education (SE). Focusing on typical techniques and technologies in educational settings, the research emphasizes the importance and potential services of localization in SE. Moreover, this work explores the feasibility and limitations of these technologies, providing a detailed account of their role in educational settings. The paper also contains in an innovative Proof of Concept (PoC), demonstrating an automatic attendance control (AAC) system that integrates 5G and WiFi technologies. This PoC effectively showcases the possibilities and effectiveness of location-based services in educational surroundings even with a limited budget, setting the stage for optimizing teaching time, enhancing the quality of education.",Education
Bivariate network meta-analysis for surrogate endpoint evaluation,"Surrogate endpoints are very important in regulatory decision-making in healthcare, in particular if they can be measured early compared to the long-term final clinical outcome and act as good predictors of clinical benefit. Bivariate meta-analysis methods can be used to evaluate surrogate endpoints and to predict the treatment effect on the final outcome from the treatment effect measured on a surrogate endpoint. However, candidate surrogate endpoints are often imperfect, and the level of association between the treatment effects on the surrogate and final outcomes may vary between treatments. This imposes a limitation on the pairwise methods which do not differentiate between the treatments. We develop bivariate network meta-analysis (bvNMA) methods which combine data on treatment effects on the surrogate and final outcomes, from trials investigating heterogeneous treatment contrasts. The bvNMA methods estimate the effects on both outcomes for all treatment contrasts individually in a single analysis. At the same time, they allow us to model the surrogacy patterns across multiple trials (different populations) within a treatment contrast and across treatment contrasts, thus enabling predictions of the treatment effect on the final outcome for a new study in a new population or investigating a new treatment. Modelling assumptions about the between-studies heterogeneity and the network consistency, and their impact on predictions, are investigated using simulated data and an illustrative example in advanced colorectal cancer. When the strength of the surrogate relationships varies across treatment contrasts, bvNMA has the advantage of identifying treatments for which surrogacy holds, thus leading to better predictions.","Bivariate network meta-analysis for surrogate endpoint evaluation Surrogate endpoints are very important in regulatory decision-making in healthcare, in particular if they can be measured early compared to the long-term final clinical outcome and act as good predictors of clinical benefit. Bivariate meta-analysis methods can be used to evaluate surrogate endpoints and to predict the treatment effect on the final outcome from the treatment effect measured on a surrogate endpoint. However, candidate surrogate endpoints are often imperfect, and the level of association between the treatment effects on the surrogate and final outcomes may vary between treatments. This imposes a limitation on the pairwise methods which do not differentiate between the treatments. We develop bivariate network meta-analysis (bvNMA) methods which combine data on treatment effects on the surrogate and final outcomes, from trials investigating heterogeneous treatment contrasts. The bvNMA methods estimate the effects on both outcomes for all treatment contrasts individually in a single analysis. At the same time, they allow us to model the surrogacy patterns across multiple trials (different populations) within a treatment contrast and across treatment contrasts, thus enabling predictions of the treatment effect on the final outcome for a new study in a new population or investigating a new treatment. Modelling assumptions about the between-studies heterogeneity and the network consistency, and their impact on predictions, are investigated using simulated data and an illustrative example in advanced colorectal cancer. When the strength of the surrogate relationships varies across treatment contrasts, bvNMA has the advantage of identifying treatments for which surrogacy holds, thus leading to better predictions.",Healthcare
PAC Learning Mixtures of Axis-Aligned Gaussians with No Separation Assumption,"We propose and analyze a new vantage point for the learning of mixtures of Gaussians: namely, the PAC-style model of learning probability distributions introduced by Kearns et al. Here the task is to construct a hypothesis mixture of Gaussians that is statistically indistinguishable from the actual mixture generating the data; specifically, the KL-divergence should be at most epsilon. In this scenario, we give a poly(nepsilon)-time algorithm that learns the class of mixtures of any constant number of axis-aligned Gaussians in n-dimensional Euclidean space. Our algorithm makes no assumptions about the separation between the means of the Gaussians, nor does it have any dependence on the minimum mixing weight. This is in contrast to learning results known in the clustering model, where such assumptions are unavoidable. Our algorithm relies on the method of moments, and a subalgorithm developed in previous work by the authors (FOCS 2005) for a discrete mixture-learning problem.","PAC Learning Mixtures of Axis-Aligned Gaussians with No Separation Assumption We propose and analyze a new vantage point for the learning of mixtures of Gaussians: namely, the PAC-style model of learning probability distributions introduced by Kearns et al. Here the task is to construct a hypothesis mixture of Gaussians that is statistically indistinguishable from the actual mixture generating the data; specifically, the KL-divergence should be at most epsilon. In this scenario, we give a poly(nepsilon)-time algorithm that learns the class of mixtures of any constant number of axis-aligned Gaussians in n-dimensional Euclidean space. Our algorithm makes no assumptions about the separation between the means of the Gaussians, nor does it have any dependence on the minimum mixing weight. This is in contrast to learning results known in the clustering model, where such assumptions are unavoidable. Our algorithm relies on the method of moments, and a subalgorithm developed in previous work by the authors (FOCS 2005) for a discrete mixture-learning problem.",Technology
Physical modelling of the airflow-walls interactions to understand the sleep apnea syndrome,"Sleep Apnea Syndrome (SAS) is defined as a partial or total closure of the patient upper airways during sleep. The term collapsus (or collapse) is used to describe this closure. From a fluid mechanical point of view, this collapse can be understood as a spectacular example of fluid-walls interaction. Indeed, the upper airways are delimited in their largest part by soft tissues having different geometrical and mechanical properties: velum, tongue and pharyngeal walls. Airway closure during SAS comes from the interaction between these soft tissues and the inspiratory flow. The aim of this work is to understand the physical phenomena at the origin of the collapsus and the metamorphosis in inspiratory flow pattern that has been reported during SAS. Indeed, a full comprehension of the physical conditions allowing this phenomenon is a prerequisite to be able to help in the planning of the surgical gesture that can be prescribed for the patients. The work presented here focuses on a simple model of fluid-walls interactions. The equations governing the airflow inside a constriction are coupled with a Finite Element (FE) biomechanical model of the velum. The geometries of this model is extracted from a single midsagittal radiography of a patient. The velar deformations induced by airflow interactions are computed, presented, discussed and compared to measurements collected onto an experimental setup.","Physical modelling of the airflow-walls interactions to understand the sleep apnea syndrome Sleep Apnea Syndrome (SAS) is defined as a partial or total closure of the patient upper airways during sleep. The term collapsus (or collapse) is used to describe this closure. From a fluid mechanical point of view, this collapse can be understood as a spectacular example of fluid-walls interaction. Indeed, the upper airways are delimited in their largest part by soft tissues having different geometrical and mechanical properties: velum, tongue and pharyngeal walls. Airway closure during SAS comes from the interaction between these soft tissues and the inspiratory flow. The aim of this work is to understand the physical phenomena at the origin of the collapsus and the metamorphosis in inspiratory flow pattern that has been reported during SAS. Indeed, a full comprehension of the physical conditions allowing this phenomenon is a prerequisite to be able to help in the planning of the surgical gesture that can be prescribed for the patients. The work presented here focuses on a simple model of fluid-walls interactions. The equations governing the airflow inside a constriction are coupled with a Finite Element (FE) biomechanical model of the velum. The geometries of this model is extracted from a single midsagittal radiography of a patient. The velar deformations induced by airflow interactions are computed, presented, discussed and compared to measurements collected onto an experimental setup.",Healthcare
Improved Neural Modeling of Real-World Systems Using Genetic Algorithm Based Variable Selection,"Neural network models of real-world systems, such as industrial processes, made from sensor data must often rely on incomplete data. System states may not all be known, sensor data may be biased or noisy, and it is not often known which sensor data may be useful for predictive modelling. Genetic algorithms may be used to help to address this problem by determining the near optimal subset of sensor variables most appropriate to produce good models. This paper describes the use of genetic search to optimize variable selection to determine inputs into the neural network model. We discuss genetic algorithm implementation issues including data representation types and genetic operators such as crossover and mutation. We present the use of this technique for neural network modelling of a typical industrial application, a liquid fed ceramic melter, and detail the results of the genetic search to optimize the neural network model for this application.","Improved Neural Modeling of Real-World Systems Using Genetic Algorithm Based Variable Selection Neural network models of real-world systems, such as industrial processes, made from sensor data must often rely on incomplete data. System states may not all be known, sensor data may be biased or noisy, and it is not often known which sensor data may be useful for predictive modelling. Genetic algorithms may be used to help to address this problem by determining the near optimal subset of sensor variables most appropriate to produce good models. This paper describes the use of genetic search to optimize variable selection to determine inputs into the neural network model. We discuss genetic algorithm implementation issues including data representation types and genetic operators such as crossover and mutation. We present the use of this technique for neural network modelling of a typical industrial application, a liquid fed ceramic melter, and detail the results of the genetic search to optimize the neural network model for this application.",Technology
Non-Market Allocation Mechanisms: Optimal Design and Investment Incentives,"We study how to optimally design selection mechanisms, accounting for agents investment incentives. A principal wishes to allocate a resource of homogeneous quality to a heterogeneous population of agents. The principal commits to a possibly random selection rule that depends on a one-dimensional characteristic of the agents she intrinsically values. Agents have a strict preference for being selected by the principal and may undertake a costly investment to improve their characteristic before it is revealed to the principal. We show that even if random selection rules foster agents investments, especially at the top of the characteristic distribution, deterministic pass-fail selection rules are in fact optimal.","Non-Market Allocation Mechanisms: Optimal Design and Investment Incentives We study how to optimally design selection mechanisms, accounting for agents investment incentives. A principal wishes to allocate a resource of homogeneous quality to a heterogeneous population of agents. The principal commits to a possibly random selection rule that depends on a one-dimensional characteristic of the agents she intrinsically values. Agents have a strict preference for being selected by the principal and may undertake a costly investment to improve their characteristic before it is revealed to the principal. We show that even if random selection rules foster agents investments, especially at the top of the characteristic distribution, deterministic pass-fail selection rules are in fact optimal.",Finance
The average behaviour of financial market by 2 scale homogenisation,"The financial market is nonpredictable, as according to the Bachelier, the mathematical expectation of the speculator is zero. Nevertheless, we observe in the price fluctuations the two distinct scales, short and long time. Behaviour of a market in long terms, such as year intervals, is different from that in short terms. A diffusion equation with a time dependent diffusion coefficient that describes the fluctuations of the financial market, is subject to a two-scale homogenisation, and long term characteristics of the market such as mean behaviour of price and variance, are obtained. We indicate also that introduction of convolution into diffusion equation permits to obtain L- stable behaviour of finance.","The average behaviour of financial market by 2 scale homogenisation The financial market is nonpredictable, as according to the Bachelier, the mathematical expectation of the speculator is zero. Nevertheless, we observe in the price fluctuations the two distinct scales, short and long time. Behaviour of a market in long terms, such as year intervals, is different from that in short terms. A diffusion equation with a time dependent diffusion coefficient that describes the fluctuations of the financial market, is subject to a two-scale homogenisation, and long term characteristics of the market such as mean behaviour of price and variance, are obtained. We indicate also that introduction of convolution into diffusion equation permits to obtain L- stable behaviour of finance.",Finance
Over-the-counter market models with several assets,"We study two classes of over-the-counter markets specified by systems of ODEs, in the spirit of Duffie-Garleanu-Pedersen, Econometrica, 2005. We first compute the steady states for many of these ODEs. Then we obtain the prices at which investors trade with each other at these steady states. Finally, we study the stability of the solutions of these ODEs.","Over-the-counter market models with several assets We study two classes of over-the-counter markets specified by systems of ODEs, in the spirit of Duffie-Garleanu-Pedersen, Econometrica, 2005. We first compute the steady states for many of these ODEs. Then we obtain the prices at which investors trade with each other at these steady states. Finally, we study the stability of the solutions of these ODEs.",Finance
Efficiency Enhancement of Probabilistic Model Building Genetic Algorithms,"This paper presents two different efficiency-enhancement techniques for probabilistic model building genetic algorithms. The first technique proposes the use of a mutation operator which performs local search in the sub-solution neighborhood identified through the probabilistic model. The second technique proposes building and using an internal probabilistic model of the fitness along with the probabilistic model of variable interactions. The fitness values of some offspring are estimated using the probabilistic model, thereby avoiding computationally expensive function evaluations. The scalability of the aforementioned techniques are analyzed using facetwise models for convergence time and population sizing. The speed-up obtained by each of the methods is predicted and verified with empirical results. The results show that for additively separable problems the competent mutation operator requires O(k 0.5 logm)--where k is the building-block size, and m is the number of building blocks--less function evaluations than its selectorecombinative counterpart. The results also show that the use of an internal probabilistic fitness model reduces the required number of function evaluations to as low as 1-10 and yields a speed-up of 2--50.","Efficiency Enhancement of Probabilistic Model Building Genetic Algorithms This paper presents two different efficiency-enhancement techniques for probabilistic model building genetic algorithms. The first technique proposes the use of a mutation operator which performs local search in the sub-solution neighborhood identified through the probabilistic model. The second technique proposes building and using an internal probabilistic model of the fitness along with the probabilistic model of variable interactions. The fitness values of some offspring are estimated using the probabilistic model, thereby avoiding computationally expensive function evaluations. The scalability of the aforementioned techniques are analyzed using facetwise models for convergence time and population sizing. The speed-up obtained by each of the methods is predicted and verified with empirical results. The results show that for additively separable problems the competent mutation operator requires O(k 0.5 logm)--where k is the building-block size, and m is the number of building blocks--less function evaluations than its selectorecombinative counterpart. The results also show that the use of an internal probabilistic fitness model reduces the required number of function evaluations to as low as 1-10 and yields a speed-up of 2--50.",Technology
Twelve Years of Education and Public Outreach with the Fermi Gamma-ray Space Telescope,"During the past twelve years, NASAs Fermi Gamma-ray Space Telescope has supported a wide range of Education and Public Outreach (EPO) activities, targeting K-14 students and the general public. The purpose of the Fermi EPO program is to increase student and public understanding of the science of the high-energy Universe, through inspiring, engaging and educational activities linked to the missions science objectives. The EPO program has additional more general goals, including increasing the diversity of students in the Science, Technology, Engineering and Mathematics (STEM) pipeline, and increasing public awareness and understanding of Fermi science and technology. Fermis multi-faceted EPO program includes elements in each major outcome category: Higher Education; Elementary and Secondary Education; Informal Education and Public Outreach.","Twelve Years of Education and Public Outreach with the Fermi Gamma-ray Space Telescope During the past twelve years, NASAs Fermi Gamma-ray Space Telescope has supported a wide range of Education and Public Outreach (EPO) activities, targeting K-14 students and the general public. The purpose of the Fermi EPO program is to increase student and public understanding of the science of the high-energy Universe, through inspiring, engaging and educational activities linked to the missions science objectives. The EPO program has additional more general goals, including increasing the diversity of students in the Science, Technology, Engineering and Mathematics (STEM) pipeline, and increasing public awareness and understanding of Fermi science and technology. Fermis multi-faceted EPO program includes elements in each major outcome category: Higher Education; Elementary and Secondary Education; Informal Education and Public Outreach.",Education
Dependence of RNA secondary structure on the energy model,"We analyze a microscopic RNA model, which includes two widely used models as limiting cases, namely it contains terms for bond as well as for stacking energies. We numerically investigate possible changes in the qualitative and quantitative behaviour while going from one model to the other; in particular we test, whether a transition occurs, when continuously moving from one model to the other. For this we calculate various thermodynamic quantities, both at zero temperature as well as at finite temperatures. All calculations can be done efficiently in polynomial time by a dynamic programming algorithm. We do not find a sign for transition between the models, but the critical exponent nu of the correlation length, describing the phase transition in all models to an ordered low-temperature phase, seems to depend continuously on the model. Finally, we apply the epsilon-Coupling method, to study low excitations. The exponent theta describing the energy-scaling of the excitations seems to depend not much on the energy model.","Dependence of RNA secondary structure on the energy model We analyze a microscopic RNA model, which includes two widely used models as limiting cases, namely it contains terms for bond as well as for stacking energies. We numerically investigate possible changes in the qualitative and quantitative behaviour while going from one model to the other; in particular we test, whether a transition occurs, when continuously moving from one model to the other. For this we calculate various thermodynamic quantities, both at zero temperature as well as at finite temperatures. All calculations can be done efficiently in polynomial time by a dynamic programming algorithm. We do not find a sign for transition between the models, but the critical exponent nu of the correlation length, describing the phase transition in all models to an ordered low-temperature phase, seems to depend continuously on the model. Finally, we apply the epsilon-Coupling method, to study low excitations. The exponent theta describing the energy-scaling of the excitations seems to depend not much on the energy model.",Healthcare
A la Recherche des Facteurs Dterminants de lIntgration Internationale des Marchs Boursiers : une Analyse sur Donnes de Panel,"The aim of this paper is to identify the determinants of international stock markets integration. Intuitively we selected a great number of factors linked to financial integration. Then, we developed an international asset-pricing model with time-varying degree of integration. This model is estimated for 30 countries (10 developed countries and 20 emerging countries) using panel data econometrics. In order to investigate whether the financial integration in emerging markets and that in developed markets react differently to the economic and financial innovations, we estimated the model as well jointly for all markets as separately for developed markets and emerging markets. Our results show that trade openness exerts a positive effect on financial integration across all markets, the global factors drive integration in developed markets whereas the factors related to economic and political stability affect financial integration in emerging markets.","A la Recherche des Facteurs Dterminants de lIntgration Internationale des Marchs Boursiers : une Analyse sur Donnes de Panel The aim of this paper is to identify the determinants of international stock markets integration. Intuitively we selected a great number of factors linked to financial integration. Then, we developed an international asset-pricing model with time-varying degree of integration. This model is estimated for 30 countries (10 developed countries and 20 emerging countries) using panel data econometrics. In order to investigate whether the financial integration in emerging markets and that in developed markets react differently to the economic and financial innovations, we estimated the model as well jointly for all markets as separately for developed markets and emerging markets. Our results show that trade openness exerts a positive effect on financial integration across all markets, the global factors drive integration in developed markets whereas the factors related to economic and political stability affect financial integration in emerging markets.",Finance
Market Making with Model Uncertainty,"Pari-mutuel markets are trading platforms through which the common market maker simultaneously clears multiple contingent claims markets. This market has several distinctive properties that began attracting the attention of the financial industry in the 2000s. For example, the platform aggregates liquidity from the individual contingent claims market into the common pool while shielding the market maker from potential financial loss. The contribution of this paper is two-fold. First, we provide a new economic interpretation of the market-clearing strategy of a pari-mutuel market that is well known in the literature. The pari-mutuel auctioneer is shown to be equivalent to the market maker with extreme ambiguity aversion for the future contingent event. Second, based on this theoretical understanding, we present a new market-clearing algorithm called the Knightian Pari-mutuel Mechanism (KPM). The KPM retains many interesting properties of pari-mutuel markets while explicitly controlling for the market makers ambiguity aversion. In addition, the KPM is computationally efficient in that it is solvable in polynomial time.","Market Making with Model Uncertainty Pari-mutuel markets are trading platforms through which the common market maker simultaneously clears multiple contingent claims markets. This market has several distinctive properties that began attracting the attention of the financial industry in the 2000s. For example, the platform aggregates liquidity from the individual contingent claims market into the common pool while shielding the market maker from potential financial loss. The contribution of this paper is two-fold. First, we provide a new economic interpretation of the market-clearing strategy of a pari-mutuel market that is well known in the literature. The pari-mutuel auctioneer is shown to be equivalent to the market maker with extreme ambiguity aversion for the future contingent event. Second, based on this theoretical understanding, we present a new market-clearing algorithm called the Knightian Pari-mutuel Mechanism (KPM). The KPM retains many interesting properties of pari-mutuel markets while explicitly controlling for the market makers ambiguity aversion. In addition, the KPM is computationally efficient in that it is solvable in polynomial time.",Finance
GPCALMA: a Grid-based tool for Mammographic Screening,"The next generation of High Energy Physics (HEP) experiments requires a GRID approach to a distributed computing system and the associated data management: the key concept is the Virtual Organisation (VO), a group of distributed users with a common goal and the will to share their resources. A similar approach is being applied to a group of Hospitals which joined the GPCALMA project (Grid Platform for Computer Assisted Library for MAmmography), which will allow common screening programs for early diagnosis of breast and, in the future, lung cancer. HEP techniques come into play in writing the application code, which makes use of neural networks for the image analysis and proved to be useful in improving the radiologists performances in the diagnosis. GRID technologies allow remote image analysis and interactive online diagnosis, with a potential for a relevant reduction of the delays presently associated to screening programs. A prototype of the system, based on AliEn GRID Services, is already available, with a central Server running common services and several clients connecting to it. Mammograms can be acquired in any location; the related information required to select and access them at any time is stored in a common service called Data Catalogue, which can be queried by any client. The result of a query can be used as input for analysis algorithms, which are executed on nodes that are in general remote to the user (but always local to the input images) thanks to the PROOF facility. The selected approach avoids data transfers for all the images with a negative diagnosis (about 95 of the sample) and allows an almost real time diagnosis for the 5 of images with high cancer probability.","GPCALMA: a Grid-based tool for Mammographic Screening The next generation of High Energy Physics (HEP) experiments requires a GRID approach to a distributed computing system and the associated data management: the key concept is the Virtual Organisation (VO), a group of distributed users with a common goal and the will to share their resources. A similar approach is being applied to a group of Hospitals which joined the GPCALMA project (Grid Platform for Computer Assisted Library for MAmmography), which will allow common screening programs for early diagnosis of breast and, in the future, lung cancer. HEP techniques come into play in writing the application code, which makes use of neural networks for the image analysis and proved to be useful in improving the radiologists performances in the diagnosis. GRID technologies allow remote image analysis and interactive online diagnosis, with a potential for a relevant reduction of the delays presently associated to screening programs. A prototype of the system, based on AliEn GRID Services, is already available, with a central Server running common services and several clients connecting to it. Mammograms can be acquired in any location; the related information required to select and access them at any time is stored in a common service called Data Catalogue, which can be queried by any client. The result of a query can be used as input for analysis algorithms, which are executed on nodes that are in general remote to the user (but always local to the input images) thanks to the PROOF facility. The selected approach avoids data transfers for all the images with a negative diagnosis (about 95 of the sample) and allows an almost real time diagnosis for the 5 of images with high cancer probability.",Healthcare
Impact of RD and AI Investments on Economic Growth and Credit Rating,"The research and development (RD) phase is essential for fostering innovation and aligns with long-term strategies in both public and private sectors. This study addresses two primary research questions: (1) assessing the relationship between RD investments and GDP through regression analysis, and (2) estimating the economic value added (EVA) that Georgia must generate to progress from a BB to a BBB credit rating. Using World Bank data from 2014-2022, this analysis found that increasing RD, with an emphasis on AI, by 30-35 has a measurable impact on GDP. Regression results reveal a coefficient of 7.02, indicating a 10 increase in RD leads to a 0.70 GDP rise, with an 81.1 determination coefficient and a strong 90.1 correlation. Georgias EVA model was calculated to determine the additional value needed for a BBB rating, comparing indicators from Greece, Hungary, India, and Kazakhstan as benchmarks. Key economic indicators considered were nominal GDP, GDP per capita, real GDP growth, and fiscal indicators (government balanceGDP, debtGDP). The EVA model projects that to achieve a BBB rating within nine years, Georgia requires 61.7 billion in investments. Utilizing EVA and comprehensive economic indicators will support informed decision-making and enhance the analysis of Georgias economic trajectory.","Impact of RD and AI Investments on Economic Growth and Credit Rating The research and development (RD) phase is essential for fostering innovation and aligns with long-term strategies in both public and private sectors. This study addresses two primary research questions: (1) assessing the relationship between RD investments and GDP through regression analysis, and (2) estimating the economic value added (EVA) that Georgia must generate to progress from a BB to a BBB credit rating. Using World Bank data from 2014-2022, this analysis found that increasing RD, with an emphasis on AI, by 30-35 has a measurable impact on GDP. Regression results reveal a coefficient of 7.02, indicating a 10 increase in RD leads to a 0.70 GDP rise, with an 81.1 determination coefficient and a strong 90.1 correlation. Georgias EVA model was calculated to determine the additional value needed for a BBB rating, comparing indicators from Greece, Hungary, India, and Kazakhstan as benchmarks. Key economic indicators considered were nominal GDP, GDP per capita, real GDP growth, and fiscal indicators (government balanceGDP, debtGDP). The EVA model projects that to achieve a BBB rating within nine years, Georgia requires 61.7 billion in investments. Utilizing EVA and comprehensive economic indicators will support informed decision-making and enhance the analysis of Georgias economic trajectory.",Finance
Indifference of Defaultable Bonds with Stochastic Intensity models,The utility-based pricing of defaultable bonds in the case of stochastic intensity models of default risk is discussed. The Hamilton-Jacobi- Bellman (HJB) equations for the value functions is derived. A finite difference method is used to solve this problem. The yield-spreads for both buyer and seller are extracted. The behaviour of the spread curve given the default intensity is analyzed. Finally the impacts of the risk aversion and the correlation coefficient are discussed.,Indifference of Defaultable Bonds with Stochastic Intensity models The utility-based pricing of defaultable bonds in the case of stochastic intensity models of default risk is discussed. The Hamilton-Jacobi- Bellman (HJB) equations for the value functions is derived. A finite difference method is used to solve this problem. The yield-spreads for both buyer and seller are extracted. The behaviour of the spread curve given the default intensity is analyzed. Finally the impacts of the risk aversion and the correlation coefficient are discussed.,Finance
Econophysics of Macro-Finance: Local Multi-fluid Models and Surface-like Waves of Financial Variables,This paper models macro financial variables alike to financial fluids with local interactions and describes surface-like waves of Investment and Profits. We regard macro-finance as ensemble of economic agents and use their risk ratings as coordinates on economic space. Aggregations of agents financial variables with risk coordinates x on economic space define macro financial variables as function of x. We describe evolution and interactions between macro financial variables alike to financial fluids by hydrodynamic-like equations. Minimum and maximum risk grades define most secure and most risky agents respectively. That determines borders of macro-finance domain that is filled by economic agents. Perturbations of agents risk coordinates near risk borders of macro domain cause disturbances of macro financial variables like Investment and Profits. Such disturbances can generate waves that propagate along risk borders. These waves may exponentially amplify perturbations inside of macro domain and impact financial sustainability. We study simple model Investment and Profits and describe linear approximation of steady state distributions of Investment and Profits on macro-finance domain that fulfill dreams of Investors: more risks-more Profits. We describe Investment and Profits waves on risk border of economic space alike to surface waves in fluids. We present simple examples that specify waves as possible origin of time fluctuations of macro financial variables. Description of possible steady state distributions of macro financial variables and financial risk waves on economic space could help for better policy-making and managing sustainable macro-finance.,Econophysics of Macro-Finance: Local Multi-fluid Models and Surface-like Waves of Financial Variables This paper models macro financial variables alike to financial fluids with local interactions and describes surface-like waves of Investment and Profits. We regard macro-finance as ensemble of economic agents and use their risk ratings as coordinates on economic space. Aggregations of agents financial variables with risk coordinates x on economic space define macro financial variables as function of x. We describe evolution and interactions between macro financial variables alike to financial fluids by hydrodynamic-like equations. Minimum and maximum risk grades define most secure and most risky agents respectively. That determines borders of macro-finance domain that is filled by economic agents. Perturbations of agents risk coordinates near risk borders of macro domain cause disturbances of macro financial variables like Investment and Profits. Such disturbances can generate waves that propagate along risk borders. These waves may exponentially amplify perturbations inside of macro domain and impact financial sustainability. We study simple model Investment and Profits and describe linear approximation of steady state distributions of Investment and Profits on macro-finance domain that fulfill dreams of Investors: more risks-more Profits. We describe Investment and Profits waves on risk border of economic space alike to surface waves in fluids. We present simple examples that specify waves as possible origin of time fluctuations of macro financial variables. Description of possible steady state distributions of macro financial variables and financial risk waves on economic space could help for better policy-making and managing sustainable macro-finance.,Finance
Bandwidth selection for kernel estimation in mixed multi-dimensional spaces,"Kernel estimation techniques, such as mean shift, suffer from one major drawback: the kernel bandwidth selection. The bandwidth can be fixed for all the data set or can vary at each points. Automatic bandwidth selection becomes a real challenge in case of multidimensional heterogeneous features. This paper presents a solution to this problem. It is an extension of citeComaniciu03a which was based on the fundamental property of normal distributions regarding the bias of the normalized density gradient. The selection is done iteratively for each type of features, by looking for the stability of local bandwidth estimates across a predefined range of bandwidths. A pseudo balloon mean shift filtering and partitioning are introduced. The validity of the method is demonstrated in the context of color image segmentation based on a 5-dimensional space.","Bandwidth selection for kernel estimation in mixed multi-dimensional spaces Kernel estimation techniques, such as mean shift, suffer from one major drawback: the kernel bandwidth selection. The bandwidth can be fixed for all the data set or can vary at each points. Automatic bandwidth selection becomes a real challenge in case of multidimensional heterogeneous features. This paper presents a solution to this problem. It is an extension of citeComaniciu03a which was based on the fundamental property of normal distributions regarding the bias of the normalized density gradient. The selection is done iteratively for each type of features, by looking for the stability of local bandwidth estimates across a predefined range of bandwidths. A pseudo balloon mean shift filtering and partitioning are introduced. The validity of the method is demonstrated in the context of color image segmentation based on a 5-dimensional space.",Technology
Statistical efficiency of curve fitting algorithms,"We study the problem of fitting parametrized curves to noisy data. Under certain assumptions (known as Cartesian and radial functional models), we derive asymptotic expressions for the bias and the covariance matrix of the parameter estimates. We also extend Kanatanis version of the Cramer-Rao lower bound, which he proved for unbiased estimates only, to more general estimates that include many popular algorithms (most notably, the orthogonal least squares and algebraic fits). We then show that the gradient-weighted algebraic fit is statistically efficient and describe all other statistically efficient algebraic fits.","Statistical efficiency of curve fitting algorithms We study the problem of fitting parametrized curves to noisy data. Under certain assumptions (known as Cartesian and radial functional models), we derive asymptotic expressions for the bias and the covariance matrix of the parameter estimates. We also extend Kanatanis version of the Cramer-Rao lower bound, which he proved for unbiased estimates only, to more general estimates that include many popular algorithms (most notably, the orthogonal least squares and algebraic fits). We then show that the gradient-weighted algebraic fit is statistically efficient and describe all other statistically efficient algebraic fits.",Technology
Tri-Level Model for Hybrid Renewable Energy Systems,"In practical scenarios, addressing real-world challenges often entails the incorporation of diverse renewable energy sources, such as solar, energy storage systems, and greenhouse gas emissions. The core purpose of these interconnected systems is to optimize a multitude of factors and objectives concurrently. Hence, it is imperative to formulate models that comprehensively cover all these objectives. This paper introduces tri-level mathematical models for Hybrid Renewable Energy Systems (HRESs), offering a framework to concurrently tackle diverse objectives and decision-making levels within the realm of renewable energy integration. The proposed model seeks to maximize the efficiency of solar PV, enhance the performance of energy storage systems, and minimize greenhouse gas emissions.","Tri-Level Model for Hybrid Renewable Energy Systems In practical scenarios, addressing real-world challenges often entails the incorporation of diverse renewable energy sources, such as solar, energy storage systems, and greenhouse gas emissions. The core purpose of these interconnected systems is to optimize a multitude of factors and objectives concurrently. Hence, it is imperative to formulate models that comprehensively cover all these objectives. This paper introduces tri-level mathematical models for Hybrid Renewable Energy Systems (HRESs), offering a framework to concurrently tackle diverse objectives and decision-making levels within the realm of renewable energy integration. The proposed model seeks to maximize the efficiency of solar PV, enhance the performance of energy storage systems, and minimize greenhouse gas emissions.",Environment
Chinese Medical Device Market and The Investment Vector,"China has attracted increasing amounts of foreign investment since it opened its doors to the world and whilst many analysts have focused on foreign investment in popular areas, little has been written about medical device investment. The purpose of this article is to analyze the status of the Chinese medical device market from the perspective of the healthcare industry and its important market drivers; the study reveals that the medical device market has significant growth potential. This article aims to identify and assess the profitable sectors of medical device technologies as a guide for international companies and investors.","Chinese Medical Device Market and The Investment Vector China has attracted increasing amounts of foreign investment since it opened its doors to the world and whilst many analysts have focused on foreign investment in popular areas, little has been written about medical device investment. The purpose of this article is to analyze the status of the Chinese medical device market from the perspective of the healthcare industry and its important market drivers; the study reveals that the medical device market has significant growth potential. This article aims to identify and assess the profitable sectors of medical device technologies as a guide for international companies and investors.",Finance
Optimal Power Flow in Renewable-Integrated Power Systems: A Comprehensive Review,"This paper explores the integration of renewable energy sources into power systems, highlighting the resulting complexities such as variability and intermittency that challenge traditional power flow dynamics. We delve into innovative Optimal Power Flow (OPF) strategies designed to manage the unpredictability of renewable sources while ensuring economically viable and stable grid operations. A thorough review of state-of-the-art OPF algorithms, particularly those that enhance systems with substantial renewable integration, is presented. The discussion spans fundamental OPF principles, adaptations to renewable energies, and categorization of the latest advancements in areas such as energy uncertainty management, energy storage integration, linearization techniques application, and data-driven strategy utilization. Each sectors application benefits and limitations are critically analyzed. The paper concludes by pinpointing ongoing challenges and suggesting future research trajectories to foster adaptable and robust power system operations in the renewable-dominant energy era.","Optimal Power Flow in Renewable-Integrated Power Systems: A Comprehensive Review This paper explores the integration of renewable energy sources into power systems, highlighting the resulting complexities such as variability and intermittency that challenge traditional power flow dynamics. We delve into innovative Optimal Power Flow (OPF) strategies designed to manage the unpredictability of renewable sources while ensuring economically viable and stable grid operations. A thorough review of state-of-the-art OPF algorithms, particularly those that enhance systems with substantial renewable integration, is presented. The discussion spans fundamental OPF principles, adaptations to renewable energies, and categorization of the latest advancements in areas such as energy uncertainty management, energy storage integration, linearization techniques application, and data-driven strategy utilization. Each sectors application benefits and limitations are critically analyzed. The paper concludes by pinpointing ongoing challenges and suggesting future research trajectories to foster adaptable and robust power system operations in the renewable-dominant energy era.",Environment
Probabilistic and Team PFIN-type Learning: General Properties,"We consider the probability hierarchy for Popperian FINite learning and study the general properties of this hierarchy. We prove that the probability hierarchy is decidable, i.e. there exists an algorithm that receives p_1 and p_2 and answers whether PFIN-type learning with the probability of success p_1 is equivalent to PFIN-type learning with the probability of success p_2. To prove our result, we analyze the topological structure of the probability hierarchy. We prove that it is well-ordered in descending ordering and order-equivalent to ordinal epsilon_0. This shows that the structure of the hierarchy is very complicated. Using similar methods, we also prove that, for PFIN-type learning, team learning and probabilistic learning are of the same power.","Probabilistic and Team PFIN-type Learning: General Properties We consider the probability hierarchy for Popperian FINite learning and study the general properties of this hierarchy. We prove that the probability hierarchy is decidable, i.e. there exists an algorithm that receives p_1 and p_2 and answers whether PFIN-type learning with the probability of success p_1 is equivalent to PFIN-type learning with the probability of success p_2. To prove our result, we analyze the topological structure of the probability hierarchy. We prove that it is well-ordered in descending ordering and order-equivalent to ordinal epsilon_0. This shows that the structure of the hierarchy is very complicated. Using similar methods, we also prove that, for PFIN-type learning, team learning and probabilistic learning are of the same power.",Technology
"Linkages among the Foreign Exchange, Stock, and Bond Markets in Japan and the United States","While economic theory explains the linkages among the financial markets of different countries, empirical studies mainly verify the linkages through Granger causality, without considering latent variables or instantaneous effects. Their findings are inconsistent regarding the existence of causal linkages among financial markets, which might be attributed to differences in the focused markets, data periods, and methods applied. Our study adopts causal discovery methods including VAR-LiNGAM and LPCMCI with domain knowledge to explore the linkages among financial markets in Japan and the United States (US) for the post Covid-19 pandemic period under divergent monetary policy directions. The VAR-LiNGAM results reveal that the previous days US market influences the following days Japanese market for both stocks and bonds, and the bond markets of the previous day impact the following days foreign exchange (FX) market directly and the following days Japanese stock market indirectly. The LPCMCI results indicate the existence of potential latent confounders. Our results demonstrate that VAR-LiNGAM uniquely identifies the directed acyclic graph (DAG), and thus provides informative insight into the causal relationship when the assumptions are considered valid. Our study contributes to a better understanding of the linkages among financial markets in the analyzed data period by supporting the existence of linkages between Japan and the US for the same financial markets and among FX, stock, and bond markets, thus highlighting the importance of leveraging causal discovery methods in the financial domain.","Linkages among the Foreign Exchange, Stock, and Bond Markets in Japan and the United States While economic theory explains the linkages among the financial markets of different countries, empirical studies mainly verify the linkages through Granger causality, without considering latent variables or instantaneous effects. Their findings are inconsistent regarding the existence of causal linkages among financial markets, which might be attributed to differences in the focused markets, data periods, and methods applied. Our study adopts causal discovery methods including VAR-LiNGAM and LPCMCI with domain knowledge to explore the linkages among financial markets in Japan and the United States (US) for the post Covid-19 pandemic period under divergent monetary policy directions. The VAR-LiNGAM results reveal that the previous days US market influences the following days Japanese market for both stocks and bonds, and the bond markets of the previous day impact the following days foreign exchange (FX) market directly and the following days Japanese stock market indirectly. The LPCMCI results indicate the existence of potential latent confounders. Our results demonstrate that VAR-LiNGAM uniquely identifies the directed acyclic graph (DAG), and thus provides informative insight into the causal relationship when the assumptions are considered valid. Our study contributes to a better understanding of the linkages among financial markets in the analyzed data period by supporting the existence of linkages between Japan and the US for the same financial markets and among FX, stock, and bond markets, thus highlighting the importance of leveraging causal discovery methods in the financial domain.",Finance
The Importance of Education for Technological Development and the Role of Internet-Based Learning in Education,"In todays world, many technologically advanced countries have realized that real power lies not in physical strength but in educated minds. As a result, every country has embarked on restructuring its education system to meet the demands of technology. As a country in the midst of these developments, we cannot remain indifferent to this transformation in education. In the Information Age of the 21st century, rapid access to information is crucial for the development of individuals and societies. To take our place among the knowledge societies in a world moving rapidly towards globalization, we must closely follow technological innovations and meet the requirements of technology. This can be achieved by providing learning opportunities to anyone interested in acquiring education in their area of interest. This study focuses on the advantages and disadvantages of internet-based learning compared to traditional teaching methods, the importance of computer usage in internet-based learning, negative factors affecting internet-based learning, and the necessary recommendations for addressing these issues. In todays world, it is impossible to talk about education without technology or technology without education.","The Importance of Education for Technological Development and the Role of Internet-Based Learning in Education In todays world, many technologically advanced countries have realized that real power lies not in physical strength but in educated minds. As a result, every country has embarked on restructuring its education system to meet the demands of technology. As a country in the midst of these developments, we cannot remain indifferent to this transformation in education. In the Information Age of the 21st century, rapid access to information is crucial for the development of individuals and societies. To take our place among the knowledge societies in a world moving rapidly towards globalization, we must closely follow technological innovations and meet the requirements of technology. This can be achieved by providing learning opportunities to anyone interested in acquiring education in their area of interest. This study focuses on the advantages and disadvantages of internet-based learning compared to traditional teaching methods, the importance of computer usage in internet-based learning, negative factors affecting internet-based learning, and the necessary recommendations for addressing these issues. In todays world, it is impossible to talk about education without technology or technology without education.",Education
Iterative Teacher-Aware Learning,"In human pedagogy, teachers and students can interact adaptively to maximize communication efficiency. The teacher adjusts her teaching method for different students, and the student, after getting familiar with the teachers instruction mechanism, can infer the teachers intention to learn faster. Recently, the benefits of integrating this cooperative pedagogy into machine concept learning in discrete spaces have been proved by multiple works. However, how cooperative pedagogy can facilitate machine parameter learning hasnt been thoroughly studied. In this paper, we propose a gradient optimization based teacher-aware learner who can incorporate teachers cooperative intention into the likelihood function and learn provably faster compared with the naive learning algorithms used in previous machine teaching works. We give theoretical proof that the iterative teacher-aware learning (ITAL) process leads to local and global improvements. We then validate our algorithms with extensive experiments on various tasks including regression, classification, and inverse reinforcement learning using synthetic and real data. We also show the advantage of modeling teacher-awareness when agents are learning from human teachers.","Iterative Teacher-Aware Learning In human pedagogy, teachers and students can interact adaptively to maximize communication efficiency. The teacher adjusts her teaching method for different students, and the student, after getting familiar with the teachers instruction mechanism, can infer the teachers intention to learn faster. Recently, the benefits of integrating this cooperative pedagogy into machine concept learning in discrete spaces have been proved by multiple works. However, how cooperative pedagogy can facilitate machine parameter learning hasnt been thoroughly studied. In this paper, we propose a gradient optimization based teacher-aware learner who can incorporate teachers cooperative intention into the likelihood function and learn provably faster compared with the naive learning algorithms used in previous machine teaching works. We give theoretical proof that the iterative teacher-aware learning (ITAL) process leads to local and global improvements. We then validate our algorithms with extensive experiments on various tasks including regression, classification, and inverse reinforcement learning using synthetic and real data. We also show the advantage of modeling teacher-awareness when agents are learning from human teachers.",Education
Pac-learning Recursive Logic Programs: Negative Results,"In a companion paper it was shown that the class of constant-depth determinate k-ary recursive clauses is efficiently learnable. In this paper we present negative results showing that any natural generalization of this class is hard to learn in Valiants model of pac-learnability. In particular, we show that the following program classes are cryptographically hard to learn: programs with an unbounded number of constant-depth linear recursive clauses; programs with one constant-depth determinate clause containing an unbounded number of recursive calls; and programs with one linear recursive clause of constant locality. These results immediately imply the non-learnability of any more general class of programs. We also show that learning a constant-depth determinate program with either two linear recursive clauses or one linear recursive clause and one non-recursive clause is as hard as learning boolean DNF. Together with positive results from the companion paper, these negative results establish a boundary of efficient learnability for recursive function-free clauses.","Pac-learning Recursive Logic Programs: Negative Results In a companion paper it was shown that the class of constant-depth determinate k-ary recursive clauses is efficiently learnable. In this paper we present negative results showing that any natural generalization of this class is hard to learn in Valiants model of pac-learnability. In particular, we show that the following program classes are cryptographically hard to learn: programs with an unbounded number of constant-depth linear recursive clauses; programs with one constant-depth determinate clause containing an unbounded number of recursive calls; and programs with one linear recursive clause of constant locality. These results immediately imply the non-learnability of any more general class of programs. We also show that learning a constant-depth determinate program with either two linear recursive clauses or one linear recursive clause and one non-recursive clause is as hard as learning boolean DNF. Together with positive results from the companion paper, these negative results establish a boundary of efficient learnability for recursive function-free clauses.",Technology
Fast Correlation Greeks by Adjoint Algorithmic Differentiation,"We show how Adjoint Algorithmic Differentiation (AAD) allows an extremely efficient calculation of correlation Risk of option prices computed with Monte Carlo simulations. A key point in the construction is the use of binning to simultaneously achieve computational efficiency and accurate confidence intervals. We illustrate the method for a copula-based Monte Carlo computation of claims written on a basket of underlying assets, and we test it numerically for Portfolio Default Options. For any number of underlying assets or names in a portfolio, the sensitivities of the option price with respect to all the pairwise correlations is obtained at a computational cost which is at most 4 times the cost of calculating the option value itself. For typical applications, this results in computational savings of several order of magnitudes with respect to standard methods.","Fast Correlation Greeks by Adjoint Algorithmic Differentiation We show how Adjoint Algorithmic Differentiation (AAD) allows an extremely efficient calculation of correlation Risk of option prices computed with Monte Carlo simulations. A key point in the construction is the use of binning to simultaneously achieve computational efficiency and accurate confidence intervals. We illustrate the method for a copula-based Monte Carlo computation of claims written on a basket of underlying assets, and we test it numerically for Portfolio Default Options. For any number of underlying assets or names in a portfolio, the sensitivities of the option price with respect to all the pairwise correlations is obtained at a computational cost which is at most 4 times the cost of calculating the option value itself. For typical applications, this results in computational savings of several order of magnitudes with respect to standard methods.",Finance
Matching Edges in Images ; Application to Face Recognition,"This communication describes a representation of images as a set of edges characterized by their position and orientation. This representation allows the comparison of two images and the computation of their similarity. The first step in this computation of similarity is the seach of a geometrical basis of the two dimensional space where the two images are represented simultaneously after transformation of one of them. Presently, this simultaneous representation takes into account a shift and a scaling ; it may be extended to rotations or other global geometrical transformations. An elementary probabilistic computation shows that a sufficient but not excessive number of trials (a few tens) ensures that the exhibition of this common basis is guaranteed in spite of possible errors in the detection of edges. When this first step is performed, the search of similarity between the two images reduces to counting the coincidence of edges in the two images. The approach may be applied to many problems of pattern matching ; it was checked on face recognition.","Matching Edges in Images ; Application to Face Recognition This communication describes a representation of images as a set of edges characterized by their position and orientation. This representation allows the comparison of two images and the computation of their similarity. The first step in this computation of similarity is the seach of a geometrical basis of the two dimensional space where the two images are represented simultaneously after transformation of one of them. Presently, this simultaneous representation takes into account a shift and a scaling ; it may be extended to rotations or other global geometrical transformations. An elementary probabilistic computation shows that a sufficient but not excessive number of trials (a few tens) ensures that the exhibition of this common basis is guaranteed in spite of possible errors in the detection of edges. When this first step is performed, the search of similarity between the two images reduces to counting the coincidence of edges in the two images. The approach may be applied to many problems of pattern matching ; it was checked on face recognition.",Technology
Probabilistic estimation of microarray data reliability and underlying gene expression,"Background: The availability of high throughput methods for measurement of mRNA concentrations makes the reliability of conclusions drawn from the data and global quality control of samples and hybridization important issues. We address these issues by an information theoretic approach, applied to discretized expression values in replicated gene expression data. Results: Our approach yields a quantitative measure of two important parameter classes: First, the probability P(sigma  S) that a gene is in the biological state sigma in a certain variety, given its observed expression S in the samples of that variety. Second, sample specific error probabilities which serve as consistency indicators of the measured samples of each variety. The method and its limitations are tested on gene expression data for developing murine B-cells and a t-test is used as reference. On a set of known genes it performs better than the t-test despite the crude discretization into only two expression levels. The consistency indicators, i.e. the error probabilities, correlate well with variations in the biological material and thus prove efficient. Conclusions: The proposed method is effective in determining differential gene expression and sample reliability in replicated microarray data. Already at two discrete expression levels in each sample, it gives a good explanation of the data and is comparable to standard techniques.","Probabilistic estimation of microarray data reliability and underlying gene expression Background: The availability of high throughput methods for measurement of mRNA concentrations makes the reliability of conclusions drawn from the data and global quality control of samples and hybridization important issues. We address these issues by an information theoretic approach, applied to discretized expression values in replicated gene expression data. Results: Our approach yields a quantitative measure of two important parameter classes: First, the probability P(sigma  S) that a gene is in the biological state sigma in a certain variety, given its observed expression S in the samples of that variety. Second, sample specific error probabilities which serve as consistency indicators of the measured samples of each variety. The method and its limitations are tested on gene expression data for developing murine B-cells and a t-test is used as reference. On a set of known genes it performs better than the t-test despite the crude discretization into only two expression levels. The consistency indicators, i.e. the error probabilities, correlate well with variations in the biological material and thus prove efficient. Conclusions: The proposed method is effective in determining differential gene expression and sample reliability in replicated microarray data. Already at two discrete expression levels in each sample, it gives a good explanation of the data and is comparable to standard techniques.",Healthcare
On the relationships between kinetic schemes and two-state single molecule trajectories,"Trajectories of a signal that fluctuates between two states which originate from single molecule activities have become ubiquitous. Common examples are trajectories of ionic flux through individual membrane-channels, and of photon counts collected from diffusion, activity, and conformational changes of biopolymers. By analyzing the trajectory, one wishes to deduce the underlying mechanism, which is usually described by a multi-substate kinetic scheme. In previous works, we divided kinetic schemes that generate two-state trajectories into two types: reducible schemes and irreducible schemes. We showed that all the information in trajectories generated from reducible schemes is contained in the waiting time probability density functions (PDFs) of the two states. It follows that reducible schemes with the same waiting time PDFs are not distinguishable. In this work, we further characterize the topologies of kinetic schemes, now of irreducible schemes, and further study two-state trajectories from the two types of scheme. We suggest various methods for extracting information about the underlying kinetic scheme from the trajectory (e. g., calculate the binned successive waiting times PDF and analyze the ordered waiting times trajectory), and point out the advantages and disadvantages of each. We show that the binned successive waiting times PDF is not only more robust than other functions when analyzing finite trajectories, but contains, in most cases, more information about the underlying kinetic scheme than other functions in the limit of infinitely long trajectories. For some cases however, analyzing the ordered waiting times trajectory may supply unique information about the underlying kinetic scheme.","On the relationships between kinetic schemes and two-state single molecule trajectories Trajectories of a signal that fluctuates between two states which originate from single molecule activities have become ubiquitous. Common examples are trajectories of ionic flux through individual membrane-channels, and of photon counts collected from diffusion, activity, and conformational changes of biopolymers. By analyzing the trajectory, one wishes to deduce the underlying mechanism, which is usually described by a multi-substate kinetic scheme. In previous works, we divided kinetic schemes that generate two-state trajectories into two types: reducible schemes and irreducible schemes. We showed that all the information in trajectories generated from reducible schemes is contained in the waiting time probability density functions (PDFs) of the two states. It follows that reducible schemes with the same waiting time PDFs are not distinguishable. In this work, we further characterize the topologies of kinetic schemes, now of irreducible schemes, and further study two-state trajectories from the two types of scheme. We suggest various methods for extracting information about the underlying kinetic scheme from the trajectory (e. g., calculate the binned successive waiting times PDF and analyze the ordered waiting times trajectory), and point out the advantages and disadvantages of each. We show that the binned successive waiting times PDF is not only more robust than other functions when analyzing finite trajectories, but contains, in most cases, more information about the underlying kinetic scheme than other functions in the limit of infinitely long trajectories. For some cases however, analyzing the ordered waiting times trajectory may supply unique information about the underlying kinetic scheme.",Healthcare
Challenges in Integrating Technology into Education,"Despite significant amount of investment, there seems to be obstacles to technology integration in education. In order to shed light on the nature of perceived obstacles to technology integration, opinions of 117 professionals, who were selected by Turkish Ministry of National Education as experts in their respective fields, about the obstacles to integration of technology into education were investigated. After categorizing the perceived obstacles by factor analysis, associations of those categories with personal and professional differences were further investigated for better contextualizing the findings. Correlations were analyzed by Pearsons product moment coefficient and point-biserial coefficient. The results revealed that its not the hardware itself that constitute obstacles to technology integration. Insufficiency of in-service and pre-service training, content support, and incentive system emerged as major perceived obstacles to technology integration. Inadequacy of physical and technological infrastructure was also found to be an important obstacle to successful integration. Novelty of the technologies compared to older ones were not found to be an obstacle to technology integration. Moreover, participants stressed the lack of education in teacher training institutions about current technologies that Ministry of National Education officially requires teachers to use as part of their jobs to be another important obstacle. There were no correlations between sex, age, level of education, job position, year of experience in other careers, and any of the categories of perceived obstacles. However, there was a strong negative correlation between year of experience in teaching and insufficiency of resources. Association between year of experience in educational administration and negative psychological state was also strong and negative.","Challenges in Integrating Technology into Education Despite significant amount of investment, there seems to be obstacles to technology integration in education. In order to shed light on the nature of perceived obstacles to technology integration, opinions of 117 professionals, who were selected by Turkish Ministry of National Education as experts in their respective fields, about the obstacles to integration of technology into education were investigated. After categorizing the perceived obstacles by factor analysis, associations of those categories with personal and professional differences were further investigated for better contextualizing the findings. Correlations were analyzed by Pearsons product moment coefficient and point-biserial coefficient. The results revealed that its not the hardware itself that constitute obstacles to technology integration. Insufficiency of in-service and pre-service training, content support, and incentive system emerged as major perceived obstacles to technology integration. Inadequacy of physical and technological infrastructure was also found to be an important obstacle to successful integration. Novelty of the technologies compared to older ones were not found to be an obstacle to technology integration. Moreover, participants stressed the lack of education in teacher training institutions about current technologies that Ministry of National Education officially requires teachers to use as part of their jobs to be another important obstacle. There were no correlations between sex, age, level of education, job position, year of experience in other careers, and any of the categories of perceived obstacles. However, there was a strong negative correlation between year of experience in teaching and insufficiency of resources. Association between year of experience in educational administration and negative psychological state was also strong and negative.",Education
Learning Interactions Between Continuous Treatments and Covariates with a Semiparametric Model,"Estimating the impact of continuous treatment variables (e.g., dosage amount) on binary outcomes presents significant challenges in modeling and estimation because many existing approaches make strong assumptions that do not hold for certain continuous treatment variables. For instance, traditional logistic regression makes strong linearity assumptions that do not hold for continuous treatment variables like time of initiation. In this work, we propose a semiparametric regression framework that decomposes effects into two interpretable components: a prognostic score that captures baseline outcome risk based on a combination of clinical, genetic, and sociodemographic features, and a treatment-interaction score that flexibly models the optimal treatment level via a nonparametric link function. By connecting these two parametric scores with Nadaraya-Watson regression, our approach is both interpretable and flexible. The potential of our approach is demonstrated through numerical simulations that show empirical estimation convergence. We conclude by applying our approach to a real-world case study using the International Warfarin Pharmacogenomics Consortium (IWPC) dataset to show our approachs clinical utility by deriving personalized warfarin dosing recommendations that integrate both genetic and clinical data, providing insights towards enhancing patient safety and therapeutic efficacy in anticoagulation therapy.","Learning Interactions Between Continuous Treatments and Covariates with a Semiparametric Model Estimating the impact of continuous treatment variables (e.g., dosage amount) on binary outcomes presents significant challenges in modeling and estimation because many existing approaches make strong assumptions that do not hold for certain continuous treatment variables. For instance, traditional logistic regression makes strong linearity assumptions that do not hold for continuous treatment variables like time of initiation. In this work, we propose a semiparametric regression framework that decomposes effects into two interpretable components: a prognostic score that captures baseline outcome risk based on a combination of clinical, genetic, and sociodemographic features, and a treatment-interaction score that flexibly models the optimal treatment level via a nonparametric link function. By connecting these two parametric scores with Nadaraya-Watson regression, our approach is both interpretable and flexible. The potential of our approach is demonstrated through numerical simulations that show empirical estimation convergence. We conclude by applying our approach to a real-world case study using the International Warfarin Pharmacogenomics Consortium (IWPC) dataset to show our approachs clinical utility by deriving personalized warfarin dosing recommendations that integrate both genetic and clinical data, providing insights towards enhancing patient safety and therapeutic efficacy in anticoagulation therapy.",Healthcare
Performance of three-photon PET imaging: Monte Carlo simulations,"We have recently introduced the idea of making use of three-photon positron annihilations in positron emission tomography. In this paper the basic characteristics of the three-gamma imaging in PET are studied by means of Monte Carlo simulations and analytical computations. Two typical configurations of human and small animal scanners are considered. Three-photon imaging requires high energy resolution detectors. Parameters currently attainable by CdZnTe semiconductor detectors, the technology of choice for the future development of radiation imaging, are assumed. Spatial resolution is calculated as a function of detector energy resolution and size, position in the field of view, scanner size, and the energies of the three gamma annihilation photons. Possible ways to improve the spatial resolution obtained for nominal parameters: 1.5 cm and 3.2 mm FWHM for human and small animal scanners, respectively, are indicated. Counting rates of true and random three-photon events for typical human and small animal scanning configurations are assessed. A simple formula for minimum size of lesions detectable in the three-gamma based images is derived. Depending on the contrast and total number of registered counts, lesions of a few mm size for human and sub mm for small animal scanners can be detected.","Performance of three-photon PET imaging: Monte Carlo simulations We have recently introduced the idea of making use of three-photon positron annihilations in positron emission tomography. In this paper the basic characteristics of the three-gamma imaging in PET are studied by means of Monte Carlo simulations and analytical computations. Two typical configurations of human and small animal scanners are considered. Three-photon imaging requires high energy resolution detectors. Parameters currently attainable by CdZnTe semiconductor detectors, the technology of choice for the future development of radiation imaging, are assumed. Spatial resolution is calculated as a function of detector energy resolution and size, position in the field of view, scanner size, and the energies of the three gamma annihilation photons. Possible ways to improve the spatial resolution obtained for nominal parameters: 1.5 cm and 3.2 mm FWHM for human and small animal scanners, respectively, are indicated. Counting rates of true and random three-photon events for typical human and small animal scanning configurations are assessed. A simple formula for minimum size of lesions detectable in the three-gamma based images is derived. Depending on the contrast and total number of registered counts, lesions of a few mm size for human and sub mm for small animal scanners can be detected.",Healthcare
Quantitative Approach to Intensity-Modulated Radiation Therapy Quality Assurance Based on Film Dosimetry and Optimization,"To accurately verify the dose of intensity-modulated radiation therapy (IMRT), we have used a global optimization method to investigate a new dose-verification algorithm. In practical application of this quality assurance (QA) procedure, verification of the dose using calculated and measured dose distributions involves a subtle problem in the region of high dose gradient. Consideration of systematic errors shows that the large dose differences in high-dose-gradient regions are due to the unexpected shift of measuring devices. We have proposed an optimization algorithm to correct this error, and an optimization method to minimize the average dose difference has been used in this study. The relationship between the dose-verification procedure and the applied optimization algorithm is explained precisely. Optimization dramatically reduced the difference between measured and calculated dose distributions in all cases investigated. The obtained results support the relevance of our explanations for the problem in the high-dose-gradient region. We have described this dose-verification procedure for IMRT and intensity-modulated radiosurgery. Through this study we have also developed an intuitive reporting method that is statistically reasonable.","Quantitative Approach to Intensity-Modulated Radiation Therapy Quality Assurance Based on Film Dosimetry and Optimization To accurately verify the dose of intensity-modulated radiation therapy (IMRT), we have used a global optimization method to investigate a new dose-verification algorithm. In practical application of this quality assurance (QA) procedure, verification of the dose using calculated and measured dose distributions involves a subtle problem in the region of high dose gradient. Consideration of systematic errors shows that the large dose differences in high-dose-gradient regions are due to the unexpected shift of measuring devices. We have proposed an optimization algorithm to correct this error, and an optimization method to minimize the average dose difference has been used in this study. The relationship between the dose-verification procedure and the applied optimization algorithm is explained precisely. Optimization dramatically reduced the difference between measured and calculated dose distributions in all cases investigated. The obtained results support the relevance of our explanations for the problem in the high-dose-gradient region. We have described this dose-verification procedure for IMRT and intensity-modulated radiosurgery. Through this study we have also developed an intuitive reporting method that is statistically reasonable.",Healthcare
Targeted Projection Pursuit for Gene Expression Data Classification and Visualisation,"We present a novel method for finding low dimensional views of high dimensional data: Targeted Projection Pursuit. The method proceeds by finding projections of the data that best approximate a target view. Two versions of the method are introduced; one version based on Procrustes analysis and one based on a single layer perceptron. These versions are capable of finding orthogonal or non-orthogonal projections respectively. The method is quantitatively and qualitatively compared with other dimension reduction techniques. It is shown to find two-dimensional views that display the classification of cancers from gene expression data with a visual separation equal to, or better than, existing dimension reduction techniques.","Targeted Projection Pursuit for Gene Expression Data Classification and Visualisation We present a novel method for finding low dimensional views of high dimensional data: Targeted Projection Pursuit. The method proceeds by finding projections of the data that best approximate a target view. Two versions of the method are introduced; one version based on Procrustes analysis and one based on a single layer perceptron. These versions are capable of finding orthogonal or non-orthogonal projections respectively. The method is quantitatively and qualitatively compared with other dimension reduction techniques. It is shown to find two-dimensional views that display the classification of cancers from gene expression data with a visual separation equal to, or better than, existing dimension reduction techniques.",Healthcare
Monodisperse approximation in the metastable phase decay,A new simple method for the first order phase transition kinetics is suggested. The metastable phase consumption can be imagined in frames of the modisperse approximation for the distribution of the droplets sizes. In all situations of the metastable phase decay this approximation leads to negligible errors in the total number of droplets appeared in the system. An evident advantage of the presented method is the possibility to investigate the situation of the metastable phase decay on several sorts of heterogeneous centers.,Monodisperse approximation in the metastable phase decay A new simple method for the first order phase transition kinetics is suggested. The metastable phase consumption can be imagined in frames of the modisperse approximation for the distribution of the droplets sizes. In all situations of the metastable phase decay this approximation leads to negligible errors in the total number of droplets appeared in the system. An evident advantage of the presented method is the possibility to investigate the situation of the metastable phase decay on several sorts of heterogeneous centers.,Environment
General Discounting versus Average Reward,"Consider an agent interacting with an environment in cycles. In every interaction cycle the agent is rewarded for its performance. We compare the average reward U from cycle 1 to m (average value) with the future discounted reward V from cycle k to infinity (discounted value). We consider essentially arbitrary (non-geometric) discount sequences and arbitrary reward sequences (non-MDP environments). We show that asymptotically U for m-infinity and V for k-infinity are equal, provided both limits exist. Further, if the effective horizon grows linearly with k or faster, then existence of the limit of U implies that the limit of V exists. Conversely, if the effective horizon grows linearly with k or slower, then existence of the limit of V implies that the limit of U exists.","General Discounting versus Average Reward Consider an agent interacting with an environment in cycles. In every interaction cycle the agent is rewarded for its performance. We compare the average reward U from cycle 1 to m (average value) with the future discounted reward V from cycle k to infinity (discounted value). We consider essentially arbitrary (non-geometric) discount sequences and arbitrary reward sequences (non-MDP environments). We show that asymptotically U for m-infinity and V for k-infinity are equal, provided both limits exist. Further, if the effective horizon grows linearly with k or faster, then existence of the limit of U implies that the limit of V exists. Conversely, if the effective horizon grows linearly with k or slower, then existence of the limit of V implies that the limit of U exists.",Technology
Climate And Resource Awareness is Imperative to Achieving Sustainable AI (and Preventing a Global AI Arms Race),"Sustainability encompasses three key facets: economic, environmental, and social. However, the nascent discourse that is emerging on sustainable artificial intelligence (AI) has predominantly focused on the environmental sustainability of AI, often neglecting the economic and social aspects. Achieving truly sustainable AI necessitates addressing the tension between its climate awareness and its social sustainability, which hinges on equitable access to AI development resources. The concept of resource awareness advocates for broader access to the infrastructure required to develop AI, fostering equity in AI innovation. Yet, this push for improving accessibility often overlooks the environmental costs of expanding such resource usage. In this position paper, we argue that reconciling climate and resource awareness is essential to realizing the full potential of sustainable AI. We use the framework of base-superstructure to analyze how the material conditions are influencing the current AI discourse. We also introduce the Climate and Resource Aware Machine Learning (CARAML) framework to address this conflict and propose actionable recommendations spanning individual, community, industry, government, and global levels to achieve sustainable AI.","Climate And Resource Awareness is Imperative to Achieving Sustainable AI (and Preventing a Global AI Arms Race) Sustainability encompasses three key facets: economic, environmental, and social. However, the nascent discourse that is emerging on sustainable artificial intelligence (AI) has predominantly focused on the environmental sustainability of AI, often neglecting the economic and social aspects. Achieving truly sustainable AI necessitates addressing the tension between its climate awareness and its social sustainability, which hinges on equitable access to AI development resources. The concept of resource awareness advocates for broader access to the infrastructure required to develop AI, fostering equity in AI innovation. Yet, this push for improving accessibility often overlooks the environmental costs of expanding such resource usage. In this position paper, we argue that reconciling climate and resource awareness is essential to realizing the full potential of sustainable AI. We use the framework of base-superstructure to analyze how the material conditions are influencing the current AI discourse. We also introduce the Climate and Resource Aware Machine Learning (CARAML) framework to address this conflict and propose actionable recommendations spanning individual, community, industry, government, and global levels to achieve sustainable AI.",Environment
Climate Modeling and Bifurcation,Many papers and monographs were written about the modeling the Earth climate and its variability. However there is still an obvious need for a module that presents the fundamentals of climate modeling to students at the undergraduate level. The present educational paper attempts to fill in this gap. To this end we collect in this paper the relevant climate data and present a simple zero and one dimensional models for the mean temperature of the Earth. These models can exhibit bifurcations from the present Earth climate to an ice age or a Venus type of climate. The models are accompanied by Matlab programs which enable the user to change the models parameters and explore the impact that these changes might have on their predictions on Earth climate.,Climate Modeling and Bifurcation Many papers and monographs were written about the modeling the Earth climate and its variability. However there is still an obvious need for a module that presents the fundamentals of climate modeling to students at the undergraduate level. The present educational paper attempts to fill in this gap. To this end we collect in this paper the relevant climate data and present a simple zero and one dimensional models for the mean temperature of the Earth. These models can exhibit bifurcations from the present Earth climate to an ice age or a Venus type of climate. The models are accompanied by Matlab programs which enable the user to change the models parameters and explore the impact that these changes might have on their predictions on Earth climate.,Environment
Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs Complex Reasoning,"In-context learning (ICL) can significantly enhance the complex reasoning capabilities of large language models (LLMs), with the key lying in the selection and ordering of demonstration examples. Previous methods typically relied on simple features to measure the relevance between examples. We argue that these features are not sufficient to reflect the intrinsic connections between examples. In this study, we propose a curriculum ICL strategy guided by problem-solving logic. We select demonstration examples by analyzing the problem-solving logic and order them based on curriculum learning. Specifically, we constructed a problem-solving logic instruction set based on the BREAK dataset and fine-tuned a language model to analyze the problem-solving logic of examples. Subsequently, we selected appropriate demonstration examples based on problem-solving logic and assessed their difficulty according to the number of problem-solving steps. In accordance with the principles of curriculum learning, we ordered the examples from easy to hard to serve as contextual prompts. Experimental results on multiple benchmarks indicate that our method outperforms previous ICL approaches in terms of performance and efficiency, effectively enhancing the complex reasoning capabilities of LLMs. Our project will be released at https:github.commaxuetaoCurriculumICL","Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs Complex Reasoning In-context learning (ICL) can significantly enhance the complex reasoning capabilities of large language models (LLMs), with the key lying in the selection and ordering of demonstration examples. Previous methods typically relied on simple features to measure the relevance between examples. We argue that these features are not sufficient to reflect the intrinsic connections between examples. In this study, we propose a curriculum ICL strategy guided by problem-solving logic. We select demonstration examples by analyzing the problem-solving logic and order them based on curriculum learning. Specifically, we constructed a problem-solving logic instruction set based on the BREAK dataset and fine-tuned a language model to analyze the problem-solving logic of examples. Subsequently, we selected appropriate demonstration examples based on problem-solving logic and assessed their difficulty according to the number of problem-solving steps. In accordance with the principles of curriculum learning, we ordered the examples from easy to hard to serve as contextual prompts. Experimental results on multiple benchmarks indicate that our method outperforms previous ICL approaches in terms of performance and efficiency, effectively enhancing the complex reasoning capabilities of LLMs. Our project will be released at https:github.commaxuetaoCurriculumICL",Education
The impact of climate policy uncertainty on financial market resilience: Evidence from China,"Resilience serves to assess the ability of the financial market to resist external shocks and to recover from shocks. The intensity and duration, used to indicate resilience, are calculated for Chinas financial market in this paper, focusing on the performance of each financial sub-market during and after several crises. Also, given that climate issues have been recognized as an important source of risk by financial market, we investigate the connectedness and mechanism of Chinas climate policy uncertainty (CPU) to its financial market resilience. We have found that the two resilience indicators of each market have a relatively consistent trend, but connectedness among markets has different sensitivities to both. In addition, Chinas CPU affects its financial market resilience by increasing the investor sentiment index and the non-performing loan ratio of commercial banks, and by reducing the capital and financial account balance. It is further found that Chinas financial markets consensus on the unswerving implementation of climate policy, which provides the reference for other countries on how to balance climate policys introduction and financial market development.","The impact of climate policy uncertainty on financial market resilience: Evidence from China Resilience serves to assess the ability of the financial market to resist external shocks and to recover from shocks. The intensity and duration, used to indicate resilience, are calculated for Chinas financial market in this paper, focusing on the performance of each financial sub-market during and after several crises. Also, given that climate issues have been recognized as an important source of risk by financial market, we investigate the connectedness and mechanism of Chinas climate policy uncertainty (CPU) to its financial market resilience. We have found that the two resilience indicators of each market have a relatively consistent trend, but connectedness among markets has different sensitivities to both. In addition, Chinas CPU affects its financial market resilience by increasing the investor sentiment index and the non-performing loan ratio of commercial banks, and by reducing the capital and financial account balance. It is further found that Chinas financial markets consensus on the unswerving implementation of climate policy, which provides the reference for other countries on how to balance climate policys introduction and financial market development.",Finance
The Benefits of Hydrogen Energy Transmission and Conversion Systems to the Renewable Power Grids: Day-ahead Unit Commitment,"The curtailment of renewable energy is more frequently observed as the renewable penetration levels are rising rapidly in modern power systems. It is a waste of free and green renewable energy and implies current power grids are unable to accommodate more renewable sources. One major reason is that higher power transmission capacity is required for higher renewable penetration level. Another major reason is the volatility of the renewable generation. The hydrogen mix or pure hydrogen pipeline can both transfer and store the energy in the form of hydrogen. However, its potential of accelerating renewable integration has not been investigated. In this paper, hydrogen pipeline networks, combined with power-to-hydrogen (P2H) and hydrogen-to-power (H2P) facilities, are organized to form a Hydrogen Energy Transmission and Conversion System (HETCS). We investigate the operation of power systems coupled with HETCS, and propose the day-ahead security-constrained unit commitment (SCUC) with HETCS. The SCUC simulation is conducted on a modified IEEE 24-bus power system with HETCS. Simulation results show HETCS can substantially reduce the renewable curtailment, CO2 emission, load payment and total operational cost. This study validates the HETCS can be a promising solution to achieve net-zero renewable grids.","The Benefits of Hydrogen Energy Transmission and Conversion Systems to the Renewable Power Grids: Day-ahead Unit Commitment The curtailment of renewable energy is more frequently observed as the renewable penetration levels are rising rapidly in modern power systems. It is a waste of free and green renewable energy and implies current power grids are unable to accommodate more renewable sources. One major reason is that higher power transmission capacity is required for higher renewable penetration level. Another major reason is the volatility of the renewable generation. The hydrogen mix or pure hydrogen pipeline can both transfer and store the energy in the form of hydrogen. However, its potential of accelerating renewable integration has not been investigated. In this paper, hydrogen pipeline networks, combined with power-to-hydrogen (P2H) and hydrogen-to-power (H2P) facilities, are organized to form a Hydrogen Energy Transmission and Conversion System (HETCS). We investigate the operation of power systems coupled with HETCS, and propose the day-ahead security-constrained unit commitment (SCUC) with HETCS. The SCUC simulation is conducted on a modified IEEE 24-bus power system with HETCS. Simulation results show HETCS can substantially reduce the renewable curtailment, CO2 emission, load payment and total operational cost. This study validates the HETCS can be a promising solution to achieve net-zero renewable grids.",Environment
EducationQ: Evaluating LLMs Teaching Capabilities Through Multi-Agent Dialogue Framework,"Large language models (LLMs) increasingly serve as educational tools, yet evaluating their teaching capabilities remains challenging due to the resource-intensive, context-dependent, and methodologically complex nature of teacher-student interactions. We introduce EducationQ, a multi-agent dialogue framework that efficiently assesses teaching capabilities through simulated dynamic educational scenarios, featuring specialized agents for teaching, learning, and evaluation. Testing 14 LLMs across major AI Organizations (OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13 disciplines and 10 difficulty levels reveals that teaching effectiveness does not correlate linearly with model scale or general reasoning capabilities - with some smaller open-source models outperforming larger commercial counterparts in teaching contexts. This finding highlights a critical gap in current evaluations that prioritize knowledge recall over interactive pedagogy. Our mixed-methods evaluation, combining quantitative metrics with qualitative analysis and expert case studies, identifies distinct pedagogical strengths employed by top-performing models (e.g., sophisticated questioning strategies, adaptive feedback mechanisms). Human expert evaluations show 78 agreement with our automated qualitative analysis of effective teaching behaviors, validating our methodology. EducationQ demonstrates that LLMs-as-teachers require specialized optimization beyond simple scaling, suggesting next-generation educational AI prioritize targeted enhancement of specific pedagogical effectiveness.","EducationQ: Evaluating LLMs Teaching Capabilities Through Multi-Agent Dialogue Framework Large language models (LLMs) increasingly serve as educational tools, yet evaluating their teaching capabilities remains challenging due to the resource-intensive, context-dependent, and methodologically complex nature of teacher-student interactions. We introduce EducationQ, a multi-agent dialogue framework that efficiently assesses teaching capabilities through simulated dynamic educational scenarios, featuring specialized agents for teaching, learning, and evaluation. Testing 14 LLMs across major AI Organizations (OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13 disciplines and 10 difficulty levels reveals that teaching effectiveness does not correlate linearly with model scale or general reasoning capabilities - with some smaller open-source models outperforming larger commercial counterparts in teaching contexts. This finding highlights a critical gap in current evaluations that prioritize knowledge recall over interactive pedagogy. Our mixed-methods evaluation, combining quantitative metrics with qualitative analysis and expert case studies, identifies distinct pedagogical strengths employed by top-performing models (e.g., sophisticated questioning strategies, adaptive feedback mechanisms). Human expert evaluations show 78 agreement with our automated qualitative analysis of effective teaching behaviors, validating our methodology. EducationQ demonstrates that LLMs-as-teachers require specialized optimization beyond simple scaling, suggesting next-generation educational AI prioritize targeted enhancement of specific pedagogical effectiveness.",Education
Intelligence Preschool Education System based on Multimodal Interaction Systems and AI,"Rapid progress in AI technologies has generated considerable interest in their potential to address challenges in every field and education is no exception. Improving learning outcomes and providing relevant education to all have been dominant themes universally, both in the developed and developing world. And they have taken on greater significance in the current era of technology driven personalization.","Intelligence Preschool Education System based on Multimodal Interaction Systems and AI Rapid progress in AI technologies has generated considerable interest in their potential to address challenges in every field and education is no exception. Improving learning outcomes and providing relevant education to all have been dominant themes universally, both in the developed and developing world. And they have taken on greater significance in the current era of technology driven personalization.",Education
Intentional Design for Empowerment,"I argue for empowering education, adapting Marxs idea of ownership of the means of production, and discuss interactive simulations as one example of a tool in which intentional design can support student ownership of learning. I propose a model that leverages affordances of educational tools to do positive work toward empowering education.","Intentional Design for Empowerment I argue for empowering education, adapting Marxs idea of ownership of the means of production, and discuss interactive simulations as one example of a tool in which intentional design can support student ownership of learning. I propose a model that leverages affordances of educational tools to do positive work toward empowering education.",Education
The Resurgence of Trumponomics: Implications for the Future of ESG Investments in a Changing Political Landscape,"Public policy shapes the economic landscape, influencing everything from corporate behavior to individual investment decisions. For Environmental, Social, and Governance (ESG) investors, these policy shifts can create opportunities and challenges as they navigate an ever-changing regulatory environment. The contrast between the Trump and Biden administrations offers a striking example of how differing political agendas can affect ESG investments. Trumps first term was marked by deregulation and policies favoring fossil fuels, which created an uncertain environment for sustainable investments. When Biden assumed office, his focus on climate action and clean energy reinvigorated the ESG sector, offering a more stable and supportive landscape for green investments. However, with Trumps return to power in his second term, these policies are being reversed again, leading to further volatility. This paper explores how such dramatic shifts in public policy influence economic strategies and directly impact ESG investors decisions, forcing them to constantly reassess their portfolios in response to changing political climates.","The Resurgence of Trumponomics: Implications for the Future of ESG Investments in a Changing Political Landscape Public policy shapes the economic landscape, influencing everything from corporate behavior to individual investment decisions. For Environmental, Social, and Governance (ESG) investors, these policy shifts can create opportunities and challenges as they navigate an ever-changing regulatory environment. The contrast between the Trump and Biden administrations offers a striking example of how differing political agendas can affect ESG investments. Trumps first term was marked by deregulation and policies favoring fossil fuels, which created an uncertain environment for sustainable investments. When Biden assumed office, his focus on climate action and clean energy reinvigorated the ESG sector, offering a more stable and supportive landscape for green investments. However, with Trumps return to power in his second term, these policies are being reversed again, leading to further volatility. This paper explores how such dramatic shifts in public policy influence economic strategies and directly impact ESG investors decisions, forcing them to constantly reassess their portfolios in response to changing political climates.",Finance
Multiplicative NonholonomicNewton -like Algorithm,"We construct new algorithms from scratch, which use the fourth order cumulant of stochastic variables for the cost function. The multiplicative updating rule here constructed is natural from the homogeneous nature of the Lie group and has numerous merits for the rigorous treatment of the dynamics. As one consequence, the second order convergence is shown. For the cost function, functions invariant under the componentwise scaling are choosen. By identifying points which can be transformed to each other by the scaling, we assume that the dynamics is in a coset space. In our method, a point can move toward any direction in this coset. Thus, no prewhitening is required.","Multiplicative NonholonomicNewton -like Algorithm We construct new algorithms from scratch, which use the fourth order cumulant of stochastic variables for the cost function. The multiplicative updating rule here constructed is natural from the homogeneous nature of the Lie group and has numerous merits for the rigorous treatment of the dynamics. As one consequence, the second order convergence is shown. For the cost function, functions invariant under the componentwise scaling are choosen. By identifying points which can be transformed to each other by the scaling, we assume that the dynamics is in a coset space. In our method, a point can move toward any direction in this coset. Thus, no prewhitening is required.",Technology
Mobile Technologies in Education,The growth of smartphone users globally is a factor that educational technologists should not ignore. This ever-growing market will eventually lead to ubiquitous learning (u-learning). The development of specific content should be replaced by the design of content in languages that are compatible with scalable technologies and that will reach the hands of students in the future in an attractive way.,Mobile Technologies in Education The growth of smartphone users globally is a factor that educational technologists should not ignore. This ever-growing market will eventually lead to ubiquitous learning (u-learning). The development of specific content should be replaced by the design of content in languages that are compatible with scalable technologies and that will reach the hands of students in the future in an attractive way.,Education
The physics of climate change: simple models in climate science,There is a perception that climate science can only be approached with complex computer simulations. But working climate scientists often use simple models to understand their simulations and make order-of-magnitude estimates. This article presents some of these simple models with the goal of making climate science more accessible and comprehensible.,The physics of climate change: simple models in climate science There is a perception that climate science can only be approached with complex computer simulations. But working climate scientists often use simple models to understand their simulations and make order-of-magnitude estimates. This article presents some of these simple models with the goal of making climate science more accessible and comprehensible.,Environment
Performative Market Making,"Financial models do not merely analyse markets, but actively shape them. This effect, known as performativity, describes how financial theories and the subsequent actions based on them influence market processes, by creating self-fulfilling prophecies. Although discussed in the literature on economic sociology, this deeply rooted phenomenon lacks mathematical formulation in financial markets. Our paper closes this gap by breaking down the canonical separation of diffusion processes between the description of the market environment and the financial model. We do that by embedding the model in the process itself, creating a closed feedback loop, and demonstrate how prices change towards greater conformity to the prevailing financial model used in the market. We further show, with closed-form solutions and machine learning, how a performative market maker can reverse engineer the current dominant strategies in the market and effectively arbitrage them while maintaining competitive quotes and superior PL.","Performative Market Making Financial models do not merely analyse markets, but actively shape them. This effect, known as performativity, describes how financial theories and the subsequent actions based on them influence market processes, by creating self-fulfilling prophecies. Although discussed in the literature on economic sociology, this deeply rooted phenomenon lacks mathematical formulation in financial markets. Our paper closes this gap by breaking down the canonical separation of diffusion processes between the description of the market environment and the financial model. We do that by embedding the model in the process itself, creating a closed feedback loop, and demonstrate how prices change towards greater conformity to the prevailing financial model used in the market. We further show, with closed-form solutions and machine learning, how a performative market maker can reverse engineer the current dominant strategies in the market and effectively arbitrage them while maintaining competitive quotes and superior PL.",Finance
Recommender Systems for Social Good: The Role of Accountability and Sustainability,"This work examines the role of recommender systems in promoting sustainability, social responsibility, and accountability, with a focus on alignment with the United Nations Sustainable Development Goals (SDGs). As recommender systems become increasingly integrated into daily interactions, they must go beyond personalization to support responsible consumption, reduce environmental impact, and foster social good. We explore strategies to mitigate the carbon footprint of recommendation models, ensure fairness, and implement accountability mechanisms. By adopting these approaches, recommender systems can contribute to sustainable and socially beneficial outcomes, aligning technological advancements with the SDGs focused on environmental sustainability and social well-being.","Recommender Systems for Social Good: The Role of Accountability and Sustainability This work examines the role of recommender systems in promoting sustainability, social responsibility, and accountability, with a focus on alignment with the United Nations Sustainable Development Goals (SDGs). As recommender systems become increasingly integrated into daily interactions, they must go beyond personalization to support responsible consumption, reduce environmental impact, and foster social good. We explore strategies to mitigate the carbon footprint of recommendation models, ensure fairness, and implement accountability mechanisms. By adopting these approaches, recommender systems can contribute to sustainable and socially beneficial outcomes, aligning technological advancements with the SDGs focused on environmental sustainability and social well-being.",Environment
Analysis of Biomass Sustainability Indicators from a Machine Learning Perspective,"Plant biomass estimation is critical due to the variability of different environmental factors and crop management practices associated with it. The assessment is largely impacted by the accurate prediction of different environmental sustainability indicators. A robust model to predict sustainability indicators is a must for the biomass community. This study proposes a robust model for biomass sustainability prediction by analyzing sustainability indicators using machine learning models. The prospect of ensemble learning was also investigated to analyze the regression problem. All experiments were carried out on a crop residue data from the Ohio state. Ten machine learning models, namely, linear regression, ridge regression, multilayer perceptron, k-nearest neighbors, support vector machine, decision tree, gradient boosting, random forest, stacking and voting, were analyzed to estimate three biomass sustainability indicators, namely soil erosion factor, soil conditioning index, and organic matter factor. The performance of the model was assessed using cross-correlation (R2), root mean squared error and mean absolute error metrics. The results showed that Random Forest was the best performing model to assess sustainability indicators. The analyzed model can now serve as a guide for assessing sustainability indicators in real time.","Analysis of Biomass Sustainability Indicators from a Machine Learning Perspective Plant biomass estimation is critical due to the variability of different environmental factors and crop management practices associated with it. The assessment is largely impacted by the accurate prediction of different environmental sustainability indicators. A robust model to predict sustainability indicators is a must for the biomass community. This study proposes a robust model for biomass sustainability prediction by analyzing sustainability indicators using machine learning models. The prospect of ensemble learning was also investigated to analyze the regression problem. All experiments were carried out on a crop residue data from the Ohio state. Ten machine learning models, namely, linear regression, ridge regression, multilayer perceptron, k-nearest neighbors, support vector machine, decision tree, gradient boosting, random forest, stacking and voting, were analyzed to estimate three biomass sustainability indicators, namely soil erosion factor, soil conditioning index, and organic matter factor. The performance of the model was assessed using cross-correlation (R2), root mean squared error and mean absolute error metrics. The results showed that Random Forest was the best performing model to assess sustainability indicators. The analyzed model can now serve as a guide for assessing sustainability indicators in real time.",Environment
Effects of Selection Logging on Rainforest Productivity,"An analysis of data from 212 permanent sample plots provided no evidence of any decline in rainforest productivity after three cycles of selection logging in the tropical rainforests of north Queensland. Relative productivity was determined as the difference between observed diameter increments and increments predicted from a diameter increment function which incorporated tree size, stand density and site quality. Analyses of variance and regression analyses revealed no significant decline in productivity after repeated harvesting. There is evidence to support the assertion that if any permanent productivity decline exists, it does not exceed six per cent per harvest.","Effects of Selection Logging on Rainforest Productivity An analysis of data from 212 permanent sample plots provided no evidence of any decline in rainforest productivity after three cycles of selection logging in the tropical rainforests of north Queensland. Relative productivity was determined as the difference between observed diameter increments and increments predicted from a diameter increment function which incorporated tree size, stand density and site quality. Analyses of variance and regression analyses revealed no significant decline in productivity after repeated harvesting. There is evidence to support the assertion that if any permanent productivity decline exists, it does not exceed six per cent per harvest.",Healthcare
Microelectromagnets for the manipulation of biological systems,"Microelectromagnet devices, a ring trap and a matrix, were developed for the microscopic control of biological systems. The ring trap is a circular Au wire with an insulator on top. The matrix has two arrays of straight Au wires, one array perpendicular to the other, that are separated and topped by insulating layers. Microelectromagnets can produce strong magnetic fields to stably manipulate magnetically tagged biological systems in a fluid. Moreover, by controlling the currents flowing through the wires, a microelectromagnet matrix can move a peak in the magnetic field magnitude continuously over the surface of the device, generate multiple peaks simultaneously and control them independently. These capabilities of a matrix can be used to trap, continuously transport, assemble, separate and sort biological samples on micrometer length scales. Combining microelectromagnets with microfluidic systems, chip-based experimental systems can be realized for novel applications in biological and biomedical studies.","Microelectromagnets for the manipulation of biological systems Microelectromagnet devices, a ring trap and a matrix, were developed for the microscopic control of biological systems. The ring trap is a circular Au wire with an insulator on top. The matrix has two arrays of straight Au wires, one array perpendicular to the other, that are separated and topped by insulating layers. Microelectromagnets can produce strong magnetic fields to stably manipulate magnetically tagged biological systems in a fluid. Moreover, by controlling the currents flowing through the wires, a microelectromagnet matrix can move a peak in the magnetic field magnitude continuously over the surface of the device, generate multiple peaks simultaneously and control them independently. These capabilities of a matrix can be used to trap, continuously transport, assemble, separate and sort biological samples on micrometer length scales. Combining microelectromagnets with microfluidic systems, chip-based experimental systems can be realized for novel applications in biological and biomedical studies.",Healthcare
Modelling horizontal and vertical concentration profiles of ozone and oxides of nitrogen within high-latitude urban area,"A Lagrangian column model has been developed to simulate the mean (monthly and annual) three-dimensional structure in ozone and nitrogen oxides concentrations in the boundary layer within and immediately around an urban area. Short time-scale photochemical processes of ozone, as well as emissions and deposition to the ground are simulated. The results show that the average surface ozone concentration in the urban area is lower than the surrounding rural areas by typically 50. Model results are compared with observations.","Modelling horizontal and vertical concentration profiles of ozone and oxides of nitrogen within high-latitude urban area A Lagrangian column model has been developed to simulate the mean (monthly and annual) three-dimensional structure in ozone and nitrogen oxides concentrations in the boundary layer within and immediately around an urban area. Short time-scale photochemical processes of ozone, as well as emissions and deposition to the ground are simulated. The results show that the average surface ozone concentration in the urban area is lower than the surrounding rural areas by typically 50. Model results are compared with observations.",Environment
rm P3: A Practice Focused Learning Environment,"There has been an increased focus on the integration of practices into physics curricula, with a particular emphasis on integrating computation into the undergraduate curriculum of scientists and engineers. In this paper, we present a university-level, introductory physics course for science and engineering majors at Michigan State University (MSU) called rm P3 (Projects and Practices in Physics) that is centered around providing introductory physics students with the opportunity to appropriate various science and engineering practices. The rm P3 design integrates computation with analytical problem solving and is built upon a curriculum foundation of problem-based learning, the principles of constructive alignment and the theoretical framework of community of practice. The design includes an innovative approach to computational physics instruction, instructional scaffolds, and a unique approach to assessment that enables instructors to guide students in the development of the practices of a physicist. We present the very positive student related outcomes of the design gathered via attitudinal and conceptual inventories and research interviews of students reflecting on their experiences in the rm P3 classroom.","rm P3: A Practice Focused Learning Environment There has been an increased focus on the integration of practices into physics curricula, with a particular emphasis on integrating computation into the undergraduate curriculum of scientists and engineers. In this paper, we present a university-level, introductory physics course for science and engineering majors at Michigan State University (MSU) called rm P3 (Projects and Practices in Physics) that is centered around providing introductory physics students with the opportunity to appropriate various science and engineering practices. The rm P3 design integrates computation with analytical problem solving and is built upon a curriculum foundation of problem-based learning, the principles of constructive alignment and the theoretical framework of community of practice. The design includes an innovative approach to computational physics instruction, instructional scaffolds, and a unique approach to assessment that enables instructors to guide students in the development of the practices of a physicist. We present the very positive student related outcomes of the design gathered via attitudinal and conceptual inventories and research interviews of students reflecting on their experiences in the rm P3 classroom.",Education
"The RR interval spectrum, the ECG signal and aliasing","A reliable spectral analysis requires sampling rate at least twice as large as the frequency bound, otherwise the analysis will be unreliable and plagued with aliasing distortions. The RR samplings do not satisfy the above requirements and therefore their spectral analysis might be unreliable. In order to demonstrate the feasibility of aliasing in RR spectral analysis, we have done an experiment which have shown clearly how the aliasing was developed. In the experiments, one of us (A.G) had kept his high breathing rate constant with the aid of metronome for more than 5 minutes. The breathing rate was larger than one-half the heart rate. Very accurate results were obtained and the resulting aliasing well understood. To our best knowledge this is the first controlled experiment of this kind coducted on humans. We compared the RR spectral analysis with the spectrum of the ECG signals from which the RR intervals were extracted. In the significant for RR analysis frequencies (below one-half Hertz) significant differences were observed. In conclusion we recommend to study the spectral analysis of the ECG signal in the free of aliasing frequency range.","The RR interval spectrum, the ECG signal and aliasing A reliable spectral analysis requires sampling rate at least twice as large as the frequency bound, otherwise the analysis will be unreliable and plagued with aliasing distortions. The RR samplings do not satisfy the above requirements and therefore their spectral analysis might be unreliable. In order to demonstrate the feasibility of aliasing in RR spectral analysis, we have done an experiment which have shown clearly how the aliasing was developed. In the experiments, one of us (A.G) had kept his high breathing rate constant with the aid of metronome for more than 5 minutes. The breathing rate was larger than one-half the heart rate. Very accurate results were obtained and the resulting aliasing well understood. To our best knowledge this is the first controlled experiment of this kind coducted on humans. We compared the RR spectral analysis with the spectrum of the ECG signals from which the RR intervals were extracted. In the significant for RR analysis frequencies (below one-half Hertz) significant differences were observed. In conclusion we recommend to study the spectral analysis of the ECG signal in the free of aliasing frequency range.",Healthcare
Mathematical modeling for sustainability: How can it promote sustainable learning in mathematics education?,"This article reviews the current state of teaching and learning mathematical modeling in the context of sustainable development goals for education at the tertiary level. While ample research on mathematical modeling education and published textbooks on the topic are available, there is a lack of focus on mathematical modeling for sustainability. This review aims to address this gap by exploring the powerful intersection of mathematical modeling and sustainability. Mathematical modeling for sustainability connects two distinct realms: learning about the mathematics of sustainability and promoting sustainable learning in mathematics education. The former involves teaching and learning sustainability quantitatively, while the latter encompasses pedagogy that enables learners to apply quantitative knowledge and skills to everyday life and continue learning and improving mathematically beyond formal education. To demonstrate the practical application of mathematical modeling for sustainability, we discuss a specific textbook suitable for a pilot liberal arts course. We illustrate how learners can grasp mathematical concepts related to sustainability through simple yet mathematically diverse examples, which can be further developed for teaching such a course. Indeed, by filling the gap in the literature and providing practical resources, this review contributes to the advancement of mathematical modeling education in the context of sustainability.","Mathematical modeling for sustainability: How can it promote sustainable learning in mathematics education? This article reviews the current state of teaching and learning mathematical modeling in the context of sustainable development goals for education at the tertiary level. While ample research on mathematical modeling education and published textbooks on the topic are available, there is a lack of focus on mathematical modeling for sustainability. This review aims to address this gap by exploring the powerful intersection of mathematical modeling and sustainability. Mathematical modeling for sustainability connects two distinct realms: learning about the mathematics of sustainability and promoting sustainable learning in mathematics education. The former involves teaching and learning sustainability quantitatively, while the latter encompasses pedagogy that enables learners to apply quantitative knowledge and skills to everyday life and continue learning and improving mathematically beyond formal education. To demonstrate the practical application of mathematical modeling for sustainability, we discuss a specific textbook suitable for a pilot liberal arts course. We illustrate how learners can grasp mathematical concepts related to sustainability through simple yet mathematically diverse examples, which can be further developed for teaching such a course. Indeed, by filling the gap in the literature and providing practical resources, this review contributes to the advancement of mathematical modeling education in the context of sustainability.",Education
Does a Plane Imitate a Bird? Does Computer Vision Have to Follow Biological Paradigms?,"We posit a new paradigm for image information processing. For the last 25 years, this task was usually approached in the frame of Treismans two-stage paradigm 1. The latter supposes an unsupervised, bottom-up directed process of preliminary information pieces gathering at the lower processing stages and a supervised, top-down directed process of information pieces binding and grouping at the higher stages. It is acknowledged that these sub-processes interact and intervene between them in a tricky and a complicated manner. Notwithstanding the prevalence of this paradigm in biological and computer vision, we nevertheless propose to replace it with a new one, which we would like to designate as a two-part paradigm. In it, information contained in an image is initially extracted in an independent top-down manner by one part of the system, and then it is examined and interpreted by another, separate system part. We argue that the new paradigm seems to be more plausible than its forerunner. We provide evidence from human attention vision studies and insights of Kolmogorovs complexity theory to support our arguments. We also provide some reasons in favor of separate image interpretation issues.","Does a Plane Imitate a Bird? Does Computer Vision Have to Follow Biological Paradigms? We posit a new paradigm for image information processing. For the last 25 years, this task was usually approached in the frame of Treismans two-stage paradigm 1. The latter supposes an unsupervised, bottom-up directed process of preliminary information pieces gathering at the lower processing stages and a supervised, top-down directed process of information pieces binding and grouping at the higher stages. It is acknowledged that these sub-processes interact and intervene between them in a tricky and a complicated manner. Notwithstanding the prevalence of this paradigm in biological and computer vision, we nevertheless propose to replace it with a new one, which we would like to designate as a two-part paradigm. In it, information contained in an image is initially extracted in an independent top-down manner by one part of the system, and then it is examined and interpreted by another, separate system part. We argue that the new paradigm seems to be more plausible than its forerunner. We provide evidence from human attention vision studies and insights of Kolmogorovs complexity theory to support our arguments. We also provide some reasons in favor of separate image interpretation issues.",Technology
Clinical Productivity System - A Decision Support Model,"Purpose: This goal of this study was to evaluate the effects of a data-driven clinical productivity system that leverages Electronic Health Record (EHR) data to provide productivity decision support functionality in a real-world clinical setting. The system was implemented for a large behavioral health care provider seeing over 75,000 distinct clients a year. Designmethodologyapproach: The key metric in this system is a VPU, which simultaneously optimizes multiple aspects of clinical care. The resulting mathematical value of clinical productivity was hypothesized to tightly link the organizations performance to its expectations and, through transparency and decision support tools at the clinician level, affect significant changes in productivity, quality, and consistency relative to traditional models of clinical productivity. Findings: In only 3 months, every single variable integrated into the VPU system showed significant improvement, including a 30 rise in revenue, 10 rise in clinical percentage, a 25 rise in treatment plan completion, a 20 rise in case rate eligibility, along with similar improvements in complianceaudit issues, outcomes collection, access, etc. Practical implications: A data-driven clinical productivity system employing decision support functionality is effective because of the impact on clinician behavior relative to traditional clinical productivity systems. Critically, the model is also extensible to integration with outcomes-based productivity. OriginalityValue: EHRs are only a first step - the problem is turning that data into useful information. Technology can leverage the data in order to produce actionable information that can inform clinical practice and decision-making. Without additional technology, EHRs are essentially just copies of paper-based records stored in electronic form.","Clinical Productivity System - A Decision Support Model Purpose: This goal of this study was to evaluate the effects of a data-driven clinical productivity system that leverages Electronic Health Record (EHR) data to provide productivity decision support functionality in a real-world clinical setting. The system was implemented for a large behavioral health care provider seeing over 75,000 distinct clients a year. Designmethodologyapproach: The key metric in this system is a VPU, which simultaneously optimizes multiple aspects of clinical care. The resulting mathematical value of clinical productivity was hypothesized to tightly link the organizations performance to its expectations and, through transparency and decision support tools at the clinician level, affect significant changes in productivity, quality, and consistency relative to traditional models of clinical productivity. Findings: In only 3 months, every single variable integrated into the VPU system showed significant improvement, including a 30 rise in revenue, 10 rise in clinical percentage, a 25 rise in treatment plan completion, a 20 rise in case rate eligibility, along with similar improvements in complianceaudit issues, outcomes collection, access, etc. Practical implications: A data-driven clinical productivity system employing decision support functionality is effective because of the impact on clinician behavior relative to traditional clinical productivity systems. Critically, the model is also extensible to integration with outcomes-based productivity. OriginalityValue: EHRs are only a first step - the problem is turning that data into useful information. Technology can leverage the data in order to produce actionable information that can inform clinical practice and decision-making. Without additional technology, EHRs are essentially just copies of paper-based records stored in electronic form.",Healthcare
"Different Length, Different Needs: Qualitative Analysis of Threads in Online Health Communities","Online health communities provide a knowledge exchange platform for a wide range of diseases and health conditions. Informational and emotional support helps forum participants orient around health issues beyond in-person doctor visits. So far, little is known about the relation between the level of participation and participants contributions in online health communities. To gain insights on the issue, we analyzed 456 posts in 56 threads from the Dermatology sub-forum of an online health community. While low participation threads (short threads) revolved around solving an individuals health issue through diagnosis suggestions and medical advice, participants in high participation threads (long threads) built collective knowledge and a sense of community, typically discussing chronic and rare conditions that medical professionals were unfamiliar with or could not treat effectively. Our results suggest that in short threads an individuals health issue is addressed, while in long threads, sub-communities about specific rare and chronic diseases emerge. This has implications for the user interface design of health forums, which could be developed to better support community building elements, even in short threads.","Different Length, Different Needs: Qualitative Analysis of Threads in Online Health Communities Online health communities provide a knowledge exchange platform for a wide range of diseases and health conditions. Informational and emotional support helps forum participants orient around health issues beyond in-person doctor visits. So far, little is known about the relation between the level of participation and participants contributions in online health communities. To gain insights on the issue, we analyzed 456 posts in 56 threads from the Dermatology sub-forum of an online health community. While low participation threads (short threads) revolved around solving an individuals health issue through diagnosis suggestions and medical advice, participants in high participation threads (long threads) built collective knowledge and a sense of community, typically discussing chronic and rare conditions that medical professionals were unfamiliar with or could not treat effectively. Our results suggest that in short threads an individuals health issue is addressed, while in long threads, sub-communities about specific rare and chronic diseases emerge. This has implications for the user interface design of health forums, which could be developed to better support community building elements, even in short threads.",Healthcare
V-like formations in flocks of artificial birds,"We consider flocks of artificial birds and study the emergence of V-like formations during flight. We introduce a small set of fully distributed positioning rules to guide the birds movements and demonstrate, by means of simulations, that they tend to lead to stabilization into several of the well-known V-like formations that have been observed in nature. We also provide quantitative indicators that we believe are closely related to achieving V-like formations, and study their behavior over a large set of independent simulations.","V-like formations in flocks of artificial birds We consider flocks of artificial birds and study the emergence of V-like formations during flight. We introduce a small set of fully distributed positioning rules to guide the birds movements and demonstrate, by means of simulations, that they tend to lead to stabilization into several of the well-known V-like formations that have been observed in nature. We also provide quantitative indicators that we believe are closely related to achieving V-like formations, and study their behavior over a large set of independent simulations.",Technology
Enhancing Higher Education with Generative AI: A Multimodal Approach for Personalised Learning,"This research explores the opportunities of Generative AI (GenAI) in the realm of higher education through the design and development of a multimodal chatbot for an undergraduate course. Leveraging the ChatGPT API for nuanced text-based interactions and Google Bard for advanced image analysis and diagram-to-code conversions, we showcase the potential of GenAI in addressing a broad spectrum of educational queries. Additionally, the chatbot presents a file-based analyser designed for educators, offering deep insights into student feedback via sentiment and emotion analysis, and summarising course evaluations with key metrics. These combinations highlight the crucial role of multimodal conversational AI in enhancing teaching and learning processes, promising significant advancements in educational adaptability, engagement, and feedback analysis. By demonstrating a practical web application, this research underlines the imperative for integrating GenAI technologies to foster more dynamic and responsive educational environments, ultimately contributing to improved educational outcomes and pedagogical strategies.","Enhancing Higher Education with Generative AI: A Multimodal Approach for Personalised Learning This research explores the opportunities of Generative AI (GenAI) in the realm of higher education through the design and development of a multimodal chatbot for an undergraduate course. Leveraging the ChatGPT API for nuanced text-based interactions and Google Bard for advanced image analysis and diagram-to-code conversions, we showcase the potential of GenAI in addressing a broad spectrum of educational queries. Additionally, the chatbot presents a file-based analyser designed for educators, offering deep insights into student feedback via sentiment and emotion analysis, and summarising course evaluations with key metrics. These combinations highlight the crucial role of multimodal conversational AI in enhancing teaching and learning processes, promising significant advancements in educational adaptability, engagement, and feedback analysis. By demonstrating a practical web application, this research underlines the imperative for integrating GenAI technologies to foster more dynamic and responsive educational environments, ultimately contributing to improved educational outcomes and pedagogical strategies.",Education
"Financial Markets, Financial Institutions and International Trade: Examining the causal links for Indian Economy","This study investigates whether a uni-directional or bi-directional causal relationship exists between financial development and international trade for Indian economy, during the time period from 1980 to 2019. The empirical analysis utilizes three measures of financial development created by IMF, namely, financial institutional development index, financial market development index and a composite index of financial development, encompassing dimensions of financial access, depth and efficiency. Johansen cointegration, vector error correction model and vector auto regressive model are estimated to examine the long run relationship and short run dynamics among the variables of interest. The econometric results indicate that there is indeed a long run causal relationship between the composite index of financial development and trade openness. Cointegration is also found to exist between trade openness and index of financial market development. However, there is no evidence of cointegration between financial institutional development and trade openness. Granger causality test results indicate the presence of uni-directional causality running from composite index of financial development to trade openness. Financial market development is also found to Granger cause trade openness. In contrast, trade openness is found to promote financial institutional development in the short run. Empirical evidence thus underlines the importance of formulating policies which recognize the role of well-developed financial markets in accelerating international trade of Indian economy.","Financial Markets, Financial Institutions and International Trade: Examining the causal links for Indian Economy This study investigates whether a uni-directional or bi-directional causal relationship exists between financial development and international trade for Indian economy, during the time period from 1980 to 2019. The empirical analysis utilizes three measures of financial development created by IMF, namely, financial institutional development index, financial market development index and a composite index of financial development, encompassing dimensions of financial access, depth and efficiency. Johansen cointegration, vector error correction model and vector auto regressive model are estimated to examine the long run relationship and short run dynamics among the variables of interest. The econometric results indicate that there is indeed a long run causal relationship between the composite index of financial development and trade openness. Cointegration is also found to exist between trade openness and index of financial market development. However, there is no evidence of cointegration between financial institutional development and trade openness. Granger causality test results indicate the presence of uni-directional causality running from composite index of financial development to trade openness. Financial market development is also found to Granger cause trade openness. In contrast, trade openness is found to promote financial institutional development in the short run. Empirical evidence thus underlines the importance of formulating policies which recognize the role of well-developed financial markets in accelerating international trade of Indian economy.",Finance
"Weather forecasts, Weather derivatives, Black-Scholes, Feynmann-Kac and Fokker-Planck","We investigate the relationships between weather forecasting, weather derivatives, the Black-Scholes equation, Feynmann-Kac theory and the Fokker-Planck equation. There is one useful result, but on the whole the relations we present seem to be more interesting than practically useful.","Weather forecasts, Weather derivatives, Black-Scholes, Feynmann-Kac and Fokker-Planck We investigate the relationships between weather forecasting, weather derivatives, the Black-Scholes equation, Feynmann-Kac theory and the Fokker-Planck equation. There is one useful result, but on the whole the relations we present seem to be more interesting than practically useful.",Environment
Epitope prediction improved by multitask support vector machines,"Motivation: In silico methods for the prediction of antigenic peptides binding to MHC class I molecules play an increasingly important role in the identification of T-cell epitopes. Statistical and machine learning methods, in particular, are widely used to score candidate epitopes based on their similarity with known epitopes and non epitopes. The genes coding for the MHC molecules, however, are highly polymorphic, and statistical methods have difficulties to build models for alleles with few known epitopes. In this case, recent works have demonstrated the utility of leveraging information across alleles to improve the performance of the prediction. Results: We design a support vector machine algorithm that is able to learn epitope models for all alleles simultaneously, by sharing information across similar alleles. The sharing of information across alleles is controlled by a user-defined measure of similarity between alleles. We show that this similarity can be defined in terms of supertypes, or more directly by comparing key residues known to play a role in the peptide-MHC binding. We illustrate the potential of this approach on various benchmark experiments where it outperforms other state-of-the-art methods.","Epitope prediction improved by multitask support vector machines Motivation: In silico methods for the prediction of antigenic peptides binding to MHC class I molecules play an increasingly important role in the identification of T-cell epitopes. Statistical and machine learning methods, in particular, are widely used to score candidate epitopes based on their similarity with known epitopes and non epitopes. The genes coding for the MHC molecules, however, are highly polymorphic, and statistical methods have difficulties to build models for alleles with few known epitopes. In this case, recent works have demonstrated the utility of leveraging information across alleles to improve the performance of the prediction. Results: We design a support vector machine algorithm that is able to learn epitope models for all alleles simultaneously, by sharing information across similar alleles. The sharing of information across alleles is controlled by a user-defined measure of similarity between alleles. We show that this similarity can be defined in terms of supertypes, or more directly by comparing key residues known to play a role in the peptide-MHC binding. We illustrate the potential of this approach on various benchmark experiments where it outperforms other state-of-the-art methods.",Healthcare
Modeling Technological Deployment and Renewal: Monotonic vs. Oscillating Industrial Dynamics,"This study proposes a new model based on a classic S-curve that describes deployment and stabilization at maximum capacity. In addition, the model extends to the post-growth plateau, where technological capacity is renewed according to the distribution of equipment lifespans. We obtain two qualitatively different results. In the case of fast deployment, characterized by a short deployment time in relation to the average equipment lifetime, production is subject to significant oscillations. In the case of slow deployment, production increases monotonically until it reaches a renewal plateau. These results are counterintuitively validated by two case studies: nuclear power plants as a fast deployment and smartphones as a slow deployment. These results are important for long-term industrial planning, as they enable us to anticipate future business cycles. Our study demonstrates that business cycles can originate endogenously from industrial dynamics of installation and renewal, contrasting with traditional views attributing fluctuations to exogenous macroeconomic factors. These endogenous cycles interact with broader trends, potentially being modulated, amplified, or attenuated by macroeconomic conditions. This dynamic of deployment and renewal is relevant for long-life infrastructure technologies, such as those supporting the renewable energy sector and has major policy implications for industry players.","Modeling Technological Deployment and Renewal: Monotonic vs. Oscillating Industrial Dynamics This study proposes a new model based on a classic S-curve that describes deployment and stabilization at maximum capacity. In addition, the model extends to the post-growth plateau, where technological capacity is renewed according to the distribution of equipment lifespans. We obtain two qualitatively different results. In the case of fast deployment, characterized by a short deployment time in relation to the average equipment lifetime, production is subject to significant oscillations. In the case of slow deployment, production increases monotonically until it reaches a renewal plateau. These results are counterintuitively validated by two case studies: nuclear power plants as a fast deployment and smartphones as a slow deployment. These results are important for long-term industrial planning, as they enable us to anticipate future business cycles. Our study demonstrates that business cycles can originate endogenously from industrial dynamics of installation and renewal, contrasting with traditional views attributing fluctuations to exogenous macroeconomic factors. These endogenous cycles interact with broader trends, potentially being modulated, amplified, or attenuated by macroeconomic conditions. This dynamic of deployment and renewal is relevant for long-life infrastructure technologies, such as those supporting the renewable energy sector and has major policy implications for industry players.",Environment
Smooth Value Functions for a Class of Nonsmooth Utility Maximization Problems,"In this paper we prove that there exists a smooth classical solution to the HJB equation for a large class of constrained problems with utility functions that are not necessarily differentiable or strictly concave. The value function is smooth if admissible controls satisfy an integrability condition or if it is continuous on the closure of its domain. The key idea is to work on the dual control problem and the dual HJB equation. We construct a smooth, strictly convex solution to the dual HJB equation and show that its conjugate function is a smooth, strictly concave solution to the primal HJB equation satisfying the terminal and boundary conditions.","Smooth Value Functions for a Class of Nonsmooth Utility Maximization Problems In this paper we prove that there exists a smooth classical solution to the HJB equation for a large class of constrained problems with utility functions that are not necessarily differentiable or strictly concave. The value function is smooth if admissible controls satisfy an integrability condition or if it is continuous on the closure of its domain. The key idea is to work on the dual control problem and the dual HJB equation. We construct a smooth, strictly convex solution to the dual HJB equation and show that its conjugate function is a smooth, strictly concave solution to the primal HJB equation satisfying the terminal and boundary conditions.",Finance
Systems-of-Systems for Environmental Sustainability: A Systematic Mapping Study,"Environmental sustainability in Systems-of-Systems (SoS) is an emerging field that seeks to integrate technological solutions to promote the efficient management of natural resources. While systematic reviews address sustainability in the context of Smart Cities (a category of SoS), a systematic study synthesizing the existing knowledge on environmental sustainability applied to SoS in general does not exist. Although literature includes other types of sustainability, such as financial and social, this study focuses on environmental sustainability, analyzing how SoS contribute to sustainable practices such as carbon emission reduction, energy efficiency, and biodiversity conservation. We conducted a Systematic Mapping Study to identify the application domains of SoS in sustainability, the challenges faced, and research opportunities. We planned and executed a research protocol including an automated search over four scientific databases. Of 926 studies retrieved, we selected, analyzed, and reported the results of 39 relevant studies. Our findings reveal that most studies focus on Smart Cities and Smart Grids, while applications such as sustainable agriculture and wildfire prevention are less explored. We identified challenges such as system interoperability, scalability, and data governance. Finally, we propose future research directions for SoS and environmental sustainability.","Systems-of-Systems for Environmental Sustainability: A Systematic Mapping Study Environmental sustainability in Systems-of-Systems (SoS) is an emerging field that seeks to integrate technological solutions to promote the efficient management of natural resources. While systematic reviews address sustainability in the context of Smart Cities (a category of SoS), a systematic study synthesizing the existing knowledge on environmental sustainability applied to SoS in general does not exist. Although literature includes other types of sustainability, such as financial and social, this study focuses on environmental sustainability, analyzing how SoS contribute to sustainable practices such as carbon emission reduction, energy efficiency, and biodiversity conservation. We conducted a Systematic Mapping Study to identify the application domains of SoS in sustainability, the challenges faced, and research opportunities. We planned and executed a research protocol including an automated search over four scientific databases. Of 926 studies retrieved, we selected, analyzed, and reported the results of 39 relevant studies. Our findings reveal that most studies focus on Smart Cities and Smart Grids, while applications such as sustainable agriculture and wildfire prevention are less explored. We identified challenges such as system interoperability, scalability, and data governance. Finally, we propose future research directions for SoS and environmental sustainability.",Environment
Teaching Network Storage Technology Assessment Outcomes and Directions,"The paper presents academic content, delivery and assessment mechanisms used, available resources including initial lessons from teaching Networked Storage Technology as a special topics course to students enrolled in two specific programs - IT and CS. The course is based on the EMC s vendor-neutral Storage Technology Fundamentals course. Furthermore, this manuscript provides a detailed review of how the course fits into our curriculum, particularly, how it helps achieving the 2008 ABET assessment requirements.","Teaching Network Storage Technology Assessment Outcomes and Directions The paper presents academic content, delivery and assessment mechanisms used, available resources including initial lessons from teaching Networked Storage Technology as a special topics course to students enrolled in two specific programs - IT and CS. The course is based on the EMC s vendor-neutral Storage Technology Fundamentals course. Furthermore, this manuscript provides a detailed review of how the course fits into our curriculum, particularly, how it helps achieving the 2008 ABET assessment requirements.",Education
Load and Renewable Energy Forecasting Using Deep Learning for Grid Stability,"As the energy landscape changes quickly, grid operators face several challenges, especially when integrating renewable energy sources with the grid. The most important challenge is to balance supply and demand because the solar and wind energy are highly unpredictable. When dealing with such uncertainty, trustworthy short-term load and renewable energy forecasting can help stabilize the grid, maximize energy storage, and guarantee the effective use of renewable resources. Physical models and statistical techniques were the previous approaches employed for this kind of forecasting tasks. In forecasting renewable energy, machine learning and deep learning techniques have recently demonstrated encouraging results. More specifically, the deep learning techniques like CNN and LSTM and the conventional machine learning techniques like regression that are mostly utilized for load and renewable energy forecasting tasks. In this article, we will focus mainly on CNN and LSTM-based forecasting methods.","Load and Renewable Energy Forecasting Using Deep Learning for Grid Stability As the energy landscape changes quickly, grid operators face several challenges, especially when integrating renewable energy sources with the grid. The most important challenge is to balance supply and demand because the solar and wind energy are highly unpredictable. When dealing with such uncertainty, trustworthy short-term load and renewable energy forecasting can help stabilize the grid, maximize energy storage, and guarantee the effective use of renewable resources. Physical models and statistical techniques were the previous approaches employed for this kind of forecasting tasks. In forecasting renewable energy, machine learning and deep learning techniques have recently demonstrated encouraging results. More specifically, the deep learning techniques like CNN and LSTM and the conventional machine learning techniques like regression that are mostly utilized for load and renewable energy forecasting tasks. In this article, we will focus mainly on CNN and LSTM-based forecasting methods.",Environment
Development of a Multiphoton Fluorescence Lifetime Imaging Microscopy (FLIM) system using a Streak Camera,We report the development and detailed calibration of a multiphoton fluorescence lifetime imaging system (FLIM) using a streak camera. The present system is versatile with high spatial (0.2 micron) and temporal (50 psec) resolution and allows rapid data acquisition and reliable and reproducible lifetime determinations. The system was calibrated with standard fluorescent dyes and the lifetime values obtained were in very good agreement with values reported in literature for these dyes. We also demonstrate the applicability of the system to FLIM studies in cellular specimens including stained pollen grains and fibroblast cells expressing green fluorescent protein. The lifetime values obtained matched well with those reported earlier by other groups for these same specimens. Potential applications of the present system include the measurement of intracellular physiology and Fluorescence Resonance Energy Transfer (FRET) imaging which are discussed in the context of live cell imaging.,Development of a Multiphoton Fluorescence Lifetime Imaging Microscopy (FLIM) system using a Streak Camera We report the development and detailed calibration of a multiphoton fluorescence lifetime imaging system (FLIM) using a streak camera. The present system is versatile with high spatial (0.2 micron) and temporal (50 psec) resolution and allows rapid data acquisition and reliable and reproducible lifetime determinations. The system was calibrated with standard fluorescent dyes and the lifetime values obtained were in very good agreement with values reported in literature for these dyes. We also demonstrate the applicability of the system to FLIM studies in cellular specimens including stained pollen grains and fibroblast cells expressing green fluorescent protein. The lifetime values obtained matched well with those reported earlier by other groups for these same specimens. Potential applications of the present system include the measurement of intracellular physiology and Fluorescence Resonance Energy Transfer (FRET) imaging which are discussed in the context of live cell imaging.,Healthcare
Analysis of Climatic Trends and Variability in Indian Topography,"The climatic change is one of the serious concerns nowadays. The impacts of climate change are global in scope and unprecedented in scale. Moreover, a small perturbation in climatic changes affects not only the pristine ecosystem but also the socioeconomic sectors. Specifically, the affect of climatic changes is related to frequent casualties. This makes it essential to dwelve deeper into analyzing the socio-climatic trends and variability. This work provides a comprehensive analysis of Indias climatic trends, emphasizing on regional variations and specifically delving into the unique climate of Delhi. Specifically, this research unveils the temporal and spatial variations in temperature patterns by amalgamating extensive datasets encompassing Indias diverse landscapes. The study uses advanced statistical tools and methodologies to scrutinize temperatures annual and seasonal variability. The insights drawn from this rigorous analysis may offer invaluable contributions to regional planning strategies, adaptive measures, and informed decision-making amidst the complex impacts of climate change. By bridging the gap between broader climatic trends and localized impacts, this research aims to facilitate more effective measures to mitigate and adapt to the multifaceted challenges of climate change, ensuring a more nuanced and tailored approaches. We utilized the Mann-Kendall test and Theil-Sens slope estimator to analyze the trends and variability of the climatic conditions over the decades. The results demonstrate that temperature variations have increased over 0.58oC on average over the last decade. Moreover, over last decade the variability of Indian states shows that Lakshadweep faced the highest change (0.87oC), highlighting coastal vulnerability, while Tripura observed the least change of 0.07oC.","Analysis of Climatic Trends and Variability in Indian Topography The climatic change is one of the serious concerns nowadays. The impacts of climate change are global in scope and unprecedented in scale. Moreover, a small perturbation in climatic changes affects not only the pristine ecosystem but also the socioeconomic sectors. Specifically, the affect of climatic changes is related to frequent casualties. This makes it essential to dwelve deeper into analyzing the socio-climatic trends and variability. This work provides a comprehensive analysis of Indias climatic trends, emphasizing on regional variations and specifically delving into the unique climate of Delhi. Specifically, this research unveils the temporal and spatial variations in temperature patterns by amalgamating extensive datasets encompassing Indias diverse landscapes. The study uses advanced statistical tools and methodologies to scrutinize temperatures annual and seasonal variability. The insights drawn from this rigorous analysis may offer invaluable contributions to regional planning strategies, adaptive measures, and informed decision-making amidst the complex impacts of climate change. By bridging the gap between broader climatic trends and localized impacts, this research aims to facilitate more effective measures to mitigate and adapt to the multifaceted challenges of climate change, ensuring a more nuanced and tailored approaches. We utilized the Mann-Kendall test and Theil-Sens slope estimator to analyze the trends and variability of the climatic conditions over the decades. The results demonstrate that temperature variations have increased over 0.58oC on average over the last decade. Moreover, over last decade the variability of Indian states shows that Lakshadweep faced the highest change (0.87oC), highlighting coastal vulnerability, while Tripura observed the least change of 0.07oC.",Environment
A Taylor series approach to pricing and implied vol for LSV models,"Using classical Taylor series techniques, we develop a unified approach to pricing and implied volatility for European-style options in a general local-stochastic volatility setting. Our price approximations require only a normal CDF and our implied volatility approximations are fully explicit (ie, they require no special functions, no infinite series and no numerical integration). As such, approximate prices can be computed as efficiently as Black-Scholes prices, and approximate implied volatilities can be computed nearly instantaneously.","A Taylor series approach to pricing and implied vol for LSV models Using classical Taylor series techniques, we develop a unified approach to pricing and implied volatility for European-style options in a general local-stochastic volatility setting. Our price approximations require only a normal CDF and our implied volatility approximations are fully explicit (ie, they require no special functions, no infinite series and no numerical integration). As such, approximate prices can be computed as efficiently as Black-Scholes prices, and approximate implied volatilities can be computed nearly instantaneously.",Finance
Efficient algorithms for decision tree cross-validation,"Cross-validation is a useful and generally applicable technique often employed in machine learning, including decision tree induction. An important disadvantage of straightforward implementation of the technique is its computational overhead. In this paper we show that, for decision trees, the computational overhead of cross-validation can be reduced significantly by integrating the cross-validation with the normal decision tree induction process. We discuss how existing decision tree algorithms can be adapted to this aim, and provide an analysis of the speedups these adaptations may yield. The analysis is supported by experimental results.","Efficient algorithms for decision tree cross-validation Cross-validation is a useful and generally applicable technique often employed in machine learning, including decision tree induction. An important disadvantage of straightforward implementation of the technique is its computational overhead. In this paper we show that, for decision trees, the computational overhead of cross-validation can be reduced significantly by integrating the cross-validation with the normal decision tree induction process. We discuss how existing decision tree algorithms can be adapted to this aim, and provide an analysis of the speedups these adaptations may yield. The analysis is supported by experimental results.",Technology
"Genetic Programming, Validation Sets, and Parsimony Pressure","Fitness functions based on test cases are very common in Genetic Programming (GP). This process can be assimilated to a learning task, with the inference of models from a limited number of samples. This paper is an investigation on two methods to improve generalization in GP-based learning: 1) the selection of the best-of-run individuals using a three data sets methodology, and 2) the application of parsimony pressure in order to reduce the complexity of the solutions. Results using GP in a binary classification setup show that while the accuracy on the test sets is preserved, with less variances compared to baseline results, the mean tree size obtained with the tested methods is significantly reduced.","Genetic Programming, Validation Sets, and Parsimony Pressure Fitness functions based on test cases are very common in Genetic Programming (GP). This process can be assimilated to a learning task, with the inference of models from a limited number of samples. This paper is an investigation on two methods to improve generalization in GP-based learning: 1) the selection of the best-of-run individuals using a three data sets methodology, and 2) the application of parsimony pressure in order to reduce the complexity of the solutions. Results using GP in a binary classification setup show that while the accuracy on the test sets is preserved, with less variances compared to baseline results, the mean tree size obtained with the tested methods is significantly reduced.",Technology
Critical pedagogy in the implementation of educational technologies,"This paper presents a critical review of the challenges to the implementation of learning technologies with particular focus on developing countries. A comprehensive literature review on learning technologies was undertaken for the purpose of understanding the challenges in developing countries. The research question is: what extent does education empower learners to be full participants in a socially democratic society? The literature review identified 25 papers relevant to this topic. Challenges are interrelated and to bring about changes in developing countries, this paper proposes two educational technology frameworks based on: 1. cultural conceptual framework, and 2. problem-based constructivist psychology simulation model. The framework and simulation model are both useful to guide practice and research.","Critical pedagogy in the implementation of educational technologies This paper presents a critical review of the challenges to the implementation of learning technologies with particular focus on developing countries. A comprehensive literature review on learning technologies was undertaken for the purpose of understanding the challenges in developing countries. The research question is: what extent does education empower learners to be full participants in a socially democratic society? The literature review identified 25 papers relevant to this topic. Challenges are interrelated and to bring about changes in developing countries, this paper proposes two educational technology frameworks based on: 1. cultural conceptual framework, and 2. problem-based constructivist psychology simulation model. The framework and simulation model are both useful to guide practice and research.",Education
Using Qualitative Hypotheses to Identify Inaccurate Data,"Identifying inaccurate data has long been regarded as a significant and difficult problem in AI. In this paper, we present a new method for identifying inaccurate data on the basis of qualitative correlations among related data. First, we introduce the definitions of related data and qualitative correlations among related data. Then we put forward a new concept called support coefficient function (SCF). SCF can be used to extract, represent, and calculate qualitative correlations among related data within a dataset. We propose an approach to determining dynamic shift intervals of inaccurate data, and an approach to calculating possibility of identifying inaccurate data, respectively. Both of the approaches are based on SCF. Finally we present an algorithm for identifying inaccurate data by using qualitative correlations among related data as confirmatory or disconfirmatory evidence. We have developed a practical system for interpreting infrared spectra by applying the method, and have fully tested the system against several hundred real spectra. The experimental results show that the method is significantly better than the conventional methods used in many similar systems.","Using Qualitative Hypotheses to Identify Inaccurate Data Identifying inaccurate data has long been regarded as a significant and difficult problem in AI. In this paper, we present a new method for identifying inaccurate data on the basis of qualitative correlations among related data. First, we introduce the definitions of related data and qualitative correlations among related data. Then we put forward a new concept called support coefficient function (SCF). SCF can be used to extract, represent, and calculate qualitative correlations among related data within a dataset. We propose an approach to determining dynamic shift intervals of inaccurate data, and an approach to calculating possibility of identifying inaccurate data, respectively. Both of the approaches are based on SCF. Finally we present an algorithm for identifying inaccurate data by using qualitative correlations among related data as confirmatory or disconfirmatory evidence. We have developed a practical system for interpreting infrared spectra by applying the method, and have fully tested the system against several hundred real spectra. The experimental results show that the method is significantly better than the conventional methods used in many similar systems.",Technology
Digital History and History Teaching in the Digital Age,"Digital technologies, such as the Internet and Artificial Intelligence, are part of our daily lives, influencing broader aspects of our way of life, as well as the way we interact with the past. Having dramatically changed the ways in which knowledge is produced and consumed, the algorithmic age has also radically changed the relationship that the general public has with History. Fields of History such as Public and Oral History have particularly benefitted from the rise of digital culture. How does our digital culture affect the way we think, study, research and teach the past, as historical evidence spreads rapidly in the public sphere? How do digital technologies promote the study, writing and teaching of History? What should historians, students of history and pre-service history teachers be critically aware of, when swarmed with digitized or born-digital content, constantly growing on the Internet? And while these changes are now visible globally, how is the discipline of History situated within the digital transformation rapidly advancing in Greece? Finally, what are the consequences of these changes for History as a subject taught at Greek secondary schools? These are some of the issues raised in the text that follows, which is part of the course materials of the undergraduate course offered during winter semester 2020-2021 at the School University of Athens, School of Philosophy, Pedagogy, Psychology. Course Title: Pedagogics of History: Theory and Practice, Academic Institution: School of Philosophy-Pedagogy-Psychology, University of Athens.","Digital History and History Teaching in the Digital Age Digital technologies, such as the Internet and Artificial Intelligence, are part of our daily lives, influencing broader aspects of our way of life, as well as the way we interact with the past. Having dramatically changed the ways in which knowledge is produced and consumed, the algorithmic age has also radically changed the relationship that the general public has with History. Fields of History such as Public and Oral History have particularly benefitted from the rise of digital culture. How does our digital culture affect the way we think, study, research and teach the past, as historical evidence spreads rapidly in the public sphere? How do digital technologies promote the study, writing and teaching of History? What should historians, students of history and pre-service history teachers be critically aware of, when swarmed with digitized or born-digital content, constantly growing on the Internet? And while these changes are now visible globally, how is the discipline of History situated within the digital transformation rapidly advancing in Greece? Finally, what are the consequences of these changes for History as a subject taught at Greek secondary schools? These are some of the issues raised in the text that follows, which is part of the course materials of the undergraduate course offered during winter semester 2020-2021 at the School University of Athens, School of Philosophy, Pedagogy, Psychology. Course Title: Pedagogics of History: Theory and Practice, Academic Institution: School of Philosophy-Pedagogy-Psychology, University of Athens.",Education
Using marginal structural models to adjust for treatment drop-in when developing clinical prediction models,"Objectives: Clinical prediction models (CPMs) can inform decision-making concerning treatment initiation. Here, one requires predicted risks assuming that no treatment is given. This is challenging since CPMs are often derived in datasets where patients receive treatment; moreover, treatment can commence post-baseline - treatment drop-ins. This study presents a novel approach of using marginal structural models (MSMs) to adjust for treatment drop-in. Study Design and Setting: We illustrate the use of MSMs in the CPM framework through simulation studies, representing randomised controlled trials and observational data. The simulations include a binary treatment and a covariate, each recorded at two timepoints and having a prognostic effect on a binary outcome. The bias in predicted risk was examined in a model ignoring treatment, a model fitted on treatment naive patients (at baseline), a model including baseline treatment, and the MSM. Results: In all simulation scenarios, all models except the MSM under-estimated the risk of outcome given absence of treatment. Consequently, CPMs that do not acknowledge treatment drop-in can lead to under-allocation of treatment. Conclusion: When developing CPMs to predict treatment-naive risk, authors should consider using MSMs to adjust for treatment drop-in. MSMs also allow estimation of individual treatment effects.","Using marginal structural models to adjust for treatment drop-in when developing clinical prediction models Objectives: Clinical prediction models (CPMs) can inform decision-making concerning treatment initiation. Here, one requires predicted risks assuming that no treatment is given. This is challenging since CPMs are often derived in datasets where patients receive treatment; moreover, treatment can commence post-baseline - treatment drop-ins. This study presents a novel approach of using marginal structural models (MSMs) to adjust for treatment drop-in. Study Design and Setting: We illustrate the use of MSMs in the CPM framework through simulation studies, representing randomised controlled trials and observational data. The simulations include a binary treatment and a covariate, each recorded at two timepoints and having a prognostic effect on a binary outcome. The bias in predicted risk was examined in a model ignoring treatment, a model fitted on treatment naive patients (at baseline), a model including baseline treatment, and the MSM. Results: In all simulation scenarios, all models except the MSM under-estimated the risk of outcome given absence of treatment. Consequently, CPMs that do not acknowledge treatment drop-in can lead to under-allocation of treatment. Conclusion: When developing CPMs to predict treatment-naive risk, authors should consider using MSMs to adjust for treatment drop-in. MSMs also allow estimation of individual treatment effects.",Healthcare
A Nonparametric Method for Value Function Guided Subgroup Identification via Gradient Tree Boosting for Censored Survival Data,"In randomized clinical trials with survival outcome, there has been an increasing interest in subgroup identification based on baseline genomic, proteomic markers or clinical characteristics. Some of the existing methods identify subgroups that benefit substantially from the experimental treatment by directly modeling outcomes or treatment effect. When the goal is to find an optimal treatment for a given patient rather than finding the right patient for a given treatment, methods under the individualized treatment regime framework estimate an individualized treatment rule that would lead to the best expected clinical outcome as measured by a value function. Connecting the concept of value function to subgroup identification, we propose a nonparametric method that searches for subgroup membership scores by maximizing a value function that directly reflects the subgroup-treatment interaction effect based on restricted mean survival time. A gradient tree boosting algorithm is proposed to search for the individual subgroup membership scores. We conduct simulation studies to evaluate the performance of the proposed method and an application to an AIDS clinical trial is performed for illustration.","A Nonparametric Method for Value Function Guided Subgroup Identification via Gradient Tree Boosting for Censored Survival Data In randomized clinical trials with survival outcome, there has been an increasing interest in subgroup identification based on baseline genomic, proteomic markers or clinical characteristics. Some of the existing methods identify subgroups that benefit substantially from the experimental treatment by directly modeling outcomes or treatment effect. When the goal is to find an optimal treatment for a given patient rather than finding the right patient for a given treatment, methods under the individualized treatment regime framework estimate an individualized treatment rule that would lead to the best expected clinical outcome as measured by a value function. Connecting the concept of value function to subgroup identification, we propose a nonparametric method that searches for subgroup membership scores by maximizing a value function that directly reflects the subgroup-treatment interaction effect based on restricted mean survival time. A gradient tree boosting algorithm is proposed to search for the individual subgroup membership scores. We conduct simulation studies to evaluate the performance of the proposed method and an application to an AIDS clinical trial is performed for illustration.",Healthcare
A Study of Teacher Educators Skill and ICT Integration in Online Teaching during the Pandemic Situation in India,Information and communication technology prompted the sharing of information over the world. For its impact on education the government and the authorities like the University Grants Commission in India have energized the higher education institutions in India to implement online education during the pandemic situation. This paper attempts to know the teaching faculties ICT skills and related online class skills in higher educational institutions in India. In India like developing countries quick as the lightning change in traditional to fully online classes are like a rumble of thunder because faculties are adopting this situation but students are challenging to adopt.,A Study of Teacher Educators Skill and ICT Integration in Online Teaching during the Pandemic Situation in India Information and communication technology prompted the sharing of information over the world. For its impact on education the government and the authorities like the University Grants Commission in India have energized the higher education institutions in India to implement online education during the pandemic situation. This paper attempts to know the teaching faculties ICT skills and related online class skills in higher educational institutions in India. In India like developing countries quick as the lightning change in traditional to fully online classes are like a rumble of thunder because faculties are adopting this situation but students are challenging to adopt.,Education
Pricing TARN Using a Finite Difference Method,"Typically options with a path dependent payoff, such as Target Accumulation Redemption Note (TARN), are evaluated by a Monte Carlo method. This paper describes a finite difference scheme for pricing a TARN option. Key steps in the proposed scheme involve tracking of multiple one-dimensional finite difference solutions, application of jump conditions at each cash flow exchange date, and a cubic spline interpolation of results after each jump. Since a finite difference scheme for TARN has significantly different features from a typical finite difference scheme for options with a path independent payoff, we give a step by step description on the implementation of the scheme, which is not available in the literature. The advantages of the proposed finite difference scheme over the Monte Carlo method are illustrated by examples with three different knockout types. In the case of constant or time dependent volatility models (where Monte Carlo requires simulation at cash flow dates only), the finite difference method can be faster by an order of magnitude than the Monte Carlo method to achieve the same accuracy in price. Finite difference method can be even more efficient in comparison with Monte Carlo in the case of local volatility model where Monte Carlo requires significantly larger number of time steps. In terms of robust and accurate estimation of Greeks, the advantage of the finite difference method will be even more pronounced.","Pricing TARN Using a Finite Difference Method Typically options with a path dependent payoff, such as Target Accumulation Redemption Note (TARN), are evaluated by a Monte Carlo method. This paper describes a finite difference scheme for pricing a TARN option. Key steps in the proposed scheme involve tracking of multiple one-dimensional finite difference solutions, application of jump conditions at each cash flow exchange date, and a cubic spline interpolation of results after each jump. Since a finite difference scheme for TARN has significantly different features from a typical finite difference scheme for options with a path independent payoff, we give a step by step description on the implementation of the scheme, which is not available in the literature. The advantages of the proposed finite difference scheme over the Monte Carlo method are illustrated by examples with three different knockout types. In the case of constant or time dependent volatility models (where Monte Carlo requires simulation at cash flow dates only), the finite difference method can be faster by an order of magnitude than the Monte Carlo method to achieve the same accuracy in price. Finite difference method can be even more efficient in comparison with Monte Carlo in the case of local volatility model where Monte Carlo requires significantly larger number of time steps. In terms of robust and accurate estimation of Greeks, the advantage of the finite difference method will be even more pronounced.",Finance
Understanding and improving social factors in education: a computational social science approach,"Over the past decade, an explosion in the availability of education-related datasets has enabled new computational research in education. Much of this work has investigated digital traces of online learners in order to better understand and optimize their cognitive learning processes. Yet cognitive learning on digital platforms does not equal education. Instead, education is an inherently social, cultural, economic, and political process manifesting in physical spaces, and educational outcomes are influenced by many factors that precede and shape the cognitive learning process. Many of these are social factors like childrens connections to schools (including teachers, counselors, and role models), parents and families, and the broader neighborhoods in which they live. In this article, we briefly discuss recent studies of learning through large-scale digital platforms, but largely focus on those exploring sociological aspects of education. We believe computational social scientists can creatively advance this emerging research frontier-and in doing so, help facilitate more equitable educational and life outcomes.","Understanding and improving social factors in education: a computational social science approach Over the past decade, an explosion in the availability of education-related datasets has enabled new computational research in education. Much of this work has investigated digital traces of online learners in order to better understand and optimize their cognitive learning processes. Yet cognitive learning on digital platforms does not equal education. Instead, education is an inherently social, cultural, economic, and political process manifesting in physical spaces, and educational outcomes are influenced by many factors that precede and shape the cognitive learning process. Many of these are social factors like childrens connections to schools (including teachers, counselors, and role models), parents and families, and the broader neighborhoods in which they live. In this article, we briefly discuss recent studies of learning through large-scale digital platforms, but largely focus on those exploring sociological aspects of education. We believe computational social scientists can creatively advance this emerging research frontier-and in doing so, help facilitate more equitable educational and life outcomes.",Education
About Unitary Rating Score Constructing,"It is offered to pool test points of different subjects and different aspects of the same subject together in order to get the unitary rating score, by the way of nonlinear transformation of indicator points in accordance with Zipfs distribution. It is proposed to use the well-studied distribution of Intellectuality Quotient IQ as the reference distribution for latent variable progress in studies.","About Unitary Rating Score Constructing It is offered to pool test points of different subjects and different aspects of the same subject together in order to get the unitary rating score, by the way of nonlinear transformation of indicator points in accordance with Zipfs distribution. It is proposed to use the well-studied distribution of Intellectuality Quotient IQ as the reference distribution for latent variable progress in studies.",Technology
On the Job Training,"We propose a new framework for building and evaluating machine learning algorithms. We argue that many real-world problems require an agent which must quickly learn to respond to demands, yet can continue to perform and respond to new training throughout its useful life. We give a framework for how such agents can be built, describe several metrics for evaluating them, and show that subtle changes in system construction can significantly affect agent performance.","On the Job Training We propose a new framework for building and evaluating machine learning algorithms. We argue that many real-world problems require an agent which must quickly learn to respond to demands, yet can continue to perform and respond to new training throughout its useful life. We give a framework for how such agents can be built, describe several metrics for evaluating them, and show that subtle changes in system construction can significantly affect agent performance.",Technology
Preliminaries to an investigation of reduced product set finance,"Principles of financial product synthesis from a few basic financial products constitute an interesting research topic inspired by Islamic finance. We make an effort to answer general questions that should be answered before starting to investigate the main issues concerning this topic with the formalization of financial products and principles of financial product synthesis. We also outline the outcome of some preparatory explorations, which have been conducted with the purpose to form a reasonable preliminary picture of the details of financial products that are relevant to the study of the principles of financial product synthesis.","Preliminaries to an investigation of reduced product set finance Principles of financial product synthesis from a few basic financial products constitute an interesting research topic inspired by Islamic finance. We make an effort to answer general questions that should be answered before starting to investigate the main issues concerning this topic with the formalization of financial products and principles of financial product synthesis. We also outline the outcome of some preparatory explorations, which have been conducted with the purpose to form a reasonable preliminary picture of the details of financial products that are relevant to the study of the principles of financial product synthesis.",Finance
Defensive forecasting for linear protocols,"We consider a general class of forecasting protocols, called linear protocols, and discuss several important special cases, including multi-class forecasting. Forecasting is formalized as a game between three players: Reality, whose role is to generate observations; Forecaster, whose goal is to predict the observations; and Skeptic, who tries to make money on any lack of agreement between Forecasters predictions and the actual observations. Our main mathematical result is that for any continuous strategy for Skeptic in a linear protocol there exists a strategy for Forecaster that does not allow Skeptics capital to grow. This result is a meta-theorem that allows one to transform any continuous law of probability in a linear protocol into a forecasting strategy whose predictions are guaranteed to satisfy this law. We apply this meta-theorem to a weak law of large numbers in Hilbert spaces to obtain a version of the K29 prediction algorithm for linear protocols and show that this version also satisfies the attractive properties of proper calibration and resolution under a suitable choice of its kernel parameter, with no assumptions about the way the data is generated.","Defensive forecasting for linear protocols We consider a general class of forecasting protocols, called linear protocols, and discuss several important special cases, including multi-class forecasting. Forecasting is formalized as a game between three players: Reality, whose role is to generate observations; Forecaster, whose goal is to predict the observations; and Skeptic, who tries to make money on any lack of agreement between Forecasters predictions and the actual observations. Our main mathematical result is that for any continuous strategy for Skeptic in a linear protocol there exists a strategy for Forecaster that does not allow Skeptics capital to grow. This result is a meta-theorem that allows one to transform any continuous law of probability in a linear protocol into a forecasting strategy whose predictions are guaranteed to satisfy this law. We apply this meta-theorem to a weak law of large numbers in Hilbert spaces to obtain a version of the K29 prediction algorithm for linear protocols and show that this version also satisfies the attractive properties of proper calibration and resolution under a suitable choice of its kernel parameter, with no assumptions about the way the data is generated.",Technology
Towards Regulatory-Confirmed Adaptive Clinical Trials: Machine Learning Opportunities and Solutions,"Randomized Controlled Trials (RCTs) are the gold standard for evaluating the effect of new medical treatments. Treatments must pass stringent regulatory conditions in order to be approved for widespread use, yet even after the regulatory barriers are crossed, real-world challenges might arise: Who should get the treatment? What is its true clinical utility? Are there discrepancies in the treatment effectiveness across diverse and under-served populations? We introduce two new objectives for future clinical trials that integrate regulatory constraints and treatment policy value for both the entire population and under-served populations, thus answering some of the questions above in advance. Designed to meet these objectives, we formulate Randomize First Augment Next (RFAN), a new framework for designing Phase III clinical trials. Our framework consists of a standard randomized component followed by an adaptive one, jointly meant to efficiently and safely acquire and assign patients into treatment arms during the trial. Then, we propose strategies for implementing RFAN based on causal, deep Bayesian active learning. Finally, we empirically evaluate the performance of our framework using synthetic and real-world semi-synthetic datasets.","Towards Regulatory-Confirmed Adaptive Clinical Trials: Machine Learning Opportunities and Solutions Randomized Controlled Trials (RCTs) are the gold standard for evaluating the effect of new medical treatments. Treatments must pass stringent regulatory conditions in order to be approved for widespread use, yet even after the regulatory barriers are crossed, real-world challenges might arise: Who should get the treatment? What is its true clinical utility? Are there discrepancies in the treatment effectiveness across diverse and under-served populations? We introduce two new objectives for future clinical trials that integrate regulatory constraints and treatment policy value for both the entire population and under-served populations, thus answering some of the questions above in advance. Designed to meet these objectives, we formulate Randomize First Augment Next (RFAN), a new framework for designing Phase III clinical trials. Our framework consists of a standard randomized component followed by an adaptive one, jointly meant to efficiently and safely acquire and assign patients into treatment arms during the trial. Then, we propose strategies for implementing RFAN based on causal, deep Bayesian active learning. Finally, we empirically evaluate the performance of our framework using synthetic and real-world semi-synthetic datasets.",Healthcare
A Review of Data Mining in Personalized Education: Current Trends and Future Prospects,"Personalized education, tailored to individual student needs, leverages educational technology and artificial intelligence (AI) in the digital age to enhance learning effectiveness. The integration of AI in educational platforms provides insights into academic performance, learning preferences, and behaviors, optimizing the personal learning process. Driven by data mining techniques, it not only benefits students but also provides educators and institutions with tools to craft customized learning experiences. To offer a comprehensive review of recent advancements in personalized educational data mining, this paper focuses on four primary scenarios: educational recommendation, cognitive diagnosis, knowledge tracing, and learning analysis. This paper presents a structured taxonomy for each area, compiles commonly used datasets, and identifies future research directions, emphasizing the role of data mining in enhancing personalized education and paving the way for future exploration and innovation.","A Review of Data Mining in Personalized Education: Current Trends and Future Prospects Personalized education, tailored to individual student needs, leverages educational technology and artificial intelligence (AI) in the digital age to enhance learning effectiveness. The integration of AI in educational platforms provides insights into academic performance, learning preferences, and behaviors, optimizing the personal learning process. Driven by data mining techniques, it not only benefits students but also provides educators and institutions with tools to craft customized learning experiences. To offer a comprehensive review of recent advancements in personalized educational data mining, this paper focuses on four primary scenarios: educational recommendation, cognitive diagnosis, knowledge tracing, and learning analysis. This paper presents a structured taxonomy for each area, compiles commonly used datasets, and identifies future research directions, emphasizing the role of data mining in enhancing personalized education and paving the way for future exploration and innovation.",Education
A Two-Stage Patient-Focused Study Design for Rare Disease Controlled Trials,"We developed a study design for rare disease clinical trials (RDTs) that efficiently evaluate treatments, promotes access to new treatments during treatment development, and optimizes healthcare resource utilization for future treatment allocation, development, and prioritization. Comprehensive literature review and focus group discussion were conducted. To address the multifaceted challenges facing RDTs, four key issues for RDTs must be addressed, which are 1) the opportunity to access the new treatment; 2) assessment of outcomes where clinically validated outcomes may be lacking; 3) patient heterogeneity; and 4) duration of the study and number of patients required. Our proposed study design has two stages. Stage 1 distinguishes patients who respond to the treatment from those who do not respond to the treatment after assigning them all to the experimental treatment. Stage 2 evaluates the treatment effect comparatively among patients responded in Stage 1. In addition to treatment effect evaluation, our design can greatly benefit rare disease patients and clinical practice by increasing opportunities to access experimental treatments and by providing relevant information that can be used for tailoring treatments to certain subgroups, aiding future research in treatment development, and improving healthcare resource utilization.","A Two-Stage Patient-Focused Study Design for Rare Disease Controlled Trials We developed a study design for rare disease clinical trials (RDTs) that efficiently evaluate treatments, promotes access to new treatments during treatment development, and optimizes healthcare resource utilization for future treatment allocation, development, and prioritization. Comprehensive literature review and focus group discussion were conducted. To address the multifaceted challenges facing RDTs, four key issues for RDTs must be addressed, which are 1) the opportunity to access the new treatment; 2) assessment of outcomes where clinically validated outcomes may be lacking; 3) patient heterogeneity; and 4) duration of the study and number of patients required. Our proposed study design has two stages. Stage 1 distinguishes patients who respond to the treatment from those who do not respond to the treatment after assigning them all to the experimental treatment. Stage 2 evaluates the treatment effect comparatively among patients responded in Stage 1. In addition to treatment effect evaluation, our design can greatly benefit rare disease patients and clinical practice by increasing opportunities to access experimental treatments and by providing relevant information that can be used for tailoring treatments to certain subgroups, aiding future research in treatment development, and improving healthcare resource utilization.",Healthcare
CLoE: Curriculum Learning on Endoscopic Images for Robust MES Classification,"Estimating disease severity from endoscopic images is essential in assessing ulcerative colitis, where the Mayo Endoscopic Subscore (MES) is widely used to grade inflammation. However, MES classification remains challenging due to label noise from inter-observer variability and the ordinal nature of the score, which standard models often ignore. We propose CLoE, a curriculum learning framework that accounts for both label reliability and ordinal structure. Image quality, estimated via a lightweight model trained on Boston Bowel Preparation Scale (BBPS) labels, is used as a proxy for annotation confidence to order samples from easy (clean) to hard (noisy). This curriculum is further combined with ResizeMix augmentation to improve robustness. Experiments on the LIMUC and HyperKvasir datasets, using both CNNs and Transformers, show that CLoE consistently improves performance over strong supervised and self-supervised baselines. For instance, ConvNeXt-Tiny reaches 82.5 accuracy and a QWK of 0.894 on LIMUC with low computational cost. These results highlight the potential of difficulty-aware training strategies for improving ordinal classification under label uncertainty. Code will be released at https:github.comzeynepozdemirCLoE.","CLoE: Curriculum Learning on Endoscopic Images for Robust MES Classification Estimating disease severity from endoscopic images is essential in assessing ulcerative colitis, where the Mayo Endoscopic Subscore (MES) is widely used to grade inflammation. However, MES classification remains challenging due to label noise from inter-observer variability and the ordinal nature of the score, which standard models often ignore. We propose CLoE, a curriculum learning framework that accounts for both label reliability and ordinal structure. Image quality, estimated via a lightweight model trained on Boston Bowel Preparation Scale (BBPS) labels, is used as a proxy for annotation confidence to order samples from easy (clean) to hard (noisy). This curriculum is further combined with ResizeMix augmentation to improve robustness. Experiments on the LIMUC and HyperKvasir datasets, using both CNNs and Transformers, show that CLoE consistently improves performance over strong supervised and self-supervised baselines. For instance, ConvNeXt-Tiny reaches 82.5 accuracy and a QWK of 0.894 on LIMUC with low computational cost. These results highlight the potential of difficulty-aware training strategies for improving ordinal classification under label uncertainty. Code will be released at https:github.comzeynepozdemirCLoE.",Education
A SentiWordNet Strategy for Curriculum Learning in Sentiment Analysis,"Curriculum Learning (CL) is the idea that learning on a training set sequenced or ordered in a manner where samples range from easy to difficult, results in an increment in performance over otherwise random ordering. The idea parallels cognitive sciences theory of how human brains learn, and that learning a difficult task can be made easier by phrasing it as a sequence of easy to difficult tasks. This idea has gained a lot of traction in machine learning and image processing for a while and recently in Natural Language Processing (NLP). In this paper, we apply the ideas of curriculum learning, driven by SentiWordNet in a sentiment analysis setting. In this setting, given a text segment, our aim is to extract its sentiment or polarity. SentiWordNet is a lexical resource with sentiment polarity annotations. By comparing performance with other curriculum strategies and with no curriculum, the effectiveness of the proposed strategy is presented. Convolutional, Recurrence, and Attention-based architectures are employed to assess this improvement. The models are evaluated on a standard sentiment dataset, Stanford Sentiment Treebank.","A SentiWordNet Strategy for Curriculum Learning in Sentiment Analysis Curriculum Learning (CL) is the idea that learning on a training set sequenced or ordered in a manner where samples range from easy to difficult, results in an increment in performance over otherwise random ordering. The idea parallels cognitive sciences theory of how human brains learn, and that learning a difficult task can be made easier by phrasing it as a sequence of easy to difficult tasks. This idea has gained a lot of traction in machine learning and image processing for a while and recently in Natural Language Processing (NLP). In this paper, we apply the ideas of curriculum learning, driven by SentiWordNet in a sentiment analysis setting. In this setting, given a text segment, our aim is to extract its sentiment or polarity. SentiWordNet is a lexical resource with sentiment polarity annotations. By comparing performance with other curriculum strategies and with no curriculum, the effectiveness of the proposed strategy is presented. Convolutional, Recurrence, and Attention-based architectures are employed to assess this improvement. The models are evaluated on a standard sentiment dataset, Stanford Sentiment Treebank.",Education
Modeling National Trends on Health in the Philippines Using ARIMA,"Health is a very important prerequisite in peoples well-being and happiness. Several studies were more focused on presenting the occurrence on specific disease like forecasting the number of dengue and malaria cases. This paper utilized the time series data for trend analysis and data forecasting using ARIMA model to visualize the trends of health data on the ten leading causes of deaths, leading cause of morbidity and leading cause of infants deaths particularly in the Philippines presented in a tabular data. Figures for each disease trend are presented individually with the use of the GRETL software. Forecasting results of the leading causes of death showed that Diseases of the heart, vascular system, accidents, Chronic lower respiratory diseases and Chronic Tuberculosis (all forms) showed a slight changed of the forecasted data, Malignant neoplasms showed unstable behavior of the forecasted data, and Pneumonia, diabetes mellitus, Nephritis, nephrotic syndrome and nephrosis and certain conditions originating in perinatal showed a decreasing patterns based on the forecasted data.","Modeling National Trends on Health in the Philippines Using ARIMA Health is a very important prerequisite in peoples well-being and happiness. Several studies were more focused on presenting the occurrence on specific disease like forecasting the number of dengue and malaria cases. This paper utilized the time series data for trend analysis and data forecasting using ARIMA model to visualize the trends of health data on the ten leading causes of deaths, leading cause of morbidity and leading cause of infants deaths particularly in the Philippines presented in a tabular data. Figures for each disease trend are presented individually with the use of the GRETL software. Forecasting results of the leading causes of death showed that Diseases of the heart, vascular system, accidents, Chronic lower respiratory diseases and Chronic Tuberculosis (all forms) showed a slight changed of the forecasted data, Malignant neoplasms showed unstable behavior of the forecasted data, and Pneumonia, diabetes mellitus, Nephritis, nephrotic syndrome and nephrosis and certain conditions originating in perinatal showed a decreasing patterns based on the forecasted data.",Healthcare
Link Climate: An Interoperable Knowledge Graph Platform for Climate Data,"Climate science has become more ambitious in recent years as global awareness about the environment has grown. To better understand climate, historical climate (e.g. archived meteorological variables such as temperature, wind, water, etc.) and climate-related data (e.g. geographical features and human activities) are widely used by todays climate research to derive models for an explainable climate change and its effects. However, such data sources are often dispersed across a multitude of disconnected data silos on the Web. Moreover, there is a lack of advanced climate data platforms to enable multi-source heterogeneous climate data analysis, therefore, researchers must face a stern challenge in collecting and analyzing multi-source data. In this paper, we address this problem by proposing a climate knowledge graph for the integration of multiple climate data and other data sources into one service, leveraging Web technologies (e.g. HTTP) for multi-source climate data analysis. The proposed knowledge graph is primarily composed of data from the National Oceanic and Atmospheric Administrations daily climate summaries, OpenStreetMap, and Wikidata, and it supports joint data queries on these widely used databases. This paper shows, with a use case in Ireland and the United Kingdom, how climate researchers could benefit from this platform as it allows them to easily integrate datasets from different domains and geographical locations.","Link Climate: An Interoperable Knowledge Graph Platform for Climate Data Climate science has become more ambitious in recent years as global awareness about the environment has grown. To better understand climate, historical climate (e.g. archived meteorological variables such as temperature, wind, water, etc.) and climate-related data (e.g. geographical features and human activities) are widely used by todays climate research to derive models for an explainable climate change and its effects. However, such data sources are often dispersed across a multitude of disconnected data silos on the Web. Moreover, there is a lack of advanced climate data platforms to enable multi-source heterogeneous climate data analysis, therefore, researchers must face a stern challenge in collecting and analyzing multi-source data. In this paper, we address this problem by proposing a climate knowledge graph for the integration of multiple climate data and other data sources into one service, leveraging Web technologies (e.g. HTTP) for multi-source climate data analysis. The proposed knowledge graph is primarily composed of data from the National Oceanic and Atmospheric Administrations daily climate summaries, OpenStreetMap, and Wikidata, and it supports joint data queries on these widely used databases. This paper shows, with a use case in Ireland and the United Kingdom, how climate researchers could benefit from this platform as it allows them to easily integrate datasets from different domains and geographical locations.",Environment
Cloud boundary height measurements using lidar and radar,"Using only lidar or radar an accurate cloud boundary height estimate is often not possible. The combination of lidar and radar can give a reliable cloud boundary estimate in a much broader range of cases. However, also this combination with standard methods still can not measure the cloud boundaries in all cases. This will be illustrated with data from the Clouds and Radiation measurement campaigns, CLARA. Rain is a problem: the radar has problems to measure the small cloud droplets in the presence of raindrops. Similarly, few large particles below cloud base can obscure the cloud base in radar measurements. And the radar reflectivity can be very low at the cloud base of water clouds or in large regions of ice clouds, due to small particles. Multiple cloud layers and clouds with specular reflections can pose problems for lidar. More advanced measurement techniques are suggested to solve these problems. An angle scanning lidar can, for example, detect specular reflections, while using information from the radars Doppler velocity spectrum may help to detect clouds during rain.","Cloud boundary height measurements using lidar and radar Using only lidar or radar an accurate cloud boundary height estimate is often not possible. The combination of lidar and radar can give a reliable cloud boundary estimate in a much broader range of cases. However, also this combination with standard methods still can not measure the cloud boundaries in all cases. This will be illustrated with data from the Clouds and Radiation measurement campaigns, CLARA. Rain is a problem: the radar has problems to measure the small cloud droplets in the presence of raindrops. Similarly, few large particles below cloud base can obscure the cloud base in radar measurements. And the radar reflectivity can be very low at the cloud base of water clouds or in large regions of ice clouds, due to small particles. Multiple cloud layers and clouds with specular reflections can pose problems for lidar. More advanced measurement techniques are suggested to solve these problems. An angle scanning lidar can, for example, detect specular reflections, while using information from the radars Doppler velocity spectrum may help to detect clouds during rain.",Environment
The use of Semantic Technologies in Computer Science Curriculum: A Systematic Review,"Semantic technologies are evolving and being applied in several research areas, including the education domain. This paper presents the outcomes of a systematic review carried out to provide an overview of the application of semantic technologies in the context of the Computer Science curriculum and discuss the limitations in this field whilst offering insights for future research. A total of 4,510 studies were reviewed, and 37 were analysed and reported. As a result, while semantic technologies have been increasingly used to develop Computer Science curricula, the alignment of ontologies and accurate curricula assessment appears to be the most significant limitations to the widespread adoption of such technologies.","The use of Semantic Technologies in Computer Science Curriculum: A Systematic Review Semantic technologies are evolving and being applied in several research areas, including the education domain. This paper presents the outcomes of a systematic review carried out to provide an overview of the application of semantic technologies in the context of the Computer Science curriculum and discuss the limitations in this field whilst offering insights for future research. A total of 4,510 studies were reviewed, and 37 were analysed and reported. As a result, while semantic technologies have been increasingly used to develop Computer Science curricula, the alignment of ontologies and accurate curricula assessment appears to be the most significant limitations to the widespread adoption of such technologies.",Education
"A Theory of Probabilistic Boosting, Decision Trees and Matryoshki","We present a theory of boosting probabilistic classifiers. We place ourselves in the situation of a user who only provides a stopping parameter and a probabilistic weak learnerclassifier and compare three types of boosting algorithms: probabilistic Adaboost, decision tree, and tree of trees of ... of trees, which we call matryoshka. Nested tree, embedded tree and recursive tree are also appropriate names for this algorithm, which is one of our contributions. Our other contribution is the theoretical analysis of the algorithms, in which we give training error bounds. This analysis suggests that the matryoshka leverages probabilistic weak classifiers more efficiently than simple decision trees.","A Theory of Probabilistic Boosting, Decision Trees and Matryoshki We present a theory of boosting probabilistic classifiers. We place ourselves in the situation of a user who only provides a stopping parameter and a probabilistic weak learnerclassifier and compare three types of boosting algorithms: probabilistic Adaboost, decision tree, and tree of trees of ... of trees, which we call matryoshka. Nested tree, embedded tree and recursive tree are also appropriate names for this algorithm, which is one of our contributions. Our other contribution is the theoretical analysis of the algorithms, in which we give training error bounds. This analysis suggests that the matryoshka leverages probabilistic weak classifiers more efficiently than simple decision trees.",Technology
Counterparty Risk Valuation: A Marked Branching Diffusion Approach,The purpose of this paper is to design an algorithm for the computation of the counterparty risk which is competitive in regards of a brute force Monte-Carlo of Monte-Carlo method (with nested simulations). This is achieved using marked branching diffusions describing a Galton-Watson random tree. Such an algorithm leads at the same time to a computation of the (bilateral) counterparty risk when we use the default-risky or counterparty-riskless option values as mark-to-market. Our method is illustrated by various numerical examples.,Counterparty Risk Valuation: A Marked Branching Diffusion Approach The purpose of this paper is to design an algorithm for the computation of the counterparty risk which is competitive in regards of a brute force Monte-Carlo of Monte-Carlo method (with nested simulations). This is achieved using marked branching diffusions describing a Galton-Watson random tree. Such an algorithm leads at the same time to a computation of the (bilateral) counterparty risk when we use the default-risky or counterparty-riskless option values as mark-to-market. Our method is illustrated by various numerical examples.,Finance
Economics and Optimal Investment Policies of Attackers and Defenders in Cybersecurity,"In our time cybersecurity has grown to be a topic of massive proportion at the national and enterprise levels. Our thesis is that the economic perspective and investment decision-making are vital factors in determining the outcome of the struggle. To build our economic framework, we borrow from the pioneering work of Gordon and Loeb in which the Defender optimally trades-off investments for lower likelihood of its system breach. Our two-sided model additionally has an Attacker, assumed to be rational and also guided by economic considerations in its decision-making, to which the Defender responds. Our model is a simplified adaptation of a model proposed during the Cold War for weapons deployment in the US. Our model may also be viewed as a Stackelberg game and, from an analytic perspective, as a Max-Min problem, the analysis of which is known to have to contend with discontinuous behavior. The complexity of our simple model is rooted in its inherent nonlinearity and, more consequentially, non-convexity of the objective function in the optimization. The possibilities of the Attackers actions add substantially to the risk to the Defender, and the Defenders rational, risk-neutral optimal investments in general substantially exceed the optimal investments predicted by the one-sided Gordon-Loeb model. We obtain a succinct set of three decision types that categorize all of the Defenders optimal investment decisions. Also, the Defenders optimal decisions exhibit discontinuous behavior as the initial vulnerability of its system is varied. The analysis is supplemented by extensive numerical illustrations. The results from our model open several major avenues for future work.","Economics and Optimal Investment Policies of Attackers and Defenders in Cybersecurity In our time cybersecurity has grown to be a topic of massive proportion at the national and enterprise levels. Our thesis is that the economic perspective and investment decision-making are vital factors in determining the outcome of the struggle. To build our economic framework, we borrow from the pioneering work of Gordon and Loeb in which the Defender optimally trades-off investments for lower likelihood of its system breach. Our two-sided model additionally has an Attacker, assumed to be rational and also guided by economic considerations in its decision-making, to which the Defender responds. Our model is a simplified adaptation of a model proposed during the Cold War for weapons deployment in the US. Our model may also be viewed as a Stackelberg game and, from an analytic perspective, as a Max-Min problem, the analysis of which is known to have to contend with discontinuous behavior. The complexity of our simple model is rooted in its inherent nonlinearity and, more consequentially, non-convexity of the objective function in the optimization. The possibilities of the Attackers actions add substantially to the risk to the Defender, and the Defenders rational, risk-neutral optimal investments in general substantially exceed the optimal investments predicted by the one-sided Gordon-Loeb model. We obtain a succinct set of three decision types that categorize all of the Defenders optimal investment decisions. Also, the Defenders optimal decisions exhibit discontinuous behavior as the initial vulnerability of its system is varied. The analysis is supplemented by extensive numerical illustrations. The results from our model open several major avenues for future work.",Finance
Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition,"This paper presents the MAXQ approach to hierarchical reinforcement learning based on decomposing the target Markov decision process (MDP) into a hierarchy of smaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs. The paper defines the MAXQ hierarchy, proves formal results on its representational power, and establishes five conditions for the safe use of state abstractions. The paper presents an online model-free learning algorithm, MAXQ-Q, and proves that it converges wih probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even in the presence of the five kinds of state abstraction. The paper evaluates the MAXQ representation and MAXQ-Q through a series of experiments in three domains and shows experimentally that MAXQ-Q (with state abstractions) converges to a recursively optimal policy much faster than flat Q learning. The fact that MAXQ learns a representation of the value function has an important benefit: it makes it possible to compute and execute an improved, non-hierarchical policy via a procedure similar to the policy improvement step of policy iteration. The paper demonstrates the effectiveness of this non-hierarchical execution experimentally. Finally, the paper concludes with a comparison to related work and a discussion of the design tradeoffs in hierarchical reinforcement learning.","Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition This paper presents the MAXQ approach to hierarchical reinforcement learning based on decomposing the target Markov decision process (MDP) into a hierarchy of smaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs. The paper defines the MAXQ hierarchy, proves formal results on its representational power, and establishes five conditions for the safe use of state abstractions. The paper presents an online model-free learning algorithm, MAXQ-Q, and proves that it converges wih probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even in the presence of the five kinds of state abstraction. The paper evaluates the MAXQ representation and MAXQ-Q through a series of experiments in three domains and shows experimentally that MAXQ-Q (with state abstractions) converges to a recursively optimal policy much faster than flat Q learning. The fact that MAXQ learns a representation of the value function has an important benefit: it makes it possible to compute and execute an improved, non-hierarchical policy via a procedure similar to the policy improvement step of policy iteration. The paper demonstrates the effectiveness of this non-hierarchical execution experimentally. Finally, the paper concludes with a comparison to related work and a discussion of the design tradeoffs in hierarchical reinforcement learning.",Technology
A quantitative model for refilling of the sarcoplasmic reticulum during vascular smooth muscle asynchronous Ca2 oscillations,"We have developed a quantitative model for the creation of cytoplasmic Ca2 gradients near the inner surface of the plasma membrane (PM). In particular we simulated the refilling of the sarcoplasmic reticulum (SR) via PM-SR junctions during asynchronous Ca2 oscillations in smooth muscle cells of the rabbit inferior vena cava. We have combined confocal microscopy data on the Ca2 oscillations, force transduction data from cell contraction studies and electron microscopic images to build a basis for computational simulations that model the transport of calcium ions from NaCa2 exchangers (NCX) on the PM to sarcoplasmicendoplasmic reticulum Ca2 ATPase (SERCA) pumps on the SR as a three-dimensional random walk through the PM-SR junctional cytoplasmic spaces. Electron microscopic ultrastructural images of the smooth muscle cells were elaborated with software algorithms to produce a very clear and dimensionally accurate picture of the PM-SR junctions. From this study, we conclude that it is plausible and possible for enough Ca2 to pass through the PM-SR junctions to replete the SR during the regenerative Ca2 release, which underlies agonist induced asynchronous Ca2 oscillations in vascular smooth muscle.","A quantitative model for refilling of the sarcoplasmic reticulum during vascular smooth muscle asynchronous Ca2 oscillations We have developed a quantitative model for the creation of cytoplasmic Ca2 gradients near the inner surface of the plasma membrane (PM). In particular we simulated the refilling of the sarcoplasmic reticulum (SR) via PM-SR junctions during asynchronous Ca2 oscillations in smooth muscle cells of the rabbit inferior vena cava. We have combined confocal microscopy data on the Ca2 oscillations, force transduction data from cell contraction studies and electron microscopic images to build a basis for computational simulations that model the transport of calcium ions from NaCa2 exchangers (NCX) on the PM to sarcoplasmicendoplasmic reticulum Ca2 ATPase (SERCA) pumps on the SR as a three-dimensional random walk through the PM-SR junctional cytoplasmic spaces. Electron microscopic ultrastructural images of the smooth muscle cells were elaborated with software algorithms to produce a very clear and dimensionally accurate picture of the PM-SR junctions. From this study, we conclude that it is plausible and possible for enough Ca2 to pass through the PM-SR junctions to replete the SR during the regenerative Ca2 release, which underlies agonist induced asynchronous Ca2 oscillations in vascular smooth muscle.",Healthcare
A Review of Sustainable Practices in Road Freight Transport,"Sustainable road freight transport becomes indispensable in the field of transportation and logistics. The new technological change, the environmental impacts, and social responsibility laid freight road transport in front of various challenges, which makes the sustainable practices a vital solution in the sector. This paper aims to provide a theoretical research findings in sustainable road freight transport. The methodology discusses the road freight transport sustainability indicators among the literature studies realized in different countries in the world. The review analysis the studies and practical applications from various countries. The result exposes that the sustainability dimensions such as economic, social, environment was discussed in different cases, which prove the efforts of many countries to reduce environmental impact, improve economic efficiency, support social well-being, and expand technological innovations to achieve a sustainable transport system.","A Review of Sustainable Practices in Road Freight Transport Sustainable road freight transport becomes indispensable in the field of transportation and logistics. The new technological change, the environmental impacts, and social responsibility laid freight road transport in front of various challenges, which makes the sustainable practices a vital solution in the sector. This paper aims to provide a theoretical research findings in sustainable road freight transport. The methodology discusses the road freight transport sustainability indicators among the literature studies realized in different countries in the world. The review analysis the studies and practical applications from various countries. The result exposes that the sustainability dimensions such as economic, social, environment was discussed in different cases, which prove the efforts of many countries to reduce environmental impact, improve economic efficiency, support social well-being, and expand technological innovations to achieve a sustainable transport system.",Environment
Stochastic Optimization Algorithms,"When looking for a solution, deterministic methods have the enormous advantage that they do find global optima. Unfortunately, they are very CPU-intensive, and are useless on untractable NP-hard problems that would require thousands of years for cutting-edge computers to explore. In order to get a result, one needs to revert to stochastic algorithms, that sample the search space without exploring it thoroughly. Such algorithms can find very good results, without any guarantee that the global optimum has been reached; but there is often no other choice than using them. This chapter is a short introduction to the main methods used in stochastic optimization.","Stochastic Optimization Algorithms When looking for a solution, deterministic methods have the enormous advantage that they do find global optima. Unfortunately, they are very CPU-intensive, and are useless on untractable NP-hard problems that would require thousands of years for cutting-edge computers to explore. In order to get a result, one needs to revert to stochastic algorithms, that sample the search space without exploring it thoroughly. Such algorithms can find very good results, without any guarantee that the global optimum has been reached; but there is often no other choice than using them. This chapter is a short introduction to the main methods used in stochastic optimization.",Technology
Evolino for recurrent support vector machines,"Traditional Support Vector Machines (SVMs) need pre-wired finite time windows to predict and classify time series. They do not have an internal state necessary to deal with sequences involving arbitrary long-term dependencies. Here we introduce a new class of recurrent, truly sequential SVM-like devices with internal adaptive states, trained by a novel method called EVOlution of systems with KErnel-based outputs (Evoke), an instance of the recent Evolino class of methods. Evoke evolves recurrent neural networks to detect and represent temporal dependencies while using quadratic programmingsupport vector regression to produce precise outputs. Evoke is the first SVM-based mechanism learning to classify a context-sensitive language. It also outperforms recent state-of-the-art gradient-based recurrent neural networks (RNNs) on various time series prediction tasks.","Evolino for recurrent support vector machines Traditional Support Vector Machines (SVMs) need pre-wired finite time windows to predict and classify time series. They do not have an internal state necessary to deal with sequences involving arbitrary long-term dependencies. Here we introduce a new class of recurrent, truly sequential SVM-like devices with internal adaptive states, trained by a novel method called EVOlution of systems with KErnel-based outputs (Evoke), an instance of the recent Evolino class of methods. Evoke evolves recurrent neural networks to detect and represent temporal dependencies while using quadratic programmingsupport vector regression to produce precise outputs. Evoke is the first SVM-based mechanism learning to classify a context-sensitive language. It also outperforms recent state-of-the-art gradient-based recurrent neural networks (RNNs) on various time series prediction tasks.",Technology
IS (Iris Security),In the paper will be presented a safety system based on iridology. The results suggest a new scenario where the security problem in supervised and unsupervised areas can be treat with the present system and the iris image recognition.,IS (Iris Security) In the paper will be presented a safety system based on iridology. The results suggest a new scenario where the security problem in supervised and unsupervised areas can be treat with the present system and the iris image recognition.,Technology
Toward environmental sustainability: an empirical study on airports efficiency,"The transition to more environmentally sustainable production processes and managerial practices is an increasingly important topic. Many industries need to undergo radical change to meet environmental sustainability requirements; the tourism industry is no exception. In this respect, a particular aspect that needs further attention is the relationship between airport performances and investments in environmental sustainability policies. This work represents a first attempt to provide empirical evidences about this relationship. Through the application of a non-parametrical method, we first assess the efficiency of the Italian airports industry. Secondly, we investigated the relationship between airports performance and management commitment toward the ecological transition using a Tobit regression model. The results show that airports adherence to formal multi-year ecological transition programs has a positive and consistent impact on their performance.","Toward environmental sustainability: an empirical study on airports efficiency The transition to more environmentally sustainable production processes and managerial practices is an increasingly important topic. Many industries need to undergo radical change to meet environmental sustainability requirements; the tourism industry is no exception. In this respect, a particular aspect that needs further attention is the relationship between airport performances and investments in environmental sustainability policies. This work represents a first attempt to provide empirical evidences about this relationship. Through the application of a non-parametrical method, we first assess the efficiency of the Italian airports industry. Secondly, we investigated the relationship between airports performance and management commitment toward the ecological transition using a Tobit regression model. The results show that airports adherence to formal multi-year ecological transition programs has a positive and consistent impact on their performance.",Environment
AI analysis of medical images at scale as a health disparities probe: a feasibility demonstration using chest radiographs,"Health disparities (differences in non-genetic conditions that influence health) can be associated with differences in burden of disease by groups within a population. Social determinants of health (SDOH) are domains such as health care access, dietary access, and economics frequently studied for potential association with health disparities. Evaluating SDOH-related phenotypes using routine medical images as data sources may enhance health disparities research. We developed a pipeline for using quantitative measures automatically extracted from medical images as inputs into health disparities index calculations. Our study focused on the use case of two SDOH demographic correlates (sex and race) and data extracted from chest radiographs of 1,571 unique patients. The likelihood of severe disease within the lung parenchyma from each image type, measured using an established deep learning model, was merged into a single numerical image-based phenotype for each patient. Patients were then separated into phenogroups by unsupervised clustering of the image-based phenotypes. The health rate for each phenogroup was defined as the median image-based phenotype for each SDOH used as inputs to four imaging-derived health disparities indices (iHDIs): one absolute measure (between-group variance) and three relative measures (index of disparity, Theil index, and mean log deviation). The iHDI measures demonstrated feasible values for each SDOH demographic correlate, showing potential for medical images to serve as a novel probe for health disparities. Large-scale AI analysis of medical images can serve as a probe for a novel data source for health disparities research.","AI analysis of medical images at scale as a health disparities probe: a feasibility demonstration using chest radiographs Health disparities (differences in non-genetic conditions that influence health) can be associated with differences in burden of disease by groups within a population. Social determinants of health (SDOH) are domains such as health care access, dietary access, and economics frequently studied for potential association with health disparities. Evaluating SDOH-related phenotypes using routine medical images as data sources may enhance health disparities research. We developed a pipeline for using quantitative measures automatically extracted from medical images as inputs into health disparities index calculations. Our study focused on the use case of two SDOH demographic correlates (sex and race) and data extracted from chest radiographs of 1,571 unique patients. The likelihood of severe disease within the lung parenchyma from each image type, measured using an established deep learning model, was merged into a single numerical image-based phenotype for each patient. Patients were then separated into phenogroups by unsupervised clustering of the image-based phenotypes. The health rate for each phenogroup was defined as the median image-based phenotype for each SDOH used as inputs to four imaging-derived health disparities indices (iHDIs): one absolute measure (between-group variance) and three relative measures (index of disparity, Theil index, and mean log deviation). The iHDI measures demonstrated feasible values for each SDOH demographic correlate, showing potential for medical images to serve as a novel probe for health disparities. Large-scale AI analysis of medical images can serve as a probe for a novel data source for health disparities research.",Healthcare
Consistency of the group Lasso and multiple kernel learning,"We consider the least-square regression problem with regularization by a block 1-norm, i.e., a sum of Euclidean norms over spaces of dimensions larger than one. This problem, referred to as the group Lasso, extends the usual regularization by the 1-norm where all spaces have dimension one, where it is commonly referred to as the Lasso. In this paper, we study the asymptotic model consistency of the group Lasso. We derive necessary and sufficient conditions for the consistency of group Lasso under practical assumptions, such as model misspecification. When the linear predictors and Euclidean norms are replaced by functions and reproducing kernel Hilbert norms, the problem is usually referred to as multiple kernel learning and is commonly used for learning from heterogeneous data sources and for non linear variable selection. Using tools from functional analysis, and in particular covariance operators, we extend the consistency results to this infinite dimensional case and also propose an adaptive scheme to obtain a consistent model estimate, even when the necessary condition required for the non adaptive scheme is not satisfied.","Consistency of the group Lasso and multiple kernel learning We consider the least-square regression problem with regularization by a block 1-norm, i.e., a sum of Euclidean norms over spaces of dimensions larger than one. This problem, referred to as the group Lasso, extends the usual regularization by the 1-norm where all spaces have dimension one, where it is commonly referred to as the Lasso. In this paper, we study the asymptotic model consistency of the group Lasso. We derive necessary and sufficient conditions for the consistency of group Lasso under practical assumptions, such as model misspecification. When the linear predictors and Euclidean norms are replaced by functions and reproducing kernel Hilbert norms, the problem is usually referred to as multiple kernel learning and is commonly used for learning from heterogeneous data sources and for non linear variable selection. Using tools from functional analysis, and in particular covariance operators, we extend the consistency results to this infinite dimensional case and also propose an adaptive scheme to obtain a consistent model estimate, even when the necessary condition required for the non adaptive scheme is not satisfied.",Technology
Environmentally-Conscious Cloud Orchestration Considering Geo-Distributed Data Centers,"This paper presents a theoretical discussion for environmentally-conscious job deployment and migration in cloud environments, aiming to minimize the environmental impact of resource provisioning while incorporating sustainability requirements. As the demand for sustainable cloud services grows, it is crucial for cloud customers to select data center operators based on sustainability metrics and to accurately report the ecological footprint of their services. To this end, we analyze sustainability reports and define comprehensive environmental impact profiles for data centers, incorporating key sustainability indicators. We formalize the problem as an optimization model, balancing multiple environmental factors while respecting user preferences. A simulative case study demonstrates the potential of our approach compared to baseline strategies that optimize for single sustainability factors.","Environmentally-Conscious Cloud Orchestration Considering Geo-Distributed Data Centers This paper presents a theoretical discussion for environmentally-conscious job deployment and migration in cloud environments, aiming to minimize the environmental impact of resource provisioning while incorporating sustainability requirements. As the demand for sustainable cloud services grows, it is crucial for cloud customers to select data center operators based on sustainability metrics and to accurately report the ecological footprint of their services. To this end, we analyze sustainability reports and define comprehensive environmental impact profiles for data centers, incorporating key sustainability indicators. We formalize the problem as an optimization model, balancing multiple environmental factors while respecting user preferences. A simulative case study demonstrates the potential of our approach compared to baseline strategies that optimize for single sustainability factors.",Environment
Idl Signal Processing Library 1.0,"We make available a library of documented IDL .pro files as well as a shareable object library that allows IDL to call routines from LAPACK. The routines are for use in the spectral analysis of time series data. The primary focus of these routines are David Thomsons multitaper methods but a whole range of functions will be made available in future revisions of the submission. At present routines are provided to carry out the following operations: calculate prolate spheroidal sequences and eigenvalues, project time-series into frequency bands, calculate spectral estimates with or without moving windows, and calculate the cross-coherence between two time series as a function of frequency as well as the coherence between frequencies for a single time series.","Idl Signal Processing Library 1.0 We make available a library of documented IDL .pro files as well as a shareable object library that allows IDL to call routines from LAPACK. The routines are for use in the spectral analysis of time series data. The primary focus of these routines are David Thomsons multitaper methods but a whole range of functions will be made available in future revisions of the submission. At present routines are provided to carry out the following operations: calculate prolate spheroidal sequences and eigenvalues, project time-series into frequency bands, calculate spectral estimates with or without moving windows, and calculate the cross-coherence between two time series as a function of frequency as well as the coherence between frequencies for a single time series.",Healthcare
Enhanced Renewable Energy Forecasting and Operations through Probabilistic Forecast Aggregation,"Accurate and reliable forecasting of renewable energy generation is crucial for the efficient integration of renewable sources into the power grid. In particular, probabilistic forecasts are becoming essential for managing the intrinsic variability and uncertainty of renewable energy production, especially wind and solar generation. This paper considers the setting where probabilistic forecasts are provided for individual renewable energy sites using, e.g., quantile regression models, but without any correlation information between sites. This setting is common if, e.g., such forecasts are provided by each individual site, or by multiple vendors. However, to effectively manage a fleet of renewable generators, it is necessary to aggregate these individual forecasts to the fleet level, while ensuring that the aggregated probabilistic forecast is statistically consistent and reliable. To address this challenge, this paper presents the integrated use of Copula and Monte-Carlo methods to aggregate individual probabilistic forecasts into a statistically calibrated, probabilistic forecast at the fleet level. The proposed framework is validated using synthetic data from several large-scale systems in the United States. This work has important implications for grid operators and energy planners, providing them with better tools to manage the variability and uncertainty inherent in renewable energy production.","Enhanced Renewable Energy Forecasting and Operations through Probabilistic Forecast Aggregation Accurate and reliable forecasting of renewable energy generation is crucial for the efficient integration of renewable sources into the power grid. In particular, probabilistic forecasts are becoming essential for managing the intrinsic variability and uncertainty of renewable energy production, especially wind and solar generation. This paper considers the setting where probabilistic forecasts are provided for individual renewable energy sites using, e.g., quantile regression models, but without any correlation information between sites. This setting is common if, e.g., such forecasts are provided by each individual site, or by multiple vendors. However, to effectively manage a fleet of renewable generators, it is necessary to aggregate these individual forecasts to the fleet level, while ensuring that the aggregated probabilistic forecast is statistically consistent and reliable. To address this challenge, this paper presents the integrated use of Copula and Monte-Carlo methods to aggregate individual probabilistic forecasts into a statistically calibrated, probabilistic forecast at the fleet level. The proposed framework is validated using synthetic data from several large-scale systems in the United States. This work has important implications for grid operators and energy planners, providing them with better tools to manage the variability and uncertainty inherent in renewable energy production.",Environment
Bias-Driven Revision of Logical Domain Theories,"The theory revision problem is the problem of how best to go about revising a deficient domain theory using information contained in examples that expose inaccuracies. In this paper we present our approach to the theory revision problem for propositional domain theories. The approach described here, called PTR, uses probabilities associated with domain theory elements to numerically track the flow of proof through the theory. This allows us to measure the precise role of a clause or literal in allowing or preventing a (desired or undesired) derivation for a given example. This information is used to efficiently locate and repair flawed elements of the theory. PTR is proved to converge to a theory which correctly classifies all examples, and shown experimentally to be fast and accurate even for deep theories.","Bias-Driven Revision of Logical Domain Theories The theory revision problem is the problem of how best to go about revising a deficient domain theory using information contained in examples that expose inaccuracies. In this paper we present our approach to the theory revision problem for propositional domain theories. The approach described here, called PTR, uses probabilities associated with domain theory elements to numerically track the flow of proof through the theory. This allows us to measure the precise role of a clause or literal in allowing or preventing a (desired or undesired) derivation for a given example. This information is used to efficiently locate and repair flawed elements of the theory. PTR is proved to converge to a theory which correctly classifies all examples, and shown experimentally to be fast and accurate even for deep theories.",Technology
Interpretable Dynamic Treatment Regimes,"Precision medicine is currently a topic of great interest in clinical and intervention science. One way to formalize precision medicine is through a treatment regime, which is a sequence of decision rules, one per stage of clinical intervention, that map up-to-date patient information to a recommended treatment. An optimal treatment regime is defined as maximizing the mean of some cumulative clinical outcome if applied to a population of interest. It is well-known that even under simple generative models an optimal treatment regime can be a highly nonlinear function of patient information. Consequently, a focal point of recent methodological research has been the development of flexible models for estimating optimal treatment regimes. However, in many settings, estimation of an optimal treatment regime is an exploratory analysis intended to generate new hypotheses for subsequent research and not to directly dictate treatment to new patients. In such settings, an estimated treatment regime that is interpretable in a domain context may be of greater value than an unintelligible treatment regime built using black-box estimation methods. We propose an estimator of an optimal treatment regime composed of a sequence of decision rules, each expressible as a list of if-then statements that can be presented as either a paragraph or as a simple flowchart that is immediately interpretable to domain experts. The discreteness of these lists precludes smooth, i.e., gradient-based, methods of estimation and leads to non-standard asymptotics. Nevertheless, we provide a computationally efficient estimation algorithm, prove consistency of the proposed estimator, and derive rates of convergence. We illustrate the proposed methods using a series of simulation examples and application to data from a sequential clinical trial on bipolar disorder.","Interpretable Dynamic Treatment Regimes Precision medicine is currently a topic of great interest in clinical and intervention science. One way to formalize precision medicine is through a treatment regime, which is a sequence of decision rules, one per stage of clinical intervention, that map up-to-date patient information to a recommended treatment. An optimal treatment regime is defined as maximizing the mean of some cumulative clinical outcome if applied to a population of interest. It is well-known that even under simple generative models an optimal treatment regime can be a highly nonlinear function of patient information. Consequently, a focal point of recent methodological research has been the development of flexible models for estimating optimal treatment regimes. However, in many settings, estimation of an optimal treatment regime is an exploratory analysis intended to generate new hypotheses for subsequent research and not to directly dictate treatment to new patients. In such settings, an estimated treatment regime that is interpretable in a domain context may be of greater value than an unintelligible treatment regime built using black-box estimation methods. We propose an estimator of an optimal treatment regime composed of a sequence of decision rules, each expressible as a list of if-then statements that can be presented as either a paragraph or as a simple flowchart that is immediately interpretable to domain experts. The discreteness of these lists precludes smooth, i.e., gradient-based, methods of estimation and leads to non-standard asymptotics. Nevertheless, we provide a computationally efficient estimation algorithm, prove consistency of the proposed estimator, and derive rates of convergence. We illustrate the proposed methods using a series of simulation examples and application to data from a sequential clinical trial on bipolar disorder.",Healthcare
Generative AI in Education: From Foundational Insights to the Socratic Playground for Learning,"This paper explores the synergy between human cognition and Large Language Models (LLMs), highlighting how generative AI can drive personalized learning at scale. We discuss parallels between LLMs and human cognition, emphasizing both the promise and new perspectives on integrating AI systems into education. After examining challenges in aligning technology with pedagogy, we review AutoTutor-one of the earliest Intelligent Tutoring Systems (ITS)-and detail its successes, limitations, and unfulfilled aspirations. We then introduce the Socratic Playground, a next-generation ITS that uses advanced transformer-based models to overcome AutoTutors constraints and provide personalized, adaptive tutoring. To illustrate its evolving capabilities, we present a JSON-based tutoring prompt that systematically guides learner reflection while tracking misconceptions. Throughout, we underscore the importance of placing pedagogy at the forefront, ensuring that technologys power is harnessed to enhance teaching and learning rather than overshadow it.","Generative AI in Education: From Foundational Insights to the Socratic Playground for Learning This paper explores the synergy between human cognition and Large Language Models (LLMs), highlighting how generative AI can drive personalized learning at scale. We discuss parallels between LLMs and human cognition, emphasizing both the promise and new perspectives on integrating AI systems into education. After examining challenges in aligning technology with pedagogy, we review AutoTutor-one of the earliest Intelligent Tutoring Systems (ITS)-and detail its successes, limitations, and unfulfilled aspirations. We then introduce the Socratic Playground, a next-generation ITS that uses advanced transformer-based models to overcome AutoTutors constraints and provide personalized, adaptive tutoring. To illustrate its evolving capabilities, we present a JSON-based tutoring prompt that systematically guides learner reflection while tracking misconceptions. Throughout, we underscore the importance of placing pedagogy at the forefront, ensuring that technologys power is harnessed to enhance teaching and learning rather than overshadow it.",Education
Fuzzy Approach Topic Discovery in Health and Medical Corpora,"The majority of medical documents and electronic health records (EHRs) are in text format that poses a challenge for data processing and finding relevant documents. Looking for ways to automatically retrieve the enormous amount of health and medical knowledge has always been an intriguing topic. Powerful methods have been developed in recent years to make the text processing automatic. One of the popular approaches to retrieve information based on discovering the themes in health  medical corpora is topic modeling, however, this approach still needs new perspectives. In this research we describe fuzzy latent semantic analysis (FLSA), a novel approach in topic modeling using fuzzy perspective. FLSA can handle health  medical corpora redundancy issue and provides a new method to estimate the number of topics. The quantitative evaluations show that FLSA produces superior performance and features to latent Dirichlet allocation (LDA), the most popular topic model.","Fuzzy Approach Topic Discovery in Health and Medical Corpora The majority of medical documents and electronic health records (EHRs) are in text format that poses a challenge for data processing and finding relevant documents. Looking for ways to automatically retrieve the enormous amount of health and medical knowledge has always been an intriguing topic. Powerful methods have been developed in recent years to make the text processing automatic. One of the popular approaches to retrieve information based on discovering the themes in health  medical corpora is topic modeling, however, this approach still needs new perspectives. In this research we describe fuzzy latent semantic analysis (FLSA), a novel approach in topic modeling using fuzzy perspective. FLSA can handle health  medical corpora redundancy issue and provides a new method to estimate the number of topics. The quantitative evaluations show that FLSA produces superior performance and features to latent Dirichlet allocation (LDA), the most popular topic model.",Healthcare
Climate Change from Large Language Models,"Climate change poses grave challenges, demanding widespread understanding and low-carbon lifestyle awareness. Large language models (LLMs) offer a powerful tool to address this crisis, yet comprehensive evaluations of their climate-crisis knowledge are lacking. This paper proposes an automated evaluation framework to assess climate-crisis knowledge within LLMs. We adopt a hybrid approach for data acquisition, combining data synthesis and manual collection, to compile a diverse set of questions encompassing various aspects of climate change. Utilizing prompt engineering based on the compiled questions, we evaluate the models knowledge by analyzing its generated answers. Furthermore, we introduce a comprehensive set of metrics to assess climate-crisis knowledge, encompassing indicators from 10 distinct perspectives. These metrics provide a multifaceted evaluation, enabling a nuanced understanding of the LLMs climate crisis comprehension. The experimental results demonstrate the efficacy of our proposed method. In our evaluation utilizing diverse high-performing LLMs, we discovered that while LLMs possess considerable climate-related knowledge, there are shortcomings in terms of timeliness, indicating a need for continuous updating and refinement of their climate-related content.","Climate Change from Large Language Models Climate change poses grave challenges, demanding widespread understanding and low-carbon lifestyle awareness. Large language models (LLMs) offer a powerful tool to address this crisis, yet comprehensive evaluations of their climate-crisis knowledge are lacking. This paper proposes an automated evaluation framework to assess climate-crisis knowledge within LLMs. We adopt a hybrid approach for data acquisition, combining data synthesis and manual collection, to compile a diverse set of questions encompassing various aspects of climate change. Utilizing prompt engineering based on the compiled questions, we evaluate the models knowledge by analyzing its generated answers. Furthermore, we introduce a comprehensive set of metrics to assess climate-crisis knowledge, encompassing indicators from 10 distinct perspectives. These metrics provide a multifaceted evaluation, enabling a nuanced understanding of the LLMs climate crisis comprehension. The experimental results demonstrate the efficacy of our proposed method. In our evaluation utilizing diverse high-performing LLMs, we discovered that while LLMs possess considerable climate-related knowledge, there are shortcomings in terms of timeliness, indicating a need for continuous updating and refinement of their climate-related content.",Environment
WaterWise: Co-optimizing Carbon- and Water-Footprint Toward Environmentally Sustainable Cloud Computing,"The carbon and water footprint of large-scale computing systems poses serious environmental sustainability risks. In this study, we discover that, unfortunately, carbon and water sustainability are at odds with each other - and, optimizing one alone hurts the other. Toward that goal, we introduce, WaterWise, a novel job scheduler for parallel workloads that intelligently co-optimizes carbon and water footprint to improve the sustainability of geographically distributed data centers.","WaterWise: Co-optimizing Carbon- and Water-Footprint Toward Environmentally Sustainable Cloud Computing The carbon and water footprint of large-scale computing systems poses serious environmental sustainability risks. In this study, we discover that, unfortunately, carbon and water sustainability are at odds with each other - and, optimizing one alone hurts the other. Toward that goal, we introduce, WaterWise, a novel job scheduler for parallel workloads that intelligently co-optimizes carbon and water footprint to improve the sustainability of geographically distributed data centers.",Environment
Clinical Challenges and AI Opportunities in Decision-Making for Cancer Treatment-Induced Cardiotoxicity,"Cardiotoxicity induced by cancer treatment has become a major clinical concern, affecting the long-term survival and quality of life of cancer patients. Effective clinical decision-making, including the detection of cancer treatment-induced cardiotoxicity and the monitoring of associated symptoms, remains a challenging task for clinicians. This study investigates the current practices and needs of clinicians in the clinical decision making of cancer treatment-induced cardiotoxicity and explores the potential of digital health technologies to support this process. Through semi-structured interviews with seven clinical experts, we identify a three-step decision-making paradigm: 1) symptom identification, 2) diagnostic testing and specialist collaboration, and 3) clinical decision-making and intervention. Our findings highlight the difficulties of diagnosing cardiotoxicity (absence of unified protocols and high variability in symptoms) and monitoring patient symptoms (lacking accurate and timely patient self-reported symptoms). The clinicians also expressed their need for effective early detection tools that can integrate remote patient monitoring capabilities. Based on these insights, we discuss the importance of understanding the dynamic nature of clinical workflows, and the design considerations for future digital tools to support cancer-treatment-induced cardiotoxicity decision-making.","Clinical Challenges and AI Opportunities in Decision-Making for Cancer Treatment-Induced Cardiotoxicity Cardiotoxicity induced by cancer treatment has become a major clinical concern, affecting the long-term survival and quality of life of cancer patients. Effective clinical decision-making, including the detection of cancer treatment-induced cardiotoxicity and the monitoring of associated symptoms, remains a challenging task for clinicians. This study investigates the current practices and needs of clinicians in the clinical decision making of cancer treatment-induced cardiotoxicity and explores the potential of digital health technologies to support this process. Through semi-structured interviews with seven clinical experts, we identify a three-step decision-making paradigm: 1) symptom identification, 2) diagnostic testing and specialist collaboration, and 3) clinical decision-making and intervention. Our findings highlight the difficulties of diagnosing cardiotoxicity (absence of unified protocols and high variability in symptoms) and monitoring patient symptoms (lacking accurate and timely patient self-reported symptoms). The clinicians also expressed their need for effective early detection tools that can integrate remote patient monitoring capabilities. Based on these insights, we discuss the importance of understanding the dynamic nature of clinical workflows, and the design considerations for future digital tools to support cancer-treatment-induced cardiotoxicity decision-making.",Healthcare
Sustainability-Driven Exploration of Topological Material,"Topological materials are at the forefront of quantum materials research, offering tremendous potential for next-generation energy and information devices. However, current investigation of these materials remains largely focused on performance and often neglects the crucial aspect of sustainability. Recognizing the pivotal role of sustainability in addressing global pollution, carbon emissions, resource conservation, and ethical labor practices, we present a comprehensive evaluation of topological materials based on their sustainability and environmental impact. Our approach involves a hierarchical analysis encompassing cost, toxicity, energy demands, environmental impact, social implications, and resilience to imports. By applying this framework to over 16,000 topological materials, we establish a sustainable topological materials database. Our endeavor unveils environmental-friendly topological materials candidates which have been previously overlooked, providing insights into their environmental ramifications and feasibility for industrial scalability. The work represents a critical step toward industrial adoption of topological materials, offering the potential for significant technological advancements and broader societal benefits.","Sustainability-Driven Exploration of Topological Material Topological materials are at the forefront of quantum materials research, offering tremendous potential for next-generation energy and information devices. However, current investigation of these materials remains largely focused on performance and often neglects the crucial aspect of sustainability. Recognizing the pivotal role of sustainability in addressing global pollution, carbon emissions, resource conservation, and ethical labor practices, we present a comprehensive evaluation of topological materials based on their sustainability and environmental impact. Our approach involves a hierarchical analysis encompassing cost, toxicity, energy demands, environmental impact, social implications, and resilience to imports. By applying this framework to over 16,000 topological materials, we establish a sustainable topological materials database. Our endeavor unveils environmental-friendly topological materials candidates which have been previously overlooked, providing insights into their environmental ramifications and feasibility for industrial scalability. The work represents a critical step toward industrial adoption of topological materials, offering the potential for significant technological advancements and broader societal benefits.",Environment
Fingerprint based bio-starter and bio-access,In the paper will be presented a safety and security system based on fingerprint technology. The results suggest a new scenario where the new cars can use a fingerprint sensor integrated in car handle to allow access and in the dashboard as starter button.,Fingerprint based bio-starter and bio-access In the paper will be presented a safety and security system based on fingerprint technology. The results suggest a new scenario where the new cars can use a fingerprint sensor integrated in car handle to allow access and in the dashboard as starter button.,Technology
Mathematical analysis of Soross theory of reflexivity,"The mathematical model proposed by George Soros for his theory of reflexivity is analyzed under the framework of discrete dynamical systems. We show the importance of the notion of fixed points for explaining the behavior of a reflexive system governed by its cognitive and manipulative functions. The interrelationship between these two functions induces fixed points with different characteristics, which in turn generate various system behaviors including the so-called boom then bust phenomenon in Soross theory.","Mathematical analysis of Soross theory of reflexivity The mathematical model proposed by George Soros for his theory of reflexivity is analyzed under the framework of discrete dynamical systems. We show the importance of the notion of fixed points for explaining the behavior of a reflexive system governed by its cognitive and manipulative functions. The interrelationship between these two functions induces fixed points with different characteristics, which in turn generate various system behaviors including the so-called boom then bust phenomenon in Soross theory.",Finance
Energy Storage Autonomy in Renewable Energy Systems Through Hydrogen Salt Caverns,"The expansion of renewable energy sources leads to volatility in electricity generation within energy systems. Subsurface storage of hydrogen in salt caverns can play an important role in long-term energy storage, but their global potential is not fully understood. This study investigates the global status quo and how much hydrogen salt caverns can contribute to stabilizing future renewable energy systems. A global geological suitability and land eligibility analysis for salt cavern placement is conducted and compared with the derived long-term storage needs of renewable energy systems. Results show that hydrogen salt caverns can balance between 43 and 66 of the global electricity demand and exist in North America, Europe, China, and Australia. By sharing the salt cavern potential with neighboring countries, up to 85 of the global electricity demand can be stabilized by salt caverns. Therefore, global hydrogen can play a significant role in stabilizing renewable energy systems.","Energy Storage Autonomy in Renewable Energy Systems Through Hydrogen Salt Caverns The expansion of renewable energy sources leads to volatility in electricity generation within energy systems. Subsurface storage of hydrogen in salt caverns can play an important role in long-term energy storage, but their global potential is not fully understood. This study investigates the global status quo and how much hydrogen salt caverns can contribute to stabilizing future renewable energy systems. A global geological suitability and land eligibility analysis for salt cavern placement is conducted and compared with the derived long-term storage needs of renewable energy systems. Results show that hydrogen salt caverns can balance between 43 and 66 of the global electricity demand and exist in North America, Europe, China, and Australia. By sharing the salt cavern potential with neighboring countries, up to 85 of the global electricity demand can be stabilized by salt caverns. Therefore, global hydrogen can play a significant role in stabilizing renewable energy systems.",Environment
Fundamental defect of the macroeconomic thinking as one of the main causes of the crisis endured,"The main points of the first section of the article written by S.I. Chernyshov, A.V. Voronin and S.A. Razumovsky arXiv:1003.4382), which deals with the fundamental bases of the macroeconomic theory, have been analyzed. An incorrectness of the Harrods model of the economical growth in its generally accepted interpretation was specifically considered. The inevitability of the economic crisis has been shown to follow directly from the premises of this model. At the same time there is an opportunity to realize the damping strategies.","Fundamental defect of the macroeconomic thinking as one of the main causes of the crisis endured The main points of the first section of the article written by S.I. Chernyshov, A.V. Voronin and S.A. Razumovsky arXiv:1003.4382), which deals with the fundamental bases of the macroeconomic theory, have been analyzed. An incorrectness of the Harrods model of the economical growth in its generally accepted interpretation was specifically considered. The inevitability of the economic crisis has been shown to follow directly from the premises of this model. At the same time there is an opportunity to realize the damping strategies.",Finance
A higher-order active contour model of a gas of circles and its application to tree crown extraction,"Many image processing problems involve identifying the region in the image domain occupied by a given entity in the scene. Automatic solution of these problems requires models that incorporate significant prior knowledge about the shape of the region. Many methods for including such knowledge run into difficulties when the topology of the region is unknown a priori, for example when the entity is composed of an unknown number of similar objects. Higher-order active contours (HOACs) represent one method for the modelling of non-trivial prior knowledge about shape without necessarily constraining region topology, via the inclusion of non-local interactions between region boundary points in the energy defining the model. The case of an unknown number of circular objects arises in a number of domains, e.g. medical, biological, nanotechnological, and remote sensing imagery. Regions composed of an a priori unknown number of circles may be referred to as a gas of circles. In this report, we present a HOAC model of a gas of circles. In order to guarantee stable circles, we conduct a stability analysis via a functional Taylor expansion of the HOAC energy around a circular shape. This analysis fixes one of the model parameters in terms of the others and constrains the rest. In conjunction with a suitable likelihood energy, we apply the model to the extraction of tree crowns from aerial imagery, and show that the new model outperforms other techniques.","A higher-order active contour model of a gas of circles and its application to tree crown extraction Many image processing problems involve identifying the region in the image domain occupied by a given entity in the scene. Automatic solution of these problems requires models that incorporate significant prior knowledge about the shape of the region. Many methods for including such knowledge run into difficulties when the topology of the region is unknown a priori, for example when the entity is composed of an unknown number of similar objects. Higher-order active contours (HOACs) represent one method for the modelling of non-trivial prior knowledge about shape without necessarily constraining region topology, via the inclusion of non-local interactions between region boundary points in the energy defining the model. The case of an unknown number of circular objects arises in a number of domains, e.g. medical, biological, nanotechnological, and remote sensing imagery. Regions composed of an a priori unknown number of circles may be referred to as a gas of circles. In this report, we present a HOAC model of a gas of circles. In order to guarantee stable circles, we conduct a stability analysis via a functional Taylor expansion of the HOAC energy around a circular shape. This analysis fixes one of the model parameters in terms of the others and constrains the rest. In conjunction with a suitable likelihood energy, we apply the model to the extraction of tree crowns from aerial imagery, and show that the new model outperforms other techniques.",Technology
Delay-optimal Data Transmission in Renewable Energy Aided Cognitive Radio Networks,"Renewable energy powered cognitive radio (CR) network has gained much attention due to its combination of the CRs spectrum efficiency and the renewable energys green nature. In the paper, we investigate the delay-optimal data transmission in the renewable energy aided CR networks. Specifically, a primary user (PU) and a secondary user (SU) share the same frequency in an area. The SUs interference to the PU is controlled by interference-signal-ratio (ISR) constraint, which means that the ISR at the PU receiver (Rx) should be less than a threshold. Under this constraint, the renewable energy powered SU aims to minimize the average data buffer delay by scheduling the renewable allocations in each slot. A constrained stochastic optimization problem is formulated when the randomness of the renewable arrival, the uncertainty of the SUs data generation, and the variability of the fading channel are taken into account. By analyzing the formulated problem, we propose two practical algorithms that is optimal for two special scenarios. And the two algorithms respectively give an upper and a lower bound for the general scenario. In addition, the availability of the PUs private information at the SU is discussed. Finally, numerical simulations verify the effectiveness of the proposed algorithm.","Delay-optimal Data Transmission in Renewable Energy Aided Cognitive Radio Networks Renewable energy powered cognitive radio (CR) network has gained much attention due to its combination of the CRs spectrum efficiency and the renewable energys green nature. In the paper, we investigate the delay-optimal data transmission in the renewable energy aided CR networks. Specifically, a primary user (PU) and a secondary user (SU) share the same frequency in an area. The SUs interference to the PU is controlled by interference-signal-ratio (ISR) constraint, which means that the ISR at the PU receiver (Rx) should be less than a threshold. Under this constraint, the renewable energy powered SU aims to minimize the average data buffer delay by scheduling the renewable allocations in each slot. A constrained stochastic optimization problem is formulated when the randomness of the renewable arrival, the uncertainty of the SUs data generation, and the variability of the fading channel are taken into account. By analyzing the formulated problem, we propose two practical algorithms that is optimal for two special scenarios. And the two algorithms respectively give an upper and a lower bound for the general scenario. In addition, the availability of the PUs private information at the SU is discussed. Finally, numerical simulations verify the effectiveness of the proposed algorithm.",Environment
Application and practice of AI technology in quantitative investment,"With the continuous development of artificial intelligence technology, using machine learning technology to predict market trends may no longer be out of reach. In recent years, artificial intelligence has become a research hotspot in the academic circle,and it has been widely used in image recognition, natural language processing and other fields, and also has a huge impact on the field of quantitative investment. As an investment method to obtain stable returns through data analysis, model construction and program trading, quantitative investment is deeply loved by financial institutions and investors. At the same time, as an important application field of quantitative investment, the quantitative investment strategy based on artificial intelligence technology arises at the historic moment.How to apply artificial intelligence to quantitative investment, so as to better achieve profit and risk control, has also become the focus and difficulty of the research. From a global perspective, inflation in the US and the Federal Reserve are the concerns of investors, which to some extent affects the direction of global assets, including the Chinese stock market. This paper studies the application of AI technology, quantitative investment, and AI technology in quantitative investment, aiming to provide investors with auxiliary decision-making, reduce the difficulty of investment analysis, and help them to obtain higher returns.","Application and practice of AI technology in quantitative investment With the continuous development of artificial intelligence technology, using machine learning technology to predict market trends may no longer be out of reach. In recent years, artificial intelligence has become a research hotspot in the academic circle,and it has been widely used in image recognition, natural language processing and other fields, and also has a huge impact on the field of quantitative investment. As an investment method to obtain stable returns through data analysis, model construction and program trading, quantitative investment is deeply loved by financial institutions and investors. At the same time, as an important application field of quantitative investment, the quantitative investment strategy based on artificial intelligence technology arises at the historic moment.How to apply artificial intelligence to quantitative investment, so as to better achieve profit and risk control, has also become the focus and difficulty of the research. From a global perspective, inflation in the US and the Federal Reserve are the concerns of investors, which to some extent affects the direction of global assets, including the Chinese stock market. This paper studies the application of AI technology, quantitative investment, and AI technology in quantitative investment, aiming to provide investors with auxiliary decision-making, reduce the difficulty of investment analysis, and help them to obtain higher returns.",Finance
"LearnLens: LLM-Enabled Personalised, Curriculum-Grounded Feedback with Educators in the Loop","Effective feedback is essential for student learning but is time-intensive for teachers. We present LearnLens, a modular, LLM-based system that generates personalised, curriculum-aligned feedback in science education. LearnLens comprises three components: (1) an error-aware assessment module that captures nuanced reasoning errors; (2) a curriculum-grounded generation module that uses a structured, topic-linked memory chain rather than traditional similarity-based retrieval, improving relevance and reducing noise; and (3) an educator-in-the-loop interface for customisation and oversight. LearnLens addresses key challenges in existing systems, offering scalable, high-quality feedback that empowers both teachers and students.","LearnLens: LLM-Enabled Personalised, Curriculum-Grounded Feedback with Educators in the Loop Effective feedback is essential for student learning but is time-intensive for teachers. We present LearnLens, a modular, LLM-based system that generates personalised, curriculum-aligned feedback in science education. LearnLens comprises three components: (1) an error-aware assessment module that captures nuanced reasoning errors; (2) a curriculum-grounded generation module that uses a structured, topic-linked memory chain rather than traditional similarity-based retrieval, improving relevance and reducing noise; and (3) an educator-in-the-loop interface for customisation and oversight. LearnLens addresses key challenges in existing systems, offering scalable, high-quality feedback that empowers both teachers and students.",Education
Carbon-Neutralized Joint User Association and Base Station Switching for Green Cellular Networks,"Mitigating climate change and its impacts is one of the sustainable development goals (SDGs) required by United Nations for an urgent action. Increasing carbon emissions due to human activities is the root cause to climate change. Telecommunication networks that provide service connectivity to mobile users contribute great amount of carbon emissions by consuming lots of non-renewable energy sources. Beyond the improvement on energy efficiency, to reduce the carbon footprint, telecom operators are increasing their adoption of renewable energy (e.g., wind power). The high variability of renewable energy in time and location; however, creates difficulties for operators when utilizing renewables for the reduction of carbon emissions. In this paper, we consider a heterogeneous network consisted of one macro base station (MBS) and multiple small base stations (SBSs) where each base station (BS) is powered by both of renewable and non-renewable energy. Different from the prior works that target on the total power consumption, we propose a novel scheme to minimize the carbon footprint of networks by dynamically switching the ONOFF modes of SBSs and adjusting the association between users and BSs to access renewables as much as possible. Our numerical analysis shows that the proposed scheme significantly reduces up to 86 of the nonrenewable energy consumption compared to two representative baselines.","Carbon-Neutralized Joint User Association and Base Station Switching for Green Cellular Networks Mitigating climate change and its impacts is one of the sustainable development goals (SDGs) required by United Nations for an urgent action. Increasing carbon emissions due to human activities is the root cause to climate change. Telecommunication networks that provide service connectivity to mobile users contribute great amount of carbon emissions by consuming lots of non-renewable energy sources. Beyond the improvement on energy efficiency, to reduce the carbon footprint, telecom operators are increasing their adoption of renewable energy (e.g., wind power). The high variability of renewable energy in time and location; however, creates difficulties for operators when utilizing renewables for the reduction of carbon emissions. In this paper, we consider a heterogeneous network consisted of one macro base station (MBS) and multiple small base stations (SBSs) where each base station (BS) is powered by both of renewable and non-renewable energy. Different from the prior works that target on the total power consumption, we propose a novel scheme to minimize the carbon footprint of networks by dynamically switching the ONOFF modes of SBSs and adjusting the association between users and BSs to access renewables as much as possible. Our numerical analysis shows that the proposed scheme significantly reduces up to 86 of the nonrenewable energy consumption compared to two representative baselines.",Environment
Learning the Past Tense of English Verbs: The Symbolic Pattern Associator vs. Connectionist Models,"Learning the past tense of English verbs - a seemingly minor aspect of language acquisition - has generated heated debates since 1986, and has become a landmark task for testing the adequacy of cognitive modeling. Several artificial neural networks (ANNs) have been implemented, and a challenge for better symbolic models has been posed. In this paper, we present a general-purpose Symbolic Pattern Associator (SPA) based upon the decision-tree learning algorithm ID3. We conduct extensive head-to-head comparisons on the generalization ability between ANN models and the SPA under different representations. We conclude that the SPA generalizes the past tense of unseen verbs better than ANN models by a wide margin, and we offer insights as to why this should be the case. We also discuss a new default strategy for decision-tree learning algorithms.","Learning the Past Tense of English Verbs: The Symbolic Pattern Associator vs. Connectionist Models Learning the past tense of English verbs - a seemingly minor aspect of language acquisition - has generated heated debates since 1986, and has become a landmark task for testing the adequacy of cognitive modeling. Several artificial neural networks (ANNs) have been implemented, and a challenge for better symbolic models has been posed. In this paper, we present a general-purpose Symbolic Pattern Associator (SPA) based upon the decision-tree learning algorithm ID3. We conduct extensive head-to-head comparisons on the generalization ability between ANN models and the SPA under different representations. We conclude that the SPA generalizes the past tense of unseen verbs better than ANN models by a wide margin, and we offer insights as to why this should be the case. We also discuss a new default strategy for decision-tree learning algorithms.",Technology
Numerical methods for optimal insurance demand under marked point processes shocks,"This paper deals with numerical solutions of maximizing expected utility from terminal wealth under a non-bankruptcy constraint. The wealth process is subject to shocks produced by a general marked point process. The problem of the agent is to derive the optimal insurance strategy which allows lowering the level of the shocks. This optimization problem is related to a suitable dual stochastic control problem in which the delicate boundary constraints disappear. In Mnif citemnif10, the dual value function is characterized as the unique viscosity solution of the corresponding Hamilton Jacobi Bellman Variational Inequality (HJBVI in short). We characterize the optimal insurance strategy by the solution of the variational inequality which we solve numerically by using an algorithm based on policy iterations.","Numerical methods for optimal insurance demand under marked point processes shocks This paper deals with numerical solutions of maximizing expected utility from terminal wealth under a non-bankruptcy constraint. The wealth process is subject to shocks produced by a general marked point process. The problem of the agent is to derive the optimal insurance strategy which allows lowering the level of the shocks. This optimization problem is related to a suitable dual stochastic control problem in which the delicate boundary constraints disappear. In Mnif citemnif10, the dual value function is characterized as the unique viscosity solution of the corresponding Hamilton Jacobi Bellman Variational Inequality (HJBVI in short). We characterize the optimal insurance strategy by the solution of the variational inequality which we solve numerically by using an algorithm based on policy iterations.",Finance
"Reform-Oriented Teaching of Introductory Statistics in the Health, Social and Behavioral Sciences-Historical Context and Rationale","There is widespread emphasis on reform in the teaching of introductory statistics at the college level. Underpinning this reform is a consensus among educators and practitioners that traditional curricular materials and pedagogical strategies have not been effective in promoting statistical literacy, a competency that is becoming increasingly necessary for effective decision-making and evidence-based practice. This paper explains the historical context of, and rationale for reform-oriented teaching of introductory statistics (at the college level) in the health, social and behavioral sciences (evidence-based disciplines). A firm understanding and appreciation of the basis for change in pedagogical approach is important, in order to facilitate commitment to reform, consensus building on appropriate strategies, and adoption and maintenance of best practices. In essence, reform-oriented pedagogy, in this context, is a function of the interaction among content, pedagogy, technology, and assessment. The challenge is to create an appropriate balance among these domains.","Reform-Oriented Teaching of Introductory Statistics in the Health, Social and Behavioral Sciences-Historical Context and Rationale There is widespread emphasis on reform in the teaching of introductory statistics at the college level. Underpinning this reform is a consensus among educators and practitioners that traditional curricular materials and pedagogical strategies have not been effective in promoting statistical literacy, a competency that is becoming increasingly necessary for effective decision-making and evidence-based practice. This paper explains the historical context of, and rationale for reform-oriented teaching of introductory statistics (at the college level) in the health, social and behavioral sciences (evidence-based disciplines). A firm understanding and appreciation of the basis for change in pedagogical approach is important, in order to facilitate commitment to reform, consensus building on appropriate strategies, and adoption and maintenance of best practices. In essence, reform-oriented pedagogy, in this context, is a function of the interaction among content, pedagogy, technology, and assessment. The challenge is to create an appropriate balance among these domains.",Education
Application of Unit Time Block Entropy to Fetal Distress Heart Rate,"Recently, multiple time scale characteristics of heart dynamics have received much attention for distinguishing healthy and pathologic cardiac systems. Despite structural peculiarities of the fetal cardiovascular system, the fetal heart rate(FHR) displays multiple time scale characteristics similar to the adult heart rate due to the autorhythmicity of its different oscillatory tissues and its interaction with other neural controllers. In this paper, we investigate the event and time scale characteristics of the normal and two pathologic fetal heart rate groups with the help of the new measure, called the Unit Time Block Entropy(UTBE), which approximates the entropy at each event and time scale based on symbolic dynamics. This method enables us to match the measurement time and the number of words between fetal heart rate data sets simultaneously. We find that in the small event scale and the large time scale, the normal fetus and the two pathologic fetus are completely distinguished. We also find that in the large event scale and the small time scale, the presumed distress fetus and the acidotic distress fetus are significantly distinguished.","Application of Unit Time Block Entropy to Fetal Distress Heart Rate Recently, multiple time scale characteristics of heart dynamics have received much attention for distinguishing healthy and pathologic cardiac systems. Despite structural peculiarities of the fetal cardiovascular system, the fetal heart rate(FHR) displays multiple time scale characteristics similar to the adult heart rate due to the autorhythmicity of its different oscillatory tissues and its interaction with other neural controllers. In this paper, we investigate the event and time scale characteristics of the normal and two pathologic fetal heart rate groups with the help of the new measure, called the Unit Time Block Entropy(UTBE), which approximates the entropy at each event and time scale based on symbolic dynamics. This method enables us to match the measurement time and the number of words between fetal heart rate data sets simultaneously. We find that in the small event scale and the large time scale, the normal fetus and the two pathologic fetus are completely distinguished. We also find that in the large event scale and the small time scale, the presumed distress fetus and the acidotic distress fetus are significantly distinguished.",Healthcare
Stratification of the phase clouds and statistical effects of the non-Markovity in chaotic time series of human gait for healthy people and Parkinson patients,"In this work we develop a new method of diagnosing the nervous system diseases and a new approach in studying human gait dynamics with the help of the theory of discrete non-Markov random processes. The stratification of the phase clouds and the statistical non-Markov effects in the time series of the dynamics of human gait are considered. We carried out the comparative analysis of the data of four age groups of healthy people: children (from 3 to 10 year olds), teenagers (from 11 to 14 year oulds), young people (from 21 up to 29 year oulds), elderly persons (from 71 to 77 year olds) and Parkinson patients. The full data set are analyzed with the help of the phase portraits of the four dynamic variables, the power spectra of the initial time correlation function and the memory functions of junior orders, the three first points in the spectra of the statistical non-Markov parameter. The received results allow to define the predisposition of the probationers to deflections in the central nervous system caused by Parkinsons disease. We have found out distinct differencies between the five submitted groups. On this basis we offer a new method of diagnostics and forecasting Parkinsons disease.","Stratification of the phase clouds and statistical effects of the non-Markovity in chaotic time series of human gait for healthy people and Parkinson patients In this work we develop a new method of diagnosing the nervous system diseases and a new approach in studying human gait dynamics with the help of the theory of discrete non-Markov random processes. The stratification of the phase clouds and the statistical non-Markov effects in the time series of the dynamics of human gait are considered. We carried out the comparative analysis of the data of four age groups of healthy people: children (from 3 to 10 year olds), teenagers (from 11 to 14 year oulds), young people (from 21 up to 29 year oulds), elderly persons (from 71 to 77 year olds) and Parkinson patients. The full data set are analyzed with the help of the phase portraits of the four dynamic variables, the power spectra of the initial time correlation function and the memory functions of junior orders, the three first points in the spectra of the statistical non-Markov parameter. The received results allow to define the predisposition of the probationers to deflections in the central nervous system caused by Parkinsons disease. We have found out distinct differencies between the five submitted groups. On this basis we offer a new method of diagnostics and forecasting Parkinsons disease.",Healthcare
Towards a Blockchain-based Software Engineering Education,"Blockchain technologies for rewards in education are gaining attraction as a promising approach to motivate student learning and promote academic achievement. By providing tangible rewards for educational attainment and engagement, such as digital tokens, educators can motivate learners to take a more active role in their learning and increase their sense of ownership and responsibility for their academic outcomes. In this context, this work proposes the Software Engineering Skill (SES) token as a way of rewarding students in order to improve their experiences in Software Engineering Education (SEE). We performed a proof of concept and conclude that SES token can be deployed in a platform to support SEE.","Towards a Blockchain-based Software Engineering Education Blockchain technologies for rewards in education are gaining attraction as a promising approach to motivate student learning and promote academic achievement. By providing tangible rewards for educational attainment and engagement, such as digital tokens, educators can motivate learners to take a more active role in their learning and increase their sense of ownership and responsibility for their academic outcomes. In this context, this work proposes the Software Engineering Skill (SES) token as a way of rewarding students in order to improve their experiences in Software Engineering Education (SEE). We performed a proof of concept and conclude that SES token can be deployed in a platform to support SEE.",Education
How to teach Quantum Mechanics,"In the spirit and style of John S. Bells well known paper on How to Teach Special Relativity it is argued, that a Bohmian pedagogyprovides a very useful tool to illustrate the relation between classical and quantum physics and illuminates the peculiar features of the latter.","How to teach Quantum Mechanics In the spirit and style of John S. Bells well known paper on How to Teach Special Relativity it is argued, that a Bohmian pedagogyprovides a very useful tool to illustrate the relation between classical and quantum physics and illuminates the peculiar features of the latter.",Education
Predicting Genetic Regulatory Response using Classification: Yeast Stress Response,"We present a novel classification-based algorithm called GeneClass for learning to predict gene regulatory response. Our approach is motivated by the hypothesis that in simple organisms such as Saccharomyces cerevisiae, we can learn a decision rule for predicting whether a gene is up- or down-regulated in a particular experiment based on (1) the presence of binding site subsequences (motifs) in the genes regulatory region and (2) the expression levels of regulators such as transcription factors in the experiment (parents). Thus our learning task integrates two qualitatively different data sources: genome-wide cDNA microarray data across multiple perturbation and mutant experiments along with motif profile data from regulatory sequences. Rather than focusing on the regression task of predicting real-valued gene expression measurements, GeneClass performs the classification task of predicting 1 and -1 labels, corresponding to up- and down-regulation beyond the levels of biological and measurement noise in microarray measurements. GeneClass uses the Adaboost learning algorithm with a margin-based generalization of decision trees called alternating decision trees. In computational experiments based on the Gasch S. cerevisiae dataset, we show that the GeneClass method predicts up- and down-regulation on held-out experiments with high accuracy. We explore a range of experimental setups related to environmental stress response, and we retrieve important regulators, binding site motifs, and relationships between regulators and binding sites that are known to be associated to specific stress response pathways. Our method thus provides predictive hypotheses, suggests biological experiments, and provides interpretable insight into the structure of genetic regulatory networks.","Predicting Genetic Regulatory Response using Classification: Yeast Stress Response We present a novel classification-based algorithm called GeneClass for learning to predict gene regulatory response. Our approach is motivated by the hypothesis that in simple organisms such as Saccharomyces cerevisiae, we can learn a decision rule for predicting whether a gene is up- or down-regulated in a particular experiment based on (1) the presence of binding site subsequences (motifs) in the genes regulatory region and (2) the expression levels of regulators such as transcription factors in the experiment (parents). Thus our learning task integrates two qualitatively different data sources: genome-wide cDNA microarray data across multiple perturbation and mutant experiments along with motif profile data from regulatory sequences. Rather than focusing on the regression task of predicting real-valued gene expression measurements, GeneClass performs the classification task of predicting 1 and -1 labels, corresponding to up- and down-regulation beyond the levels of biological and measurement noise in microarray measurements. GeneClass uses the Adaboost learning algorithm with a margin-based generalization of decision trees called alternating decision trees. In computational experiments based on the Gasch S. cerevisiae dataset, we show that the GeneClass method predicts up- and down-regulation on held-out experiments with high accuracy. We explore a range of experimental setups related to environmental stress response, and we retrieve important regulators, binding site motifs, and relationships between regulators and binding sites that are known to be associated to specific stress response pathways. Our method thus provides predictive hypotheses, suggests biological experiments, and provides interpretable insight into the structure of genetic regulatory networks.",Healthcare
An Effective Incorporating Heterogeneous Knowledge Curriculum Learning for Sequence Labeling,"Sequence labeling models often benefit from incorporating external knowledge. However, this practice introduces data heterogeneity and complicates the model with additional modules, leading to increased expenses for training a high-performing model. To address this challenge, we propose a two-stage curriculum learning (TCL) framework specifically designed for sequence labeling tasks. The TCL framework enhances training by gradually introducing data instances from easy to hard, aiming to improve both performance and training speed. Furthermore, we explore different metrics for assessing the difficulty levels of sequence labeling tasks. Through extensive experimentation on six Chinese word segmentation (CWS) and Part-of-speech tagging (POS) datasets, we demonstrate the effectiveness of our model in enhancing the performance of sequence labeling models. Additionally, our analysis indicates that TCL accelerates training and alleviates the slow training problem associated with complex models.","An Effective Incorporating Heterogeneous Knowledge Curriculum Learning for Sequence Labeling Sequence labeling models often benefit from incorporating external knowledge. However, this practice introduces data heterogeneity and complicates the model with additional modules, leading to increased expenses for training a high-performing model. To address this challenge, we propose a two-stage curriculum learning (TCL) framework specifically designed for sequence labeling tasks. The TCL framework enhances training by gradually introducing data instances from easy to hard, aiming to improve both performance and training speed. Furthermore, we explore different metrics for assessing the difficulty levels of sequence labeling tasks. Through extensive experimentation on six Chinese word segmentation (CWS) and Part-of-speech tagging (POS) datasets, we demonstrate the effectiveness of our model in enhancing the performance of sequence labeling models. Additionally, our analysis indicates that TCL accelerates training and alleviates the slow training problem associated with complex models.",Education
Constrained-Hamiltonian Shallow-Water Dynamics on the Sphere,"Salmons nearly geostrophic model for rotating shallow-water flow is derived in full spherical geometry. The model, which results upon constraining the velocity field to the height field in Hamiltons principle for rotating shallow-water dynamics, constitutes an important prototype of Hamiltonian balanced models. Instead of Salmons original approach, which consists in taking variations of particle paths at fixed Lagrangian labels and time, Holms approach is considered here, namely variations are taken on Lagrangian particle labels at fixed Eulerian positions and time. Unlike the classical quasigeostrophic model, Salmons is found to be sensitive to the differences between geographic and geodesic coordinates. One consequence of this result is that the beta  plane approximation, which is included in Salmons original derivation, is not consistent for this class of model.","Constrained-Hamiltonian Shallow-Water Dynamics on the Sphere Salmons nearly geostrophic model for rotating shallow-water flow is derived in full spherical geometry. The model, which results upon constraining the velocity field to the height field in Hamiltons principle for rotating shallow-water dynamics, constitutes an important prototype of Hamiltonian balanced models. Instead of Salmons original approach, which consists in taking variations of particle paths at fixed Lagrangian labels and time, Holms approach is considered here, namely variations are taken on Lagrangian particle labels at fixed Eulerian positions and time. Unlike the classical quasigeostrophic model, Salmons is found to be sensitive to the differences between geographic and geodesic coordinates. One consequence of this result is that the beta  plane approximation, which is included in Salmons original derivation, is not consistent for this class of model.",Environment
Overcoming Barriers to Engagement with Educational Video Games for Self-Directed Learning: A Mixed-Methods Case Study,"Research has established increased engagement and positive behavioral, attitudinal, and learning outcomes from educational games. Although engagement begets these benefits, there is a lack of research on how students engage with educational games, especially when self-directed. Additionally, research on the effects of engagement on performance is conflicting. This study aimed to identify barriers to students self-directed engagement with two anatomy educational games, methods to overcome identified barriers, and the impact of engagement on learning performance. Employing the within-case and cross-case approach and triangulation of data, four educational-game specific barriers emerged (from most common to least common): 1) negative perceptions of educational games, 2) incompatible audience, 3) incompatible difficulty, and 4) price. Path analysis found that engagement may have insignificant, positive effects on performance. These findings indicate that students self-directed engagement with educational games is impeded by unique barriers and engagement may not be vital for performance. Suggestions for game design and marketing emerged to help developers, teachers, and researchers overcome identified barriers.","Overcoming Barriers to Engagement with Educational Video Games for Self-Directed Learning: A Mixed-Methods Case Study Research has established increased engagement and positive behavioral, attitudinal, and learning outcomes from educational games. Although engagement begets these benefits, there is a lack of research on how students engage with educational games, especially when self-directed. Additionally, research on the effects of engagement on performance is conflicting. This study aimed to identify barriers to students self-directed engagement with two anatomy educational games, methods to overcome identified barriers, and the impact of engagement on learning performance. Employing the within-case and cross-case approach and triangulation of data, four educational-game specific barriers emerged (from most common to least common): 1) negative perceptions of educational games, 2) incompatible audience, 3) incompatible difficulty, and 4) price. Path analysis found that engagement may have insignificant, positive effects on performance. These findings indicate that students self-directed engagement with educational games is impeded by unique barriers and engagement may not be vital for performance. Suggestions for game design and marketing emerged to help developers, teachers, and researchers overcome identified barriers.",Education
Financial Market Trend Forecasting and Performance Analysis Using LSTM,"The financial market trend forecasting method is emerging as a hot topic in financial markets today. Many challenges still currently remain, and various researches related thereto have been actively conducted. Especially, recent research of neural network-based financial market trend prediction has attracted much attention. However, previous researches do not deal with the financial market forecasting method based on LSTM which has good performance in time series data. There is also a lack of comparative analysis in the performance of neural network-based prediction techniques and traditional prediction techniques. In this paper, we propose a financial market trend forecasting method using LSTM and analyze the performance with existing financial market trend forecasting methods through experiments. This method prepares the input data set through the data preprocessing process so as to reflect all the fundamental data, technical data and qualitative data used in the financial data analysis, and makes comprehensive financial market analysis through LSTM. In this paper, we experiment and compare performances of existing financial market trend forecasting models, and performance according to the financial market environment. In addition, we implement the proposed method using open sources and platform and forecast financial market trends using various financial data indicators.","Financial Market Trend Forecasting and Performance Analysis Using LSTM The financial market trend forecasting method is emerging as a hot topic in financial markets today. Many challenges still currently remain, and various researches related thereto have been actively conducted. Especially, recent research of neural network-based financial market trend prediction has attracted much attention. However, previous researches do not deal with the financial market forecasting method based on LSTM which has good performance in time series data. There is also a lack of comparative analysis in the performance of neural network-based prediction techniques and traditional prediction techniques. In this paper, we propose a financial market trend forecasting method using LSTM and analyze the performance with existing financial market trend forecasting methods through experiments. This method prepares the input data set through the data preprocessing process so as to reflect all the fundamental data, technical data and qualitative data used in the financial data analysis, and makes comprehensive financial market analysis through LSTM. In this paper, we experiment and compare performances of existing financial market trend forecasting models, and performance according to the financial market environment. In addition, we implement the proposed method using open sources and platform and forecast financial market trends using various financial data indicators.",Finance
A new framework for climate sensitivity and prediction: a modelling perspective,"The sensitivity of climate models to increasing CO2 concentration and the climate response at decadal time scales are still major factors of uncertainty for the assessment of the long and short term effects of anthropogenic climate change. While the relative slow progress on these issues is partly due to the inherent inaccuracies of numerical climate models, this also hints at the need for stronger theoretical foundations to the problem of studying climate sensitivity and performing climate change predictions with numerical models. Here we demonstrate that it is possible to use Ruelles response theory to predict the impact of an arbitrary CO2 forcing scenario on the global surface temperature of a general circulation model. Response theory puts the concept of climate sensitivity on firm theoretical grounds, and addresses rigorously the problem of predictability at different time scales. Conceptually, our results show that performing climate change experiments with general circulation models is a well defined problem from a physical and mathematical point of view. Practically, our results show that considering one single CO2 forcing scenario is enough to construct operators able to predict the response of climatic observables to any other CO2 forcing scenario, without the need to perform additional numerical simulations. We also introduce a general relationship between climate sensitivity and climate response at different time scales, thus providing an explicit definition of the inertia of the system at different time scales. While what we report here refers to the linear response, the general theory allows for treating nonlinear effects as well. Our results pave the way for redesigning and interpreting climate change experiments from a radically new perspective.","A new framework for climate sensitivity and prediction: a modelling perspective The sensitivity of climate models to increasing CO2 concentration and the climate response at decadal time scales are still major factors of uncertainty for the assessment of the long and short term effects of anthropogenic climate change. While the relative slow progress on these issues is partly due to the inherent inaccuracies of numerical climate models, this also hints at the need for stronger theoretical foundations to the problem of studying climate sensitivity and performing climate change predictions with numerical models. Here we demonstrate that it is possible to use Ruelles response theory to predict the impact of an arbitrary CO2 forcing scenario on the global surface temperature of a general circulation model. Response theory puts the concept of climate sensitivity on firm theoretical grounds, and addresses rigorously the problem of predictability at different time scales. Conceptually, our results show that performing climate change experiments with general circulation models is a well defined problem from a physical and mathematical point of view. Practically, our results show that considering one single CO2 forcing scenario is enough to construct operators able to predict the response of climatic observables to any other CO2 forcing scenario, without the need to perform additional numerical simulations. We also introduce a general relationship between climate sensitivity and climate response at different time scales, thus providing an explicit definition of the inertia of the system at different time scales. While what we report here refers to the linear response, the general theory allows for treating nonlinear effects as well. Our results pave the way for redesigning and interpreting climate change experiments from a radically new perspective.",Environment
Diagnostics of Rational Expectation Financial Bubbles with Stochastic Mean-Reverting Termination Times,"We propose two rational expectation models of transient financial bubbles with heterogeneous arbitrageurs and positive feedbacks leading to self-reinforcing transient stochastic faster-than-exponential price dynamics. As a result of the nonlinear feedbacks, the termination of a bubble is found to be characterized by a finite-time singularity in the bubble price formation process ending at some potential critical time tildet_c, which follows a mean-reversing stationary dynamics. Because of the heterogeneity of the rational agents expectations, there is a synchronization problem for the optimal exit times determined by these arbitrageurs, which leads to the survival of the bubble almost all the way to its theoretical end time. The explicit exact analytical solutions of the two models provide nonlinear transformations which allow us to develop novel tests for the presence of bubbles in financial time series. Avoiding the difficult problem of parameter estimation of the stochastic differential equation describing the price dynamics, the derived operational procedures allow us to diagnose bubbles that are in the making and to forecast their termination time. The tests performed on three financial markets, the US SP500 index from 1 February 1980 to 31 October 2008, the US NASDAQ composite index from 1 January 1980 to 31 July 2008 and the Hong Kong Hang Seng index from 1 December 1986 to 30 November 2008, suggest the feasibility of advance bubble warning.","Diagnostics of Rational Expectation Financial Bubbles with Stochastic Mean-Reverting Termination Times We propose two rational expectation models of transient financial bubbles with heterogeneous arbitrageurs and positive feedbacks leading to self-reinforcing transient stochastic faster-than-exponential price dynamics. As a result of the nonlinear feedbacks, the termination of a bubble is found to be characterized by a finite-time singularity in the bubble price formation process ending at some potential critical time tildet_c, which follows a mean-reversing stationary dynamics. Because of the heterogeneity of the rational agents expectations, there is a synchronization problem for the optimal exit times determined by these arbitrageurs, which leads to the survival of the bubble almost all the way to its theoretical end time. The explicit exact analytical solutions of the two models provide nonlinear transformations which allow us to develop novel tests for the presence of bubbles in financial time series. Avoiding the difficult problem of parameter estimation of the stochastic differential equation describing the price dynamics, the derived operational procedures allow us to diagnose bubbles that are in the making and to forecast their termination time. The tests performed on three financial markets, the US SP500 index from 1 February 1980 to 31 October 2008, the US NASDAQ composite index from 1 January 1980 to 31 July 2008 and the Hong Kong Hang Seng index from 1 December 1986 to 30 November 2008, suggest the feasibility of advance bubble warning.",Finance
Learning Analytics in Higher Education -- Exploring Students and Teachers Expectations in Germany,"Technology enhanced learning analytics has the potential to play a significant role in higher education in the future. Opinions and expectations towards technology and learning analytics, thus, are vital to consider for institutional developments in higher education institutions. The Sheila framework offers instruments to yield exploratory knowledge about stakeholder aspirations towards technology, such as learning analytics in higher education. The sample of the study consists of students (N  1169) and teachers (N  497) at a higher education institution in Germany. Using self-report questionnaires, we assessed students and teachers attitudes towards learning analytics in higher education teaching, comparing ideal and expected circumstances. We report results on the attitudes of students, teachers, as well as comparisons of the two groups and different disciplines. We discuss the results with regard to practical implications for the implementation and further developments of learning analytics in higher education.","Learning Analytics in Higher Education -- Exploring Students and Teachers Expectations in Germany Technology enhanced learning analytics has the potential to play a significant role in higher education in the future. Opinions and expectations towards technology and learning analytics, thus, are vital to consider for institutional developments in higher education institutions. The Sheila framework offers instruments to yield exploratory knowledge about stakeholder aspirations towards technology, such as learning analytics in higher education. The sample of the study consists of students (N  1169) and teachers (N  497) at a higher education institution in Germany. Using self-report questionnaires, we assessed students and teachers attitudes towards learning analytics in higher education teaching, comparing ideal and expected circumstances. We report results on the attitudes of students, teachers, as well as comparisons of the two groups and different disciplines. We discuss the results with regard to practical implications for the implementation and further developments of learning analytics in higher education.",Education
Why Markets are Inefficient: A Gambling Theory of Financial Markets For Practitioners and Theorists,"The purpose of this article is to propose a new theory, the Strategic Analysis of Financial Markets (SAFM) theory, that explains the operation of financial markets using the analytical perspective of an enlightened gambler. The gambler understands that all opportunities for superior performance arise from suboptimal decisions by humans, but understands also that knowledge of human decision making alone is not enough to understand market behavior --- one must still model how those decisions lead to market prices. Thus are there three parts to the model: gambling theory, human decision making, and strategic problem solving. A new theory is necessary because at this writing in 2017, there is no theory of financial markets acceptable to both practitioners and theorists. Theorists efficient market theory, for example, cannot explain bubbles and crashes nor the exceptional returns of famous investors and speculators such as Warren Buffett and George Soros. At the same time, a new theory must be sufficiently quantitative, explain market anomalies and provide predictions in order to satisfy theorists. It is hoped that the SAFM framework will meet these requirements.","Why Markets are Inefficient: A Gambling Theory of Financial Markets For Practitioners and Theorists The purpose of this article is to propose a new theory, the Strategic Analysis of Financial Markets (SAFM) theory, that explains the operation of financial markets using the analytical perspective of an enlightened gambler. The gambler understands that all opportunities for superior performance arise from suboptimal decisions by humans, but understands also that knowledge of human decision making alone is not enough to understand market behavior --- one must still model how those decisions lead to market prices. Thus are there three parts to the model: gambling theory, human decision making, and strategic problem solving. A new theory is necessary because at this writing in 2017, there is no theory of financial markets acceptable to both practitioners and theorists. Theorists efficient market theory, for example, cannot explain bubbles and crashes nor the exceptional returns of famous investors and speculators such as Warren Buffett and George Soros. At the same time, a new theory must be sufficiently quantitative, explain market anomalies and provide predictions in order to satisfy theorists. It is hoped that the SAFM framework will meet these requirements.",Finance
AI for Accessible Education: Personalized Audio-Based Learning for Blind Students,"Blind and visually impaired (BVI) students face significant challenges in traditional educational settings. While screen readers and braille materials offer some accessibility, they often lack interactivity and real-time adaptability to individual learning needs. This paper presents Audemy, an AI-powered audio-based learning platform designed to provide personalized, accessible, and engaging educational experiences for BVI students. Audemy uses adaptive learning techniques to customize content based on student accuracy, pacing preferences, and engagement patterns. The platform has been iteratively developed with input from over 20 educators specializing in accessibility and currently serves over 2,000 BVI students. Educator insights show key considerations for accessible AI, including the importance of engagement, intuitive design, compatibility with existing assistive technologies, and the role of positive reinforcement in maintaining student motivation. Beyond accessibility, this paper explores the ethical implications of AI in education, emphasizing data privacy, security, and transparency. Audemy demonstrates how AI can empower BVI students with personalized and equitable learning opportunities, advancing the broader goal of inclusive education.","AI for Accessible Education: Personalized Audio-Based Learning for Blind Students Blind and visually impaired (BVI) students face significant challenges in traditional educational settings. While screen readers and braille materials offer some accessibility, they often lack interactivity and real-time adaptability to individual learning needs. This paper presents Audemy, an AI-powered audio-based learning platform designed to provide personalized, accessible, and engaging educational experiences for BVI students. Audemy uses adaptive learning techniques to customize content based on student accuracy, pacing preferences, and engagement patterns. The platform has been iteratively developed with input from over 20 educators specializing in accessibility and currently serves over 2,000 BVI students. Educator insights show key considerations for accessible AI, including the importance of engagement, intuitive design, compatibility with existing assistive technologies, and the role of positive reinforcement in maintaining student motivation. Beyond accessibility, this paper explores the ethical implications of AI in education, emphasizing data privacy, security, and transparency. Audemy demonstrates how AI can empower BVI students with personalized and equitable learning opportunities, advancing the broader goal of inclusive education.",Education
Control Strategies for Microgrids with Renewable Energy Generation and Battery Energy Storage Systems,"In this report, several control strategies for microgrids with renewable energy resources and battery energy storage systems are proposed. Renewable energy has the potential to reduce global carbon emissions, while higher penetration of renewable energy is hindered by the inherent variability and intermittency of renewable resources. This motivates the development of microgrids supplied by renewable energy resources. With proper control of storage units and communications with the energy market, the non-dispatchable energy can be regulated to reduce the difficulty of power scheduling in the grid operation. Battery energy storage systems (BESS) are widely used to make renewable energy more controllable and usable on demand. With suitable prediction techniques and control algorithms, BESS can store part of the generated renewable energy and supply some stored energy at different periods, so that the actual power dispatched can meet the required operating criteria. The control algorithms introduced in this report are developed to maximize the profit and minimize the energy cost in microgrids, which require predicted data on renewable power, temperature, electricity price, and load demand.","Control Strategies for Microgrids with Renewable Energy Generation and Battery Energy Storage Systems In this report, several control strategies for microgrids with renewable energy resources and battery energy storage systems are proposed. Renewable energy has the potential to reduce global carbon emissions, while higher penetration of renewable energy is hindered by the inherent variability and intermittency of renewable resources. This motivates the development of microgrids supplied by renewable energy resources. With proper control of storage units and communications with the energy market, the non-dispatchable energy can be regulated to reduce the difficulty of power scheduling in the grid operation. Battery energy storage systems (BESS) are widely used to make renewable energy more controllable and usable on demand. With suitable prediction techniques and control algorithms, BESS can store part of the generated renewable energy and supply some stored energy at different periods, so that the actual power dispatched can meet the required operating criteria. The control algorithms introduced in this report are developed to maximize the profit and minimize the energy cost in microgrids, which require predicted data on renewable power, temperature, electricity price, and load demand.",Environment
A Contextual-bandit-based Approach for Informed Decision-making in Clinical Trials,"Clinical trials involving multiple treatments utilize randomization of the treatment assignments to enable the evaluation of treatment efficacies in an unbiased manner. Such evaluation is performed in post hoc studies that usually use supervised-learning methods that rely on large amounts of data collected in a randomized fashion. That approach often proves to be suboptimal in that some participants may suffer and even die as a result of having not received the most appropriate treatments during the trial. Reinforcement-learning methods improve the situation by making it possible to learn the treatment efficacies dynamically during the course of the trial, and to adapt treatment assignments accordingly. Recent efforts using textitmulti-arm bandits, a type of reinforcement-learning methods, have focused on maximizing clinical outcomes for a population that was assumed to be homogeneous. However, those approaches have failed to account for the variability among participants that is becoming increasingly evident as a result of recent clinical-trial-based studies. We present a contextual-bandit-based online treatment optimization algorithm that, in choosing treatments for new participants in the study, takes into account not only the maximization of the clinical outcomes but also the patient characteristics. We evaluated our algorithm using a real clinical trial dataset from the International Stroke Trial. The results of our retrospective analysis indicate that the proposed approach performs significantly better than either a random assignment of treatments (the current gold standard) or a multi-arm-bandit-based approach, providing substantial gains in the percentage of participants who are assigned the most suitable treatments. The contextual-bandit and multi-arm bandit approaches provide 72.63 and 64.34 gains, respectively, compared to a random assignment.","A Contextual-bandit-based Approach for Informed Decision-making in Clinical Trials Clinical trials involving multiple treatments utilize randomization of the treatment assignments to enable the evaluation of treatment efficacies in an unbiased manner. Such evaluation is performed in post hoc studies that usually use supervised-learning methods that rely on large amounts of data collected in a randomized fashion. That approach often proves to be suboptimal in that some participants may suffer and even die as a result of having not received the most appropriate treatments during the trial. Reinforcement-learning methods improve the situation by making it possible to learn the treatment efficacies dynamically during the course of the trial, and to adapt treatment assignments accordingly. Recent efforts using textitmulti-arm bandits, a type of reinforcement-learning methods, have focused on maximizing clinical outcomes for a population that was assumed to be homogeneous. However, those approaches have failed to account for the variability among participants that is becoming increasingly evident as a result of recent clinical-trial-based studies. We present a contextual-bandit-based online treatment optimization algorithm that, in choosing treatments for new participants in the study, takes into account not only the maximization of the clinical outcomes but also the patient characteristics. We evaluated our algorithm using a real clinical trial dataset from the International Stroke Trial. The results of our retrospective analysis indicate that the proposed approach performs significantly better than either a random assignment of treatments (the current gold standard) or a multi-arm-bandit-based approach, providing substantial gains in the percentage of participants who are assigned the most suitable treatments. The contextual-bandit and multi-arm bandit approaches provide 72.63 and 64.34 gains, respectively, compared to a random assignment.",Healthcare
Regulation Simulation,"A deterministic trading strategy by a representative investor on a single market asset, which generates complex and realistic returns with its first four moments similar to the empirical values of European stock indices, is used to simulate the effects of financial regulation that either pricks bubbles, props up crashes, or both. The results suggest that regulation makes the market process appear more Gaussian and less complex, with the difference more pronounced for more frequent intervention, though particular periods can be worse than the non-regulated version, and that pricking bubbles and propping up crashes are not symmetrical.","Regulation Simulation A deterministic trading strategy by a representative investor on a single market asset, which generates complex and realistic returns with its first four moments similar to the empirical values of European stock indices, is used to simulate the effects of financial regulation that either pricks bubbles, props up crashes, or both. The results suggest that regulation makes the market process appear more Gaussian and less complex, with the difference more pronounced for more frequent intervention, though particular periods can be worse than the non-regulated version, and that pricking bubbles and propping up crashes are not symmetrical.",Finance
The Day-night Variation of Cosmic Rays Instensity at Sea Level Under the Influence of Meteorological Fronts and Troughs,"The day-night variation of cosmic rays intensity at sea level has been observed by a simple G-M counter telescope. We preform two 5 hours counting during the day and night and find that the pattern of variation is closely related to the atmospheric disturbance, especially when the observation station is being affected by meterological front or trough. Such effects may lasts for a few days until the trough or fronts weakened. The pattern of variation may be negatively correlated to the altitude of the 0 degC level of the atmosphere. This is closely related to the muon decay effect in the atmosphere. The phenomenon should be further investigation for possible application in weather forecasting.","The Day-night Variation of Cosmic Rays Instensity at Sea Level Under the Influence of Meteorological Fronts and Troughs The day-night variation of cosmic rays intensity at sea level has been observed by a simple G-M counter telescope. We preform two 5 hours counting during the day and night and find that the pattern of variation is closely related to the atmospheric disturbance, especially when the observation station is being affected by meterological front or trough. Such effects may lasts for a few days until the trough or fronts weakened. The pattern of variation may be negatively correlated to the altitude of the 0 degC level of the atmosphere. This is closely related to the muon decay effect in the atmosphere. The phenomenon should be further investigation for possible application in weather forecasting.",Environment
The Basic Kak Neural Network with Complex Inputs,"The Kak family of neural networks is able to learn patterns quickly, and this speed of learning can be a decisive advantage over other competing models in many applications. Amongst the implementations of these networks are those using reconfigurable networks, FPGAs and optical networks. In some applications, it is useful to use complex data, and it is with that in mind that this introduction to the basic Kak network with complex inputs is being presented. The training algorithm is prescriptive and the network weights are assigned simply upon examining the inputs. The input is mapped using quaternary encoding for purpose of efficienty. This network family is part of a larger hierarchy of learning schemes that include quantum models.","The Basic Kak Neural Network with Complex Inputs The Kak family of neural networks is able to learn patterns quickly, and this speed of learning can be a decisive advantage over other competing models in many applications. Amongst the implementations of these networks are those using reconfigurable networks, FPGAs and optical networks. In some applications, it is useful to use complex data, and it is with that in mind that this introduction to the basic Kak network with complex inputs is being presented. The training algorithm is prescriptive and the network weights are assigned simply upon examining the inputs. The input is mapped using quaternary encoding for purpose of efficienty. This network family is part of a larger hierarchy of learning schemes that include quantum models.",Technology
"A Conceptual Model and Methodology for Sustainability-aware, IoT-enhanced Business Processes","The real-time data collection and automation capabilities offered by the Internet of Things (IoT) are revolutionizing and transforming Business Processes (BPs) into IoT-enhanced BPs, showing high potential for improving sustainability. Although already studied in Business Process Management (BPM), sustainability research has primarily focused on environmental concerns. However, achieving a holistic and lasting impact requires a systematic approach to address sustainability beyond the environmental dimension. This work proposes a conceptual model and a structured methodology with the goal of analyzing the potential of IoT to measure and improve the sustainability of BPs. The conceptual model formally represents key sustainability concepts, linking BPM and IoT by highlighting how IoT devices support and contribute to sustainability. The methodology guides the systematic analysis of existing BPs, identifies opportunities, and implements sustainability-aware, IoT-enhanced BPs. The approach is illustrated through a running example from the tourism domain and a case study in healthcare.","A Conceptual Model and Methodology for Sustainability-aware, IoT-enhanced Business Processes The real-time data collection and automation capabilities offered by the Internet of Things (IoT) are revolutionizing and transforming Business Processes (BPs) into IoT-enhanced BPs, showing high potential for improving sustainability. Although already studied in Business Process Management (BPM), sustainability research has primarily focused on environmental concerns. However, achieving a holistic and lasting impact requires a systematic approach to address sustainability beyond the environmental dimension. This work proposes a conceptual model and a structured methodology with the goal of analyzing the potential of IoT to measure and improve the sustainability of BPs. The conceptual model formally represents key sustainability concepts, linking BPM and IoT by highlighting how IoT devices support and contribute to sustainability. The methodology guides the systematic analysis of existing BPs, identifies opportunities, and implements sustainability-aware, IoT-enhanced BPs. The approach is illustrated through a running example from the tourism domain and a case study in healthcare.",Environment
Visual Character Recognition using Artificial Neural Networks,"The recognition of optical characters is known to be one of the earliest applications of Artificial Neural Networks, which partially emulate human thinking in the domain of artificial intelligence. In this paper, a simplified neural approach to recognition of optical or visual characters is portrayed and discussed. The document is expected to serve as a resource for learners and amateur investigators in pattern recognition, neural networking and related disciplines.","Visual Character Recognition using Artificial Neural Networks The recognition of optical characters is known to be one of the earliest applications of Artificial Neural Networks, which partially emulate human thinking in the domain of artificial intelligence. In this paper, a simplified neural approach to recognition of optical or visual characters is portrayed and discussed. The document is expected to serve as a resource for learners and amateur investigators in pattern recognition, neural networking and related disciplines.",Technology
Stability Analysis for Regularized Least Squares Regression,"We discuss stability for a class of learning algorithms with respect to noisy labels. The algorithms we consider are for regression, and they involve the minimization of regularized risk functionals, such as L(f) : 1N sum_i (f(x_i)-y_i)2 lambda f_H2. We shall call the algorithm stable if, when y_i is a noisy version of f(x_i) for some function f in H, the output of the algorithm converges to f as the regularization term and noise simultaneously vanish. We consider two flavors of this problem, one where a data set of N points remains fixed, and the other where N - infinity. For the case where N - infinity, we give conditions for convergence to f_E (the function which is the expectation of y(x) for each x), as lambda - 0. For the fixed N case, we describe the limiting non-noisy, non-regularized function f, and give conditions for convergence. In the process, we develop a set of tools for dealing with functionals such as L(f), which are applicable to many other problems in learning theory.","Stability Analysis for Regularized Least Squares Regression We discuss stability for a class of learning algorithms with respect to noisy labels. The algorithms we consider are for regression, and they involve the minimization of regularized risk functionals, such as L(f) : 1N sum_i (f(x_i)-y_i)2 lambda f_H2. We shall call the algorithm stable if, when y_i is a noisy version of f(x_i) for some function f in H, the output of the algorithm converges to f as the regularization term and noise simultaneously vanish. We consider two flavors of this problem, one where a data set of N points remains fixed, and the other where N - infinity. For the case where N - infinity, we give conditions for convergence to f_E (the function which is the expectation of y(x) for each x), as lambda - 0. For the fixed N case, we describe the limiting non-noisy, non-regularized function f, and give conditions for convergence. In the process, we develop a set of tools for dealing with functionals such as L(f), which are applicable to many other problems in learning theory.",Technology
Geometrical Complexity of Classification Problems,"Despite encouraging recent progresses in ensemble approaches, classification methods seem to have reached a plateau in development. Further advances depend on a better understanding of geometrical and topological characteristics of point sets in high-dimensional spaces, the preservation of such characteristics under feature transformations and sampling processes, and their interaction with geometrical models used in classifiers. We discuss an attempt to measure such properties from data sets and relate them to classifier accuracies.","Geometrical Complexity of Classification Problems Despite encouraging recent progresses in ensemble approaches, classification methods seem to have reached a plateau in development. Further advances depend on a better understanding of geometrical and topological characteristics of point sets in high-dimensional spaces, the preservation of such characteristics under feature transformations and sampling processes, and their interaction with geometrical models used in classifiers. We discuss an attempt to measure such properties from data sets and relate them to classifier accuracies.",Technology
"Analisis de la incidencia de la inversion extranjera directa y la inversion nacional, en el crecimiento economico de Chile","The research aims to assess the impact of foreign direct investment (FDI) and domestic investment on Chiles economic growth. By elucidating the relationship between FDI and domestic investment, the study contributes valuable insights for economic policy formulation and future investments. The findings hold significance in shaping Chiles international perception as an investment destination, potentially influencing its standing in the global economic landscape. Demonstrating that FDI is a significant driver of economic growth could enhance confidence among foreign investors. The projects importance lies in contributing to economic knowledge and guiding strategic decisions for sustainable economic growth in Chile. Understanding the interplay of FDI and domestic investment allows for a balanced approach, promoting stable economic development and mitigating issues like excessive reliance on foreign investment. The study highlights the theory of internationalization as a conceptual framework for understanding the motives and strategies of multinational companies investing abroad. Leveraging data from sources like the Central Bank of Chile, the research analyzes variables such as Chiles economic growth (GDP), FDI, and domestic investment. The hypothesis posits a significant long-term causal relationship between FDI, National Investment (NI), and Chiles Economic Growth (GDP). Statistical analysis using the Eviews 6 software tool confirms that attracting foreign investments and promoting internal investment are imperative for sustainable economic growth in Chile.","Analisis de la incidencia de la inversion extranjera directa y la inversion nacional, en el crecimiento economico de Chile The research aims to assess the impact of foreign direct investment (FDI) and domestic investment on Chiles economic growth. By elucidating the relationship between FDI and domestic investment, the study contributes valuable insights for economic policy formulation and future investments. The findings hold significance in shaping Chiles international perception as an investment destination, potentially influencing its standing in the global economic landscape. Demonstrating that FDI is a significant driver of economic growth could enhance confidence among foreign investors. The projects importance lies in contributing to economic knowledge and guiding strategic decisions for sustainable economic growth in Chile. Understanding the interplay of FDI and domestic investment allows for a balanced approach, promoting stable economic development and mitigating issues like excessive reliance on foreign investment. The study highlights the theory of internationalization as a conceptual framework for understanding the motives and strategies of multinational companies investing abroad. Leveraging data from sources like the Central Bank of Chile, the research analyzes variables such as Chiles economic growth (GDP), FDI, and domestic investment. The hypothesis posits a significant long-term causal relationship between FDI, National Investment (NI), and Chiles Economic Growth (GDP). Statistical analysis using the Eviews 6 software tool confirms that attracting foreign investments and promoting internal investment are imperative for sustainable economic growth in Chile.",Finance
Graph-based Local Climate Classification in Iran,"In this paper, we introduce a novel graph-based method to classify the regions with similar climate in a local area. We refer our proposed method as Graph Partition Based Method (GPBM). Our proposed method attempts to overcome the shortcomings of the current state-of-the-art methods in the literature. It has no limit on the number of variables that can be used and also preserves the nature of climate data. To illustrate the capability of our proposed algorithm, we benchmark its performance with other state-of-the-art climate classification techniques. The climate data is collected from 24 synoptic stations in Fars province in southern Iran. The data includes seven climate variables stored as time series from 1951 to 2017. Our results exhibit that our proposed method performs a more realistic climate classification with less computational time. It can save more information during the climate classification process and is therefore efficient in further data analysis. Furthermore, using our method, we can introduce seasonal graphs to better investigate seasonal climate changes. To the best of our knowledge, our proposed method is the first graph-based climate classification system.","Graph-based Local Climate Classification in Iran In this paper, we introduce a novel graph-based method to classify the regions with similar climate in a local area. We refer our proposed method as Graph Partition Based Method (GPBM). Our proposed method attempts to overcome the shortcomings of the current state-of-the-art methods in the literature. It has no limit on the number of variables that can be used and also preserves the nature of climate data. To illustrate the capability of our proposed algorithm, we benchmark its performance with other state-of-the-art climate classification techniques. The climate data is collected from 24 synoptic stations in Fars province in southern Iran. The data includes seven climate variables stored as time series from 1951 to 2017. Our results exhibit that our proposed method performs a more realistic climate classification with less computational time. It can save more information during the climate classification process and is therefore efficient in further data analysis. Furthermore, using our method, we can introduce seasonal graphs to better investigate seasonal climate changes. To the best of our knowledge, our proposed method is the first graph-based climate classification system.",Environment
Explicit implied volatilities for multifactor local-stochastic volatility models,"We consider an asset whose risk-neutral dynamics are described by a general class of local-stochastic volatility models and derive a family of asymptotic expansions for European-style option prices and implied volatilities. Our implied volatility expansions are explicit; they do not require any special functions nor do they require numerical integration. To illustrate the accuracy and versatility of our method, we implement it under five different model dynamics: CEV local volatility, quadratic local volatility, Heston stochastic volatility, 32 stochastic volatility, and SABR local-stochastic volatility.","Explicit implied volatilities for multifactor local-stochastic volatility models We consider an asset whose risk-neutral dynamics are described by a general class of local-stochastic volatility models and derive a family of asymptotic expansions for European-style option prices and implied volatilities. Our implied volatility expansions are explicit; they do not require any special functions nor do they require numerical integration. To illustrate the accuracy and versatility of our method, we implement it under five different model dynamics: CEV local volatility, quadratic local volatility, Heston stochastic volatility, 32 stochastic volatility, and SABR local-stochastic volatility.",Finance
Distorted English Alphabet Identification : An application of Difference Boosting Algorithm,"The difference-boosting algorithm is used on letters dataset from the UCI repository to classify distorted raster images of English alphabets. In contrast to rather complex networks, the difference-boosting is found to produce comparable or better classification efficiency on this complex problem.","Distorted English Alphabet Identification : An application of Difference Boosting Algorithm The difference-boosting algorithm is used on letters dataset from the UCI repository to classify distorted raster images of English alphabets. In contrast to rather complex networks, the difference-boosting is found to produce comparable or better classification efficiency on this complex problem.",Technology
Weather-Driven Priority Charging for Battery Storage Systems in Hybrid Renewable Energy Grids,"The integration of renewable energy into the power grid is often hindered by its fragmented infrastructure, leading to inefficient utilization due to the variability of energy production and its reliance on weather conditions. Battery storage systems, while essential for stabilizing energy supply, face challenges like sub-optimal energy distribution, accelerating battery degradation, and reducing operational efficiency. This paper presents a novel solution to these challenges by developing a large-scale, interconnected renewable energy network that optimizes energy storage and distribution. The proposed system includes strategically placed battery storage facilities that stabilize energy production by compensating for fluctuations in renewable output. A priority charging algorithm, informed by real-time weather forecasting and load monitoring, ensures that the most suitable battery systems are charged under varying conditions. Within each storage facility, a secondary priority charging algorithm minimizes battery degradation by ranking batteries based on critical parameters such as state of health (SoH) and state of charge (SoC) and deciding which to charge. This comprehensive approach enhances the efficiency and longevity of battery storage systems, offering a more reliable and resilient renewable energy infrastructure.","Weather-Driven Priority Charging for Battery Storage Systems in Hybrid Renewable Energy Grids The integration of renewable energy into the power grid is often hindered by its fragmented infrastructure, leading to inefficient utilization due to the variability of energy production and its reliance on weather conditions. Battery storage systems, while essential for stabilizing energy supply, face challenges like sub-optimal energy distribution, accelerating battery degradation, and reducing operational efficiency. This paper presents a novel solution to these challenges by developing a large-scale, interconnected renewable energy network that optimizes energy storage and distribution. The proposed system includes strategically placed battery storage facilities that stabilize energy production by compensating for fluctuations in renewable output. A priority charging algorithm, informed by real-time weather forecasting and load monitoring, ensures that the most suitable battery systems are charged under varying conditions. Within each storage facility, a secondary priority charging algorithm minimizes battery degradation by ranking batteries based on critical parameters such as state of health (SoH) and state of charge (SoC) and deciding which to charge. This comprehensive approach enhances the efficiency and longevity of battery storage systems, offering a more reliable and resilient renewable energy infrastructure.",Environment
A Novel Framework for Electronic Global Health Record Access,"When most patients visit physicians in a clinic or a hospital, they are asked about their medical history and related medical tests results which might not exist or might simply have been lost over time. In emergency situations, many patients suffer or sadly die because of lack of pertinent medical information. Patients Health information (PHI) saved by Electronic Medical Record (EMR) could be accessible only by a hospital using their EMR system. Furthermore, Personal Health Record (PHR) information cannot be solely relied on since it is controlled solely by patients. This paper introduces a novel framework for accessing, sharing, and controlling the medical records for patients and their physicians globally, while patients PHI are securely stored and their privacy is taken into consideration. Based on the framework, a proof of concept prototype is implemented. Preliminary performance evaluation results indicate the validity and viability of the proposed framework.","A Novel Framework for Electronic Global Health Record Access When most patients visit physicians in a clinic or a hospital, they are asked about their medical history and related medical tests results which might not exist or might simply have been lost over time. In emergency situations, many patients suffer or sadly die because of lack of pertinent medical information. Patients Health information (PHI) saved by Electronic Medical Record (EMR) could be accessible only by a hospital using their EMR system. Furthermore, Personal Health Record (PHR) information cannot be solely relied on since it is controlled solely by patients. This paper introduces a novel framework for accessing, sharing, and controlling the medical records for patients and their physicians globally, while patients PHI are securely stored and their privacy is taken into consideration. Based on the framework, a proof of concept prototype is implemented. Preliminary performance evaluation results indicate the validity and viability of the proposed framework.",Healthcare
Optimal Regulation and Investment Incentives in Financial Networks,"We examine optimal regulation of financial networks with debt interdependencies between financial firms. We first characterize when it is firms have an incentive to choose excessively risky portfolios and overly correlate their portfolios with those of their counterparties. We then characterize how optimal regulation depends on a firms financial centrality and its available investment opportunities. In standard core-periphery networks, optimal regulation depends non-monotonically on the correlation of banks investments, with maximal restrictions for intermediate levels of correlation. Moreover, it can be uniquely optimal to treat banks asymmetrically: restricting the investments of one core bank while allowing an otherwise identical core bank (in all aspects, including network centrality) to invest freely.","Optimal Regulation and Investment Incentives in Financial Networks We examine optimal regulation of financial networks with debt interdependencies between financial firms. We first characterize when it is firms have an incentive to choose excessively risky portfolios and overly correlate their portfolios with those of their counterparties. We then characterize how optimal regulation depends on a firms financial centrality and its available investment opportunities. In standard core-periphery networks, optimal regulation depends non-monotonically on the correlation of banks investments, with maximal restrictions for intermediate levels of correlation. Moreover, it can be uniquely optimal to treat banks asymmetrically: restricting the investments of one core bank while allowing an otherwise identical core bank (in all aspects, including network centrality) to invest freely.",Finance
Does economics need a scientific revolution?,Economics does not need a scientific revolution. Economics needs accurate measurements according to high standards of natural sciences and meticulous work on revealing empirical relationships between measured variables.,Does economics need a scientific revolution? Economics does not need a scientific revolution. Economics needs accurate measurements according to high standards of natural sciences and meticulous work on revealing empirical relationships between measured variables.,Finance
Teaching the Foundations of Data Science: An Interdisciplinary Approach,"The astronomical growth of data has necessitated the need for educating well-qualified data scientists to derive deep insights from large and complex data sets generated by organizations. In this paper, we present our interdisciplinary approach and experiences in teaching a Data Science course, the first of its kind offered at the Wright State University. Two faculty members from the Management Information Systems (MIS) and Computer Science (CS) departments designed and co-taught the course with perspectives from their previous research and teaching experiences. Students in the class had mix backgrounds with mainly MIS and CS majors. Students learning outcomes and post course survey responses suggested that the course delivered a broad overview of data science as desired, and that students worked synergistically with those of different majors in collaborative lab assignments and in a semester long project. The interdisciplinary pedagogy helped build collaboration and create satisfaction among learners.","Teaching the Foundations of Data Science: An Interdisciplinary Approach The astronomical growth of data has necessitated the need for educating well-qualified data scientists to derive deep insights from large and complex data sets generated by organizations. In this paper, we present our interdisciplinary approach and experiences in teaching a Data Science course, the first of its kind offered at the Wright State University. Two faculty members from the Management Information Systems (MIS) and Computer Science (CS) departments designed and co-taught the course with perspectives from their previous research and teaching experiences. Students in the class had mix backgrounds with mainly MIS and CS majors. Students learning outcomes and post course survey responses suggested that the course delivered a broad overview of data science as desired, and that students worked synergistically with those of different majors in collaborative lab assignments and in a semester long project. The interdisciplinary pedagogy helped build collaboration and create satisfaction among learners.",Education
Time and symmetry in models of economic markets,"These notes discuss several topics in neoclassical economics and alternatives, with an aim of reviewing fundamental issues in modeling economic markets. I start with a brief, non-rigorous summary of the basic Arrow-Debreu model of general equilibrium, as well as its extensions to include time and contingency. I then argue that symmetries due to similarly endowed individuals and similar products are generically broken by the constraints of scarcity, leading to the existence of multiple equilibria. This is followed by an evaluation of the strengths and weaknesses of the model generally. Several of the weaknesses are concerned with the treatments of time and contingency. To address these we discuss a class of agent based models. Another set of issues has to do with the fundamental meaning of prices and the related question of what the observables of a non-equilibrium, dynamic model of an economic market should be. We argue that these issues are addressed by formulating economics in the language of a gauge theory, as proposed originally by Malaney and Weinstein. We review some of their work and provide a sketch of how gauge invariance can be incorporated into the formulation of agent based models.","Time and symmetry in models of economic markets These notes discuss several topics in neoclassical economics and alternatives, with an aim of reviewing fundamental issues in modeling economic markets. I start with a brief, non-rigorous summary of the basic Arrow-Debreu model of general equilibrium, as well as its extensions to include time and contingency. I then argue that symmetries due to similarly endowed individuals and similar products are generically broken by the constraints of scarcity, leading to the existence of multiple equilibria. This is followed by an evaluation of the strengths and weaknesses of the model generally. Several of the weaknesses are concerned with the treatments of time and contingency. To address these we discuss a class of agent based models. Another set of issues has to do with the fundamental meaning of prices and the related question of what the observables of a non-equilibrium, dynamic model of an economic market should be. We argue that these issues are addressed by formulating economics in the language of a gauge theory, as proposed originally by Malaney and Weinstein. We review some of their work and provide a sketch of how gauge invariance can be incorporated into the formulation of agent based models.",Finance
Optimal Investment with Transaction Costs under Cumulative Prospect Theory in Discrete Time,We study optimal investment problems under the framework of cumulative prospect theory (CPT). A CPT investor makes investment decisions in a single-period financial market with transaction costs. The objective is to seek the optimal investment strategy that maximizes the prospect value of the investors final wealth. We obtain the optimal investment strategy explicitly in two examples. An economic analysis is conducted to investigate the impact of the transaction costs and risk aversion on the optimal investment strategy.,Optimal Investment with Transaction Costs under Cumulative Prospect Theory in Discrete Time We study optimal investment problems under the framework of cumulative prospect theory (CPT). A CPT investor makes investment decisions in a single-period financial market with transaction costs. The objective is to seek the optimal investment strategy that maximizes the prospect value of the investors final wealth. We obtain the optimal investment strategy explicitly in two examples. An economic analysis is conducted to investigate the impact of the transaction costs and risk aversion on the optimal investment strategy.,Finance
Nuclear Magnetic Resonance with the Distant Dipolar Field,"Distant dipolar field (DDF)-based nuclear magnetic resonance is an active research area with many fundamental properties still not well understood. Already several intriguing applications have developed, like HOMOGENIZED and IDEAL spectroscopy, that allow high resolution spectra to be obtained in inhomogeneous fields, such as in-vivo. The theoretical and experimental research in this thesis concentrates on the fundamental signal properties of DDF-based sequences in the presence of relaxation (T1 and T2) and diffusion. A general introduction to magnetic resonance phenomenon is followed by a more in depth introduction to the DDF and its effects. A novel analytical signal equation has been developed to describe the effects of T2 relaxation and diffusing spatially modulated longitudinal spins during the signal build period of an HOMOGENIZED cross peak. Diffusion of the longitudinal spins results in a lengthening of the effective dipolar demagnetization time, delaying the re-phasing of coupled anti-phase states in the quantum picture. In the classical picture the unwinding rate of spatially twisted magnetization is no longer constant, but decays exponentially with time. The expression is experimentally verified for the HOMOGENIZED spectrum of 100mM TSP in H2O at 4.7T. Equations have also been developed for the case of multiple repetition steady state 1d and 2d spectroscopic sequences with incomplete magnetization recovery, leading to spatially varying longitudinal magnetization. Experimental verification has been accomplished by imaging the profile. The equations should be found generally applicable for those interested in DDF-based spectroscopy and imaging.","Nuclear Magnetic Resonance with the Distant Dipolar Field Distant dipolar field (DDF)-based nuclear magnetic resonance is an active research area with many fundamental properties still not well understood. Already several intriguing applications have developed, like HOMOGENIZED and IDEAL spectroscopy, that allow high resolution spectra to be obtained in inhomogeneous fields, such as in-vivo. The theoretical and experimental research in this thesis concentrates on the fundamental signal properties of DDF-based sequences in the presence of relaxation (T1 and T2) and diffusion. A general introduction to magnetic resonance phenomenon is followed by a more in depth introduction to the DDF and its effects. A novel analytical signal equation has been developed to describe the effects of T2 relaxation and diffusing spatially modulated longitudinal spins during the signal build period of an HOMOGENIZED cross peak. Diffusion of the longitudinal spins results in a lengthening of the effective dipolar demagnetization time, delaying the re-phasing of coupled anti-phase states in the quantum picture. In the classical picture the unwinding rate of spatially twisted magnetization is no longer constant, but decays exponentially with time. The expression is experimentally verified for the HOMOGENIZED spectrum of 100mM TSP in H2O at 4.7T. Equations have also been developed for the case of multiple repetition steady state 1d and 2d spectroscopic sequences with incomplete magnetization recovery, leading to spatially varying longitudinal magnetization. Experimental verification has been accomplished by imaging the profile. The equations should be found generally applicable for those interested in DDF-based spectroscopy and imaging.",Healthcare
Verifying the Medical Specialty from User Profile of Online Community for Health-Related Advices,"The paper describes the verifying methods of medical specialty from user profile of online community for health-related advices. To avoid critical situations with the proliferation of unverified and inaccurate information in medical online community, it is necessary to develop a comprehensive software solution for verifying the user medical specialty of online community for health-related advices. The algorithm for forming the information profile of a medical online community user is designed. The scheme systems of formation of indicators of user specialization in the profession based on a training sample is presented. The method of forming the user information profile of online community for healthrelated advices by computer-linguistic analysis of the information content is suggested. The system of indicators based on a training sample of users in medical online communities is formed. The matrix of medical specialties indicators and method of determining weight coefficients these indicators is investigated. The proposed method of verifying the medical specialty from user profile is tested in online medical community.","Verifying the Medical Specialty from User Profile of Online Community for Health-Related Advices The paper describes the verifying methods of medical specialty from user profile of online community for health-related advices. To avoid critical situations with the proliferation of unverified and inaccurate information in medical online community, it is necessary to develop a comprehensive software solution for verifying the user medical specialty of online community for health-related advices. The algorithm for forming the information profile of a medical online community user is designed. The scheme systems of formation of indicators of user specialization in the profession based on a training sample is presented. The method of forming the user information profile of online community for healthrelated advices by computer-linguistic analysis of the information content is suggested. The system of indicators based on a training sample of users in medical online communities is formed. The matrix of medical specialties indicators and method of determining weight coefficients these indicators is investigated. The proposed method of verifying the medical specialty from user profile is tested in online medical community.",Healthcare
Dynamic Backtracking,"Because of their occasional need to return to shallow points in a search tree, existing backtracking methods can sometimes erase meaningful progress toward solving a search problem. In this paper, we present a method by which backtrack points can be moved deeper in the search space, thereby avoiding this difficulty. The technique developed is a variant of dependency-directed backtracking that uses only polynomial space while still providing useful control information and retaining the completeness guarantees provided by earlier approaches.","Dynamic Backtracking Because of their occasional need to return to shallow points in a search tree, existing backtracking methods can sometimes erase meaningful progress toward solving a search problem. In this paper, we present a method by which backtrack points can be moved deeper in the search space, thereby avoiding this difficulty. The technique developed is a variant of dependency-directed backtracking that uses only polynomial space while still providing useful control information and retaining the completeness guarantees provided by earlier approaches.",Technology
Influences of tongue biomechanics on speech movements during the production of velar stop consonants: a modeling study,"This study explores the following hypothesis: forward looping movements of the tongue that are observed in VCV sequences are due partly to the anatomical arrangement of the tongue muscles and how they are used to produce a velar closure. The study uses an anatomically based 2D biomechanical tongue model. Tissue elastic properties are accounted for in finite-element modeling, and movement is controlled by constant-rate control parameter shifts. Tongue raising and lowering movements are produced by the model with the combined actions of the genioglossus, styloglossus and hyoglossus. Simulations of V1CV2 movements were made, where C is a velar consonant and V is a, i or u. If V1 is one of the vowels a and u, the resulting trajectories describe movements that begin to loop forward before consonant closure and continue to slide along the palate during the closure. This prediction is in agreement with classical data published in the literature. If V1 is vowel i, we observe a small backward movement. This is also in agreement with some measurements on human speakers, but it is also in contradiction with the original data published by Houde (1967). These observations support the idea that the biomechanical properties of the tongue could be the main factor responsible for the forward loops when V1 is a back vowel. In the left i context, it seems that additional factors have to be taken into considerations, in order to explain the observations made on some speakers","Influences of tongue biomechanics on speech movements during the production of velar stop consonants: a modeling study This study explores the following hypothesis: forward looping movements of the tongue that are observed in VCV sequences are due partly to the anatomical arrangement of the tongue muscles and how they are used to produce a velar closure. The study uses an anatomically based 2D biomechanical tongue model. Tissue elastic properties are accounted for in finite-element modeling, and movement is controlled by constant-rate control parameter shifts. Tongue raising and lowering movements are produced by the model with the combined actions of the genioglossus, styloglossus and hyoglossus. Simulations of V1CV2 movements were made, where C is a velar consonant and V is a, i or u. If V1 is one of the vowels a and u, the resulting trajectories describe movements that begin to loop forward before consonant closure and continue to slide along the palate during the closure. This prediction is in agreement with classical data published in the literature. If V1 is vowel i, we observe a small backward movement. This is also in agreement with some measurements on human speakers, but it is also in contradiction with the original data published by Houde (1967). These observations support the idea that the biomechanical properties of the tongue could be the main factor responsible for the forward loops when V1 is a back vowel. In the left i context, it seems that additional factors have to be taken into considerations, in order to explain the observations made on some speakers",Healthcare
High-level environmental sustainability guidelines for large accelerator facilities,"The proposed construction of new particle accelerator-based facilities in the coming decades -- and upgrades to existing facilities -- provides the unique opportunity to embed innovative environmental impact reduction techniques into their design. This living document provides high-level guidelines to improve environmental sustainability in the planning, construction, operational and decommissioning stages of large accelerator facilities. A collection of various resources is provided, with examples of some existing and suggested practices.","High-level environmental sustainability guidelines for large accelerator facilities The proposed construction of new particle accelerator-based facilities in the coming decades -- and upgrades to existing facilities -- provides the unique opportunity to embed innovative environmental impact reduction techniques into their design. This living document provides high-level guidelines to improve environmental sustainability in the planning, construction, operational and decommissioning stages of large accelerator facilities. A collection of various resources is provided, with examples of some existing and suggested practices.",Environment
High-performance Uncertainty Quantification in Large-scale Virtual Clinical Trials of Closed-loop Diabetes Treatment,"In this paper, we propose a virtual clinical trial for assessing the performance and identifying risks in closed-loop diabetes treatments. Virtual clinical trials enable fast and risk-free tests of many treatment variations for large populations of fictive patients (represented by mathematical models). We use closed-loop Monte Carlo simulation, implemented in high-performance software and hardware, to quantify the uncertainty in treatment performance as well as to compare the performance in different scenarios or of different closed-loop treatments. Our software can be used for testing a wide variety of control strategies ranging from heuristical approaches to nonlinear model predictive control. We present an example of a virtual clinical trial with one million patients over 52 weeks, and we use high-performance software and hardware to conduct the virtual trial in 1 h and 22 min.","High-performance Uncertainty Quantification in Large-scale Virtual Clinical Trials of Closed-loop Diabetes Treatment In this paper, we propose a virtual clinical trial for assessing the performance and identifying risks in closed-loop diabetes treatments. Virtual clinical trials enable fast and risk-free tests of many treatment variations for large populations of fictive patients (represented by mathematical models). We use closed-loop Monte Carlo simulation, implemented in high-performance software and hardware, to quantify the uncertainty in treatment performance as well as to compare the performance in different scenarios or of different closed-loop treatments. Our software can be used for testing a wide variety of control strategies ranging from heuristical approaches to nonlinear model predictive control. We present an example of a virtual clinical trial with one million patients over 52 weeks, and we use high-performance software and hardware to conduct the virtual trial in 1 h and 22 min.",Healthcare
Comment on Enhanced TKE Dissipation under Breaking Waves,"It is noted that the results of recent experiments on the enhancement of turbulent kinetic energy (TKE) dissipation below surface waves can be stated as follows. TKE dissipation is enhanced by a factor 15 H_wsz at depths 0.5 H_ws  z  20 H_ws with respect to the wall-layer result epsilon  u_w3kappa z, where u_w is the friction velocity in water and H_ws is the significant wind-sea wave height. For open ocean conditions, this reduces in most cases to an enhancement factor 106 u_w2gz approx U_102gz.","Comment on Enhanced TKE Dissipation under Breaking Waves It is noted that the results of recent experiments on the enhancement of turbulent kinetic energy (TKE) dissipation below surface waves can be stated as follows. TKE dissipation is enhanced by a factor 15 H_wsz at depths 0.5 H_ws  z  20 H_ws with respect to the wall-layer result epsilon  u_w3kappa z, where u_w is the friction velocity in water and H_ws is the significant wind-sea wave height. For open ocean conditions, this reduces in most cases to an enhancement factor 106 u_w2gz approx U_102gz.",Environment
Exploring the Efficiency of Renewable Energy-based Modular Data Centers at Scale,"Modular data centers (MDCs) that can be placed right at the energy farms and powered mostly by renewable energy, are proven to be a flexible and effective approach to lowering the carbon footprint of data centers. However, the main challenge of using renewable energy is the high variability of power produced, which implies large volatility in powering computing resources at MDCs, and degraded application performance due to the task evictions and migrations. This causes challenges for platform operators to decide the MDC deployment. To this end, we present SkyBox, a framework that employs a holistic and learning-based approach for platform operators to explore the efficient use of renewable energy with MDC deployment across geographical regions. SkyBox is driven by the insights based on our study of real-world power traces from a variety of renewable energy farms -- the predictable production of renewable energy and the complementary nature of energy production patterns across different renewable energy sources and locations. With these insights, SkyBox first uses the coefficient of variation metric to select the qualified renewable farms, and proposes a subgraph identification algorithm to identify a set of farms with complementary energy production patterns. After that, SkyBox enables smart workload placement and migrations to further tolerate the power variability. Our experiments with real power traces and datacenter workloads show that SkyBox has the lowest carbon emissions in comparison with current MDC deployment approaches. SkyBox also minimizes the impact of the power variability on cloud virtual machines, enabling rMDCs a practical solution of efficiently using renewable energy.","Exploring the Efficiency of Renewable Energy-based Modular Data Centers at Scale Modular data centers (MDCs) that can be placed right at the energy farms and powered mostly by renewable energy, are proven to be a flexible and effective approach to lowering the carbon footprint of data centers. However, the main challenge of using renewable energy is the high variability of power produced, which implies large volatility in powering computing resources at MDCs, and degraded application performance due to the task evictions and migrations. This causes challenges for platform operators to decide the MDC deployment. To this end, we present SkyBox, a framework that employs a holistic and learning-based approach for platform operators to explore the efficient use of renewable energy with MDC deployment across geographical regions. SkyBox is driven by the insights based on our study of real-world power traces from a variety of renewable energy farms -- the predictable production of renewable energy and the complementary nature of energy production patterns across different renewable energy sources and locations. With these insights, SkyBox first uses the coefficient of variation metric to select the qualified renewable farms, and proposes a subgraph identification algorithm to identify a set of farms with complementary energy production patterns. After that, SkyBox enables smart workload placement and migrations to further tolerate the power variability. Our experiments with real power traces and datacenter workloads show that SkyBox has the lowest carbon emissions in comparison with current MDC deployment approaches. SkyBox also minimizes the impact of the power variability on cloud virtual machines, enabling rMDCs a practical solution of efficiently using renewable energy.",Environment
Total Variation Minimization and Graph Cuts for Moving Objects Segmentation,"In this paper, we are interested in the application to video segmentation of the discrete shape optimization problem involving the shape weighted perimeter and an additional term depending on a parameter. Based on recent works and in particular the one of Darbon and Sigelle, we justify the equivalence of the shape optimization problem and a weighted total variation regularization. For solving this problem, we adapt the projection algorithm proposed recently for solving the basic TV regularization problem. Another solution to the shape optimization investigated here is the graph cut technique. Both methods have the advantage to lead to a global minimum. Since we can distinguish moving objects from static elements of a scene by analyzing norm of the optical flow vectors, we choose the optical flow norm as initial data. In order to have the contour as close as possible to an edge in the image, we use a classical edge detector function as the weight of the weighted total variation. This model has been used in one of our former works. We also apply the same methods to a video segmentation model used by Jehan-Besson, Barlaud and Aubert. In this case, only standard perimeter is incorporated in the shape functional. We also propose another way for finding moving objects by using an a contrario detection of objects on the image obtained by solving the Rudin-Osher-Fatemi Total Variation regularization problem.We can notice the segmentation can be associated to a level set in the former methods.","Total Variation Minimization and Graph Cuts for Moving Objects Segmentation In this paper, we are interested in the application to video segmentation of the discrete shape optimization problem involving the shape weighted perimeter and an additional term depending on a parameter. Based on recent works and in particular the one of Darbon and Sigelle, we justify the equivalence of the shape optimization problem and a weighted total variation regularization. For solving this problem, we adapt the projection algorithm proposed recently for solving the basic TV regularization problem. Another solution to the shape optimization investigated here is the graph cut technique. Both methods have the advantage to lead to a global minimum. Since we can distinguish moving objects from static elements of a scene by analyzing norm of the optical flow vectors, we choose the optical flow norm as initial data. In order to have the contour as close as possible to an edge in the image, we use a classical edge detector function as the weight of the weighted total variation. This model has been used in one of our former works. We also apply the same methods to a video segmentation model used by Jehan-Besson, Barlaud and Aubert. In this case, only standard perimeter is incorporated in the shape functional. We also propose another way for finding moving objects by using an a contrario detection of objects on the image obtained by solving the Rudin-Osher-Fatemi Total Variation regularization problem.We can notice the segmentation can be associated to a level set in the former methods.",Technology
Causal inference with multiple versions of treatment and application to personalized medicine,"The development of high-throughput sequencing and targeted therapies has led to the emergence of personalized medicine: a patients molecular profile or the presence of a specific biomarker of drug response will correspond to a treatment recommendation made either by a physician or by a treatment assignment algorithm. The growing number of such algorithms raises the question of how to quantify their clinical impact knowing that a personalized medicine strategy will inherently include different versions of treatment. We thus specify an appropriate causal framework with multiple versions of treatment to define the causal effects of interest for precision medicine strategies and estimate them emulating clinical trials with observational data. Therefore, we determine whether the treatment assignment algorithm is more efficient than different control arms: gold standard treatment, observed treatments or random assignment of targeted treatments. Causal estimates of the precision medicine effects are first evaluated on simulated data and they demonstrate a lower biases and variances compared with naive estimation of the difference in expected outcome between treatment arms. The various simulations scenarios also point out the different bias sources depending on the clinical situation (heterogeneity of response, assignment of observed treatments etc.). A RShiny interactive application is also provided to further explore other user-defined scenarios. The method is then applied to data from patient-derived xenografts (PDX): each patient tumour is implanted in several immunodeficient cloned mice later treated with different drugs, thus providing access to all corresponding drug sensitivities for all patients. Access to these unique pre-clinical data emulating counterfactual outcomes allows to validate the reliability of causal estimates obtained with the proposed method.","Causal inference with multiple versions of treatment and application to personalized medicine The development of high-throughput sequencing and targeted therapies has led to the emergence of personalized medicine: a patients molecular profile or the presence of a specific biomarker of drug response will correspond to a treatment recommendation made either by a physician or by a treatment assignment algorithm. The growing number of such algorithms raises the question of how to quantify their clinical impact knowing that a personalized medicine strategy will inherently include different versions of treatment. We thus specify an appropriate causal framework with multiple versions of treatment to define the causal effects of interest for precision medicine strategies and estimate them emulating clinical trials with observational data. Therefore, we determine whether the treatment assignment algorithm is more efficient than different control arms: gold standard treatment, observed treatments or random assignment of targeted treatments. Causal estimates of the precision medicine effects are first evaluated on simulated data and they demonstrate a lower biases and variances compared with naive estimation of the difference in expected outcome between treatment arms. The various simulations scenarios also point out the different bias sources depending on the clinical situation (heterogeneity of response, assignment of observed treatments etc.). A RShiny interactive application is also provided to further explore other user-defined scenarios. The method is then applied to data from patient-derived xenografts (PDX): each patient tumour is implanted in several immunodeficient cloned mice later treated with different drugs, thus providing access to all corresponding drug sensitivities for all patients. Access to these unique pre-clinical data emulating counterfactual outcomes allows to validate the reliability of causal estimates obtained with the proposed method.",Healthcare
General Theory of Image Normalization,"We give a systematic, abstract formulation of the image normalization method as applied to a general group of image transformations, and then illustrate the abstract analysis by applying it to the hierarchy of viewing transformations of a planar object.","General Theory of Image Normalization We give a systematic, abstract formulation of the image normalization method as applied to a general group of image transformations, and then illustrate the abstract analysis by applying it to the hierarchy of viewing transformations of a planar object.",Technology
A new space-time model for volatility clustering in the financial market,"A new space-time model for interacting agents on the financial market is presented. It is a combination of the Curie-Weiss model and a space-time model introduced by Jarpe 2005. Properties of the model are derived with focus on the critical temperature and magnetization. It turns out that the Hamiltonian is a sufficient statistic for the temperature parameter and thus statistical inference about this parameter can be performed. Thus e.g. statements about how far the current financial situation is from a financial crisis can be made, and financial trading stability be monitored for detection of malicious risk indicating signals.","A new space-time model for volatility clustering in the financial market A new space-time model for interacting agents on the financial market is presented. It is a combination of the Curie-Weiss model and a space-time model introduced by Jarpe 2005. Properties of the model are derived with focus on the critical temperature and magnetization. It turns out that the Hamiltonian is a sufficient statistic for the temperature parameter and thus statistical inference about this parameter can be performed. Thus e.g. statements about how far the current financial situation is from a financial crisis can be made, and financial trading stability be monitored for detection of malicious risk indicating signals.",Finance
What is Fair Pay for Executives? An Information Theoretic Analysis of Wage Distributions,The high pay packages of U.S. CEOs have raised serious concerns about what would constitute a fair pay.,What is Fair Pay for Executives? An Information Theoretic Analysis of Wage Distributions The high pay packages of U.S. CEOs have raised serious concerns about what would constitute a fair pay.,Finance
On-line regression competitive with reproducing kernel Hilbert spaces,"We consider the problem of on-line prediction of real-valued labels, assumed bounded in absolute value by a known constant, of new objects from known labeled objects. The prediction algorithms performance is measured by the squared deviation of the predictions from the actual labels. No stochastic assumptions are made about the way the labels and objects are generated. Instead, we are given a benchmark class of prediction rules some of which are hoped to produce good predictions. We show that for a wide range of infinite-dimensional benchmark classes one can construct a prediction algorithm whose cumulative loss over the first N examples does not exceed the cumulative loss of any prediction rule in the class plus O(sqrt(N)); the main differences from the known results are that we do not impose any upper bound on the norm of the considered prediction rules and that we achieve an optimal leading term in the excess loss of our algorithm. If the benchmark class is universal (dense in the class of continuous functions on each compact set), this provides an on-line non-stochastic analogue of universally consistent prediction in non-parametric statistics. We use two proof techniques: one is based on the Aggregating Algorithm and the other on the recently developed method of defensive forecasting.","On-line regression competitive with reproducing kernel Hilbert spaces We consider the problem of on-line prediction of real-valued labels, assumed bounded in absolute value by a known constant, of new objects from known labeled objects. The prediction algorithms performance is measured by the squared deviation of the predictions from the actual labels. No stochastic assumptions are made about the way the labels and objects are generated. Instead, we are given a benchmark class of prediction rules some of which are hoped to produce good predictions. We show that for a wide range of infinite-dimensional benchmark classes one can construct a prediction algorithm whose cumulative loss over the first N examples does not exceed the cumulative loss of any prediction rule in the class plus O(sqrt(N)); the main differences from the known results are that we do not impose any upper bound on the norm of the considered prediction rules and that we achieve an optimal leading term in the excess loss of our algorithm. If the benchmark class is universal (dense in the class of continuous functions on each compact set), this provides an on-line non-stochastic analogue of universally consistent prediction in non-parametric statistics. We use two proof techniques: one is based on the Aggregating Algorithm and the other on the recently developed method of defensive forecasting.",Technology
Renewable Energy-Aware Inter-datacenter Virtual Machine Migration over Elastic Optical Networks,"Datacenters (DCs) are deployed in a large scale to support the ever increasing demand for data processing to support various applications. The energy consumption of DCs becomes a critical issue. Powering DCs with renewable energy can effectively reduce the brown energy consumption and thus alleviates the energy consumption problem. Owing to geographical deployments of DCs, the renewable energy generation and the data processing demands usually vary in different DCs. Migrating virtual machines (VMs) among DCs according to the availability of renewable energy helps match the energy demands and the renewable energy generation in DCs, and thus maximizes the utilization of renewable energy. Since migrating VMs incurs additional traffic in the network, the VM migration is constrained by the network capacity. The inter-datacenter (inter-DC) VM migration with network capacity constraints is an NP-hard problem. In this paper, we propose two heuristic algorithms that approximate the optimal VM migration solution. Through extensive simulations, we show that the proposed algorithms, by migrating VM among DCs, can reduce up to 31 of brown energy consumption.","Renewable Energy-Aware Inter-datacenter Virtual Machine Migration over Elastic Optical Networks Datacenters (DCs) are deployed in a large scale to support the ever increasing demand for data processing to support various applications. The energy consumption of DCs becomes a critical issue. Powering DCs with renewable energy can effectively reduce the brown energy consumption and thus alleviates the energy consumption problem. Owing to geographical deployments of DCs, the renewable energy generation and the data processing demands usually vary in different DCs. Migrating virtual machines (VMs) among DCs according to the availability of renewable energy helps match the energy demands and the renewable energy generation in DCs, and thus maximizes the utilization of renewable energy. Since migrating VMs incurs additional traffic in the network, the VM migration is constrained by the network capacity. The inter-datacenter (inter-DC) VM migration with network capacity constraints is an NP-hard problem. In this paper, we propose two heuristic algorithms that approximate the optimal VM migration solution. Through extensive simulations, we show that the proposed algorithms, by migrating VM among DCs, can reduce up to 31 of brown energy consumption.",Environment
Automatic Detection of Pulmonary Embolism using Computational Intelligence,"This article describes the implementation of a system designed to automatically detect the presence of pulmonary embolism in lung scans. These images are firstly segmented, before alignment and feature extraction using PCA. The neural network was trained using the Hybrid Monte Carlo method, resulting in a committee of 250 neural networks and good results are obtained.","Automatic Detection of Pulmonary Embolism using Computational Intelligence This article describes the implementation of a system designed to automatically detect the presence of pulmonary embolism in lung scans. These images are firstly segmented, before alignment and feature extraction using PCA. The neural network was trained using the Hybrid Monte Carlo method, resulting in a committee of 250 neural networks and good results are obtained.",Technology
The El Nino Stochastic Oscillator,"Anomalies during an El Nino are dominated by a single, irregularly oscillating, mode. Equatorial dynamics has been linked to delayed-oscillator models of this mode. Usually, the El Nino mode is regarded as an unstable mode of the coupled atmosphere system and the irregularity is attributed to noise and possibly chaos. Here a variation on the delayed oscillator is explored. In this stochastic-oscillator view, El Nino is a stable mode excited by noise. It is shown that the autocorrelation function of the observed NINO3.4 index is that of a stochastic oscillator, within the measurement uncertainty. Decadal variations as would occur in a stochastic oscillator are shown to be comparable to those observed, only the increase in the long-term mean around 1980 is rather large. The observed dependence of the seasonal cycle on the variance and the correlation is so large that it can not be attributed to the natural variability of a stationary stochastic oscillator. So the El Nino stochastic-oscillator parameters must depend on the season. A forecast model based on the stochastic oscillator with a variance that depends on the season has a skill that approaches that of more comprehensive statistical models: over the period 1982-1993, the anomaly correlation is 0.65 for two-season lead forecasts.","The El Nino Stochastic Oscillator Anomalies during an El Nino are dominated by a single, irregularly oscillating, mode. Equatorial dynamics has been linked to delayed-oscillator models of this mode. Usually, the El Nino mode is regarded as an unstable mode of the coupled atmosphere system and the irregularity is attributed to noise and possibly chaos. Here a variation on the delayed oscillator is explored. In this stochastic-oscillator view, El Nino is a stable mode excited by noise. It is shown that the autocorrelation function of the observed NINO3.4 index is that of a stochastic oscillator, within the measurement uncertainty. Decadal variations as would occur in a stochastic oscillator are shown to be comparable to those observed, only the increase in the long-term mean around 1980 is rather large. The observed dependence of the seasonal cycle on the variance and the correlation is so large that it can not be attributed to the natural variability of a stationary stochastic oscillator. So the El Nino stochastic-oscillator parameters must depend on the season. A forecast model based on the stochastic oscillator with a variance that depends on the season has a skill that approaches that of more comprehensive statistical models: over the period 1982-1993, the anomaly correlation is 0.65 for two-season lead forecasts.",Environment
DKINet: Medication Recommendation via Domain Knowledge Informed Deep Learning,"Medication recommendation is a fundamental yet crucial branch of healthcare that presents opportunities to assist physicians in making more accurate medication prescriptions for patients with complex health conditions. Previous studies have primarily focused on learning patient representation from electronic health records (EHR). While considering the clinical manifestations of the patient is important, incorporating domain-specific prior knowledge is equally significant in diagnosing the patients health conditions. However, effectively integrating domain knowledge with the patients clinical manifestations can be challenging, particularly when dealing with complex clinical manifestations. Therefore, in this paper, we first identify comprehensive domain-specific prior knowledge, namely the Unified Medical Language System (UMLS), which is a comprehensive repository of biomedical vocabularies and standards, for knowledge extraction. Subsequently, we propose a knowledge injection module that addresses the effective integration of domain knowledge with complex clinical manifestations, enabling an effective characterization of the health conditions of the patient. Furthermore, considering the significant impact of a patients medication history on their current medication, we introduce a historical medication-aware patient representation module to capture the longitudinal influence of historical medication information on the representation of current patients. Extensive experiments on three publicly benchmark datasets verify the superiority of our proposed method, which outperformed other methods by a significant margin. The code is available at: https:github.comsherry6247DKINet.","DKINet: Medication Recommendation via Domain Knowledge Informed Deep Learning Medication recommendation is a fundamental yet crucial branch of healthcare that presents opportunities to assist physicians in making more accurate medication prescriptions for patients with complex health conditions. Previous studies have primarily focused on learning patient representation from electronic health records (EHR). While considering the clinical manifestations of the patient is important, incorporating domain-specific prior knowledge is equally significant in diagnosing the patients health conditions. However, effectively integrating domain knowledge with the patients clinical manifestations can be challenging, particularly when dealing with complex clinical manifestations. Therefore, in this paper, we first identify comprehensive domain-specific prior knowledge, namely the Unified Medical Language System (UMLS), which is a comprehensive repository of biomedical vocabularies and standards, for knowledge extraction. Subsequently, we propose a knowledge injection module that addresses the effective integration of domain knowledge with complex clinical manifestations, enabling an effective characterization of the health conditions of the patient. Furthermore, considering the significant impact of a patients medication history on their current medication, we introduce a historical medication-aware patient representation module to capture the longitudinal influence of historical medication information on the representation of current patients. Extensive experiments on three publicly benchmark datasets verify the superiority of our proposed method, which outperformed other methods by a significant margin. The code is available at: https:github.comsherry6247DKINet.",Healthcare
Response kinetics of tethered bacteria to stepwise changes in nutrient concentration,"We examined the changes in swimming behaviour of the bacterium Rhodobacter sphaeroides in response to stepwise changes in a nutrient (propionate), following the prestimulus motion, the initial response and the adaptation to the sustained concentration of the chemical. This was carried out by tethering motile cells by their flagella to glass slides and following the rotational behaviour of their cell bodies in response to the nutrient change. Computerised motion analysis was used to analyse the behaviour. Distributions of run and stop times were obtained from rotation data for tethered cells. Exponential and Weibull fits for these distributions, and variability in individual responses are discussed. In terms of parameters derived from the run and stop time distributions, we compare the responses to stepwise changes in the nutrient concentration and the long-term behaviour of 84 cells under twelve propionate concentration levels from 1 nM to 25 mM. We discuss traditional assumptions for the random walk approximation to bacterial swimming and compare them with the observed R. sphaeroides motile behaviour.","Response kinetics of tethered bacteria to stepwise changes in nutrient concentration We examined the changes in swimming behaviour of the bacterium Rhodobacter sphaeroides in response to stepwise changes in a nutrient (propionate), following the prestimulus motion, the initial response and the adaptation to the sustained concentration of the chemical. This was carried out by tethering motile cells by their flagella to glass slides and following the rotational behaviour of their cell bodies in response to the nutrient change. Computerised motion analysis was used to analyse the behaviour. Distributions of run and stop times were obtained from rotation data for tethered cells. Exponential and Weibull fits for these distributions, and variability in individual responses are discussed. In terms of parameters derived from the run and stop time distributions, we compare the responses to stepwise changes in the nutrient concentration and the long-term behaviour of 84 cells under twelve propionate concentration levels from 1 nM to 25 mM. We discuss traditional assumptions for the random walk approximation to bacterial swimming and compare them with the observed R. sphaeroides motile behaviour.",Healthcare
Scaling Up Inductive Logic Programming by Learning from Interpretations,"When comparing inductive logic programming (ILP) and attribute-value learning techniques, there is a trade-off between expressive power and efficiency. Inductive logic programming techniques are typically more expressive but also less efficient. Therefore, the data sets handled by current inductive logic programming systems are small according to general standards within the data mining community. The main source of inefficiency lies in the assumption that several examples may be related to each other, so they cannot be handled independently. Within the learning from interpretations framework for inductive logic programming this assumption is unnecessary, which allows to scale up existing ILP algorithms. In this paper we explain this learning setting in the context of relational databases. We relate the setting to propositional data mining and to the classical ILP setting, and show that learning from interpretations corresponds to learning from multiple relations and thus extends the expressiveness of propositional learning, while maintaining its efficiency to a large extent (which is not the case in the classical ILP setting). As a case study, we present two alternative implementations of the ILP system Tilde (Top-down Induction of Logical DEcision trees): Tilde-classic, which loads all data in main memory, and Tilde-LDS, which loads the examples one by one. We experimentally compare the implementations, showing Tilde-LDS can handle large data sets (in the order of 100,000 examples or 100 MB) and indeed scales up linearly in the number of examples.","Scaling Up Inductive Logic Programming by Learning from Interpretations When comparing inductive logic programming (ILP) and attribute-value learning techniques, there is a trade-off between expressive power and efficiency. Inductive logic programming techniques are typically more expressive but also less efficient. Therefore, the data sets handled by current inductive logic programming systems are small according to general standards within the data mining community. The main source of inefficiency lies in the assumption that several examples may be related to each other, so they cannot be handled independently. Within the learning from interpretations framework for inductive logic programming this assumption is unnecessary, which allows to scale up existing ILP algorithms. In this paper we explain this learning setting in the context of relational databases. We relate the setting to propositional data mining and to the classical ILP setting, and show that learning from interpretations corresponds to learning from multiple relations and thus extends the expressiveness of propositional learning, while maintaining its efficiency to a large extent (which is not the case in the classical ILP setting). As a case study, we present two alternative implementations of the ILP system Tilde (Top-down Induction of Logical DEcision trees): Tilde-classic, which loads all data in main memory, and Tilde-LDS, which loads the examples one by one. We experimentally compare the implementations, showing Tilde-LDS can handle large data sets (in the order of 100,000 examples or 100 MB) and indeed scales up linearly in the number of examples.",Technology
Robustness of Regional Matching Scheme over Global Matching Scheme,"The paper has established and verified the theory prevailing widely among image and pattern recognition specialists that the bottom-up indirect regional matching process is the more stable and the more robust than the global matching process against concentrated types of noise represented by clutter, outlier or occlusion in the imagery. We have demonstrated this by analyzing the effect of concentrated noise on a typical decision making process of a simplified two candidate voting model where our theorem establishes the lower bounds to a critical breakdown point of election (or decision) result by the bottom-up matching process are greater than the exact bound of the global matching process implying that the former regional process is capable of accommodating a higher level of noise than the latter global process before the result of decision overturns. We present a convincing experimental verification supporting not only the theory by a white-black flag recognition problem in the presence of localized noise but also the validity of the conjecture by a facial recognition problem that the theorem remains valid for other decision making processes involving an important dimension-reducing transform such as principal component analysis or a Gabor transform.","Robustness of Regional Matching Scheme over Global Matching Scheme The paper has established and verified the theory prevailing widely among image and pattern recognition specialists that the bottom-up indirect regional matching process is the more stable and the more robust than the global matching process against concentrated types of noise represented by clutter, outlier or occlusion in the imagery. We have demonstrated this by analyzing the effect of concentrated noise on a typical decision making process of a simplified two candidate voting model where our theorem establishes the lower bounds to a critical breakdown point of election (or decision) result by the bottom-up matching process are greater than the exact bound of the global matching process implying that the former regional process is capable of accommodating a higher level of noise than the latter global process before the result of decision overturns. We present a convincing experimental verification supporting not only the theory by a white-black flag recognition problem in the presence of localized noise but also the validity of the conjecture by a facial recognition problem that the theorem remains valid for other decision making processes involving an important dimension-reducing transform such as principal component analysis or a Gabor transform.",Technology
Heterogeneous Cell Population Dynamics: Equation-Free Uncertainty Quantification Computations,"We propose a computational approach to modeling the collective dynamics of populations of coupled heterogeneous biological oscillators. In contrast to Monte Carlo simulation, this approach utilizes generalized Polynomial Chaos (gPC) to represent random properties of the population, thus reducing the dynamics of ensembles of oscillators to dynamics of their (typically significantly fewer) representative gPC coefficients. Equation-Free (EF) methods are employed to efficiently evolve these gPC coefficients in time and compute their coarse-grained stationary state andor limit cycle solutions, circumventing the derivation of explicit, closed-form evolution equations. Ensemble realizations of the oscillators and their statistics can be readily reconstructed from these gPC coefficients. We apply this methodology to the synchronization of yeast glycolytic oscillators coupled by the membrane exchange of an intracellular metabolite. The heterogeneity consists of a single random parameter, which accounts for glucose influx into a cell, with a Gaussian distribution over the population. Coarse projective integration is used to accelerate the evolution of the population statistics in time. Coarse fixed-point algorithms in conjunction with a Poincare return map are used to compute oscillatory solutions for the cell population and to quantify their stability.","Heterogeneous Cell Population Dynamics: Equation-Free Uncertainty Quantification Computations We propose a computational approach to modeling the collective dynamics of populations of coupled heterogeneous biological oscillators. In contrast to Monte Carlo simulation, this approach utilizes generalized Polynomial Chaos (gPC) to represent random properties of the population, thus reducing the dynamics of ensembles of oscillators to dynamics of their (typically significantly fewer) representative gPC coefficients. Equation-Free (EF) methods are employed to efficiently evolve these gPC coefficients in time and compute their coarse-grained stationary state andor limit cycle solutions, circumventing the derivation of explicit, closed-form evolution equations. Ensemble realizations of the oscillators and their statistics can be readily reconstructed from these gPC coefficients. We apply this methodology to the synchronization of yeast glycolytic oscillators coupled by the membrane exchange of an intracellular metabolite. The heterogeneity consists of a single random parameter, which accounts for glucose influx into a cell, with a Gaussian distribution over the population. Coarse projective integration is used to accelerate the evolution of the population statistics in time. Coarse fixed-point algorithms in conjunction with a Poincare return map are used to compute oscillatory solutions for the cell population and to quantify their stability.",Healthcare
GATE: An Integrated Assessment Model for AI Automation,"Assessing the economic impacts of artificial intelligence requires integrating insights from both computer science and economics. We present the Growth and AI Transition Endogenous model (GATE), a dynamic integrated assessment model that simulates the economic effects of AI automation. GATE combines three key ingredients that have not been brought together in previous work: (1) a compute-based model of AI development, (2) an AI automation framework, and (3) a semi-endogenous growth model featuring endogenous investment and adjustment costs. The model allows users to simulate the economic effects of the transition to advanced AI across a range of potential scenarios. GATE captures the interactions between economic variables, including investment, automation, innovation, and growth, as well as AI-related inputs such as compute and algorithms. This paper explains the models structure and functionality, emphasizing AI development for economists and economic modeling for the AI community. The model is implemented in an interactive sandbox, enabling users to explore the impact of AI under different parameter choices and policy interventions. The modeling sandbox is available at: www.epoch.aiGATE.","GATE: An Integrated Assessment Model for AI Automation Assessing the economic impacts of artificial intelligence requires integrating insights from both computer science and economics. We present the Growth and AI Transition Endogenous model (GATE), a dynamic integrated assessment model that simulates the economic effects of AI automation. GATE combines three key ingredients that have not been brought together in previous work: (1) a compute-based model of AI development, (2) an AI automation framework, and (3) a semi-endogenous growth model featuring endogenous investment and adjustment costs. The model allows users to simulate the economic effects of the transition to advanced AI across a range of potential scenarios. GATE captures the interactions between economic variables, including investment, automation, innovation, and growth, as well as AI-related inputs such as compute and algorithms. This paper explains the models structure and functionality, emphasizing AI development for economists and economic modeling for the AI community. The model is implemented in an interactive sandbox, enabling users to explore the impact of AI under different parameter choices and policy interventions. The modeling sandbox is available at: www.epoch.aiGATE.",Finance
GePEToS : A Geant4 Monte Carlo simulation package for Positron Emission Tomography,"GePEToS is a simulation framework developed over the last few years for assessing the instrumental performance of future PET scanners. It is based on Geant4, written in Object-Oriented C and runs on Linux platforms. The validity of GePEToS has been tested on the well-known Siemens ECAT EXACT HR camera. The results of two application examples are presented : the design optimization of a liquid Xe micro-PET camera dedicated to small animal imaging as well as the evaluation of the effect of a strong axial magnetic field on the image resolution of a Concorde P4 micro-PET camera.","GePEToS : A Geant4 Monte Carlo simulation package for Positron Emission Tomography GePEToS is a simulation framework developed over the last few years for assessing the instrumental performance of future PET scanners. It is based on Geant4, written in Object-Oriented C and runs on Linux platforms. The validity of GePEToS has been tested on the well-known Siemens ECAT EXACT HR camera. The results of two application examples are presented : the design optimization of a liquid Xe micro-PET camera dedicated to small animal imaging as well as the evaluation of the effect of a strong axial magnetic field on the image resolution of a Concorde P4 micro-PET camera.",Healthcare
Distributionally Robust Optimal Power Flow with Uncertain Renewable Energy Output,"Optimal power flow (OPF) is an important tool for Independent System Operators (ISOs) to deal with the power generation management. With the increasing penetration of renewable energy into power grids, challenges arise in tackling the OPF problem due to the intermittent nature of renewable energy output. To address these challenges, we develop a multi-stage distributionally robust approach for the direct-current optimal power flow (DC-OPF) problem to minimize total generation cost under renewable energy uncertainty. In our model, we assume the renewable energy output follows an ambiguous distribution that can be characterized by a confidence set. By utilizing the revealed data sequentially, the proposed approach can provide a reliable and robust optimal OPF decision without restricting the renewable energy output distribution to any particular distribution class. The computational results also verify the effectiveness of our approach to reduce the conservativeness and meanwhile maintain the reliability.","Distributionally Robust Optimal Power Flow with Uncertain Renewable Energy Output Optimal power flow (OPF) is an important tool for Independent System Operators (ISOs) to deal with the power generation management. With the increasing penetration of renewable energy into power grids, challenges arise in tackling the OPF problem due to the intermittent nature of renewable energy output. To address these challenges, we develop a multi-stage distributionally robust approach for the direct-current optimal power flow (DC-OPF) problem to minimize total generation cost under renewable energy uncertainty. In our model, we assume the renewable energy output follows an ambiguous distribution that can be characterized by a confidence set. By utilizing the revealed data sequentially, the proposed approach can provide a reliable and robust optimal OPF decision without restricting the renewable energy output distribution to any particular distribution class. The computational results also verify the effectiveness of our approach to reduce the conservativeness and meanwhile maintain the reliability.",Environment
Economic law of increase of Kolmogorov complexity. Transition from financial crisis 2008 to the zero-order phase transition (social explosion),"In Maslov (2003), a two level model of the occurrence of financial pyramid (bubbles) has been considered. We also considered the mathematical analogy of this model to Bose condensation. In the present paper, we explain why Ponzi schemes and bubbles result in a crisis in real economics. In Maslov (2005), the law of increase of entropy in financial systems, and consequently increase of Kolmogorov complexity, is formulated. If this law is broken, the financial system makes a phase transition to a different state. In Maslov (2005) the author considered a two level model of the zeroth-order phase transition which was interpreted in Maslov (2006) as an analog of social catastrophe. In the present paper we also examine this model.","Economic law of increase of Kolmogorov complexity. Transition from financial crisis 2008 to the zero-order phase transition (social explosion) In Maslov (2003), a two level model of the occurrence of financial pyramid (bubbles) has been considered. We also considered the mathematical analogy of this model to Bose condensation. In the present paper, we explain why Ponzi schemes and bubbles result in a crisis in real economics. In Maslov (2005), the law of increase of entropy in financial systems, and consequently increase of Kolmogorov complexity, is formulated. If this law is broken, the financial system makes a phase transition to a different state. In Maslov (2005) the author considered a two level model of the zeroth-order phase transition which was interpreted in Maslov (2006) as an analog of social catastrophe. In the present paper we also examine this model.",Finance
Optimal Microgrid Sizing of Offshore Renewable Energy Sources for Offshore Platforms and Coastal Communities,"The global energy landscape is undergoing a transformative shift towards renewable energy and advanced storage solutions, driven by the urgent need for sustainable and resilient power systems. Isolated offshore communities, such as islands and offshore platforms, which traditionally rely on mainland grids or diesel generators, stand to gain significantly from renewable energy integration. Promising offshore renewable technologies include wind turbines, wave and tidal energy converters, and floating photovoltaic systems, paired with a storage solution like battery energy storage systems. This paper introduces a renewable energy microgrid optimizer (REMO), a tool designed to identify the optimal sizes of renewable generation and storage resources for offshore microgrids. A key challenge in such models is accurately accounting for battery degradation costs. To address this, the REMO model integrates a deep neural network-based battery degradation (DNN-BD) module, which factors in variables like ambient temperature, chargedischarge rates, state of charge, depth of discharge and battery health. Simulations on six test regions demonstrate that the REMO-DNN-BD approach minimizes lifetime energy costs while maintaining high reliability and sustainability, making it a viable design solution for offshore microgrid systems.","Optimal Microgrid Sizing of Offshore Renewable Energy Sources for Offshore Platforms and Coastal Communities The global energy landscape is undergoing a transformative shift towards renewable energy and advanced storage solutions, driven by the urgent need for sustainable and resilient power systems. Isolated offshore communities, such as islands and offshore platforms, which traditionally rely on mainland grids or diesel generators, stand to gain significantly from renewable energy integration. Promising offshore renewable technologies include wind turbines, wave and tidal energy converters, and floating photovoltaic systems, paired with a storage solution like battery energy storage systems. This paper introduces a renewable energy microgrid optimizer (REMO), a tool designed to identify the optimal sizes of renewable generation and storage resources for offshore microgrids. A key challenge in such models is accurately accounting for battery degradation costs. To address this, the REMO model integrates a deep neural network-based battery degradation (DNN-BD) module, which factors in variables like ambient temperature, chargedischarge rates, state of charge, depth of discharge and battery health. Simulations on six test regions demonstrate that the REMO-DNN-BD approach minimizes lifetime energy costs while maintaining high reliability and sustainability, making it a viable design solution for offshore microgrid systems.",Environment
An Anarchist Approach to the Undergraduate Mathematics Curriculum,"Contemporary anarchism centers around three tenets: (1) a constant challenge of and resistance to all forms of domination, (2) so-called prefigurative politics, in which all decisions are made in a manner that is consistent with a set of non-hierarchical values such as equality, decentralization and voluntary cooperation, (3) a focus on diversity and open-endedness (Gordon, 2008). Within this philosophy the notion of end goals becomes moot; progress, then, is measured by process, in which the values of diversity, pluralism, cooperation, autonomy and experimentation are celebrated. In this perspective piece we propose anarchism as a philosophical framework to address the perceived cognitive dissonances of the current undergraduate mathematics curriculum. Are learning outcomes appropriate in an anarchist approach to education? How can we address the power dynamics of grading and assessment? How can assessment be done in the context of a process-based and horizontal approach that celebrates diversity and autonomy? Should grades be used, and if so, how could they be assigned non-hierarchically? At its core, anarchism aims at aligning thoughts and actions, and we argue that an anarchist viewpoint on undergraduate mathematics addresses the cognitive dissonances that currently plague our curriculum. We propose food for thought for individual instructors practice, including ideas for incremental and large-scale changes.","An Anarchist Approach to the Undergraduate Mathematics Curriculum Contemporary anarchism centers around three tenets: (1) a constant challenge of and resistance to all forms of domination, (2) so-called prefigurative politics, in which all decisions are made in a manner that is consistent with a set of non-hierarchical values such as equality, decentralization and voluntary cooperation, (3) a focus on diversity and open-endedness (Gordon, 2008). Within this philosophy the notion of end goals becomes moot; progress, then, is measured by process, in which the values of diversity, pluralism, cooperation, autonomy and experimentation are celebrated. In this perspective piece we propose anarchism as a philosophical framework to address the perceived cognitive dissonances of the current undergraduate mathematics curriculum. Are learning outcomes appropriate in an anarchist approach to education? How can we address the power dynamics of grading and assessment? How can assessment be done in the context of a process-based and horizontal approach that celebrates diversity and autonomy? Should grades be used, and if so, how could they be assigned non-hierarchically? At its core, anarchism aims at aligning thoughts and actions, and we argue that an anarchist viewpoint on undergraduate mathematics addresses the cognitive dissonances that currently plague our curriculum. We propose food for thought for individual instructors practice, including ideas for incremental and large-scale changes.",Education
Time-to-event estimands and loss to follow-up in oncology in light of the estimands framework,"Time-to-event estimands are central to many oncology clinical trials. The estimand framework (addendum to the ICH E9 guideline) calls for precisely defining the treatment effect of interest to align with the clinical question of interest and requires predefining the handling of intercurrent events that occur after treatment initiation and either preclude the observation of an event of interest or impact the interpretation of the treatment effect. We discuss a practical problem in clinical trial design and execution, i.e. in some clinical contexts it is not feasible to systematically follow patients to an event of interest. Loss to follow-up in the presence of intercurrent events can affect the meaning and interpretation of the study results. We provide recommendations for trial design, stressing the need for close alignment of the clinical question of interest and study design, impact on data collection and other practical implications. When patients cannot be systematically followed, compromise may be necessary to select the best available estimand that can be feasibly estimated under the circumstances. We discuss the use of sensitivity and supplementary analyses to examine assumptions of interest.","Time-to-event estimands and loss to follow-up in oncology in light of the estimands framework Time-to-event estimands are central to many oncology clinical trials. The estimand framework (addendum to the ICH E9 guideline) calls for precisely defining the treatment effect of interest to align with the clinical question of interest and requires predefining the handling of intercurrent events that occur after treatment initiation and either preclude the observation of an event of interest or impact the interpretation of the treatment effect. We discuss a practical problem in clinical trial design and execution, i.e. in some clinical contexts it is not feasible to systematically follow patients to an event of interest. Loss to follow-up in the presence of intercurrent events can affect the meaning and interpretation of the study results. We provide recommendations for trial design, stressing the need for close alignment of the clinical question of interest and study design, impact on data collection and other practical implications. When patients cannot be systematically followed, compromise may be necessary to select the best available estimand that can be feasibly estimated under the circumstances. We discuss the use of sensitivity and supplementary analyses to examine assumptions of interest.",Healthcare
GPCALMA: A Tool For Mammography With A GRID-Connected Distributed Database,"The GPCALMA (Grid Platform for Computer Assisted Library for MAmmography) collaboration involves several departments of physics, INFN sections, and italian hospitals. The aim of this collaboration is developing a tool that can help radiologists in early detection of breast cancer. GPCALMA has built a large distributed database of digitised mammographic images (about 5500 images corresponding to 1650 patients) and developed a CAD (Computer Aided Detection) software which is integrated in a station that can also be used for acquire new images, as archive and to perform statistical analysis. The images are completely described: pathological ones have a consistent characterization with radiologists diagnosis and histological data, non pathological ones correspond to patients with a follow up at least three years. The distributed database is realized throught the connection of all the hospitals and research centers in GRID tecnology. In each hospital local patients digital images are stored in the local database. Using GRID connection, GPCALMA will allow each node to work on distributed database data as well as local database data. Using its database the GPCALMA tools perform several analysis. A texture analysis, i.e. an automated classification on adipose, dense or glandular texture, can be provided by the system. GPCALMA software also allows classification of pathological features, in particular massive lesions analysis and microcalcification clusters analysis. The performance of the GPCALMA system will be presented in terms of the ROC (Receiver Operating Characteristic) curves. The results of GPCALMA system as second reader will also be presented.","GPCALMA: A Tool For Mammography With A GRID-Connected Distributed Database The GPCALMA (Grid Platform for Computer Assisted Library for MAmmography) collaboration involves several departments of physics, INFN sections, and italian hospitals. The aim of this collaboration is developing a tool that can help radiologists in early detection of breast cancer. GPCALMA has built a large distributed database of digitised mammographic images (about 5500 images corresponding to 1650 patients) and developed a CAD (Computer Aided Detection) software which is integrated in a station that can also be used for acquire new images, as archive and to perform statistical analysis. The images are completely described: pathological ones have a consistent characterization with radiologists diagnosis and histological data, non pathological ones correspond to patients with a follow up at least three years. The distributed database is realized throught the connection of all the hospitals and research centers in GRID tecnology. In each hospital local patients digital images are stored in the local database. Using GRID connection, GPCALMA will allow each node to work on distributed database data as well as local database data. Using its database the GPCALMA tools perform several analysis. A texture analysis, i.e. an automated classification on adipose, dense or glandular texture, can be provided by the system. GPCALMA software also allows classification of pathological features, in particular massive lesions analysis and microcalcification clusters analysis. The performance of the GPCALMA system will be presented in terms of the ROC (Receiver Operating Characteristic) curves. The results of GPCALMA system as second reader will also be presented.",Healthcare
Stability of Rossby waves in the beta-plane approximation,"Floquet theory is used to describe the unstable spectrum at large scales of the beta-plane equation linearized about Rossby waves. Base flows consisting of one to three Rossby wave are considered analytically using continued fractions and the method of multiple scales, while base flow with more than three Rossby waves are studied numerically. It is demonstrated that the mechanism for instability changes from inflectional to triad resonance at an O(1) transition Rhines number Rh, independent of the Reynolds number. For a single Rossby wave base flow, the critical Reynolds number Rec for instability is found in various limits. In the limits Rh -- infinity and k -- 0, the classical value Rec  sqrt(2) is recovered. For Rh -- 0 and all orientations of the Rossby wave except zonal and meridional, the base flow is unstable for all Reynolds numbers; a zonal Rossby wave is stable, while a meridional Rossby wave has critical Reynolds number Rec  sqrt(2). For more isotropic base flows consisting of many Rossby waves (up to forty), the most unstable mode is purely zonal for 2  Rh  infinity and is nearly zonal for Rh  12, where the transition Rhines number is again O(1), independent of the Reynolds number and consistent with a change in the mechanism for instability from inflectional to triad resonance.","Stability of Rossby waves in the beta-plane approximation Floquet theory is used to describe the unstable spectrum at large scales of the beta-plane equation linearized about Rossby waves. Base flows consisting of one to three Rossby wave are considered analytically using continued fractions and the method of multiple scales, while base flow with more than three Rossby waves are studied numerically. It is demonstrated that the mechanism for instability changes from inflectional to triad resonance at an O(1) transition Rhines number Rh, independent of the Reynolds number. For a single Rossby wave base flow, the critical Reynolds number Rec for instability is found in various limits. In the limits Rh -- infinity and k -- 0, the classical value Rec  sqrt(2) is recovered. For Rh -- 0 and all orientations of the Rossby wave except zonal and meridional, the base flow is unstable for all Reynolds numbers; a zonal Rossby wave is stable, while a meridional Rossby wave has critical Reynolds number Rec  sqrt(2). For more isotropic base flows consisting of many Rossby waves (up to forty), the most unstable mode is purely zonal for 2  Rh  infinity and is nearly zonal for Rh  12, where the transition Rhines number is again O(1), independent of the Reynolds number and consistent with a change in the mechanism for instability from inflectional to triad resonance.",Environment
Locally Adaptive Block Thresholding Method with Continuity Constraint,"We present an algorithm that enables one to perform locally adaptive block thresholding, while maintaining image continuity. Images are divided into sub-images based some standard image attributes and thresholding technique is employed over the sub-images. The present algorithm makes use of the thresholds of neighboring sub-images to calculate a range of values. The image continuity is taken care by choosing the threshold of the sub-image under consideration to lie within the above range. After examining the average range values for various sub-image sizes of a variety of images, it was found that the range of acceptable threshold values is substantially high, justifying our assumption of exploiting the freedom of range for bringing out local details.","Locally Adaptive Block Thresholding Method with Continuity Constraint We present an algorithm that enables one to perform locally adaptive block thresholding, while maintaining image continuity. Images are divided into sub-images based some standard image attributes and thresholding technique is employed over the sub-images. The present algorithm makes use of the thresholds of neighboring sub-images to calculate a range of values. The image continuity is taken care by choosing the threshold of the sub-image under consideration to lie within the above range. After examining the average range values for various sub-image sizes of a variety of images, it was found that the range of acceptable threshold values is substantially high, justifying our assumption of exploiting the freedom of range for bringing out local details.",Technology
Pac-Learning Recursive Logic Programs: Efficient Algorithms,"We present algorithms that learn certain classes of function-free recursive logic programs in polynomial time from equivalence queries. In particular, we show that a single k-ary recursive constant-depth determinate clause is learnable. Two-clause programs consisting of one learnable recursive clause and one constant-depth determinate non-recursive clause are also learnable, if an additional basecase oracle is assumed. These results immediately imply the pac-learnability of these classes. Although these classes of learnable recursive programs are very constrained, it is shown in a companion paper that they are maximally general, in that generalizing either class in any natural way leads to a computationally difficult learning problem. Thus, taken together with its companion paper, this paper establishes a boundary of efficient learnability for recursive logic programs.","Pac-Learning Recursive Logic Programs: Efficient Algorithms We present algorithms that learn certain classes of function-free recursive logic programs in polynomial time from equivalence queries. In particular, we show that a single k-ary recursive constant-depth determinate clause is learnable. Two-clause programs consisting of one learnable recursive clause and one constant-depth determinate non-recursive clause are also learnable, if an additional basecase oracle is assumed. These results immediately imply the pac-learnability of these classes. Although these classes of learnable recursive programs are very constrained, it is shown in a companion paper that they are maximally general, in that generalizing either class in any natural way leads to a computationally difficult learning problem. Thus, taken together with its companion paper, this paper establishes a boundary of efficient learnability for recursive logic programs.",Technology
A Prediction Market for Toxic Assets Prices,"We propose the development of a prediction market for forecasting prices for toxic assets to be transferred from Irish banks to the National Asset Management Agency (NAMA). Such a market allows market participants to assume a stake in a security whose value is tied to a future event. We propose that securities are created whose value hinges on the transfer amount paid for loans from NAMA to a bank. In essence, bets are accepted on whether the price is higher or lower than a certain quoted figure. The prices of the securities represent transfer prices for toxic assets increases or decreases in line with market opinion. Prediction markets offer a proven means of aggregating distributed knowledge pertaining to fair market values in a scalable and transparent manner. They are incentive compatible (i.e. induce truthful reporting) and robust to strategic manipulation. We propose that a prediction market is run in parallel with the pricing procedure recommended by the European Commission. This procedure need not necessarily take heed of the prediction markets view in all cases but it may offer guidance and a means of anomaly detection. An online prediction market would offer everybody an opportunity to have their say in an open and transparent manner.","A Prediction Market for Toxic Assets Prices We propose the development of a prediction market for forecasting prices for toxic assets to be transferred from Irish banks to the National Asset Management Agency (NAMA). Such a market allows market participants to assume a stake in a security whose value is tied to a future event. We propose that securities are created whose value hinges on the transfer amount paid for loans from NAMA to a bank. In essence, bets are accepted on whether the price is higher or lower than a certain quoted figure. The prices of the securities represent transfer prices for toxic assets increases or decreases in line with market opinion. Prediction markets offer a proven means of aggregating distributed knowledge pertaining to fair market values in a scalable and transparent manner. They are incentive compatible (i.e. induce truthful reporting) and robust to strategic manipulation. We propose that a prediction market is run in parallel with the pricing procedure recommended by the European Commission. This procedure need not necessarily take heed of the prediction markets view in all cases but it may offer guidance and a means of anomaly detection. An online prediction market would offer everybody an opportunity to have their say in an open and transparent manner.",Finance
Optimizing GoTools Search Heuristics using Genetic Algorithms,"GoTools is a program which solves life  death problems in the game of Go. This paper describes experiments using a Genetic Algorithm to optimize heuristic weights used by GoTools tree-search. The complete set of heuristic weights is composed of different subgroups, each of which can be optimized with a suitable fitness function. As a useful side product, an MPI interface for FreePascal was implemented to allow the use of a parallelized fitness function running on a Beowulf cluster. The aim of this exercise is to optimize the current version of GoTools, and to make tools available in preparation of an extension of GoTools for solving open boundary life  death problems, which will introduce more heuristic parameters to be fine tuned.","Optimizing GoTools Search Heuristics using Genetic Algorithms GoTools is a program which solves life  death problems in the game of Go. This paper describes experiments using a Genetic Algorithm to optimize heuristic weights used by GoTools tree-search. The complete set of heuristic weights is composed of different subgroups, each of which can be optimized with a suitable fitness function. As a useful side product, an MPI interface for FreePascal was implemented to allow the use of a parallelized fitness function running on a Beowulf cluster. The aim of this exercise is to optimize the current version of GoTools, and to make tools available in preparation of an extension of GoTools for solving open boundary life  death problems, which will introduce more heuristic parameters to be fine tuned.",Technology
U-CNNpred: A Universal CNN-based Predictor for Stock Markets,"The performance of financial market prediction systems depends heavily on the quality of features it is using. While researchers have used various techniques for enhancing the stock specific features, less attention has been paid to extracting features that represent general mechanism of financial markets. In this paper, we investigate the importance of extracting such general features in stock market prediction domain and show how it can improve the performance of financial market prediction. We present a framework called U-CNNpred, that uses a CNN-based structure. A base model is trained in a specially designed layer-wise training procedure over a pool of historical data from many financial markets, in order to extract the common patterns from different markets. Our experiments, in which we have used hundreds of stocks in SP 500 as well as 14 famous indices around the world, show that this model can outperform baseline algorithms when predicting the directional movement of the markets for which it has been trained for. We also show that the base model can be fine-tuned for predicting new markets and achieve a better performance compared to the state of the art baseline algorithms that focus on constructing market-specific models from scratch.","U-CNNpred: A Universal CNN-based Predictor for Stock Markets The performance of financial market prediction systems depends heavily on the quality of features it is using. While researchers have used various techniques for enhancing the stock specific features, less attention has been paid to extracting features that represent general mechanism of financial markets. In this paper, we investigate the importance of extracting such general features in stock market prediction domain and show how it can improve the performance of financial market prediction. We present a framework called U-CNNpred, that uses a CNN-based structure. A base model is trained in a specially designed layer-wise training procedure over a pool of historical data from many financial markets, in order to extract the common patterns from different markets. Our experiments, in which we have used hundreds of stocks in SP 500 as well as 14 famous indices around the world, show that this model can outperform baseline algorithms when predicting the directional movement of the markets for which it has been trained for. We also show that the base model can be fine-tuned for predicting new markets and achieve a better performance compared to the state of the art baseline algorithms that focus on constructing market-specific models from scratch.",Finance
Generative AI in Modern Education Society,"Transitioning from Education 1.0 to Education 5.0, the integration of generative artificial intelligence (GenAI) revolutionizes the learning environment by fostering enhanced human-machine collaboration, enabling personalized, adaptive and experiential learning, and preparing students with the skills and adaptability needed for the future workforce. Our understanding of academic integrity and the scholarship of teaching, learning, and research has been revolutionised by GenAI. Schools and universities around the world are experimenting and exploring the integration of GenAI in their education systems (like, curriculum design, teaching process and assessments, administrative tasks, results generation and so on). The findings of the literature study demonstrate how well GenAI has been incorporated into the global educational system. This study explains the roles of GenAI in the schooling and university education systems with respect to the different stakeholders (students, teachers, researchers etc,). It highlights the current challenges of integrating Generative AI into the education system and outlines future directions for leveraging GenAI to enhance educational practices.","Generative AI in Modern Education Society Transitioning from Education 1.0 to Education 5.0, the integration of generative artificial intelligence (GenAI) revolutionizes the learning environment by fostering enhanced human-machine collaboration, enabling personalized, adaptive and experiential learning, and preparing students with the skills and adaptability needed for the future workforce. Our understanding of academic integrity and the scholarship of teaching, learning, and research has been revolutionised by GenAI. Schools and universities around the world are experimenting and exploring the integration of GenAI in their education systems (like, curriculum design, teaching process and assessments, administrative tasks, results generation and so on). The findings of the literature study demonstrate how well GenAI has been incorporated into the global educational system. This study explains the roles of GenAI in the schooling and university education systems with respect to the different stakeholders (students, teachers, researchers etc,). It highlights the current challenges of integrating Generative AI into the education system and outlines future directions for leveraging GenAI to enhance educational practices.",Education
Recognition of expression variant faces using masked log-Gabor features and Principal Component Analysis,In this article we propose a method for the recognition of faces with different facial expressions. For recognition we extract feature vectors by using log-Gabor filters of multiple orientations and scales. Using sliding window algorithm and variances -based masking these features are extracted at image regions that are less affected by the changes of facial expressions. Extracted features are passed to the Principal Component Analysis (PCA) -based recognition method. The results of face recognition experiments using expression variant faces showed that the proposed method could achieve higher recognition accuracy than many other methods. For development and testing we used facial images from the AR and FERET databases. Using facial photographs of more than one thousand persons from the FERET database the proposed method achieved 96.6-98.9 first one recognition rate and 0.2-0.6 Equal Error Rate (EER).,Recognition of expression variant faces using masked log-Gabor features and Principal Component Analysis In this article we propose a method for the recognition of faces with different facial expressions. For recognition we extract feature vectors by using log-Gabor filters of multiple orientations and scales. Using sliding window algorithm and variances -based masking these features are extracted at image regions that are less affected by the changes of facial expressions. Extracted features are passed to the Principal Component Analysis (PCA) -based recognition method. The results of face recognition experiments using expression variant faces showed that the proposed method could achieve higher recognition accuracy than many other methods. For development and testing we used facial images from the AR and FERET databases. Using facial photographs of more than one thousand persons from the FERET database the proposed method achieved 96.6-98.9 first one recognition rate and 0.2-0.6 Equal Error Rate (EER).,Technology
Irreversible investment under weighted discounting: effects of decreasing impatience,"This paper employs an intra-personal game-theoretic framework to investigate how decreasing impatience influences irreversible investment behaviors in a continuous-time setting. We consider a capacity expansion problem under weighted discount functions, a class of nonexponential functions that exhibit decreasing impatience, including the hyperbolic discount function as a special case. By deriving the Bellman system that characterizes the equilibrium, we establish the framework for analyzing investment behaviors of agents subject to decreasing impatience. From an economic perspective, we demonstrates that decreasing impatience prompts early investment. From a technical standpoint, we warn that decreasing impatience can lead to the failure of the smooth pasting principle.","Irreversible investment under weighted discounting: effects of decreasing impatience This paper employs an intra-personal game-theoretic framework to investigate how decreasing impatience influences irreversible investment behaviors in a continuous-time setting. We consider a capacity expansion problem under weighted discount functions, a class of nonexponential functions that exhibit decreasing impatience, including the hyperbolic discount function as a special case. By deriving the Bellman system that characterizes the equilibrium, we establish the framework for analyzing investment behaviors of agents subject to decreasing impatience. From an economic perspective, we demonstrates that decreasing impatience prompts early investment. From a technical standpoint, we warn that decreasing impatience can lead to the failure of the smooth pasting principle.",Finance
Conditional Expressions for Blind Deconvolution: Multi-point form,We present conditional expression (CE) for finding blurs convolved in given images. The CE is given in terms of the zero-values of the blurs evaluated at multi-point. The CE can detect multiple blur all at once. We illustrate the multiple blur-detection by using a test image.,Conditional Expressions for Blind Deconvolution: Multi-point form We present conditional expression (CE) for finding blurs convolved in given images. The CE is given in terms of the zero-values of the blurs evaluated at multi-point. The CE can detect multiple blur all at once. We illustrate the multiple blur-detection by using a test image.,Technology
Improved and Developed Upper Bound of Price of Anarchy in Two Echelon Case,"Price of anarchy, the performance ratio, which could characterize the loss of efficiency of the distributed supply chain management compared with the integrated supply chain management is discussed by utilizing newsvendor problem in single period which is well-known. In particular, some of remarkable distributed policies are handled, the performance ratios in each case which have been investigated in the previous works are analyzed theoretically and the tighter upper bound of price of anarchy and the lower bound are presented. Furthermore our approach is developed based on a generalized framework and a geometric interpretation of price of anarchy is appeared via the literature of convex optimization.","Improved and Developed Upper Bound of Price of Anarchy in Two Echelon Case Price of anarchy, the performance ratio, which could characterize the loss of efficiency of the distributed supply chain management compared with the integrated supply chain management is discussed by utilizing newsvendor problem in single period which is well-known. In particular, some of remarkable distributed policies are handled, the performance ratios in each case which have been investigated in the previous works are analyzed theoretically and the tighter upper bound of price of anarchy and the lower bound are presented. Furthermore our approach is developed based on a generalized framework and a geometric interpretation of price of anarchy is appeared via the literature of convex optimization.",Finance
Developing the Next Generation of Physics Assessments,"Science education at all levels is currently undergoing dramatic changes to its curricula and developing assessments for these new curricula is paramount. We have used the basis of many of these new changes (scientific practices, crosscutting concepts, and core ideas) to develop sets of criteria that can be used to guide assessment development for this new curriculum. We present a case study that uses the criteria we have developed to revise a traditional physics assessment item into an assessment item that is much more aligned with the goals of current transformation efforts. Assessment items developed using this criteria can be used to assess student learning of both the concepts and process of science.","Developing the Next Generation of Physics Assessments Science education at all levels is currently undergoing dramatic changes to its curricula and developing assessments for these new curricula is paramount. We have used the basis of many of these new changes (scientific practices, crosscutting concepts, and core ideas) to develop sets of criteria that can be used to guide assessment development for this new curriculum. We present a case study that uses the criteria we have developed to revise a traditional physics assessment item into an assessment item that is much more aligned with the goals of current transformation efforts. Assessment items developed using this criteria can be used to assess student learning of both the concepts and process of science.",Education
An Insurance Paradigm for Improving Power System Resilience via Distributed Investment,"Extreme events, exacerbated by climate change, pose significant risks to the energy system and its consumers. However there are natural limits to the degree of protection that can be delivered from a centralised market architecture. Distributed energy resources provide resilience to the energy system, but their value remains inadequately recognized by regulatory frameworks. We propose an insurance framework to align residual outage risk exposure with locational incentives for distributed investment. We demonstrate that leveraging this framework in large-scale electricity systems could improve consumer welfare outcomes in the face of growing risks from extreme events via investment in distributed energy.","An Insurance Paradigm for Improving Power System Resilience via Distributed Investment Extreme events, exacerbated by climate change, pose significant risks to the energy system and its consumers. However there are natural limits to the degree of protection that can be delivered from a centralised market architecture. Distributed energy resources provide resilience to the energy system, but their value remains inadequately recognized by regulatory frameworks. We propose an insurance framework to align residual outage risk exposure with locational incentives for distributed investment. We demonstrate that leveraging this framework in large-scale electricity systems could improve consumer welfare outcomes in the face of growing risks from extreme events via investment in distributed energy.",Finance
Learning Optimal Dynamic Treatment Regimes from Longitudinal Data,"Studies often report estimates of the average treatment effect. While the ATE summarizes the effect of a treatment on average, it does not provide any information about the effect of treatment within any individual. A treatment strategy that uses an individuals information to tailor treatment to maximize benefit is known as an optimal dynamic treatment rule. Treatment, however, is typically not limited to a single point in time; consequently, learning an optimal rule for a time-varying treatment may involve not just learning the extent to which the comparative treatments benefits vary across the characteristics of individuals, but also learning the extent to which the comparative treatments benefits vary as relevant circumstances evolve within an individual. The goal of this paper is to provide a tutorial for estimating ODTR from longitudinal observational and clinical trial data for applied researchers. We describe an approach that uses a doubly-robust unbiased transformation of the conditional average treatment effect. We then learn a time-varying ODTR for when to increase buprenorphine-naloxone dose to minimize return-to-regular-opioid-use among patients with opioid use disorder. Our analysis highlights the utility of ODTRs in the context of sequential decision making: the learned ODTR outperforms a clinically defined strategy.","Learning Optimal Dynamic Treatment Regimes from Longitudinal Data Studies often report estimates of the average treatment effect. While the ATE summarizes the effect of a treatment on average, it does not provide any information about the effect of treatment within any individual. A treatment strategy that uses an individuals information to tailor treatment to maximize benefit is known as an optimal dynamic treatment rule. Treatment, however, is typically not limited to a single point in time; consequently, learning an optimal rule for a time-varying treatment may involve not just learning the extent to which the comparative treatments benefits vary across the characteristics of individuals, but also learning the extent to which the comparative treatments benefits vary as relevant circumstances evolve within an individual. The goal of this paper is to provide a tutorial for estimating ODTR from longitudinal observational and clinical trial data for applied researchers. We describe an approach that uses a doubly-robust unbiased transformation of the conditional average treatment effect. We then learn a time-varying ODTR for when to increase buprenorphine-naloxone dose to minimize return-to-regular-opioid-use among patients with opioid use disorder. Our analysis highlights the utility of ODTRs in the context of sequential decision making: the learned ODTR outperforms a clinically defined strategy.",Healthcare
Approximating Incomplete Kernel Matrices by the em Algorithm,"In biological data, it is often the case that observed data are available only for a subset of samples. When a kernel matrix is derived from such data, we have to leave the entries for unavailable samples as missing. In this paper, we make use of a parametric model of kernel matrices, and estimate missing entries by fitting the model to existing entries. The parametric model is created as a set of spectral variants of a complete kernel matrix derived from another information source. For model fitting, we adopt the em algorithm based on the information geometry of positive definite matrices. We will report promising results on bacteria clustering experiments using two marker sequences: 16S and gyrB.","Approximating Incomplete Kernel Matrices by the em Algorithm In biological data, it is often the case that observed data are available only for a subset of samples. When a kernel matrix is derived from such data, we have to leave the entries for unavailable samples as missing. In this paper, we make use of a parametric model of kernel matrices, and estimate missing entries by fitting the model to existing entries. The parametric model is created as a set of spectral variants of a complete kernel matrix derived from another information source. For model fitting, we adopt the em algorithm based on the information geometry of positive definite matrices. We will report promising results on bacteria clustering experiments using two marker sequences: 16S and gyrB.",Technology
Efficient Algorithms for Renewable Energy Allocation to Delay Tolerant Consumers,"We investigate the problem of allocating energy from renewable sources to flexible consumers in electricity markets. We assume there is a renewable energy supplier that provides energy according to a time-varying (and possibly unpredictable) supply process. The plant must serve consumers within a specified delay window, and incurs a cost of drawing energy from other (possibly non-renewable) sources if its own supply is not sufficient to meet the deadlines. We formulate two stochastic optimization problems: The first seeks to minimize the time average cost of using the other sources (and hence strives for the most efficient utilization of the renewable source). The second allows the renewable source to dynamically set a price for its service, and seeks to maximize the resulting time average profit. These problems are solved via the Lyapunov optimization technique. Our resulting algorithms do not require knowledge of the statistics of the time-varying supply and demand processes and are robust to arbitrary sample path variations.","Efficient Algorithms for Renewable Energy Allocation to Delay Tolerant Consumers We investigate the problem of allocating energy from renewable sources to flexible consumers in electricity markets. We assume there is a renewable energy supplier that provides energy according to a time-varying (and possibly unpredictable) supply process. The plant must serve consumers within a specified delay window, and incurs a cost of drawing energy from other (possibly non-renewable) sources if its own supply is not sufficient to meet the deadlines. We formulate two stochastic optimization problems: The first seeks to minimize the time average cost of using the other sources (and hence strives for the most efficient utilization of the renewable source). The second allows the renewable source to dynamically set a price for its service, and seeks to maximize the resulting time average profit. These problems are solved via the Lyapunov optimization technique. Our resulting algorithms do not require knowledge of the statistics of the time-varying supply and demand processes and are robust to arbitrary sample path variations.",Environment
Multilevel Monte Carlo method for jump-diffusion SDEs,"We investigate the extension of the multilevel Monte Carlo path simulation method to jump-diffusion SDEs. We consider models with finite rate activity, using a jump-adapted discretisation in which the jump times are computed and added to the standard uniform dis- cretisation times. The key component in multilevel analysis is the calculation of an expected payoff difference between a coarse path simulation and a fine path simulation with twice as many timesteps. If the Poisson jump rate is constant, the jump times are the same on both paths and the multilevel extension is relatively straightforward, but the implementation is more complex in the case of state-dependent jump rates for which the jump times naturally differ.","Multilevel Monte Carlo method for jump-diffusion SDEs We investigate the extension of the multilevel Monte Carlo path simulation method to jump-diffusion SDEs. We consider models with finite rate activity, using a jump-adapted discretisation in which the jump times are computed and added to the standard uniform dis- cretisation times. The key component in multilevel analysis is the calculation of an expected payoff difference between a coarse path simulation and a fine path simulation with twice as many timesteps. If the Poisson jump rate is constant, the jump times are the same on both paths and the multilevel extension is relatively straightforward, but the implementation is more complex in the case of state-dependent jump rates for which the jump times naturally differ.",Finance
A Computational Model of Spatial Memory Anticipation during Visual Search,"Some visual search tasks require to memorize the location of stimuli that have been previously scanned. Considerations about the eye movements raise the question of how we are able to maintain a coherent memory, despite the frequent drastically changes in the perception. In this article, we present a computational model that is able to anticipate the consequences of the eye movements on the visual perception in order to update a spatial memory","A Computational Model of Spatial Memory Anticipation during Visual Search Some visual search tasks require to memorize the location of stimuli that have been previously scanned. Considerations about the eye movements raise the question of how we are able to maintain a coherent memory, despite the frequent drastically changes in the perception. In this article, we present a computational model that is able to anticipate the consequences of the eye movements on the visual perception in order to update a spatial memory",Technology
Distant generalization by feedforward neural networks,"This paper discusses the notion of generalization of training samples over long distances in the input space of a feedforward neural network. Such a generalization might occur in various ways, that differ in how great the contribution of different training features should be. The structure of a neuron in a feedforward neural network is analyzed and it is concluded, that the actual performance of the discussed generalization in such neural networks may be problematic -- while such neural networks might be capable for such a distant generalization, a random and spurious generalization may occur as well. To illustrate the differences in generalizing of the same function by different learning machines, results given by the support vector machines are also presented.","Distant generalization by feedforward neural networks This paper discusses the notion of generalization of training samples over long distances in the input space of a feedforward neural network. Such a generalization might occur in various ways, that differ in how great the contribution of different training features should be. The structure of a neuron in a feedforward neural network is analyzed and it is concluded, that the actual performance of the discussed generalization in such neural networks may be problematic -- while such neural networks might be capable for such a distant generalization, a random and spurious generalization may occur as well. To illustrate the differences in generalizing of the same function by different learning machines, results given by the support vector machines are also presented.",Technology
A possible explanation for Earths climatic changes in the past few million years,"The astronomical theory of Milankovitch relates the changes of Earth past climate to variations in insolation caused by oscillations of the orbital parameters. However, this theory has problems to account for some major observed phenomena of the past few million years. Here, we present an alternative explanation for these phenomena. It is based on the idea that the solar system until quite recently contained an additional massive object of planetary size. This object, called Z, is assumed to have moved on a highly eccentric orbit bound to the sun. It influenced Earths climate through a gas cloud of evaporated material. Calculations show that more than once during the last 3.2 Myr it even approached the Earth close enough to provoke a significant shift of the geographic position of the poles. The last of these shifts terminated Earths Ice Age epoch about 11.5 kyr ago. The origin and fate of Z is also discussed.","A possible explanation for Earths climatic changes in the past few million years The astronomical theory of Milankovitch relates the changes of Earth past climate to variations in insolation caused by oscillations of the orbital parameters. However, this theory has problems to account for some major observed phenomena of the past few million years. Here, we present an alternative explanation for these phenomena. It is based on the idea that the solar system until quite recently contained an additional massive object of planetary size. This object, called Z, is assumed to have moved on a highly eccentric orbit bound to the sun. It influenced Earths climate through a gas cloud of evaporated material. Calculations show that more than once during the last 3.2 Myr it even approached the Earth close enough to provoke a significant shift of the geographic position of the poles. The last of these shifts terminated Earths Ice Age epoch about 11.5 kyr ago. The origin and fate of Z is also discussed.",Environment
Boosting the Differences: A fast Bayesian classifier neural network,"A Bayesian classifier that up-weights the differences in the attribute values is discussed. Using four popular datasets from the UCI repository, some interesting features of the network are illustrated. The network is suitable for classification problems.","Boosting the Differences: A fast Bayesian classifier neural network A Bayesian classifier that up-weights the differences in the attribute values is discussed. Using four popular datasets from the UCI repository, some interesting features of the network are illustrated. The network is suitable for classification problems.",Technology
Engaged pedagogy: An Innovative method to Teach Physics,"Most science classes and in particular Physics are delivered through traditional teaching methods. More specifically, in the traditional stand-and-deliver lecture, the information was transmitted unilaterally from the teacher to the student and hence the students are only passive participants. Recent studies revealed that traditional lectures are inefficient and even more do not help students understand key concepts or develop critical thinking skills. Mazur described the traditional lecture as the illusion of teaching for teachers, and the illusion of learning for learners. However, one of the approaches that can support students to study Physics more efficiently as well as develop critical and quantitative thinking skills is Engaged pedagogy. Engaged pedagogy, as a novel interactive teaching method and as well as a vector of success in teaching and learning Physics, is discussed in this paper. Additionally, the impact of technology on the proposed teaching pathway is presented.","Engaged pedagogy: An Innovative method to Teach Physics Most science classes and in particular Physics are delivered through traditional teaching methods. More specifically, in the traditional stand-and-deliver lecture, the information was transmitted unilaterally from the teacher to the student and hence the students are only passive participants. Recent studies revealed that traditional lectures are inefficient and even more do not help students understand key concepts or develop critical thinking skills. Mazur described the traditional lecture as the illusion of teaching for teachers, and the illusion of learning for learners. However, one of the approaches that can support students to study Physics more efficiently as well as develop critical and quantitative thinking skills is Engaged pedagogy. Engaged pedagogy, as a novel interactive teaching method and as well as a vector of success in teaching and learning Physics, is discussed in this paper. Additionally, the impact of technology on the proposed teaching pathway is presented.",Education
Counterfactual Fairness Evaluation of Machine Learning Models on Educational Datasets,"As machine learning models are increasingly used in educational settings, from detecting at-risk students to predicting student performance, algorithmic bias and its potential impacts on students raise critical concerns about algorithmic fairness. Although group fairness is widely explored in education, works on individual fairness in a causal context are understudied, especially on counterfactual fairness. This paper explores the notion of counterfactual fairness for educational data by conducting counterfactual fairness analysis of machine learning models on benchmark educational datasets. We demonstrate that counterfactual fairness provides meaningful insight into the causality of sensitive attributes and causal-based individual fairness in education.","Counterfactual Fairness Evaluation of Machine Learning Models on Educational Datasets As machine learning models are increasingly used in educational settings, from detecting at-risk students to predicting student performance, algorithmic bias and its potential impacts on students raise critical concerns about algorithmic fairness. Although group fairness is widely explored in education, works on individual fairness in a causal context are understudied, especially on counterfactual fairness. This paper explores the notion of counterfactual fairness for educational data by conducting counterfactual fairness analysis of machine learning models on benchmark educational datasets. We demonstrate that counterfactual fairness provides meaningful insight into the causality of sensitive attributes and causal-based individual fairness in education.",Education
HiFAR: Multi-Stage Curriculum Learning for High-Dynamics Humanoid Fall Recovery,"Humanoid robots encounter considerable difficulties in autonomously recovering from falls, especially within dynamic and unstructured environments. Conventional control methodologies are often inadequate in addressing the complexities associated with high-dimensional dynamics and the contact-rich nature of fall recovery. Meanwhile, reinforcement learning techniques are hindered by issues related to sparse rewards, intricate collision scenarios, and discrepancies between simulation and real-world applications. In this study, we introduce a multi-stage curriculum learning framework, termed HiFAR. This framework employs a staged learning approach that progressively incorporates increasingly complex and high-dimensional recovery tasks, thereby facilitating the robots acquisition of efficient and stable fall recovery strategies. Furthermore, it enables the robot to adapt its policy to effectively manage real-world fall incidents. We assess the efficacy of the proposed method using a real humanoid robot, showcasing its capability to autonomously recover from a diverse range of falls with high success rates, rapid recovery times, robustness, and generalization.","HiFAR: Multi-Stage Curriculum Learning for High-Dynamics Humanoid Fall Recovery Humanoid robots encounter considerable difficulties in autonomously recovering from falls, especially within dynamic and unstructured environments. Conventional control methodologies are often inadequate in addressing the complexities associated with high-dimensional dynamics and the contact-rich nature of fall recovery. Meanwhile, reinforcement learning techniques are hindered by issues related to sparse rewards, intricate collision scenarios, and discrepancies between simulation and real-world applications. In this study, we introduce a multi-stage curriculum learning framework, termed HiFAR. This framework employs a staged learning approach that progressively incorporates increasingly complex and high-dimensional recovery tasks, thereby facilitating the robots acquisition of efficient and stable fall recovery strategies. Furthermore, it enables the robot to adapt its policy to effectively manage real-world fall incidents. We assess the efficacy of the proposed method using a real humanoid robot, showcasing its capability to autonomously recover from a diverse range of falls with high success rates, rapid recovery times, robustness, and generalization.",Education
"Graph rigidity, Cyclic Belief Propagation and Point Pattern Matching","A recent paper citeCaeCaeSchBar06 proposed a provably optimal, polynomial time method for performing near-isometric point pattern matching by means of exact probabilistic inference in a chordal graphical model. Their fundamental result is that the chordal graph in question is shown to be globally rigid, implying that exact inference provides the same matching solution as exact inference in a complete graphical model. This implies that the algorithm is optimal when there is no noise in the point patterns. In this paper, we present a new graph which is also globally rigid but has an advantage over the graph proposed in citeCaeCaeSchBar06: its maximal clique size is smaller, rendering inference significantly more efficient. However, our graph is not chordal and thus standard Junction Tree algorithms cannot be directly applied. Nevertheless, we show that loopy belief propagation in such a graph converges to the optimal solution. This allows us to retain the optimality guarantee in the noiseless case, while substantially reducing both memory requirements and processing time. Our experimental results show that the accuracy of the proposed solution is indistinguishable from that of citeCaeCaeSchBar06 when there is noise in the point patterns.","Graph rigidity, Cyclic Belief Propagation and Point Pattern Matching A recent paper citeCaeCaeSchBar06 proposed a provably optimal, polynomial time method for performing near-isometric point pattern matching by means of exact probabilistic inference in a chordal graphical model. Their fundamental result is that the chordal graph in question is shown to be globally rigid, implying that exact inference provides the same matching solution as exact inference in a complete graphical model. This implies that the algorithm is optimal when there is no noise in the point patterns. In this paper, we present a new graph which is also globally rigid but has an advantage over the graph proposed in citeCaeCaeSchBar06: its maximal clique size is smaller, rendering inference significantly more efficient. However, our graph is not chordal and thus standard Junction Tree algorithms cannot be directly applied. Nevertheless, we show that loopy belief propagation in such a graph converges to the optimal solution. This allows us to retain the optimality guarantee in the noiseless case, while substantially reducing both memory requirements and processing time. Our experimental results show that the accuracy of the proposed solution is indistinguishable from that of citeCaeCaeSchBar06 when there is noise in the point patterns.",Technology
Generalization of Clauses under Implication,"In the area of inductive learning, generalization is a main operation, and the usual definition of induction is based on logical implication. Recently there has been a rising interest in clausal representation of knowledge in machine learning. Almost all inductive learning systems that perform generalization of clauses use the relation theta-subsumption instead of implication. The main reason is that there is a well-known and simple technique to compute least general generalizations under theta-subsumption, but not under implication. However generalization under theta-subsumption is inappropriate for learning recursive clauses, which is a crucial problem since recursion is the basic program structure of logic programs. We note that implication between clauses is undecidable, and we therefore introduce a stronger form of implication, called T-implication, which is decidable between clauses. We show that for every finite set of clauses there exists a least general generalization under T-implication. We describe a technique to reduce generalizations under implication of a clause to generalizations under theta-subsumption of what we call an expansion of the original clause. Moreover we show that for every non-tautological clause there exists a T-complete expansion, which means that every generalization under T-implication of the clause is reduced to a generalization under theta-subsumption of the expansion.","Generalization of Clauses under Implication In the area of inductive learning, generalization is a main operation, and the usual definition of induction is based on logical implication. Recently there has been a rising interest in clausal representation of knowledge in machine learning. Almost all inductive learning systems that perform generalization of clauses use the relation theta-subsumption instead of implication. The main reason is that there is a well-known and simple technique to compute least general generalizations under theta-subsumption, but not under implication. However generalization under theta-subsumption is inappropriate for learning recursive clauses, which is a crucial problem since recursion is the basic program structure of logic programs. We note that implication between clauses is undecidable, and we therefore introduce a stronger form of implication, called T-implication, which is decidable between clauses. We show that for every finite set of clauses there exists a least general generalization under T-implication. We describe a technique to reduce generalizations under implication of a clause to generalizations under theta-subsumption of what we call an expansion of the original clause. Moreover we show that for every non-tautological clause there exists a T-complete expansion, which means that every generalization under T-implication of the clause is reduced to a generalization under theta-subsumption of the expansion.",Technology
A Massive Local Rules Search Approach to the Classification Problem,"An approach to the classification problem of machine learning, based on building local classification rules, is developed. The local rules are considered as projections of the global classification rules to the event we want to classify. A massive global optimization algorithm is used for optimization of quality criterion. The algorithm, which has polynomial complexity in typical case, is used to find all high--quality local rules. The other distinctive feature of the algorithm is the integration of attributes levels selection (for ordered attributes) with rules searching and original conflicting rules resolution strategy. The algorithm is practical; it was tested on a number of data sets from UCI repository, and a comparison with the other predicting techniques is presented.","A Massive Local Rules Search Approach to the Classification Problem An approach to the classification problem of machine learning, based on building local classification rules, is developed. The local rules are considered as projections of the global classification rules to the event we want to classify. A massive global optimization algorithm is used for optimization of quality criterion. The algorithm, which has polynomial complexity in typical case, is used to find all high--quality local rules. The other distinctive feature of the algorithm is the integration of attributes levels selection (for ordered attributes) with rules searching and original conflicting rules resolution strategy. The algorithm is practical; it was tested on a number of data sets from UCI repository, and a comparison with the other predicting techniques is presented.",Technology
Interdisciplinary Translation of Comparative Visualization,"Spatial visualisation skills and interpretations are critical in the design professions, but traditionally difficult to effectively teach. Visualization and multimedia presentation studies show positive improvements in learner outcomes for specific learning domains. But the development and translation of a comparative visualization pedagogy between disciplines is poorly understood. This research seeks to identify an approach to developing comparable multimodal and interactive visualizations and attendant student reflections for curriculum designers in courses that can utilize visualizations and manipulations. Results from previous use of comparative multimodal visualization pedagogy in a multimedia 3D modelling class are used as a guide to translation of pedagogy to architecture design. The focus is how to guide the use of comparative multimodal visualizations through media properties, lesson sequencing, and reflection to inform effective instruction and learning.","Interdisciplinary Translation of Comparative Visualization Spatial visualisation skills and interpretations are critical in the design professions, but traditionally difficult to effectively teach. Visualization and multimedia presentation studies show positive improvements in learner outcomes for specific learning domains. But the development and translation of a comparative visualization pedagogy between disciplines is poorly understood. This research seeks to identify an approach to developing comparable multimodal and interactive visualizations and attendant student reflections for curriculum designers in courses that can utilize visualizations and manipulations. Results from previous use of comparative multimodal visualization pedagogy in a multimedia 3D modelling class are used as a guide to translation of pedagogy to architecture design. The focus is how to guide the use of comparative multimodal visualizations through media properties, lesson sequencing, and reflection to inform effective instruction and learning.",Education
Basket Options Valuation for a Local Volatility Jump-Diffusion Model with the Asymptotic Expansion Method,In this paper we discuss the basket options valuation for a jump-diffusion model. The underlying asset prices follow some correlated local volatility diffusion processes with systematic jumps. We derive a forward partial integral differential equation (PIDE) for general stochastic processes and use the asymptotic expansion method to approximate the conditional expectation of the stochastic variance associated with the basket value process. The numerical tests show that the suggested method is fast and accurate in comparison with the Monte Carlo and other methods in most cases.,Basket Options Valuation for a Local Volatility Jump-Diffusion Model with the Asymptotic Expansion Method In this paper we discuss the basket options valuation for a jump-diffusion model. The underlying asset prices follow some correlated local volatility diffusion processes with systematic jumps. We derive a forward partial integral differential equation (PIDE) for general stochastic processes and use the asymptotic expansion method to approximate the conditional expectation of the stochastic variance associated with the basket value process. The numerical tests show that the suggested method is fast and accurate in comparison with the Monte Carlo and other methods in most cases.,Finance
Anti-correlation and subsector structure in financial systems,"With the random matrix theory, we study the spatial structure of the Chinese stock market, American stock market and global market indices. After taking into account the signs of the components in the eigenvectors of the cross-correlation matrix, we detect the subsector structure of the financial systems. The positive and negative subsectors are anti-correlated each other in the corresponding eigenmode. The subsector structure is strong in the Chinese stock market, while somewhat weaker in the American stock market and global market indices. Characteristics of the subsector structures in different markets are revealed.","Anti-correlation and subsector structure in financial systems With the random matrix theory, we study the spatial structure of the Chinese stock market, American stock market and global market indices. After taking into account the signs of the components in the eigenvectors of the cross-correlation matrix, we detect the subsector structure of the financial systems. The positive and negative subsectors are anti-correlated each other in the corresponding eigenmode. The subsector structure is strong in the Chinese stock market, while somewhat weaker in the American stock market and global market indices. Characteristics of the subsector structures in different markets are revealed.",Finance
An Analytical Theory for the Early Stage of the Development of Hurricanes: Part I,"A theoretical formulation for the early stage (Tropical Storm stage) of hurricane development is proposed. These solutions are not only consistent with observations but also offer some new insights into hurricane properties. This is the first time a theoretical model for the early growing of hurricanes is proposed for which analytical solutions have been found, based on an assumption of a positive feedback of a self-induced developing system.","An Analytical Theory for the Early Stage of the Development of Hurricanes: Part I A theoretical formulation for the early stage (Tropical Storm stage) of hurricane development is proposed. These solutions are not only consistent with observations but also offer some new insights into hurricane properties. This is the first time a theoretical model for the early growing of hurricanes is proposed for which analytical solutions have been found, based on an assumption of a positive feedback of a self-induced developing system.",Environment
Optimal Workload Allocation for Distributed Edge Clouds With Renewable Energy and Battery Storage,"This paper studies an optimal workload allocation problem for a network of renewable energy-powered edge clouds that serve users located across various geographical areas. Specifically, each edge cloud is furnished with both an on-site renewable energy generation unit and a battery storage unit. Due to the discrepancy in electricity pricing and the diverse temporal-spatial characteristics of renewable energy generation, how to optimally allocate workload to different edge clouds to minimize the total operating cost while maximizing renewable energy utilization is a crucial and challenging problem. To this end, we introduce and formulate an optimization-based framework designed for Edge Service Providers (ESPs) with the overarching goal of simultaneously reducing energy costs and environmental impacts through the integration of renewable energy sources and battery storage systems, all while maintaining essential quality-of-service standards. Numerical results demonstrate the effectiveness of the proposed model and solution in maintaining service quality as well as reducing operational costs and emissions. Furthermore, the impacts of renewable energy generation and battery storage on optimal system operations are rigorously analyzed.","Optimal Workload Allocation for Distributed Edge Clouds With Renewable Energy and Battery Storage This paper studies an optimal workload allocation problem for a network of renewable energy-powered edge clouds that serve users located across various geographical areas. Specifically, each edge cloud is furnished with both an on-site renewable energy generation unit and a battery storage unit. Due to the discrepancy in electricity pricing and the diverse temporal-spatial characteristics of renewable energy generation, how to optimally allocate workload to different edge clouds to minimize the total operating cost while maximizing renewable energy utilization is a crucial and challenging problem. To this end, we introduce and formulate an optimization-based framework designed for Edge Service Providers (ESPs) with the overarching goal of simultaneously reducing energy costs and environmental impacts through the integration of renewable energy sources and battery storage systems, all while maintaining essential quality-of-service standards. Numerical results demonstrate the effectiveness of the proposed model and solution in maintaining service quality as well as reducing operational costs and emissions. Furthermore, the impacts of renewable energy generation and battery storage on optimal system operations are rigorously analyzed.",Environment
Residual Weighted Learning for Estimating Individualized Treatment Rules,"Personalized medicine has received increasing attention among statisticians, computer scientists, and clinical practitioners. A major component of personalized medicine is the estimation of individualized treatment rules (ITRs). Recently, Zhao et al. (2012) proposed outcome weighted learning (OWL) to construct ITRs that directly optimize the clinical outcome. Although OWL opens the door to introducing machine learning techniques to optimal treatment regimes, it still has some problems in performance. In this article, we propose a general framework, called Residual Weighted Learning (RWL), to improve finite sample performance. Unlike OWL which weights misclassification errors by clinical outcomes, RWL weights these errors by residuals of the outcome from a regression fit on clinical covariates excluding treatment assignment. We utilize the smoothed ramp loss function in RWL, and provide a difference of convex (d.c.) algorithm to solve the corresponding non-convex optimization problem. By estimating residuals with linear models or generalized linear models, RWL can effectively deal with different types of outcomes, such as continuous, binary and count outcomes. We also propose variable selection methods for linear and nonlinear rules, respectively, to further improve the performance. We show that the resulting estimator of the treatment rule is consistent. We further obtain a rate of convergence for the difference between the expected outcome using the estimated ITR and that of the optimal treatment rule. The performance of the proposed RWL methods is illustrated in simulation studies and in an analysis of cystic fibrosis clinical trial data.","Residual Weighted Learning for Estimating Individualized Treatment Rules Personalized medicine has received increasing attention among statisticians, computer scientists, and clinical practitioners. A major component of personalized medicine is the estimation of individualized treatment rules (ITRs). Recently, Zhao et al. (2012) proposed outcome weighted learning (OWL) to construct ITRs that directly optimize the clinical outcome. Although OWL opens the door to introducing machine learning techniques to optimal treatment regimes, it still has some problems in performance. In this article, we propose a general framework, called Residual Weighted Learning (RWL), to improve finite sample performance. Unlike OWL which weights misclassification errors by clinical outcomes, RWL weights these errors by residuals of the outcome from a regression fit on clinical covariates excluding treatment assignment. We utilize the smoothed ramp loss function in RWL, and provide a difference of convex (d.c.) algorithm to solve the corresponding non-convex optimization problem. By estimating residuals with linear models or generalized linear models, RWL can effectively deal with different types of outcomes, such as continuous, binary and count outcomes. We also propose variable selection methods for linear and nonlinear rules, respectively, to further improve the performance. We show that the resulting estimator of the treatment rule is consistent. We further obtain a rate of convergence for the difference between the expected outcome using the estimated ITR and that of the optimal treatment rule. The performance of the proposed RWL methods is illustrated in simulation studies and in an analysis of cystic fibrosis clinical trial data.",Healthcare
Temporal ordering of clinical events,"This report describes a minimalistic set of methods engineered to anchor clinical events onto a temporal space. Specifically, we describe methods to extract clinical events (e.g., Problems, Treatments and Tests), temporal expressions (i.e., time, date, duration, and frequency), and temporal links (e.g., Before, After, Overlap) between events and temporal entities. These methods are developed and validated using high quality datasets.","Temporal ordering of clinical events This report describes a minimalistic set of methods engineered to anchor clinical events onto a temporal space. Specifically, we describe methods to extract clinical events (e.g., Problems, Treatments and Tests), temporal expressions (i.e., time, date, duration, and frequency), and temporal links (e.g., Before, After, Overlap) between events and temporal entities. These methods are developed and validated using high quality datasets.",Healthcare
Rerepresenting and Restructuring Domain Theories: A Constructive Induction Approach,"Theory revision integrates inductive learning and background knowledge by combining training examples with a coarse domain theory to produce a more accurate theory. There are two challenges that theory revision and other theory-guided systems face. First, a representation language appropriate for the initial theory may be inappropriate for an improved theory. While the original representation may concisely express the initial theory, a more accurate theory forced to use that same representation may be bulky, cumbersome, and difficult to reach. Second, a theory structure suitable for a coarse domain theory may be insufficient for a fine-tuned theory. Systems that produce only small, local changes to a theory have limited value for accomplishing complex structural alterations that may be required. Consequently, advanced theory-guided learning systems require flexible representation and flexible structure. An analysis of various theory revision systems and theory-guided learning systems reveals specific strengths and weaknesses in terms of these two desired properties. Designed to capture the underlying qualities of each system, a new system uses theory-guided constructive induction. Experiments in three domains show improvement over previous theory-guided systems. This leads to a study of the behavior, limitations, and potential of theory-guided constructive induction.","Rerepresenting and Restructuring Domain Theories: A Constructive Induction Approach Theory revision integrates inductive learning and background knowledge by combining training examples with a coarse domain theory to produce a more accurate theory. There are two challenges that theory revision and other theory-guided systems face. First, a representation language appropriate for the initial theory may be inappropriate for an improved theory. While the original representation may concisely express the initial theory, a more accurate theory forced to use that same representation may be bulky, cumbersome, and difficult to reach. Second, a theory structure suitable for a coarse domain theory may be insufficient for a fine-tuned theory. Systems that produce only small, local changes to a theory have limited value for accomplishing complex structural alterations that may be required. Consequently, advanced theory-guided learning systems require flexible representation and flexible structure. An analysis of various theory revision systems and theory-guided learning systems reveals specific strengths and weaknesses in terms of these two desired properties. Designed to capture the underlying qualities of each system, a new system uses theory-guided constructive induction. Experiments in three domains show improvement over previous theory-guided systems. This leads to a study of the behavior, limitations, and potential of theory-guided constructive induction.",Technology
PABED A Tool for Big Education Data Analysis,"Cloud computing and big data have risen to become the most popular technologies of the modern world. Apparently, the reason behind their immense popularity is their wide range of applicability as far as the areas of interest are concerned. Education and research remain one of the most obvious and befitting application areas. This research paper introduces a big data analytics tool, PABED Project Analyzing Big Education Data, for the education sector that makes use of cloud-based technologies. This tool is implemented using Google BigQuery and R programming language and allows comparison of undergraduate enrollment data for different academic years. Although, there are many proposed applications of big data in education, there is a lack of tools that can actualize the concept into practice. PABED is an effort in this direction. The implementation and testing details of the project have been described in this paper. This tool validates the use of cloud computing and big data technologies in education and shall head start development of more sophisticated educational intelligence tools.","PABED A Tool for Big Education Data Analysis Cloud computing and big data have risen to become the most popular technologies of the modern world. Apparently, the reason behind their immense popularity is their wide range of applicability as far as the areas of interest are concerned. Education and research remain one of the most obvious and befitting application areas. This research paper introduces a big data analytics tool, PABED Project Analyzing Big Education Data, for the education sector that makes use of cloud-based technologies. This tool is implemented using Google BigQuery and R programming language and allows comparison of undergraduate enrollment data for different academic years. Although, there are many proposed applications of big data in education, there is a lack of tools that can actualize the concept into practice. PABED is an effort in this direction. The implementation and testing details of the project have been described in this paper. This tool validates the use of cloud computing and big data technologies in education and shall head start development of more sophisticated educational intelligence tools.",Education
The Network of Inter-Regional Direct Investment Stocks across Europe,"We propose a methodological framework to study the dynamics of inter-regional investment flow in Europe from a Complex Networks perspective, an approach with recent proven success in many fields including economics. In this work we study the network of investment stocks in Europe at two different levels: first, we compute the inward-outward investment stocks at the level of firms, based on ownership shares and number of employees; then we estimate the inward-outward investment stock at the level of regions in Europe, by aggregating the ownership network of firms, based on their headquarter location. Despite the intuitive value of this approach for EU policy making in economic development, to our knowledge there are no similar works in the literature yet. In this paper we focus on statistical distributions and scaling laws of activity, investment stock and connectivity degree both at the level of firms and at the level of regions. In particular we find that investment stock of firms is power law distributed with an exponent very close to the one found for firm activity. On the other hand investment stock and activity of regions turn out to be log-normal distributed. At both levels we find scaling laws relating investment to activity and connectivity. In particular, we find that investment stock scales with connectivity in a similar way as has been previously found for stock market data, calling for further investigations on a possible general scaling law holding true in economical networks.","The Network of Inter-Regional Direct Investment Stocks across Europe We propose a methodological framework to study the dynamics of inter-regional investment flow in Europe from a Complex Networks perspective, an approach with recent proven success in many fields including economics. In this work we study the network of investment stocks in Europe at two different levels: first, we compute the inward-outward investment stocks at the level of firms, based on ownership shares and number of employees; then we estimate the inward-outward investment stock at the level of regions in Europe, by aggregating the ownership network of firms, based on their headquarter location. Despite the intuitive value of this approach for EU policy making in economic development, to our knowledge there are no similar works in the literature yet. In this paper we focus on statistical distributions and scaling laws of activity, investment stock and connectivity degree both at the level of firms and at the level of regions. In particular we find that investment stock of firms is power law distributed with an exponent very close to the one found for firm activity. On the other hand investment stock and activity of regions turn out to be log-normal distributed. At both levels we find scaling laws relating investment to activity and connectivity. In particular, we find that investment stock scales with connectivity in a similar way as has been previously found for stock market data, calling for further investigations on a possible general scaling law holding true in economical networks.",Finance
Assessing a GTA professional development program,"For the last four years, the School of Physics at Georgia Tech have been preparing new Graduate Teaching Assistants (GTAs) through a program that integrates pedagogy, physics content, and professional development strategies. Here we discuss various assessments we have used to evaluate the program, among them surveys, GTA self-reporting, and end-of-semester student evaluations. Our results indicate that GTAs who participate in the program find its practical activities useful, feel better prepared for teaching, make use of learner-centered teaching strategies, and receive higher scores in teaching evaluations.","Assessing a GTA professional development program For the last four years, the School of Physics at Georgia Tech have been preparing new Graduate Teaching Assistants (GTAs) through a program that integrates pedagogy, physics content, and professional development strategies. Here we discuss various assessments we have used to evaluate the program, among them surveys, GTA self-reporting, and end-of-semester student evaluations. Our results indicate that GTAs who participate in the program find its practical activities useful, feel better prepared for teaching, make use of learner-centered teaching strategies, and receive higher scores in teaching evaluations.",Education
Generalized Paraxial Ray Trace Procedure Derived from Geodesic Deviation,"Paraxial ray tracing procedures have become widely accepted techniques for acoustic models in seismology and underwater acoustics. To date a generic form of these procedures including fluid motion and time dependence has not appeared in the literature. A detailed investigation of the characteristic curves of the equations of hydrodynamics allows for an immediate generalization of the procedure to be extracted from the equation form geodesic deviation. The general paraxial ray trace equations serve as an ideal supplement to ordinary ray tracing in predicting the deformation of acoustic beams in random environments. The general procedure is derived in terms of affine parameterization and in a coordinate time parameterization ideal for application to physical acoustic ray propagation. The formalism is applied to layered media, where the deviation equation reduces to a second order differential equation for a single field with a general solution in terms of a depth integral along the ray path. Some features are illustrated through special cases which lead to exact solutions in terms of either ordinary or special functions.","Generalized Paraxial Ray Trace Procedure Derived from Geodesic Deviation Paraxial ray tracing procedures have become widely accepted techniques for acoustic models in seismology and underwater acoustics. To date a generic form of these procedures including fluid motion and time dependence has not appeared in the literature. A detailed investigation of the characteristic curves of the equations of hydrodynamics allows for an immediate generalization of the procedure to be extracted from the equation form geodesic deviation. The general paraxial ray trace equations serve as an ideal supplement to ordinary ray tracing in predicting the deformation of acoustic beams in random environments. The general procedure is derived in terms of affine parameterization and in a coordinate time parameterization ideal for application to physical acoustic ray propagation. The formalism is applied to layered media, where the deviation equation reduces to a second order differential equation for a single field with a general solution in terms of a depth integral along the ray path. Some features are illustrated through special cases which lead to exact solutions in terms of either ordinary or special functions.",Environment
A simulator for maxillo-facial surgery integrating cephalometry and orthodontia,"Objectives : This paper presents a new simulator for maxillo-facial surgery, that gathers the dental and the maxillo-facial analyses together into a single computer-assisted procedure. The idea is first to propose a repositioning of the maxilla, via the introduction of a 3D cephalometry, applied to a 3D virtual model of the patients skull. Then, orthodontic data are integrated into this model, thanks to optical measurements of teeth plaster casts. Materials and Methods : The feasibility of the maxillo-facial demonstrator was first evaluated on a dry skull. To simulate malformations (and thus to simulate a real patient), the skull was modified and manually cut by the surgeon, in order to generate a given maxillo-facial malformation (with asymmetries in the sagittal, frontal and axial planes). Results : The validation of our simulator consisted in evaluating its ability to propose a bone repositioning diagnosis that will put the skull as it was in its original configuration. A first qualitative validation is provided in this paper, with a 1.5-mm error in the repositioning diagnosis. Conclusions : These results mainly validate the concept of a maxillo-facial numerical simulator that integrates 3D cephalometry and guarantees a correct dental occlusion.","A simulator for maxillo-facial surgery integrating cephalometry and orthodontia Objectives : This paper presents a new simulator for maxillo-facial surgery, that gathers the dental and the maxillo-facial analyses together into a single computer-assisted procedure. The idea is first to propose a repositioning of the maxilla, via the introduction of a 3D cephalometry, applied to a 3D virtual model of the patients skull. Then, orthodontic data are integrated into this model, thanks to optical measurements of teeth plaster casts. Materials and Methods : The feasibility of the maxillo-facial demonstrator was first evaluated on a dry skull. To simulate malformations (and thus to simulate a real patient), the skull was modified and manually cut by the surgeon, in order to generate a given maxillo-facial malformation (with asymmetries in the sagittal, frontal and axial planes). Results : The validation of our simulator consisted in evaluating its ability to propose a bone repositioning diagnosis that will put the skull as it was in its original configuration. A first qualitative validation is provided in this paper, with a 1.5-mm error in the repositioning diagnosis. Conclusions : These results mainly validate the concept of a maxillo-facial numerical simulator that integrates 3D cephalometry and guarantees a correct dental occlusion.",Healthcare
Enhancing flipped classroom pedagogy in linear algebra through machine learning,"We implemented active learning pedagogy in teaching and learning an introductory course of linear algebra at the tertiary level. We adopted a flipped classroom approach for several semesters and collected students perceptions regarding the pedagogy. A questionnaire is distributed and collected at the end of the semester, and it collects students demographics, including gender, their year of study, their major, and their expected grade. The students were asked to respond to ten statements on a five-item Likert scale, five of them gauging their opinion and attitude toward flipped classroom pedagogy, whereas the other five regarding the instructor. Using a machine learning algorithm of the support vector machine (SVM) classification, we investigated whether there are observable variations in the survey response when stratified by gender difference. We seek a hyperplane that best divides this dataset into two distinct classes based on gender differences. After implementing the principal component analysis (PCA) for reducing the data dimensionality, we observed that there exists a discernible hyperplane that segregates the responses based on gender difference. This implies that SVM has learned to capture and represent the underlying gender-related characteristics within the dataset. Furthermore, this finding suggests that we need to tailor our approach when implementing flipped classroom pedagogy because different gender learns differently.","Enhancing flipped classroom pedagogy in linear algebra through machine learning We implemented active learning pedagogy in teaching and learning an introductory course of linear algebra at the tertiary level. We adopted a flipped classroom approach for several semesters and collected students perceptions regarding the pedagogy. A questionnaire is distributed and collected at the end of the semester, and it collects students demographics, including gender, their year of study, their major, and their expected grade. The students were asked to respond to ten statements on a five-item Likert scale, five of them gauging their opinion and attitude toward flipped classroom pedagogy, whereas the other five regarding the instructor. Using a machine learning algorithm of the support vector machine (SVM) classification, we investigated whether there are observable variations in the survey response when stratified by gender difference. We seek a hyperplane that best divides this dataset into two distinct classes based on gender differences. After implementing the principal component analysis (PCA) for reducing the data dimensionality, we observed that there exists a discernible hyperplane that segregates the responses based on gender difference. This implies that SVM has learned to capture and represent the underlying gender-related characteristics within the dataset. Furthermore, this finding suggests that we need to tailor our approach when implementing flipped classroom pedagogy because different gender learns differently.",Education
Climate Change and Open Science,Obtaining reliable answers to the major scientific questions raised by climate change in time to take appropriate action gives added urgency to the open access program.,Climate Change and Open Science Obtaining reliable answers to the major scientific questions raised by climate change in time to take appropriate action gives added urgency to the open access program.,Environment
Leveraging Technology for Healthcare and Retaining Access to Personal Health Data to Enhance Personal Health and Well-being,"Health data is a sensitive category of personal data. It might result in a high risk to individual and health information handling rights and opportunities unless there is a palatable defense. Reasonable security standards are needed to protect electronic health records (EHR). All personal data handling needs adequate explanation. Maintaining access to medical data even in the developing world would favor health and well-being across the world. Unfortunately, there are still countries that hinder the portability of medical records. Numerous occurrences have shown that it still takes weeks for the medical data to be ported from one general physician (GP) to another. Cross border portability is nearly impossible due to the lack of technical infrastructure and standardization. We demonstrate the difficulty of the portability of medical records with some example case studies as a collaborative engagement exercise through a data mapping process to describe how different people and datapoints interact and evaluate EHR portability techniques. We then propose a blockchain-based EHR system that allows secure, and cross border sharing of medical data. The ethical and technical challenges around having such a system have also been discussed in this study.","Leveraging Technology for Healthcare and Retaining Access to Personal Health Data to Enhance Personal Health and Well-being Health data is a sensitive category of personal data. It might result in a high risk to individual and health information handling rights and opportunities unless there is a palatable defense. Reasonable security standards are needed to protect electronic health records (EHR). All personal data handling needs adequate explanation. Maintaining access to medical data even in the developing world would favor health and well-being across the world. Unfortunately, there are still countries that hinder the portability of medical records. Numerous occurrences have shown that it still takes weeks for the medical data to be ported from one general physician (GP) to another. Cross border portability is nearly impossible due to the lack of technical infrastructure and standardization. We demonstrate the difficulty of the portability of medical records with some example case studies as a collaborative engagement exercise through a data mapping process to describe how different people and datapoints interact and evaluate EHR portability techniques. We then propose a blockchain-based EHR system that allows secure, and cross border sharing of medical data. The ethical and technical challenges around having such a system have also been discussed in this study.",Healthcare
On Preparing a List of Random treatment Assigns,This paper presents the foundations of a computer oriented approach for preparing a list of random treatment assignments to be adopted in randomised controlled trials. Software is presented which can be applied in the earliest stage of clinical trials and bioequivalence assays. This allocation of patients to treatment in clinical trials ensures exactly equal treatment numbers. The investigation of the randomness properties of an assignment leads to the concept of a strong randomised list. The new approach introduced in this note is based on thresholds and produces a strong randomised list of treatment assignments.,On Preparing a List of Random treatment Assigns This paper presents the foundations of a computer oriented approach for preparing a list of random treatment assignments to be adopted in randomised controlled trials. Software is presented which can be applied in the earliest stage of clinical trials and bioequivalence assays. This allocation of patients to treatment in clinical trials ensures exactly equal treatment numbers. The investigation of the randomness properties of an assignment leads to the concept of a strong randomised list. The new approach introduced in this note is based on thresholds and produces a strong randomised list of treatment assignments.,Healthcare
Multiple sequence alignment based on set covers,"We introduce a new heuristic for the multiple alignment of a set of sequences. The heuristic is based on a set cover of the residue alphabet of the sequences, and also on the determination of a significant set of blocks comprising subsequences of the sequences to be aligned. These blocks are obtained with the aid of a new data structure, called a suffix-set tree, which is constructed from the input sequences with the guidance of the residue-alphabet set cover and generalizes the well-known suffix tree of the sequence set. We provide performance results on selected BAliBASE amino-acid sequences and compare them with those yielded by some prominent approaches.","Multiple sequence alignment based on set covers We introduce a new heuristic for the multiple alignment of a set of sequences. The heuristic is based on a set cover of the residue alphabet of the sequences, and also on the determination of a significant set of blocks comprising subsequences of the sequences to be aligned. These blocks are obtained with the aid of a new data structure, called a suffix-set tree, which is constructed from the input sequences with the guidance of the residue-alphabet set cover and generalizes the well-known suffix tree of the sequence set. We provide performance results on selected BAliBASE amino-acid sequences and compare them with those yielded by some prominent approaches.",Healthcare
Educational data mining and learning analytics: An updated survey,"This survey is an updated and improved version of the previous one published in 2013 in this journal with the title data mining in education. It reviews in a comprehensible and very general way how Educational Data Mining and Learning Analytics have been applied over educational data. In the last decade, this research area has evolved enormously and a wide range of related terms are now used in the bibliography such as Academic Analytics, Institutional Analytics, Teaching Analytics, Data-Driven Education, Data-Driven Decision-Making in Education, Big Data in Education, and Educational Data Science. This paper provides the current state of the art by reviewing the main publications, the key milestones, the knowledge discovery cycle, the main educational environments, the specific tools, the free available datasets, the most used methods, the main objectives, and the future trends in this research area.","Educational data mining and learning analytics: An updated survey This survey is an updated and improved version of the previous one published in 2013 in this journal with the title data mining in education. It reviews in a comprehensible and very general way how Educational Data Mining and Learning Analytics have been applied over educational data. In the last decade, this research area has evolved enormously and a wide range of related terms are now used in the bibliography such as Academic Analytics, Institutional Analytics, Teaching Analytics, Data-Driven Education, Data-Driven Decision-Making in Education, Big Data in Education, and Educational Data Science. This paper provides the current state of the art by reviewing the main publications, the key milestones, the knowledge discovery cycle, the main educational environments, the specific tools, the free available datasets, the most used methods, the main objectives, and the future trends in this research area.",Education
On the sensitivity of coastal quasigeostrophic edge wave interaction to bottom boundary characteristics: possible implications for eddy parameterizations,"The Eady problem of baroclinic instability as applicable to quasi-geostrophic oceanic flows with zero internal PV gradients is revisited by introducing a mild slope and Ekman pumping on the lower boundary. The solution behaviour is determined by the isopycnal slope relative to either the bottom slope or the ratio of Ekman depth to horizontal wavenumber. Attention is paid to the physical interpretation of the growing, decaying and stable disturbances, with emphasis on the intimate connection between the quasigeostrophic edge waves and Eady waves, and the role of the isopycnal slope for the stability properties as opposed to the bottom density gradient. The disturbance structure is found to be strongly influenced by the boundary conditions. For a sloping bottom boundary, the growth rate is enhanced for the most unstable waves if the isopycnals tilt in the same direction as the bottom, but in general non-standard boundary conditions tend to retard the growth of disturbances. In particular, the existence of the long- and short-wave cutoffs is found to be very sensitive to boundary conditions, both for the sloping topography and the Ekman pumping. It is suggested that any cutoffs for the growth rate in an Eady-like problem actually result from the chosen boundary conditions. However, for a certain range of parameters, the maximum growth rate is comparable to that found in the original Eady problem, which may explain the fair success enjoyed by recent eddy parameterizations basing their timescale on the Eady growth rate.","On the sensitivity of coastal quasigeostrophic edge wave interaction to bottom boundary characteristics: possible implications for eddy parameterizations The Eady problem of baroclinic instability as applicable to quasi-geostrophic oceanic flows with zero internal PV gradients is revisited by introducing a mild slope and Ekman pumping on the lower boundary. The solution behaviour is determined by the isopycnal slope relative to either the bottom slope or the ratio of Ekman depth to horizontal wavenumber. Attention is paid to the physical interpretation of the growing, decaying and stable disturbances, with emphasis on the intimate connection between the quasigeostrophic edge waves and Eady waves, and the role of the isopycnal slope for the stability properties as opposed to the bottom density gradient. The disturbance structure is found to be strongly influenced by the boundary conditions. For a sloping bottom boundary, the growth rate is enhanced for the most unstable waves if the isopycnals tilt in the same direction as the bottom, but in general non-standard boundary conditions tend to retard the growth of disturbances. In particular, the existence of the long- and short-wave cutoffs is found to be very sensitive to boundary conditions, both for the sloping topography and the Ekman pumping. It is suggested that any cutoffs for the growth rate in an Eady-like problem actually result from the chosen boundary conditions. However, for a certain range of parameters, the maximum growth rate is comparable to that found in the original Eady problem, which may explain the fair success enjoyed by recent eddy parameterizations basing their timescale on the Eady growth rate.",Environment
On Estimation of Optimal Treatment Regimes For Maximizing t-Year Survival Probability,"A treatment regime is a deterministic function that dictates personalized treatment based on patients individual prognostic information. There is a fast-growing interest in finding optimal treatment regimes to maximize expected long-term clinical outcomes of patients for complex diseases, such as cancer and AIDS. For many clinical studies with survival time as a primary endpoint, a main goal is to maximize patients survival probabilities given treatments. In this article, we first propose two nonparametric estimators for survival function of patients following a given treatment regime. Then, we derive the estimation of the optimal treatment regime based on a value-based searching algorithm within a set of treatment regimes indexed by parameters. The asymptotic properties of the proposed estimators for survival probabilities under derived optimal treatment regimes are established under suitable regularity conditions. Simulations are conducted to evaluate the numerical performance of the proposed estimators under various scenarios. An application to an AIDS clinical trial data is also given to illustrate the methods.","On Estimation of Optimal Treatment Regimes For Maximizing t-Year Survival Probability A treatment regime is a deterministic function that dictates personalized treatment based on patients individual prognostic information. There is a fast-growing interest in finding optimal treatment regimes to maximize expected long-term clinical outcomes of patients for complex diseases, such as cancer and AIDS. For many clinical studies with survival time as a primary endpoint, a main goal is to maximize patients survival probabilities given treatments. In this article, we first propose two nonparametric estimators for survival function of patients following a given treatment regime. Then, we derive the estimation of the optimal treatment regime based on a value-based searching algorithm within a set of treatment regimes indexed by parameters. The asymptotic properties of the proposed estimators for survival probabilities under derived optimal treatment regimes are established under suitable regularity conditions. Simulations are conducted to evaluate the numerical performance of the proposed estimators under various scenarios. An application to an AIDS clinical trial data is also given to illustrate the methods.",Healthcare
Ranking medical jargon in electronic health record notes by adapted distant supervision,"Objective: Allowing patients to access their own electronic health record (EHR) notes through online patient portals has the potential to improve patient-centered care. However, medical jargon, which abounds in EHR notes, has been shown to be a barrier for patient EHR comprehension. Existing knowledge bases that link medical jargon to lay terms or definitions play an important role in alleviating this problem but have low coverage of medical jargon in EHRs. We developed a data-driven approach that mines EHRs to identify and rank medical jargon based on its importance to patients, to support the building of EHR-centric lay language resources. Methods: We developed an innovative adapted distant supervision (ADS) model based on support vector machines to rank medical jargon from EHRs. For distant supervision, we utilized the open-access, collaborative consumer health vocabulary, a large, publicly available resource that links lay terms to medical jargon. We explored both knowledge-based features from the Unified Medical Language System and distributed word representations learned from unlabeled large corpora. We evaluated the ADS model using physician-identified important medical terms. Results: Our ADS model significantly surpassed two state-of-the-art automatic term recognition methods, TFIDF and C-Value, yielding 0.810 ROC-AUC versus 0.710 and 0.667, respectively. Our model identified 10K important medical jargon terms after ranking over 100K candidate terms mined from over 7,500 EHR narratives. Conclusion: Our work is an important step towards enriching lexical resources that link medical jargon to lay termsdefinitions to support patient EHR comprehension. The identified medical jargon terms and their rankings are available upon request.","Ranking medical jargon in electronic health record notes by adapted distant supervision Objective: Allowing patients to access their own electronic health record (EHR) notes through online patient portals has the potential to improve patient-centered care. However, medical jargon, which abounds in EHR notes, has been shown to be a barrier for patient EHR comprehension. Existing knowledge bases that link medical jargon to lay terms or definitions play an important role in alleviating this problem but have low coverage of medical jargon in EHRs. We developed a data-driven approach that mines EHRs to identify and rank medical jargon based on its importance to patients, to support the building of EHR-centric lay language resources. Methods: We developed an innovative adapted distant supervision (ADS) model based on support vector machines to rank medical jargon from EHRs. For distant supervision, we utilized the open-access, collaborative consumer health vocabulary, a large, publicly available resource that links lay terms to medical jargon. We explored both knowledge-based features from the Unified Medical Language System and distributed word representations learned from unlabeled large corpora. We evaluated the ADS model using physician-identified important medical terms. Results: Our ADS model significantly surpassed two state-of-the-art automatic term recognition methods, TFIDF and C-Value, yielding 0.810 ROC-AUC versus 0.710 and 0.667, respectively. Our model identified 10K important medical jargon terms after ranking over 100K candidate terms mined from over 7,500 EHR narratives. Conclusion: Our work is an important step towards enriching lexical resources that link medical jargon to lay termsdefinitions to support patient EHR comprehension. The identified medical jargon terms and their rankings are available upon request.",Healthcare
Scalable Educational Question Generation with Pre-trained Language Models,"The automatic generation of educational questions will play a key role in scaling online education, enabling self-assessment at scale when a global population is manoeuvring their personalised learning journeys. We develop textitEduQG, a novel educational question generation model built by adapting a large language model. Our extensive experiments demonstrate that textitEduQG can produce superior educational questions by further pre-training and fine-tuning a pre-trained language model on the scientific text and science question data.","Scalable Educational Question Generation with Pre-trained Language Models The automatic generation of educational questions will play a key role in scaling online education, enabling self-assessment at scale when a global population is manoeuvring their personalised learning journeys. We develop textitEduQG, a novel educational question generation model built by adapting a large language model. Our extensive experiments demonstrate that textitEduQG can produce superior educational questions by further pre-training and fine-tuning a pre-trained language model on the scientific text and science question data.",Education
Camera motion estimation through planar deformation determination,"In this paper, we propose a global method for estimating the motion of a camera which films a static scene. Our approach is direct, fast and robust, and deals with adjacent frames of a sequence. It is based on a quadratic approximation of the deformation between two images, in the case of a scene with constant depth in the camera coordinate system. This condition is very restrictive but we show that provided translation and depth inverse variations are small enough, the error on optical flow involved by the approximation of depths by a constant is small. In this context, we propose a new model of camera motion, that allows to separate the image deformation in a similarity and a purely projective application, due to change of optical axis direction. This model leads to a quadratic approximation of image deformation that we estimate with an M-estimator; we can immediatly deduce camera motion parameters.","Camera motion estimation through planar deformation determination In this paper, we propose a global method for estimating the motion of a camera which films a static scene. Our approach is direct, fast and robust, and deals with adjacent frames of a sequence. It is based on a quadratic approximation of the deformation between two images, in the case of a scene with constant depth in the camera coordinate system. This condition is very restrictive but we show that provided translation and depth inverse variations are small enough, the error on optical flow involved by the approximation of depths by a constant is small. In this context, we propose a new model of camera motion, that allows to separate the image deformation in a similarity and a purely projective application, due to change of optical axis direction. This model leads to a quadratic approximation of image deformation that we estimate with an M-estimator; we can immediatly deduce camera motion parameters.",Technology
Operations for Learning with Graphical Models,"This paper is a multidisciplinary review of empirical, statistical learning from a graphical model perspective. Well-known examples of graphical models include Bayesian networks, directed graphs representing a Markov chain, and undirected networks representing a Markov field. These graphical models are extended to model data analysis and empirical learning using the notation of plates. Graphical operations for simplifying and manipulating a problem are provided including decomposition, differentiation, and the manipulation of probability models from the exponential family. Two standard algorithm schemas for learning are reviewed in a graphical framework: Gibbs sampling and the expectation maximization algorithm. Using these operations and schemas, some popular algorithms can be synthesized from their graphical specification. This includes versions of linear regression, techniques for feed-forward networks, and learning Gaussian and discrete Bayesian networks from data. The paper concludes by sketching some implications for data analysis and summarizing how some popular algorithms fall within the framework presented. The main original contributions here are the decomposition techniques and the demonstration that graphical models provide a framework for understanding and developing complex learning algorithms.","Operations for Learning with Graphical Models This paper is a multidisciplinary review of empirical, statistical learning from a graphical model perspective. Well-known examples of graphical models include Bayesian networks, directed graphs representing a Markov chain, and undirected networks representing a Markov field. These graphical models are extended to model data analysis and empirical learning using the notation of plates. Graphical operations for simplifying and manipulating a problem are provided including decomposition, differentiation, and the manipulation of probability models from the exponential family. Two standard algorithm schemas for learning are reviewed in a graphical framework: Gibbs sampling and the expectation maximization algorithm. Using these operations and schemas, some popular algorithms can be synthesized from their graphical specification. This includes versions of linear regression, techniques for feed-forward networks, and learning Gaussian and discrete Bayesian networks from data. The paper concludes by sketching some implications for data analysis and summarizing how some popular algorithms fall within the framework presented. The main original contributions here are the decomposition techniques and the demonstration that graphical models provide a framework for understanding and developing complex learning algorithms.",Technology
Measurement of coupling development level of new infrastructure investment and digital transformation and its temporal and spatial evolution trend,"Based on the coupling mechanism between new infrastructure investment level and digital transformation level, a comprehensive index system is constructed. By using entropy weight method, coupling coordination evaluation model, exploratory spatial data analysis (ESDA) and standard deviation ellipse model, the coupling coordination development level of new infrastructure investment level and digital transformation level in 31 provinces and cities in China from 2014 to 2021 is calculated, and its spatial agglomeration level and spatial-temporal evolution characteristics are analyzed.","Measurement of coupling development level of new infrastructure investment and digital transformation and its temporal and spatial evolution trend Based on the coupling mechanism between new infrastructure investment level and digital transformation level, a comprehensive index system is constructed. By using entropy weight method, coupling coordination evaluation model, exploratory spatial data analysis (ESDA) and standard deviation ellipse model, the coupling coordination development level of new infrastructure investment level and digital transformation level in 31 provinces and cities in China from 2014 to 2021 is calculated, and its spatial agglomeration level and spatial-temporal evolution characteristics are analyzed.",Finance
The Paradox of Industrial Involvement in Engineering Higher Education,"This paper discusses the importance of reflective and socially conscious education in engineering schools, particularly within the EECS sector. While most engineering disciplines have historically aligned themselves with the demands of the technology industry, the lack of critical examination of industry practices and their impact on justice, equality, and sustainability is self-evident. Today, the for-profit engineeringtechnology companies, some of which are among the largest in the world, also shape the narrative of engineering education and research in universities. As engineering graduates form the largest cohorts within STEM disciplines in Western countries, they become future professionals who will work, lead, or even establish companies in this industry. Unfortunately, the curriculum within engineering education often lacks a deep understanding of social realities, an essential component of a comprehensive university education. Here we establish this unusual connection with the industry that has driven engineering higher education for several decades and its obvious negative impacts to society. We analyse this nexus and highlight the need for engineering schools to hold a more critical viewpoint. Given the wealth and power of modern technology companies, particularly in the ICT domain, questioning their techno-solutionism narrative is essential within the institutes of higher education.","The Paradox of Industrial Involvement in Engineering Higher Education This paper discusses the importance of reflective and socially conscious education in engineering schools, particularly within the EECS sector. While most engineering disciplines have historically aligned themselves with the demands of the technology industry, the lack of critical examination of industry practices and their impact on justice, equality, and sustainability is self-evident. Today, the for-profit engineeringtechnology companies, some of which are among the largest in the world, also shape the narrative of engineering education and research in universities. As engineering graduates form the largest cohorts within STEM disciplines in Western countries, they become future professionals who will work, lead, or even establish companies in this industry. Unfortunately, the curriculum within engineering education often lacks a deep understanding of social realities, an essential component of a comprehensive university education. Here we establish this unusual connection with the industry that has driven engineering higher education for several decades and its obvious negative impacts to society. We analyse this nexus and highlight the need for engineering schools to hold a more critical viewpoint. Given the wealth and power of modern technology companies, particularly in the ICT domain, questioning their techno-solutionism narrative is essential within the institutes of higher education.",Education
The potential approach in practice,"The potential approach is a general and simple method for modelling interest rates, foreign exchange rates, and in principle other types of financial assets. This paper takes data on some liquid interest rate derivatives, and fits potential models using a small finite-state Markov chain as the base Markov process.","The potential approach in practice The potential approach is a general and simple method for modelling interest rates, foreign exchange rates, and in principle other types of financial assets. This paper takes data on some liquid interest rate derivatives, and fits potential models using a small finite-state Markov chain as the base Markov process.",Finance
2Planning for Contingencies: A Decision-based Approach,"A fundamental assumption made by classical AI planners is that there is no uncertainty in the world: the planner has full knowledge of the conditions under which the plan will be executed and the outcome of every action is fully predictable. These planners cannot therefore construct contingency plans, i.e., plans in which different actions are performed in different circumstances. In this paper we discuss some issues that arise in the representation and construction of contingency plans and describe Cassandra, a partial-order contingency planner. Cassandra uses explicit decision-steps that enable the agent executing the plan to decide which plan branch to follow. The decision-steps in a plan result in subgoals to acquire knowledge, which are planned for in the same way as any other subgoals. Cassandra thus distinguishes the process of gathering information from the process of making decisions. The explicit representation of decisions in Cassandra allows a coherent approach to the problems of contingent planning, and provides a solid base for extensions such as the use of different decision-making procedures.","2Planning for Contingencies: A Decision-based Approach A fundamental assumption made by classical AI planners is that there is no uncertainty in the world: the planner has full knowledge of the conditions under which the plan will be executed and the outcome of every action is fully predictable. These planners cannot therefore construct contingency plans, i.e., plans in which different actions are performed in different circumstances. In this paper we discuss some issues that arise in the representation and construction of contingency plans and describe Cassandra, a partial-order contingency planner. Cassandra uses explicit decision-steps that enable the agent executing the plan to decide which plan branch to follow. The decision-steps in a plan result in subgoals to acquire knowledge, which are planned for in the same way as any other subgoals. Cassandra thus distinguishes the process of gathering information from the process of making decisions. The explicit representation of decisions in Cassandra allows a coherent approach to the problems of contingent planning, and provides a solid base for extensions such as the use of different decision-making procedures.",Technology
Statistical Predictive Models in Ecology: Comparison of Performances and Assessment of Applicability,"Ecological systems are governed by complex interactions which are mainly nonlinear. In order to capture this complexity and nonlinearity, statistical models recently gained popularity. However, although these models are commonly applied in ecology, there are no studies to date aiming to assess the applicability and performance. We provide an overview for nature of the wide range of the data sets and predictive variables, from both aquatic and terrestrial ecosystems with different scales of time-dependent dynamics, and the applicability and robustness of predictive modeling methods on such data sets by comparing different statistical modeling approaches. The methods considered k-NN, LDA, QDA, generalized linear models (GLM) feedforward multilayer backpropagation networks and pseudo-supervised network ARTMAP. For ecosystems involving time-dependent dynamics and periodicities whose frequency are possibly less than the time scale of the data considered, GLM and connectionist neural network models appear to be most suitable and robust, provided that a predictive variable reflecting these time-dependent dynamics included in the model either implicitly or explicitly. For spatial data, which does not include any time-dependence comparable to the time scale covered by the data, on the other hand, neighborhood based methods such as k-NN and ARTMAP proved to be more robust than other methods considered in this study. In addition, for predictive modeling purposes, first a suitable, computationally inexpensive method should be applied to the problem at hand a good predictive performance of which would render the computational cost and efforts associated with complex variants unnecessary.","Statistical Predictive Models in Ecology: Comparison of Performances and Assessment of Applicability Ecological systems are governed by complex interactions which are mainly nonlinear. In order to capture this complexity and nonlinearity, statistical models recently gained popularity. However, although these models are commonly applied in ecology, there are no studies to date aiming to assess the applicability and performance. We provide an overview for nature of the wide range of the data sets and predictive variables, from both aquatic and terrestrial ecosystems with different scales of time-dependent dynamics, and the applicability and robustness of predictive modeling methods on such data sets by comparing different statistical modeling approaches. The methods considered k-NN, LDA, QDA, generalized linear models (GLM) feedforward multilayer backpropagation networks and pseudo-supervised network ARTMAP. For ecosystems involving time-dependent dynamics and periodicities whose frequency are possibly less than the time scale of the data considered, GLM and connectionist neural network models appear to be most suitable and robust, provided that a predictive variable reflecting these time-dependent dynamics included in the model either implicitly or explicitly. For spatial data, which does not include any time-dependence comparable to the time scale covered by the data, on the other hand, neighborhood based methods such as k-NN and ARTMAP proved to be more robust than other methods considered in this study. In addition, for predictive modeling purposes, first a suitable, computationally inexpensive method should be applied to the problem at hand a good predictive performance of which would render the computational cost and efforts associated with complex variants unnecessary.",Healthcare
Individual and Social Requirement Aspects of Sustainable eLearning Systems,"Internationalization of the higher education has created the so-called borderless university, which provides better opportunities for learning and increases the human and social sustainability. eLearning systems are a special kind of software systems, developed to provide a platform for accessible teaching and learning, including also online access to learning materials and online support for learning and teaching. The aim of our current work is to extract, analyse, and combine the results from multiple studies in order to develop an RE framework for sustainable eLearning systems. We call a system sustainable, if it has a positive effect on and whose direct and indirect negative impacts resulting from its development, deployment, and usage are minimal. Sustainability has various dimensions. We classify sustainability requirements of eLearning system to five dimensions: individual (human), social, technical, environmental, and economic. In this paper, we focus on human and social aspects (i.e., individual needs the relationship of people within society), as the eLearning systems have a very strong impact on human dimension of sustainability, where their impact on environmental dimension is rather small. This also provides us a basis to identify the corresponding requirements for sustainable eLearning systems. These requirements include collaboration, learner-centred features, leadership development and the reuse of the learning materials. As a result, achieving individual and social requirements for eLearning systems would provide higher quality of leaning and teaching, as well as better opportunities for learning and increasing the human and social sustainability.","Individual and Social Requirement Aspects of Sustainable eLearning Systems Internationalization of the higher education has created the so-called borderless university, which provides better opportunities for learning and increases the human and social sustainability. eLearning systems are a special kind of software systems, developed to provide a platform for accessible teaching and learning, including also online access to learning materials and online support for learning and teaching. The aim of our current work is to extract, analyse, and combine the results from multiple studies in order to develop an RE framework for sustainable eLearning systems. We call a system sustainable, if it has a positive effect on and whose direct and indirect negative impacts resulting from its development, deployment, and usage are minimal. Sustainability has various dimensions. We classify sustainability requirements of eLearning system to five dimensions: individual (human), social, technical, environmental, and economic. In this paper, we focus on human and social aspects (i.e., individual needs the relationship of people within society), as the eLearning systems have a very strong impact on human dimension of sustainability, where their impact on environmental dimension is rather small. This also provides us a basis to identify the corresponding requirements for sustainable eLearning systems. These requirements include collaboration, learner-centred features, leadership development and the reuse of the learning materials. As a result, achieving individual and social requirements for eLearning systems would provide higher quality of leaning and teaching, as well as better opportunities for learning and increasing the human and social sustainability.",Environment
Surprises in approximating Levenshtein distances,"The Levenshtein distance is an important tool for the comparison of symbolic sequences, with many appearances in genome research, linguistics and other areas. For efficient applications, an approximation by a distance of smaller computational complexity is highly desirable. However, our comparison of the Levenshtein with a generic dictionary-based distance indicates their statistical independence. This suggests that a simplification along this line might not be possible without restricting the class of sequences. Several other probabilistic properties are briefly discussed, emphasizing various questions that deserve further investigation.","Surprises in approximating Levenshtein distances The Levenshtein distance is an important tool for the comparison of symbolic sequences, with many appearances in genome research, linguistics and other areas. For efficient applications, an approximation by a distance of smaller computational complexity is highly desirable. However, our comparison of the Levenshtein with a generic dictionary-based distance indicates their statistical independence. This suggests that a simplification along this line might not be possible without restricting the class of sequences. Several other probabilistic properties are briefly discussed, emphasizing various questions that deserve further investigation.",Healthcare
Learning by Teaching: Key Challenges and Design Implications,"Benefits of learning by teaching (LbT) have been highlighted by previous studies from a pedagogical lens, as well as through computer-supported systems. However, the challenges that university students face in technology-mediated LbTunicodex2013whether it be teaching oneself, teaching a peer, or teaching an agentunicodex2013are not well understood. Furthermore, there is a gap in knowledge on the challenges that students encounter throughout the process of teaching (content selection, preparation, teaching, receiving and giving feedback, and reflection) despite its importance to the design of LbT platforms. Thus, we conducted a study with 24 university students where they taught content they had not fully grasped, without guidance, and participated in a semi-structured interview. Results demonstrate that participants encountered the following challenges: psychological barriers relating to self and others, and lack of know-how. Furthermore, we illuminate design implications required to overcome these challenges and benefit from LbT without requiring prior training in pedagogy.","Learning by Teaching: Key Challenges and Design Implications Benefits of learning by teaching (LbT) have been highlighted by previous studies from a pedagogical lens, as well as through computer-supported systems. However, the challenges that university students face in technology-mediated LbTunicodex2013whether it be teaching oneself, teaching a peer, or teaching an agentunicodex2013are not well understood. Furthermore, there is a gap in knowledge on the challenges that students encounter throughout the process of teaching (content selection, preparation, teaching, receiving and giving feedback, and reflection) despite its importance to the design of LbT platforms. Thus, we conducted a study with 24 university students where they taught content they had not fully grasped, without guidance, and participated in a semi-structured interview. Results demonstrate that participants encountered the following challenges: psychological barriers relating to self and others, and lack of know-how. Furthermore, we illuminate design implications required to overcome these challenges and benefit from LbT without requiring prior training in pedagogy.",Education
A finite element study of the influence of the osteotomy surface on the backward displacement during exophthalmia reduction,"Exophthalmia is characterized by a protrusion of the eyeball. The most frequent surgery consists in an osteotomy of the orbit walls to increase the orbital volume and to retrieve a normal eye position. Only a few clinical obser-vations have estimated the relationship between the eyeball backward dis-placement and the decompressed fat tissue volume. This paper presents a method to determine the relationship between the eyeball backward displace-ment and the osteotomy surface made by the surgeon, in order to improve ex-ophthalmia reduction planning. A poroelastic finite element model involving morphology, material properties of orbital components, and surgical gesture is proposed to perform this study on 12 patients. As a result, the osteotomy sur-face seems to have a non-linear influence on the backward displacement. More-over, the FE model permits to give a first estimation of an average law linking those two parameters. This law may be helpful in a surgical planning frame-work.","A finite element study of the influence of the osteotomy surface on the backward displacement during exophthalmia reduction Exophthalmia is characterized by a protrusion of the eyeball. The most frequent surgery consists in an osteotomy of the orbit walls to increase the orbital volume and to retrieve a normal eye position. Only a few clinical obser-vations have estimated the relationship between the eyeball backward dis-placement and the decompressed fat tissue volume. This paper presents a method to determine the relationship between the eyeball backward displace-ment and the osteotomy surface made by the surgeon, in order to improve ex-ophthalmia reduction planning. A poroelastic finite element model involving morphology, material properties of orbital components, and surgical gesture is proposed to perform this study on 12 patients. As a result, the osteotomy sur-face seems to have a non-linear influence on the backward displacement. More-over, the FE model permits to give a first estimation of an average law linking those two parameters. This law may be helpful in a surgical planning frame-work.",Healthcare
LLMCL-GEC: Advancing Grammatical Error Correction with LLM-Driven Curriculum Learning,"While large-scale language models (LLMs) have demonstrated remarkable capabilities in specific natural language processing (NLP) tasks, they may still lack proficiency compared to specialized models in certain domains, such as grammatical error correction (GEC). Drawing inspiration from the concept of curriculum learning, we have delved into refining LLMs into proficient GEC experts by devising effective curriculum learning (CL) strategies. In this paper, we introduce a novel approach, termed LLM-based curriculum learning, which capitalizes on the robust semantic comprehension and discriminative prowess inherent in LLMs to gauge the complexity of GEC training data. Unlike traditional curriculum learning techniques, our method closely mirrors human expert-designed curriculums. Leveraging the proposed LLM-based CL method, we sequentially select varying levels of curriculums ranging from easy to hard, and iteratively train and refine using the pretrianed T5 and LLaMA series models. Through rigorous testing and analysis across diverse benchmark assessments in English GEC, including the CoNLL14 test, BEA19 test, and BEA19 development sets, our approach showcases a significant performance boost over baseline models and conventional curriculum learning methodologies.","LLMCL-GEC: Advancing Grammatical Error Correction with LLM-Driven Curriculum Learning While large-scale language models (LLMs) have demonstrated remarkable capabilities in specific natural language processing (NLP) tasks, they may still lack proficiency compared to specialized models in certain domains, such as grammatical error correction (GEC). Drawing inspiration from the concept of curriculum learning, we have delved into refining LLMs into proficient GEC experts by devising effective curriculum learning (CL) strategies. In this paper, we introduce a novel approach, termed LLM-based curriculum learning, which capitalizes on the robust semantic comprehension and discriminative prowess inherent in LLMs to gauge the complexity of GEC training data. Unlike traditional curriculum learning techniques, our method closely mirrors human expert-designed curriculums. Leveraging the proposed LLM-based CL method, we sequentially select varying levels of curriculums ranging from easy to hard, and iteratively train and refine using the pretrianed T5 and LLaMA series models. Through rigorous testing and analysis across diverse benchmark assessments in English GEC, including the CoNLL14 test, BEA19 test, and BEA19 development sets, our approach showcases a significant performance boost over baseline models and conventional curriculum learning methodologies.",Education
Implementing New Technology in Educational Systems,"Educators are more than workers within educational systems; they are stewards of educational systems. They must analyze student performance data, identify patterns that inform targeted interventions and personalized learning plans, continuously develop the curriculum, set ambitious learning goals and use up-to-date pedagogical theory to adapt instructional strategies, act as advocates for educational policies that promote inclusivity and equity, and much more. Most educators deeply care about the learning and wellbeing of their students and colleagues. Given the chance, they will do whatever they can to make improvements to these ends. In this role as architects of change, educators deal with conflicting definitions of success, multiple stakeholders, complex causal relationships, ambiguous data, and intricate human factors. Amid all this, most educators and the educational systems around them are strained to the capacity of what their time, training, and budgets allow. The problem is not merely that they must perform demanding tasks, but more so that they must constantly implement improvements and interventions amid the complex challenges of the organizations in which they work. These challenges can be especially difficult in implementation of related education technology, which is continuously developing at sometimes rapid pace. Whether the context is an individual classroom, a school district, or a postsecondary institution, implementing beneficial human-technology partnerships requires attending to the needs and constraints of these classrooms, districts, institutions, and so forth as organizations and engaging in this work as a partnership with educators. This chapter lays out the principles and processes of developing successful educator-technology partnerships including key considerations for each step and an example protocol for engaging in this endeavor.","Implementing New Technology in Educational Systems Educators are more than workers within educational systems; they are stewards of educational systems. They must analyze student performance data, identify patterns that inform targeted interventions and personalized learning plans, continuously develop the curriculum, set ambitious learning goals and use up-to-date pedagogical theory to adapt instructional strategies, act as advocates for educational policies that promote inclusivity and equity, and much more. Most educators deeply care about the learning and wellbeing of their students and colleagues. Given the chance, they will do whatever they can to make improvements to these ends. In this role as architects of change, educators deal with conflicting definitions of success, multiple stakeholders, complex causal relationships, ambiguous data, and intricate human factors. Amid all this, most educators and the educational systems around them are strained to the capacity of what their time, training, and budgets allow. The problem is not merely that they must perform demanding tasks, but more so that they must constantly implement improvements and interventions amid the complex challenges of the organizations in which they work. These challenges can be especially difficult in implementation of related education technology, which is continuously developing at sometimes rapid pace. Whether the context is an individual classroom, a school district, or a postsecondary institution, implementing beneficial human-technology partnerships requires attending to the needs and constraints of these classrooms, districts, institutions, and so forth as organizations and engaging in this work as a partnership with educators. This chapter lays out the principles and processes of developing successful educator-technology partnerships including key considerations for each step and an example protocol for engaging in this endeavor.",Education
What Can Learn from PER: Physics Education Research?,"I believe that most teachers develop a belief in a set of pedagogical practices. As we teach, we try different ways to teach topics and then judge how successful the methods were. After several years, we have a compilation of techniques in our teaching toolbox. New teachers are at a disadvantage because they have fewer prior experiences to draw upon. Luckily, there is a group of physicists and physics educators who are researching how students learn physics, and have been able to show evidence of effective education practices in physics. They field of study is called PER: Physics Education Research. I asked Chandralekha Singh, one of the leaders in PER, to summarize some of the most relevant PER findings and her response follows.","What Can Learn from PER: Physics Education Research? I believe that most teachers develop a belief in a set of pedagogical practices. As we teach, we try different ways to teach topics and then judge how successful the methods were. After several years, we have a compilation of techniques in our teaching toolbox. New teachers are at a disadvantage because they have fewer prior experiences to draw upon. Luckily, there is a group of physicists and physics educators who are researching how students learn physics, and have been able to show evidence of effective education practices in physics. They field of study is called PER: Physics Education Research. I asked Chandralekha Singh, one of the leaders in PER, to summarize some of the most relevant PER findings and her response follows.",Education
Stochastic Optimal Power Flow with Network Reconfiguration: Congestion Management and Facilitating Grid Integration of Renewables,"There has been a significant growth of variable renewable generation in the power grid today. However, the industry still uses deterministic optimization to model and solve the optimal power flow (OPF) problem for real-time generation dispatch that ignores the uncertainty associated with intermittent renewable power. Thus, it is necessary to study stochastic OPF (SOPF) that can better handle uncertainty since SOPF is able to consider the probabilistic forecasting information of intermittent renewables. Transmission network congestion is one of the main reasons for renewable energy curtailment. Prior efforts in the literature show that utilizing transmission network reconfiguration can relieve congestion and resolve congestion-induced issues. This paper enhances SOPF by incorporating network reconfiguration into the dispatch model. Numerical simulations show that renewable curtailment can be avoided with the proposed network reconfiguration scheme that relieves transmission congestion in post-contingency situations. It is also shown that network reconfiguration can substantially reduce congestion cost, especially the contingency-case congestion cost.","Stochastic Optimal Power Flow with Network Reconfiguration: Congestion Management and Facilitating Grid Integration of Renewables There has been a significant growth of variable renewable generation in the power grid today. However, the industry still uses deterministic optimization to model and solve the optimal power flow (OPF) problem for real-time generation dispatch that ignores the uncertainty associated with intermittent renewable power. Thus, it is necessary to study stochastic OPF (SOPF) that can better handle uncertainty since SOPF is able to consider the probabilistic forecasting information of intermittent renewables. Transmission network congestion is one of the main reasons for renewable energy curtailment. Prior efforts in the literature show that utilizing transmission network reconfiguration can relieve congestion and resolve congestion-induced issues. This paper enhances SOPF by incorporating network reconfiguration into the dispatch model. Numerical simulations show that renewable curtailment can be avoided with the proposed network reconfiguration scheme that relieves transmission congestion in post-contingency situations. It is also shown that network reconfiguration can substantially reduce congestion cost, especially the contingency-case congestion cost.",Environment
Physics Faculty and Educational Researchers: Divergent Expectations as Barriers to the Diffusion of Innovations,"Physics Education Research (PER) practitioners have engaged in substantial curriculum development and dissemination work in recent years. Yet, it appears that this work has had minimal influence on the fundamental teaching practices of typical physics faculty. To better understand this situation interviews were conducted with 5 likely users of physics education research. All reported making changes in their instructional practices and all were influenced, to some extent, by educational research. Yet, none made full use of educational research and most had complaints about their interactions with educational researchers. In this paper we examine how these instructors used educational research in making instructional decisions and identify divergent expectations about how researchers and faculty can work together to improve student learning. Although different instructors emphasized different aspects of this discrepancy between expectations, we believe that they are all related to a single underlying issue: the typical dissemination model is to disseminate curricular innovations and have faculty adopt them with minimal changes while faculty expect researchers to work with them to incorporate research-based knowledge and materials into their unique instructional situations. Implications and recommendations are discussed.","Physics Faculty and Educational Researchers: Divergent Expectations as Barriers to the Diffusion of Innovations Physics Education Research (PER) practitioners have engaged in substantial curriculum development and dissemination work in recent years. Yet, it appears that this work has had minimal influence on the fundamental teaching practices of typical physics faculty. To better understand this situation interviews were conducted with 5 likely users of physics education research. All reported making changes in their instructional practices and all were influenced, to some extent, by educational research. Yet, none made full use of educational research and most had complaints about their interactions with educational researchers. In this paper we examine how these instructors used educational research in making instructional decisions and identify divergent expectations about how researchers and faculty can work together to improve student learning. Although different instructors emphasized different aspects of this discrepancy between expectations, we believe that they are all related to a single underlying issue: the typical dissemination model is to disseminate curricular innovations and have faculty adopt them with minimal changes while faculty expect researchers to work with them to incorporate research-based knowledge and materials into their unique instructional situations. Implications and recommendations are discussed.",Education
Is an historical economic crisis upcoming?,"In this work, the time chart of Dow Jones Industrial Average (DJIA) index is analyzed and approach of recession time term is predicted, which may be hallmark of a worldwide economic crisis. However, the methods used for the prediction will be disclosed a few years from now. On the other hand, this work will be updated by the author frequently once in every few months where no revisions will be made on the previous uploads and a timestamp will designate each part. Thus, the time evolution of the crisis can be followed and the prediction may be verified by the readers in time.","Is an historical economic crisis upcoming? In this work, the time chart of Dow Jones Industrial Average (DJIA) index is analyzed and approach of recession time term is predicted, which may be hallmark of a worldwide economic crisis. However, the methods used for the prediction will be disclosed a few years from now. On the other hand, this work will be updated by the author frequently once in every few months where no revisions will be made on the previous uploads and a timestamp will designate each part. Thus, the time evolution of the crisis can be followed and the prediction may be verified by the readers in time.",Finance
Pandemic Pedagogy: Evaluating Remote Education Strategies during COVID-19,"The COVID-19 pandemic precipitated an abrupt shift in the educational landscape, compelling universities to transition from in-person to online instruction. This sudden shift left many university instructors grappling with the intricacies of remote teaching. Now, with the pandemic behind us, we present a retrospective study aimed at understanding and evaluating the remote teaching practices employed during that period. Drawing from a cross-sectional analysis of 300 computer science students who underwent a full year of online education during the lockdown, our findings indicate that while remote teaching practices moderately influenced students learning outcomes, they had a pronounced positive impact on student satisfaction. Remarkably, these outcomes were consistent across various demographics, including country, gender, and educational level. As we reflect on the lessons from this global event, this research offers evidence-based recommendations that could inform educational strategies in unwelcoming future scenarios of a similar nature, ensuring both student satisfaction and effective learning outcomes in online settings.","Pandemic Pedagogy: Evaluating Remote Education Strategies during COVID-19 The COVID-19 pandemic precipitated an abrupt shift in the educational landscape, compelling universities to transition from in-person to online instruction. This sudden shift left many university instructors grappling with the intricacies of remote teaching. Now, with the pandemic behind us, we present a retrospective study aimed at understanding and evaluating the remote teaching practices employed during that period. Drawing from a cross-sectional analysis of 300 computer science students who underwent a full year of online education during the lockdown, our findings indicate that while remote teaching practices moderately influenced students learning outcomes, they had a pronounced positive impact on student satisfaction. Remarkably, these outcomes were consistent across various demographics, including country, gender, and educational level. As we reflect on the lessons from this global event, this research offers evidence-based recommendations that could inform educational strategies in unwelcoming future scenarios of a similar nature, ensuring both student satisfaction and effective learning outcomes in online settings.",Education
Financial Markets and ESG: How Big Data is Transforming Sustainable Investing in Developing countries,"This study explores the role of big data adoption and financial market development in driving ESG investments in developing countries, using an instrumental variable (IV) approach to address endogeneity. The results show that big data adoption significantly enhances ESG investing, as data-driven analytics improve sustainability assessments and capital allocation. Financial market development also positively influences ESG investments, but its effect is relatively small. A key finding is that inflation negatively impacts ESG investment, highlighting the importance of macroeconomic stability in fostering sustainable finance. In contrast, GDP per capita and foreign direct investment (FDI) are not significant determinants, suggesting that economic growth alone does not drive sustainability efforts. Overall, this study provides empirical evidence that leveraging big data and financial market improvements can accelerate sustainable investing in emerging economies. Policymakers should focus on technological advancements, financial reforms, and inflation control to strengthen ESG investments and long-term sustainability commitments.","Financial Markets and ESG: How Big Data is Transforming Sustainable Investing in Developing countries This study explores the role of big data adoption and financial market development in driving ESG investments in developing countries, using an instrumental variable (IV) approach to address endogeneity. The results show that big data adoption significantly enhances ESG investing, as data-driven analytics improve sustainability assessments and capital allocation. Financial market development also positively influences ESG investments, but its effect is relatively small. A key finding is that inflation negatively impacts ESG investment, highlighting the importance of macroeconomic stability in fostering sustainable finance. In contrast, GDP per capita and foreign direct investment (FDI) are not significant determinants, suggesting that economic growth alone does not drive sustainability efforts. Overall, this study provides empirical evidence that leveraging big data and financial market improvements can accelerate sustainable investing in emerging economies. Policymakers should focus on technological advancements, financial reforms, and inflation control to strengthen ESG investments and long-term sustainability commitments.",Finance
Baumols Climate Disease,"We investigate optimal carbon abatement in a dynamic general equilibrium climate-economy model with endogenous structural change. By differentiating the production of investment from consumption, we show that social cost of carbon can be conceived as a reduction in physical capital. In addition, we distinguish two final sectors in terms of productivity growth and climate vulnerability. We theoretically show that heterogeneous climate vulnerability results in a climate-induced version of Baumols cost disease. Further, if climate-vulnerable sectors have high (low) productivity growth, climate impact can either ameliorate (aggravate) the Baumols cost disease, call for less (more) stringent climate policy. We conclude that carbon abatement should not only factor in unpriced climate capital, but also be tailored to Baumols cost and climate diseases.","Baumols Climate Disease We investigate optimal carbon abatement in a dynamic general equilibrium climate-economy model with endogenous structural change. By differentiating the production of investment from consumption, we show that social cost of carbon can be conceived as a reduction in physical capital. In addition, we distinguish two final sectors in terms of productivity growth and climate vulnerability. We theoretically show that heterogeneous climate vulnerability results in a climate-induced version of Baumols cost disease. Further, if climate-vulnerable sectors have high (low) productivity growth, climate impact can either ameliorate (aggravate) the Baumols cost disease, call for less (more) stringent climate policy. We conclude that carbon abatement should not only factor in unpriced climate capital, but also be tailored to Baumols cost and climate diseases.",Environment
Revealing Treatment Non-Adherence Bias in Clinical Machine Learning Using Large Language Models,"Machine learning systems trained on electronic health records (EHRs) increasingly guide treatment decisions, but their reliability depends on the critical assumption that patients follow the prescribed treatments recorded in EHRs. Using EHR data from 3,623 hypertension patients, we investigate how treatment non-adherence introduces implicit bias that can fundamentally distort both causal inference and predictive modeling. By extracting patient adherence information from clinical notes using a large language model (LLM), we identify 786 patients (21.7) with medication non-adherence. We further uncover key demographic and clinical factors associated with non-adherence, as well as patient-reported reasons including side effects and difficulties obtaining refills. Our findings demonstrate that this implicit bias can not only reverse estimated treatment effects, but also degrade model performance by up to 5 while disproportionately affecting vulnerable populations by exacerbating disparities in decision outcomes and model error rates. This highlights the importance of accounting for treatment non-adherence in developing responsible and equitable clinical machine learning systems.","Revealing Treatment Non-Adherence Bias in Clinical Machine Learning Using Large Language Models Machine learning systems trained on electronic health records (EHRs) increasingly guide treatment decisions, but their reliability depends on the critical assumption that patients follow the prescribed treatments recorded in EHRs. Using EHR data from 3,623 hypertension patients, we investigate how treatment non-adherence introduces implicit bias that can fundamentally distort both causal inference and predictive modeling. By extracting patient adherence information from clinical notes using a large language model (LLM), we identify 786 patients (21.7) with medication non-adherence. We further uncover key demographic and clinical factors associated with non-adherence, as well as patient-reported reasons including side effects and difficulties obtaining refills. Our findings demonstrate that this implicit bias can not only reverse estimated treatment effects, but also degrade model performance by up to 5 while disproportionately affecting vulnerable populations by exacerbating disparities in decision outcomes and model error rates. This highlights the importance of accounting for treatment non-adherence in developing responsible and equitable clinical machine learning systems.",Healthcare
Developing a Robust Computable Phenotype Definition Workflow to Describe Health and Disease in Observational Health Research,"Health informatics can inform decisions that practitioners, patients, policymakers, and researchers need to make about health and disease. Health informatics is built upon patient health data leading to the need to codify patient health information. Such standardization is required to compute population statistics (such as prevalence, incidence, etc.) that are common metrics used in fields such as epidemiology. Reliable decision-making about health and disease rests on our ability to organize, analyze, and assess data repositories that contain patient health data. While standards exist to structure and analyze patient data across patient data sources such as health information exchanges, clinical data repositories, and health data marketplaces, analogous best practices for rigorously defining patient populations in health informatics contexts do not exist. Codifying best practices for developing disease definitions could support the effective development of clinical guidelines, inform algorithms used in clinical decision support systems, and additional patient guidelines. In this paper, we present a workflow for the development of phenotype definitions. This workflow presents a series of recommendations for defining health and disease. Various examples within this paper are presented to demonstrate this workflow in health informatics contexts.","Developing a Robust Computable Phenotype Definition Workflow to Describe Health and Disease in Observational Health Research Health informatics can inform decisions that practitioners, patients, policymakers, and researchers need to make about health and disease. Health informatics is built upon patient health data leading to the need to codify patient health information. Such standardization is required to compute population statistics (such as prevalence, incidence, etc.) that are common metrics used in fields such as epidemiology. Reliable decision-making about health and disease rests on our ability to organize, analyze, and assess data repositories that contain patient health data. While standards exist to structure and analyze patient data across patient data sources such as health information exchanges, clinical data repositories, and health data marketplaces, analogous best practices for rigorously defining patient populations in health informatics contexts do not exist. Codifying best practices for developing disease definitions could support the effective development of clinical guidelines, inform algorithms used in clinical decision support systems, and additional patient guidelines. In this paper, we present a workflow for the development of phenotype definitions. This workflow presents a series of recommendations for defining health and disease. Various examples within this paper are presented to demonstrate this workflow in health informatics contexts.",Healthcare
Spiraling toward market completeness and financial instability,"I study the limit of a large random economy, where a set of consumers invests in financial instruments engineered by banks, in order to optimize their future consumption. This exercise shows that, even in the ideal case of perfect competition, where full information is available to all market participants, the equilibrium develops a marked vulnerability (or susceptibility) to market imperfections, as markets approach completeness and transaction costs vanish. The decrease in transaction costs arises because financial institutions exploit trading instruments to hedge other instruments. This entails trading volumes in the interbank market which diverge in the limit of complete markets. These results suggest that the proliferation of financial instruments exacerbates the effects of market imperfections, calling for theories of market as interacting systems. From a different perspective, in order to prevent an escalation of perverse effects, markets may necessitate institutional structures which are more and more conspicuous as their complexity expands.","Spiraling toward market completeness and financial instability I study the limit of a large random economy, where a set of consumers invests in financial instruments engineered by banks, in order to optimize their future consumption. This exercise shows that, even in the ideal case of perfect competition, where full information is available to all market participants, the equilibrium develops a marked vulnerability (or susceptibility) to market imperfections, as markets approach completeness and transaction costs vanish. The decrease in transaction costs arises because financial institutions exploit trading instruments to hedge other instruments. This entails trading volumes in the interbank market which diverge in the limit of complete markets. These results suggest that the proliferation of financial instruments exacerbates the effects of market imperfections, calling for theories of market as interacting systems. From a different perspective, in order to prevent an escalation of perverse effects, markets may necessitate institutional structures which are more and more conspicuous as their complexity expands.",Finance
Evolutionary Curriculum Training for DRL-Based Navigation Systems,"In recent years, Deep Reinforcement Learning (DRL) has emerged as a promising method for robot collision avoidance. However, such DRL models often come with limitations, such as adapting effectively to structured environments containing various pedestrians. In order to solve this difficulty, previous research has attempted a few approaches, including training an end-to-end solution by integrating a waypoint planner with DRL and developing a multimodal solution to mitigate the drawbacks of the DRL model. However, these approaches have encountered several issues, including slow training times, scalability challenges, and poor coordination among different models. To address these challenges, this paper introduces a novel approach called evolutionary curriculum training to tackle these challenges. The primary goal of evolutionary curriculum training is to evaluate the collision avoidance models competency in various scenarios and create curricula to enhance its insufficient skills. The paper introduces an innovative evaluation technique to assess the DRL models performance in navigating structured maps and avoiding dynamic obstacles. Additionally, an evolutionary training environment generates all the curriculum to improve the DRL models inadequate skills tested in the previous evaluation. We benchmark the performance of our model across five structured environments to validate the hypothesis that this evolutionary training environment leads to a higher success rate and a lower average number of collisions. Further details and results at our project website.","Evolutionary Curriculum Training for DRL-Based Navigation Systems In recent years, Deep Reinforcement Learning (DRL) has emerged as a promising method for robot collision avoidance. However, such DRL models often come with limitations, such as adapting effectively to structured environments containing various pedestrians. In order to solve this difficulty, previous research has attempted a few approaches, including training an end-to-end solution by integrating a waypoint planner with DRL and developing a multimodal solution to mitigate the drawbacks of the DRL model. However, these approaches have encountered several issues, including slow training times, scalability challenges, and poor coordination among different models. To address these challenges, this paper introduces a novel approach called evolutionary curriculum training to tackle these challenges. The primary goal of evolutionary curriculum training is to evaluate the collision avoidance models competency in various scenarios and create curricula to enhance its insufficient skills. The paper introduces an innovative evaluation technique to assess the DRL models performance in navigating structured maps and avoiding dynamic obstacles. Additionally, an evolutionary training environment generates all the curriculum to improve the DRL models inadequate skills tested in the previous evaluation. We benchmark the performance of our model across five structured environments to validate the hypothesis that this evolutionary training environment leads to a higher success rate and a lower average number of collisions. Further details and results at our project website.",Education
Is TinyML Sustainable? Assessing the Environmental Impacts of Machine Learning on Microcontrollers,"The sustained growth of carbon emissions and global waste elicits significant sustainability concerns for our environments future. The growing Internet of Things (IoT) has the potential to exacerbate this issue. However, an emerging area known as Tiny Machine Learning (TinyML) has the opportunity to help address these environmental challenges through sustainable computing practices. TinyML, the deployment of machine learning (ML) algorithms onto low-cost, low-power microcontroller systems, enables on-device sensor analytics that unlocks numerous always-on ML applications. This article discusses both the potential of these TinyML applications to address critical sustainability challenges, as well as the environmental footprint of this emerging technology. Through a complete life cycle analysis (LCA), we find that TinyML systems present opportunities to offset their carbon emissions by enabling applications that reduce the emissions of other sectors. Nevertheless, when globally scaled, the carbon footprint of TinyML systems is not negligible, necessitating that designers factor in environmental impact when formulating new devices. Finally, we outline research directions to enable further sustainable contributions of TinyML.","Is TinyML Sustainable? Assessing the Environmental Impacts of Machine Learning on Microcontrollers The sustained growth of carbon emissions and global waste elicits significant sustainability concerns for our environments future. The growing Internet of Things (IoT) has the potential to exacerbate this issue. However, an emerging area known as Tiny Machine Learning (TinyML) has the opportunity to help address these environmental challenges through sustainable computing practices. TinyML, the deployment of machine learning (ML) algorithms onto low-cost, low-power microcontroller systems, enables on-device sensor analytics that unlocks numerous always-on ML applications. This article discusses both the potential of these TinyML applications to address critical sustainability challenges, as well as the environmental footprint of this emerging technology. Through a complete life cycle analysis (LCA), we find that TinyML systems present opportunities to offset their carbon emissions by enabling applications that reduce the emissions of other sectors. Nevertheless, when globally scaled, the carbon footprint of TinyML systems is not negligible, necessitating that designers factor in environmental impact when formulating new devices. Finally, we outline research directions to enable further sustainable contributions of TinyML.",Environment
Improving Medical Systems in the United States using Knowledge-Based Systems,"America has one of the best medical systems in the world. The medical treatment care options offered by the medical system make it sophisticated. However, many American patients are not receiving health care on a regular basis, and at the same time, they cannot afford it. Also, the current medical system has many flaws such as high medical treatment costs and lack of doctors to accommodate many patients. This paper presents the principles of medical artificial intelligence called the knowledge based system. Doctors can remotely check and monitor their patients health data, medical history, how and what medical tests were done, and the lab results. The patients have access to detailed health information online and do not need to make an appointment with doctors to check their health on a daily basis. One doctor can check many patients simultaneously online (when medical centers are understaffed) and do not need to spend a lot of time with patients. Thus, doctors save more money for patients, because patients will no longer be transporting to medical centers to receive routine health check-ups. Patients do not need to overpay for their insurance because they will have access to the knowledge-based system, and the system will save the patients money to have their health checked and reduce the number of unnecessary medical exams. This paper undertakes a brief overview of research work done in a knowledge based system rule based expert systems in the field of medical practices.","Improving Medical Systems in the United States using Knowledge-Based Systems America has one of the best medical systems in the world. The medical treatment care options offered by the medical system make it sophisticated. However, many American patients are not receiving health care on a regular basis, and at the same time, they cannot afford it. Also, the current medical system has many flaws such as high medical treatment costs and lack of doctors to accommodate many patients. This paper presents the principles of medical artificial intelligence called the knowledge based system. Doctors can remotely check and monitor their patients health data, medical history, how and what medical tests were done, and the lab results. The patients have access to detailed health information online and do not need to make an appointment with doctors to check their health on a daily basis. One doctor can check many patients simultaneously online (when medical centers are understaffed) and do not need to spend a lot of time with patients. Thus, doctors save more money for patients, because patients will no longer be transporting to medical centers to receive routine health check-ups. Patients do not need to overpay for their insurance because they will have access to the knowledge-based system, and the system will save the patients money to have their health checked and reduce the number of unnecessary medical exams. This paper undertakes a brief overview of research work done in a knowledge based system rule based expert systems in the field of medical practices.",Healthcare
Building Multilingual Datasets for Predicting Mental Health Severity through LLMs: Prospects and Challenges,"Large Language Models (LLMs) are increasingly being integrated into various medical fields, including mental health support systems. However, there is a gap in research regarding the effectiveness of LLMs in non-English mental health support applications. To address this problem, we present a novel multilingual adaptation of widely-used mental health datasets, translated from English into six languages (e.g., Greek, Turkish, French, Portuguese, German, and Finnish). This dataset enables a comprehensive evaluation of LLM performance in detecting mental health conditions and assessing their severity across multiple languages. By experimenting with GPT and Llama, we observe considerable variability in performance across languages, despite being evaluated on the same translated dataset. This inconsistency underscores the complexities inherent in multilingual mental health support, where language-specific nuances and mental health data coverage can affect the accuracy of the models. Through comprehensive error analysis, we emphasize the risks of relying exclusively on LLMs in medical settings (e.g., their potential to contribute to misdiagnoses). Moreover, our proposed approach offers significant cost savings for multilingual tasks, presenting a major advantage for broad-scale implementation.","Building Multilingual Datasets for Predicting Mental Health Severity through LLMs: Prospects and Challenges Large Language Models (LLMs) are increasingly being integrated into various medical fields, including mental health support systems. However, there is a gap in research regarding the effectiveness of LLMs in non-English mental health support applications. To address this problem, we present a novel multilingual adaptation of widely-used mental health datasets, translated from English into six languages (e.g., Greek, Turkish, French, Portuguese, German, and Finnish). This dataset enables a comprehensive evaluation of LLM performance in detecting mental health conditions and assessing their severity across multiple languages. By experimenting with GPT and Llama, we observe considerable variability in performance across languages, despite being evaluated on the same translated dataset. This inconsistency underscores the complexities inherent in multilingual mental health support, where language-specific nuances and mental health data coverage can affect the accuracy of the models. Through comprehensive error analysis, we emphasize the risks of relying exclusively on LLMs in medical settings (e.g., their potential to contribute to misdiagnoses). Moreover, our proposed approach offers significant cost savings for multilingual tasks, presenting a major advantage for broad-scale implementation.",Healthcare
Reliable and Efficient Inference of Bayesian Networks from Sparse Data by Statistical Learning Theory,"To learn (statistical) dependencies among random variables requires exponentially large sample size in the number of observed random variables if any arbitrary joint probability distribution can occur. We consider the case that sparse data strongly suggest that the probabilities can be described by a simple Bayesian network, i.e., by a graph with small in-degree Delta. Then this simple law will also explain further data with high confidence. This is shown by calculating bounds on the VC dimension of the set of those probability measures that correspond to simple graphs. This allows to select networks by structural risk minimization and gives reliability bounds on the error of the estimated joint measure without (in contrast to a previous paper) any prior assumptions on the set of possible joint measures. The complexity for searching the optimal Bayesian networks of in-degree Delta increases only polynomially in the number of random varibales for constant Delta and the optimal joint measure associated with a given graph can be found by convex optimization.","Reliable and Efficient Inference of Bayesian Networks from Sparse Data by Statistical Learning Theory To learn (statistical) dependencies among random variables requires exponentially large sample size in the number of observed random variables if any arbitrary joint probability distribution can occur. We consider the case that sparse data strongly suggest that the probabilities can be described by a simple Bayesian network, i.e., by a graph with small in-degree Delta. Then this simple law will also explain further data with high confidence. This is shown by calculating bounds on the VC dimension of the set of those probability measures that correspond to simple graphs. This allows to select networks by structural risk minimization and gives reliability bounds on the error of the estimated joint measure without (in contrast to a previous paper) any prior assumptions on the set of possible joint measures. The complexity for searching the optimal Bayesian networks of in-degree Delta increases only polynomially in the number of random varibales for constant Delta and the optimal joint measure associated with a given graph can be found by convex optimization.",Technology
Cybersecurity and Sustainable Development,"Growing interdependencies between organizations lead them towards the creation of inter-organizational networks where cybersecurity and sustainable development have become one of the most important issues. The Environmental Goods and Services Sector (EGSS) is one of the fastest developing sectors of the economy fueled by the growing relationships between network entities based on ICT usage. In this sector, Green Cybersecurity is an emerging issue because it secures processes related directly and indirectly to environmental management and protection. In the future, the multidimensional development of the EGSS can help European Union to overcome the upcoming crises. At the same time, computer technologies and cybersecurity can contribute to the implementation of the concept of sustainable development. The development of environmental technologies along with their cybersecurity is one of the aims of the realization of sustainable production and domestic security concepts among the EU countries. Hence, the aim of this article is a theoretical discussion and research on the relationships between cybersecurity and sustainable development in inter-organizational networks. Therefore, the article is an attempt to give an answer to the question about the current state of the implementation of cybersecurity in relation to the EGSS part of the economy in different EU countries.","Cybersecurity and Sustainable Development Growing interdependencies between organizations lead them towards the creation of inter-organizational networks where cybersecurity and sustainable development have become one of the most important issues. The Environmental Goods and Services Sector (EGSS) is one of the fastest developing sectors of the economy fueled by the growing relationships between network entities based on ICT usage. In this sector, Green Cybersecurity is an emerging issue because it secures processes related directly and indirectly to environmental management and protection. In the future, the multidimensional development of the EGSS can help European Union to overcome the upcoming crises. At the same time, computer technologies and cybersecurity can contribute to the implementation of the concept of sustainable development. The development of environmental technologies along with their cybersecurity is one of the aims of the realization of sustainable production and domestic security concepts among the EU countries. Hence, the aim of this article is a theoretical discussion and research on the relationships between cybersecurity and sustainable development in inter-organizational networks. Therefore, the article is an attempt to give an answer to the question about the current state of the implementation of cybersecurity in relation to the EGSS part of the economy in different EU countries.",Environment
Interactive Digital Learning Materials for Kindergarten Students in Bangladesh,"The pedagogy of teaching and learning has changed with the proliferation of communication technology and it is necessary to develop interactive learning materials for children that may improve their learning, catching, and memorizing capabilities. Perhaps, one of the most important innovations in the age of technology is multimedia and its application. It is imperative to create high quality and realistic learning environment for children. Interactive learning materials can be easier to understand and deal with their first learning. We developed some interactive learning materials in the form of a video for Playgroup using multimedia application tools. This study investigated the impact of students abilities to acquire new knowledge or skills through interactive learning materials. We visited one kindergartens (Nursery schools), interviewed class teachers about their teaching methods and level of students ability of recognizing English alphabets, pictures, etc. The course teachers were provided interactive learning materials to show their playgroups for a number of sessions. The video included English alphabets with related words and pictures, and motivational fun. We noticed that almost all children were very interested to interact with their leaning video. The students were assessed individually and asked to recognize the alphabets, and pictures. The students adapted with their first alphabets very quickly. However, there were individual differences in their cognitive development. This interactive multimedia can be an alternative to traditional pedagogy for teaching playgroups.","Interactive Digital Learning Materials for Kindergarten Students in Bangladesh The pedagogy of teaching and learning has changed with the proliferation of communication technology and it is necessary to develop interactive learning materials for children that may improve their learning, catching, and memorizing capabilities. Perhaps, one of the most important innovations in the age of technology is multimedia and its application. It is imperative to create high quality and realistic learning environment for children. Interactive learning materials can be easier to understand and deal with their first learning. We developed some interactive learning materials in the form of a video for Playgroup using multimedia application tools. This study investigated the impact of students abilities to acquire new knowledge or skills through interactive learning materials. We visited one kindergartens (Nursery schools), interviewed class teachers about their teaching methods and level of students ability of recognizing English alphabets, pictures, etc. The course teachers were provided interactive learning materials to show their playgroups for a number of sessions. The video included English alphabets with related words and pictures, and motivational fun. We noticed that almost all children were very interested to interact with their leaning video. The students were assessed individually and asked to recognize the alphabets, and pictures. The students adapted with their first alphabets very quickly. However, there were individual differences in their cognitive development. This interactive multimedia can be an alternative to traditional pedagogy for teaching playgroups.",Education
Optimal dividend distribution under Markov-regime switching,"We investigate the problem of optimal dividend distribution for a company in the presence of regime shifts. We consider a company whose cumulative net revenues evolve as a Brownian motion with positive drift that is modulated by a finite state Markov chain, and model the discount rate as a deterministic function of the current state of the chain. In this setting the objective of the company is to maximize the expected cumulative discounted dividend payments until the moment of bankruptcy, which is taken to be the first time that the cash reserves (the cumulative net revenues minus cumulative dividend payments) are zero. We show that, if the drift is positive in each state, it is optimal to adopt a barrier strategy at certain positive regime-dependent levels, and provide an explicit characterization of the value function as the fixed point of a contraction. In the case that the drift is small and negative in one state, the optimal strategy takes a different form, which we explicitly identify if there are two regimes. We also provide a numerical illustration of the sensitivities of the optimal barriers and the influence of regime-switching.","Optimal dividend distribution under Markov-regime switching We investigate the problem of optimal dividend distribution for a company in the presence of regime shifts. We consider a company whose cumulative net revenues evolve as a Brownian motion with positive drift that is modulated by a finite state Markov chain, and model the discount rate as a deterministic function of the current state of the chain. In this setting the objective of the company is to maximize the expected cumulative discounted dividend payments until the moment of bankruptcy, which is taken to be the first time that the cash reserves (the cumulative net revenues minus cumulative dividend payments) are zero. We show that, if the drift is positive in each state, it is optimal to adopt a barrier strategy at certain positive regime-dependent levels, and provide an explicit characterization of the value function as the fixed point of a contraction. In the case that the drift is small and negative in one state, the optimal strategy takes a different form, which we explicitly identify if there are two regimes. We also provide a numerical illustration of the sensitivities of the optimal barriers and the influence of regime-switching.",Finance
"Understanding the Practice, Perception, and Challenge of Blind or Low Vision Students Learning through Accessible Technologies in Non-Inclusive Blind Colleges","In developing and underdeveloped regions, many Blind Colleges exclusively enroll individuals with Blindness or Vision Impairment (BLV) for higher education. While advancements in accessible technologies have facilitated BLV student integration into Integrated Colleges, their implementation in Blind Colleges remains uneven due to complex economic, social, and policy challenges. This study investigates the practices, perceptions, and challenges of BLV students using accessible technologies in a Chinese Blind College through a two-part empirical approach. Our findings demonstrate that tactile and digital technologies enhance access to education but face significant integration barriers. We emphasize the critical role of early education in addressing capability gaps, BLV students aspirations for more inclusive educational environments, and the systemic obstacles within existing frameworks. We advocate for leveraging accessible technologies to transition Blind Colleges into Integrated Colleges, offering actionable insights for policymakers, designers, and educators. Finally, we outline future research directions on accessible technology innovation and its implications for BLV education in resource-constrained settings.","Understanding the Practice, Perception, and Challenge of Blind or Low Vision Students Learning through Accessible Technologies in Non-Inclusive Blind Colleges In developing and underdeveloped regions, many Blind Colleges exclusively enroll individuals with Blindness or Vision Impairment (BLV) for higher education. While advancements in accessible technologies have facilitated BLV student integration into Integrated Colleges, their implementation in Blind Colleges remains uneven due to complex economic, social, and policy challenges. This study investigates the practices, perceptions, and challenges of BLV students using accessible technologies in a Chinese Blind College through a two-part empirical approach. Our findings demonstrate that tactile and digital technologies enhance access to education but face significant integration barriers. We emphasize the critical role of early education in addressing capability gaps, BLV students aspirations for more inclusive educational environments, and the systemic obstacles within existing frameworks. We advocate for leveraging accessible technologies to transition Blind Colleges into Integrated Colleges, offering actionable insights for policymakers, designers, and educators. Finally, we outline future research directions on accessible technology innovation and its implications for BLV education in resource-constrained settings.",Education
Asymptotic expansion for characteristic function in Heston stochastic volatility model with fast mean-reverting correction,"In this note, we derive the characteristic function expansion for logarithm of the underlying asset price in corrected Heston model as proposed by Fouque and Lorig.","Asymptotic expansion for characteristic function in Heston stochastic volatility model with fast mean-reverting correction In this note, we derive the characteristic function expansion for logarithm of the underlying asset price in corrected Heston model as proposed by Fouque and Lorig.",Finance
Intrinsic dimension of a dataset: what properties does one expect?,"We propose an axiomatic approach to the concept of an intrinsic dimension of a dataset, based on a viewpoint of geometry of high-dimensional structures. Our first axiom postulates that high values of dimension be indicative of the presence of the curse of dimensionality (in a certain precise mathematical sense). The second axiom requires the dimension to depend smoothly on a distance between datasets (so that the dimension of a dataset and that of an approximating principal manifold would be close to each other). The third axiom is a normalization condition: the dimension of the Euclidean n-sphere sn is Theta(n). We give an example of a dimension function satisfying our axioms, even though it is in general computationally unfeasible, and discuss a computationally cheap function satisfying most but not all of our axioms (the intrinsic dimensionality of Chavez et al.)","Intrinsic dimension of a dataset: what properties does one expect? We propose an axiomatic approach to the concept of an intrinsic dimension of a dataset, based on a viewpoint of geometry of high-dimensional structures. Our first axiom postulates that high values of dimension be indicative of the presence of the curse of dimensionality (in a certain precise mathematical sense). The second axiom requires the dimension to depend smoothly on a distance between datasets (so that the dimension of a dataset and that of an approximating principal manifold would be close to each other). The third axiom is a normalization condition: the dimension of the Euclidean n-sphere sn is Theta(n). We give an example of a dimension function satisfying our axioms, even though it is in general computationally unfeasible, and discuss a computationally cheap function satisfying most but not all of our axioms (the intrinsic dimensionality of Chavez et al.)",Technology
Utilizing Smartphone-Based Machine Learning in Medical Monitor Data Collection: Seven Segment Digit Recognition,"Biometric measurements captured from medical devices, such as blood pressure gauges, glucose monitors, and weighing scales, are essential to tracking a patients health. Trends in these measurements can accurately track diabetes, cardiovascular issues, and assist medication management for patients. Currently, patients record their results and data of measurement in a physical notebook. It may be weeks before a doctor sees a patients records and can assess the health of the patient. With a predicted 6.8 billion smartphones in the world by 2022, health monitoring platforms, such as Apples HealthKit, can be leveraged to provide the right care at the right time. This research presents a mobile application that enables users to capture medical monitor data and send it to their doctor swiftly. A key contribution of this paper is a robust engine that can recognize digits from medical monitors with an accuracy of 98.2.","Utilizing Smartphone-Based Machine Learning in Medical Monitor Data Collection: Seven Segment Digit Recognition Biometric measurements captured from medical devices, such as blood pressure gauges, glucose monitors, and weighing scales, are essential to tracking a patients health. Trends in these measurements can accurately track diabetes, cardiovascular issues, and assist medication management for patients. Currently, patients record their results and data of measurement in a physical notebook. It may be weeks before a doctor sees a patients records and can assess the health of the patient. With a predicted 6.8 billion smartphones in the world by 2022, health monitoring platforms, such as Apples HealthKit, can be leveraged to provide the right care at the right time. This research presents a mobile application that enables users to capture medical monitor data and send it to their doctor swiftly. A key contribution of this paper is a robust engine that can recognize digits from medical monitors with an accuracy of 98.2.",Healthcare
An extension of Paulsen-Gjessings risk model with stochastic return on investments,"We consider in this paper a general two-sided jump-diffusion risk model that allows for risky investments as well as for correlation between the two Brownian motions driving insurance risk and investment return. We first introduce the model and then find the integro-differential equations satisfied by the Gerber-Shiu functions as well as the expected discounted penalty functions at ruin caused by a claim or by oscillation; We also study the dividend problem for the threshold and barrier strategies, the moments and moment-generating function of the total discounted dividends until ruin are discussed. Some examples are given for special cases.","An extension of Paulsen-Gjessings risk model with stochastic return on investments We consider in this paper a general two-sided jump-diffusion risk model that allows for risky investments as well as for correlation between the two Brownian motions driving insurance risk and investment return. We first introduce the model and then find the integro-differential equations satisfied by the Gerber-Shiu functions as well as the expected discounted penalty functions at ruin caused by a claim or by oscillation; We also study the dividend problem for the threshold and barrier strategies, the moments and moment-generating function of the total discounted dividends until ruin are discussed. Some examples are given for special cases.",Finance
Pricing approximations and error estimates for local Lvy-type models with default,"We find approximate solutions of partial integro-differential equations, which arise in financial models when defaultable assets are described by general scalar Levy-type stochastic processes. We derive rigorous error bounds for the approximate solutions. We also provide numerical examples illustrating the usefulness and versatility of our methods in a variety of financial settings.","Pricing approximations and error estimates for local Lvy-type models with default We find approximate solutions of partial integro-differential equations, which arise in financial models when defaultable assets are described by general scalar Levy-type stochastic processes. We derive rigorous error bounds for the approximate solutions. We also provide numerical examples illustrating the usefulness and versatility of our methods in a variety of financial settings.",Finance
Learning Unions of (1)-Dimensional Rectangles,"We consider the problem of learning unions of rectangles over the domain bn, in the uniform distribution membership query learning setting, where both b and n are large. We obtain poly(n, log b)-time algorithms for the following classes: - poly(n log b)-way Majority of O(fraclog(n log b) log log(n log b))-dimensional rectangles. - Union of poly(log(n log b)) many O(fraclog2 (n log b) (log log(n log b) log log log (n log b))2)-dimensional rectangles. - poly(n log b)-way Majority of poly(n log b)-Or of disjoint O(fraclog(n log b) log log(n log b))-dimensional rectangles. Our main algorithmic tool is an extension of Jacksons boosting- and Fourier-based Harmonic Sieve algorithm Jackson 1997 to the domain bn, building on work of Akavia, Goldwasser, Safra 2003. Other ingredients used to obtain the results stated above are techniques from exact learning Beimel, Kushilevitz 1998 and ideas from recent work on learning augmented AC0 circuits Jackson, Klivans, Servedio 2002 and on representing Boolean functions as thresholds of parities Klivans, Servedio 2001.","Learning Unions of (1)-Dimensional Rectangles We consider the problem of learning unions of rectangles over the domain bn, in the uniform distribution membership query learning setting, where both b and n are large. We obtain poly(n, log b)-time algorithms for the following classes: - poly(n log b)-way Majority of O(fraclog(n log b) log log(n log b))-dimensional rectangles. - Union of poly(log(n log b)) many O(fraclog2 (n log b) (log log(n log b) log log log (n log b))2)-dimensional rectangles. - poly(n log b)-way Majority of poly(n log b)-Or of disjoint O(fraclog(n log b) log log(n log b))-dimensional rectangles. Our main algorithmic tool is an extension of Jacksons boosting- and Fourier-based Harmonic Sieve algorithm Jackson 1997 to the domain bn, building on work of Akavia, Goldwasser, Safra 2003. Other ingredients used to obtain the results stated above are techniques from exact learning Beimel, Kushilevitz 1998 and ideas from recent work on learning augmented AC0 circuits Jackson, Klivans, Servedio 2002 and on representing Boolean functions as thresholds of parities Klivans, Servedio 2001.",Technology
Unlocking the Potential of Renewable Energy Through Curtailment Prediction,A significant fraction (5-15) of renewable energy generated goes into waste in the grids around the world today due to oversupply issues and transmission constraints. Being able to predict when and where renewable curtailment occurs would improve renewable utilization. The core of this work is to enable the machine learning community to help decarbonize electricity grids by unlocking the potential of renewable energy through curtailment prediction.,Unlocking the Potential of Renewable Energy Through Curtailment Prediction A significant fraction (5-15) of renewable energy generated goes into waste in the grids around the world today due to oversupply issues and transmission constraints. Being able to predict when and where renewable curtailment occurs would improve renewable utilization. The core of this work is to enable the machine learning community to help decarbonize electricity grids by unlocking the potential of renewable energy through curtailment prediction.,Environment
Steady coexistence of the subjects of the market representing the private and state capital,"The sustainability conditions for the market participants with a different ownership model were also determined. It was revealed, that the nonlinear form of the equations describing the market behavior with the prevailing private capital, predetermines the development of such a market according to the subharmonic cascade scenario. The latter is presumably the reason of the periodically arising economic crises.","Steady coexistence of the subjects of the market representing the private and state capital The sustainability conditions for the market participants with a different ownership model were also determined. It was revealed, that the nonlinear form of the equations describing the market behavior with the prevailing private capital, predetermines the development of such a market according to the subharmonic cascade scenario. The latter is presumably the reason of the periodically arising economic crises.",Finance
Pedagogy-R1: Pedagogically-Aligned Reasoning Model with Balanced Educational Benchmark,"Recent advances in large reasoning models (LRMs) show strong performance in structured domains such as mathematics and programming; however, they often lack pedagogical coherence and realistic teaching behaviors. To bridge this gap, we introduce Pedagogy-R1, a framework that adapts LRMs for classroom use through three innovations: (1) a distillation-based pipeline that filters and refines model outputs for instruction-tuning, (2) the Well-balanced Educational Benchmark (WBEB), which evaluates performance across subject knowledge, pedagogical knowledge, tracing, essay scoring, and teacher decision-making, and (3) a Chain-of-Pedagogy (CoP) prompting strategy for generating and eliciting teacher-style reasoning. Our mixed-method evaluation combines quantitative metrics with qualitative analysis, providing the first systematic assessment of LRMs pedagogical strengths and limitations.","Pedagogy-R1: Pedagogically-Aligned Reasoning Model with Balanced Educational Benchmark Recent advances in large reasoning models (LRMs) show strong performance in structured domains such as mathematics and programming; however, they often lack pedagogical coherence and realistic teaching behaviors. To bridge this gap, we introduce Pedagogy-R1, a framework that adapts LRMs for classroom use through three innovations: (1) a distillation-based pipeline that filters and refines model outputs for instruction-tuning, (2) the Well-balanced Educational Benchmark (WBEB), which evaluates performance across subject knowledge, pedagogical knowledge, tracing, essay scoring, and teacher decision-making, and (3) a Chain-of-Pedagogy (CoP) prompting strategy for generating and eliciting teacher-style reasoning. Our mixed-method evaluation combines quantitative metrics with qualitative analysis, providing the first systematic assessment of LRMs pedagogical strengths and limitations.",Education
LXPER Index: a curriculum-specific text readability assessment model for EFL students in Korea,"Automatic readability assessment is one of the most important applications of Natural Language Processing (NLP) in education. Since automatic readability assessment allows the fast selection of appropriate reading material for readers at all levels of proficiency, it can be particularly useful for the English education of English as Foreign Language (EFL) students around the world. Most readability assessment models are developed for the native readers of English and have low accuracy for texts in the non-native English Language Training (ELT) curriculum. We introduce LXPER Index, which is a readability assessment model for non-native EFL readers in the ELT curriculum of Korea. Our experiments show that our new model, trained with CoKEC-text (Text Corpus of the Korean ELT Curriculum), significantly improves the accuracy of automatic readability assessment for texts in the Korean ELT curriculum.","LXPER Index: a curriculum-specific text readability assessment model for EFL students in Korea Automatic readability assessment is one of the most important applications of Natural Language Processing (NLP) in education. Since automatic readability assessment allows the fast selection of appropriate reading material for readers at all levels of proficiency, it can be particularly useful for the English education of English as Foreign Language (EFL) students around the world. Most readability assessment models are developed for the native readers of English and have low accuracy for texts in the non-native English Language Training (ELT) curriculum. We introduce LXPER Index, which is a readability assessment model for non-native EFL readers in the ELT curriculum of Korea. Our experiments show that our new model, trained with CoKEC-text (Text Corpus of the Korean ELT Curriculum), significantly improves the accuracy of automatic readability assessment for texts in the Korean ELT curriculum.",Education
Generative AI and Teachers -- For Us or Against Us? A Case Study,"We present insightful results of a survey on the adoption of generative artificial intelligence (GenAI) by university teachers in their teaching activities. The transformation of education by GenAI, particularly large language models (LLMs), has been presenting both opportunities and challenges, including cheating by students. We prepared the online survey according to best practices and the questions were created by the authors, who have pedagogy experience. The survey contained 12 questions and a pilot study was first conducted. The survey was then sent to all teachers in multiple departments across different campuses of the university of interest in Sweden: Luleaa University of Technology. The survey was available in both Swedish and English. The results show that 35 teachers (more than half) use GenAI out of 67 respondents. Preparation is the teaching activity with the most frequency that GenAI is used for and ChatGPT is the most commonly used GenAI. 59 say it has impacted their teaching, however, 55 say there should be legislation around the use of GenAI, especially as inaccuracies and cheating are the biggest concerns.","Generative AI and Teachers -- For Us or Against Us? A Case Study We present insightful results of a survey on the adoption of generative artificial intelligence (GenAI) by university teachers in their teaching activities. The transformation of education by GenAI, particularly large language models (LLMs), has been presenting both opportunities and challenges, including cheating by students. We prepared the online survey according to best practices and the questions were created by the authors, who have pedagogy experience. The survey contained 12 questions and a pilot study was first conducted. The survey was then sent to all teachers in multiple departments across different campuses of the university of interest in Sweden: Luleaa University of Technology. The survey was available in both Swedish and English. The results show that 35 teachers (more than half) use GenAI out of 67 respondents. Preparation is the teaching activity with the most frequency that GenAI is used for and ChatGPT is the most commonly used GenAI. 59 say it has impacted their teaching, however, 55 say there should be legislation around the use of GenAI, especially as inaccuracies and cheating are the biggest concerns.",Education
A Semantics and Complete Algorithm for Subsumption in the CLASSIC Description Logic,"This paper analyzes the correctness of the subsumption algorithm used in CLASSIC, a description logic-based knowledge representation system that is being used in practical applications. In order to deal efficiently with individuals in CLASSIC descriptions, the developers have had to use an algorithm that is incomplete with respect to the standard, model-theoretic semantics for description logics. We provide a variant semantics for descriptions with respect to which the current implementation is complete, and which can be independently motivated. The soundness and completeness of the polynomial-time subsumption algorithm is established using description graphs, which are an abstracted version of the implementation structures used in CLASSIC, and are of independent interest.","A Semantics and Complete Algorithm for Subsumption in the CLASSIC Description Logic This paper analyzes the correctness of the subsumption algorithm used in CLASSIC, a description logic-based knowledge representation system that is being used in practical applications. In order to deal efficiently with individuals in CLASSIC descriptions, the developers have had to use an algorithm that is incomplete with respect to the standard, model-theoretic semantics for description logics. We provide a variant semantics for descriptions with respect to which the current implementation is complete, and which can be independently motivated. The soundness and completeness of the polynomial-time subsumption algorithm is established using description graphs, which are an abstracted version of the implementation structures used in CLASSIC, and are of independent interest.",Technology
Mobile Apps for Childrens Health and Wellbeing: Design Features and Future Opportunities,"Mobile health apps hold great potential for promoting childrens health and wellbeing. However, there is limited understanding of how these technologies are currently designed to support children with their health concerns or wellness goals. To gain insight into the current landscape of mobile apps designed for childrens health, we retrieved and reviewed 43 apps from IOS and Google Play store that are specifically marketed for children. Our qualitative analysis identified the dominant health focuses and goals of childrens mobile health apps. We analyzed the primary users and their expectations as well as the methods of engagement and involvement adopted. Based on our findings, we discussed the opportunities to support children with chronic illnesses through mobile apps, design for dual use, and design for age appropriateness and digital health safety. This study provides insights and recommendations for app designers, health researchers, and policymakers on strategies for engaging children and parents while also promoting childrens health and wellbeing through mobile technology.","Mobile Apps for Childrens Health and Wellbeing: Design Features and Future Opportunities Mobile health apps hold great potential for promoting childrens health and wellbeing. However, there is limited understanding of how these technologies are currently designed to support children with their health concerns or wellness goals. To gain insight into the current landscape of mobile apps designed for childrens health, we retrieved and reviewed 43 apps from IOS and Google Play store that are specifically marketed for children. Our qualitative analysis identified the dominant health focuses and goals of childrens mobile health apps. We analyzed the primary users and their expectations as well as the methods of engagement and involvement adopted. Based on our findings, we discussed the opportunities to support children with chronic illnesses through mobile apps, design for dual use, and design for age appropriateness and digital health safety. This study provides insights and recommendations for app designers, health researchers, and policymakers on strategies for engaging children and parents while also promoting childrens health and wellbeing through mobile technology.",Healthcare
Assessing m-Learning Adoption in Higher Education,"New mobile platforms, connected seamlessly to the Internet via wireless access have become increasingly more powerful and have found usage in a diverse set of application areas, including the education sector. The educational institutions are becoming more open to embracing new learning platforms, which in turn has sparked the interest in developing new assessment frameworks. This paper has used the framework of the Capability Maturity Model (CMM) to design a model for M-learning within educational institutions. The framework has been validated with studies cases from higher education institutions.","Assessing m-Learning Adoption in Higher Education New mobile platforms, connected seamlessly to the Internet via wireless access have become increasingly more powerful and have found usage in a diverse set of application areas, including the education sector. The educational institutions are becoming more open to embracing new learning platforms, which in turn has sparked the interest in developing new assessment frameworks. This paper has used the framework of the Capability Maturity Model (CMM) to design a model for M-learning within educational institutions. The framework has been validated with studies cases from higher education institutions.",Education
Comprehending environmental and economic sustainability: Comparative analysis of stability principles in the biosphere and free market economy,"Using the formalism of Lyapunov potential function it is shown that the stability principles for biomass in the ecosystem and for employment in economics are mathematically similar. The ecosystem is found to have a stable and an unstable stationary state with high (forest) and low (grasslands) biomass, respectively. In economics, there is a stable stationary state with high employment, which corresponds to mass production of conventional goods sold at low cost price, and an unstable stationary state with lower employment, which corresponds to production of novel goods appearing in the course of technological progress. An additional stable stationary state is described for economics, the one corresponding to very low employment in production of life essentials such as energy and raw materials. In this state the civilization currently pays 10 of global GDP for energy produced by a negligible minority of the working population (currently 0.2) and sold at prices greatly exceeding the cost price by 40 times. It is shown that economic ownership over energy sources is equivalent to equating measurable variables of different dimensions (stores and fluxes), which leads to effective violation of the laws of energy and matter conservation.","Comprehending environmental and economic sustainability: Comparative analysis of stability principles in the biosphere and free market economy Using the formalism of Lyapunov potential function it is shown that the stability principles for biomass in the ecosystem and for employment in economics are mathematically similar. The ecosystem is found to have a stable and an unstable stationary state with high (forest) and low (grasslands) biomass, respectively. In economics, there is a stable stationary state with high employment, which corresponds to mass production of conventional goods sold at low cost price, and an unstable stationary state with lower employment, which corresponds to production of novel goods appearing in the course of technological progress. An additional stable stationary state is described for economics, the one corresponding to very low employment in production of life essentials such as energy and raw materials. In this state the civilization currently pays 10 of global GDP for energy produced by a negligible minority of the working population (currently 0.2) and sold at prices greatly exceeding the cost price by 40 times. It is shown that economic ownership over energy sources is equivalent to equating measurable variables of different dimensions (stores and fluxes), which leads to effective violation of the laws of energy and matter conservation.",Finance
Decision-Theoretic Foundations for Causal Reasoning,"We present a definition of cause and effect in terms of decision-theoretic primitives and thereby provide a principled foundation for causal reasoning. Our definition departs from the traditional view of causation in that causal assertions may vary with the set of decisions available. We argue that this approach provides added clarity to the notion of cause. Also in this paper, we examine the encoding of causal relationships in directed acyclic graphs. We describe a special class of influence diagrams, those in canonical form, and show its relationship to Pearls representation of cause and effect. Finally, we show how canonical form facilitates counterfactual reasoning.","Decision-Theoretic Foundations for Causal Reasoning We present a definition of cause and effect in terms of decision-theoretic primitives and thereby provide a principled foundation for causal reasoning. Our definition departs from the traditional view of causation in that causal assertions may vary with the set of decisions available. We argue that this approach provides added clarity to the notion of cause. Also in this paper, we examine the encoding of causal relationships in directed acyclic graphs. We describe a special class of influence diagrams, those in canonical form, and show its relationship to Pearls representation of cause and effect. Finally, we show how canonical form facilitates counterfactual reasoning.",Technology
Biologically Inspired Hierarchical Model for Feature Extraction and Localization,"Feature extraction and matching are among central problems of computer vision. It is inefficent to search features over all locations and scales. Neurophysiological evidence shows that to locate objects in a digital image the human visual system employs visual attention to a specific object while ignoring others. The brain also has a mechanism to search from coarse to fine. In this paper, we present a feature extractor and an associated hierarchical searching model to simulate such processes. With the hierarchical representation of the object, coarse scanning is done through the matching of the larger scale and precise localization is conducted through the matching of the smaller scale. Experimental results justify the proposed model in its effectiveness and efficiency to localize features.","Biologically Inspired Hierarchical Model for Feature Extraction and Localization Feature extraction and matching are among central problems of computer vision. It is inefficent to search features over all locations and scales. Neurophysiological evidence shows that to locate objects in a digital image the human visual system employs visual attention to a specific object while ignoring others. The brain also has a mechanism to search from coarse to fine. In this paper, we present a feature extractor and an associated hierarchical searching model to simulate such processes. With the hierarchical representation of the object, coarse scanning is done through the matching of the larger scale and precise localization is conducted through the matching of the smaller scale. Experimental results justify the proposed model in its effectiveness and efficiency to localize features.",Technology
Sentiment-Aware Recommendation System for Healthcare using Social Media,"Over the last decade, health communities (known as forums) have evolved into platforms where more and more users share their medical experiences, thereby seeking guidance and interacting with people of the community. The shared content, though informal and unstructured in nature, contains valuable medical andor health-related information and can be leveraged to produce structured suggestions to the common people. In this paper, at first we propose a stacked deep learning model for sentiment analysis from the medical forum data. The stacked model comprises of Convolutional Neural Network (CNN) followed by a Long Short Term Memory (LSTM) and then by another CNN. For a blog classified with positive sentiment, we retrieve the top-n similar posts. Thereafter, we develop a probabilistic model for suggesting the suitable treatments or procedures for a particular disease or health condition. We believe that integration of medical sentiment and suggestion would be beneficial to the users for finding the relevant contents regarding medications and medical conditions, without having to manually stroll through a large amount of unstructured contents.","Sentiment-Aware Recommendation System for Healthcare using Social Media Over the last decade, health communities (known as forums) have evolved into platforms where more and more users share their medical experiences, thereby seeking guidance and interacting with people of the community. The shared content, though informal and unstructured in nature, contains valuable medical andor health-related information and can be leveraged to produce structured suggestions to the common people. In this paper, at first we propose a stacked deep learning model for sentiment analysis from the medical forum data. The stacked model comprises of Convolutional Neural Network (CNN) followed by a Long Short Term Memory (LSTM) and then by another CNN. For a blog classified with positive sentiment, we retrieve the top-n similar posts. Thereafter, we develop a probabilistic model for suggesting the suitable treatments or procedures for a particular disease or health condition. We believe that integration of medical sentiment and suggestion would be beneficial to the users for finding the relevant contents regarding medications and medical conditions, without having to manually stroll through a large amount of unstructured contents.",Healthcare
Constrained NonSmooth Utility Maximization on the Positive Real Line,We maximize the expected utility of terminal wealth in an incomplete market where there are cone constraints on the investors portfolio process and the utility function is not assumed to be strictly concave or differentiable. We establish the existence of the optimal solutions to the primal and dual problems and their dual relationship. We simplify the present proofs in this area and extend the existing duality theory to the constrained nonsmooth setting.,Constrained NonSmooth Utility Maximization on the Positive Real Line We maximize the expected utility of terminal wealth in an incomplete market where there are cone constraints on the investors portfolio process and the utility function is not assumed to be strictly concave or differentiable. We establish the existence of the optimal solutions to the primal and dual problems and their dual relationship. We simplify the present proofs in this area and extend the existing duality theory to the constrained nonsmooth setting.,Finance
A methodology for determining amino-acid substitution matrices from set covers,"We introduce a new methodology for the determination of amino-acid substitution matrices for use in the alignment of proteins. The new methodology is based on a pre-existing set cover on the set of residues and on the undirected graph that describes residue exchangeability given the set cover. For fixed functional forms indicating how to obtain edge weights from the set cover and, after that, substitution-matrix elements from weighted distances on the graph, the resulting substitution matrix can be checked for performance against some known set of reference alignments and for given gap costs. Finding the appropriate functional forms and gap costs can then be formulated as an optimization problem that seeks to maximize the performance of the substitution matrix on the reference alignment set. We give computational results on the BAliBASE suite using a genetic algorithm for optimization. Our results indicate that it is possible to obtain substitution matrices whose performance is either comparable to or surpasses that of several others, depending on the particular scenario under consideration.","A methodology for determining amino-acid substitution matrices from set covers We introduce a new methodology for the determination of amino-acid substitution matrices for use in the alignment of proteins. The new methodology is based on a pre-existing set cover on the set of residues and on the undirected graph that describes residue exchangeability given the set cover. For fixed functional forms indicating how to obtain edge weights from the set cover and, after that, substitution-matrix elements from weighted distances on the graph, the resulting substitution matrix can be checked for performance against some known set of reference alignments and for given gap costs. Finding the appropriate functional forms and gap costs can then be formulated as an optimization problem that seeks to maximize the performance of the substitution matrix on the reference alignment set. We give computational results on the BAliBASE suite using a genetic algorithm for optimization. Our results indicate that it is possible to obtain substitution matrices whose performance is either comparable to or surpasses that of several others, depending on the particular scenario under consideration.",Healthcare
"Advancing Education Through Extended Reality and Internet of Everything Enabled Metaverses: Applications, Challenges, and Open Issues","Metaverse has evolved as one of the popular research agendas that let the users learn, socialize, and collaborate in a networked 3D immersive virtual world. Due to the rich multimedia streaming capability and immersive user experience with high-speed communication, the metaverse is an ideal model for education, training, and skill development tasks. To facilitate research in this area, we provide a comprehensive review of the various educational use cases and explore how enabling technologies such as Extended reality (XR) and Internet of Everything (IoE) will play a major role in educational services in future metaverses. Secondly, we provide an overview of metaverse-based educational applications focusing on education, training, and skill development and analyze the technologies they are built upon. We identify common research problems and future research directions in the domain. The paper also identifies core ethical considerations of metaverse for education and potential pitfalls. We believe this survey can fully demonstrate the versatility of metaverse-driven education, which could serve as a potential guideline for the researchers.","Advancing Education Through Extended Reality and Internet of Everything Enabled Metaverses: Applications, Challenges, and Open Issues Metaverse has evolved as one of the popular research agendas that let the users learn, socialize, and collaborate in a networked 3D immersive virtual world. Due to the rich multimedia streaming capability and immersive user experience with high-speed communication, the metaverse is an ideal model for education, training, and skill development tasks. To facilitate research in this area, we provide a comprehensive review of the various educational use cases and explore how enabling technologies such as Extended reality (XR) and Internet of Everything (IoE) will play a major role in educational services in future metaverses. Secondly, we provide an overview of metaverse-based educational applications focusing on education, training, and skill development and analyze the technologies they are built upon. We identify common research problems and future research directions in the domain. The paper also identifies core ethical considerations of metaverse for education and potential pitfalls. We believe this survey can fully demonstrate the versatility of metaverse-driven education, which could serve as a potential guideline for the researchers.",Education
Combating the Bucket Effect:Multi-Knowledge Alignment for Medication Recommendation,"Medication recommendation is crucial in healthcare, offering effective treatments based on patients electronic health records (EHR). Previous studies show that integrating more medication-related knowledge improves medication representation accuracy. However, not all medications encompass multiple types of knowledge data simultaneously. For instance, some medications provide only textual descriptions without structured data. This imbalance in data availability limits the performance of existing models, a challenge we term the bucket effect in medication recommendation. Our data analysis uncovers the severity of the bucket effect in medication recommendation. To fill this gap, we introduce a cross-modal medication encoder capable of seamlessly aligning data from different modalities and propose a medication recommendation framework to integrate Multiple types of Knowledge, named MKMed. Specifically, we first pre-train a cross-modal encoder with contrastive learning on five knowledge modalities, aligning them into a unified space. Then, we combine the multi-knowledge medication representations with patient records for recommendations. Extensive experiments on the MIMIC-III and MIMIC-IV datasets demonstrate that MKMed mitigates the bucket effect in data, and significantly outperforms state-of-the-art baselines in recommendation accuracy and safety.","Combating the Bucket Effect:Multi-Knowledge Alignment for Medication Recommendation Medication recommendation is crucial in healthcare, offering effective treatments based on patients electronic health records (EHR). Previous studies show that integrating more medication-related knowledge improves medication representation accuracy. However, not all medications encompass multiple types of knowledge data simultaneously. For instance, some medications provide only textual descriptions without structured data. This imbalance in data availability limits the performance of existing models, a challenge we term the bucket effect in medication recommendation. Our data analysis uncovers the severity of the bucket effect in medication recommendation. To fill this gap, we introduce a cross-modal medication encoder capable of seamlessly aligning data from different modalities and propose a medication recommendation framework to integrate Multiple types of Knowledge, named MKMed. Specifically, we first pre-train a cross-modal encoder with contrastive learning on five knowledge modalities, aligning them into a unified space. Then, we combine the multi-knowledge medication representations with patient records for recommendations. Extensive experiments on the MIMIC-III and MIMIC-IV datasets demonstrate that MKMed mitigates the bucket effect in data, and significantly outperforms state-of-the-art baselines in recommendation accuracy and safety.",Healthcare
On the Fragility of Third-party Punishment: The Context Effect of a Dominated Risky Investment Option,"Experimental studies regularly show that third-party punishment (TPP) substantially exists in various settings. This study further investigates the robustness of TPP under an environment where context effects are involved. In our experiment, we offer a third party an additional but unattractive risky investment option. We find that, when the dominated investment option irrelevant to prosocial behavior is available, the demand for punishment decreases, whereas the demand for investment increases. These findings support our hypothesis that the seemingly unrelated and dominated investment option may work as a compromise and suggest the fragility of TPP in this setting.","On the Fragility of Third-party Punishment: The Context Effect of a Dominated Risky Investment Option Experimental studies regularly show that third-party punishment (TPP) substantially exists in various settings. This study further investigates the robustness of TPP under an environment where context effects are involved. In our experiment, we offer a third party an additional but unattractive risky investment option. We find that, when the dominated investment option irrelevant to prosocial behavior is available, the demand for punishment decreases, whereas the demand for investment increases. These findings support our hypothesis that the seemingly unrelated and dominated investment option may work as a compromise and suggest the fragility of TPP in this setting.",Finance
Inequality reversal: effects of the savings propensity and correlated returns,"In the last decade, a large body of literature has been developed to explain the universal features of inequality in terms of income and wealth. By now, it is established that the distributions of income and wealth in various economies show a number of statistical regularities. There are several models to explain such static features of inequality in an unifying framework and the kinetic exchange models, in particular, provide one such framework. Here we focus on the dynamic features of inequality. In the process of development and growth, inequality in an economy in terms of income and wealth follows a particular pattern of rising in the initial stage followed by an eventual fall. This inverted U-shaped curve is known as the Kuznets Curve. We examine the possibilities of such behavior of an economy in the context of a generalized kinetic exchange model. It is shown that under some specific conditions, our model economy indeed shows inequality reversal.","Inequality reversal: effects of the savings propensity and correlated returns In the last decade, a large body of literature has been developed to explain the universal features of inequality in terms of income and wealth. By now, it is established that the distributions of income and wealth in various economies show a number of statistical regularities. There are several models to explain such static features of inequality in an unifying framework and the kinetic exchange models, in particular, provide one such framework. Here we focus on the dynamic features of inequality. In the process of development and growth, inequality in an economy in terms of income and wealth follows a particular pattern of rising in the initial stage followed by an eventual fall. This inverted U-shaped curve is known as the Kuznets Curve. We examine the possibilities of such behavior of an economy in the context of a generalized kinetic exchange model. It is shown that under some specific conditions, our model economy indeed shows inequality reversal.",Finance
"A Survey of Sustainability in Large Language Models: Applications, Economics, and Challenges","Large Language Models (LLMs) have transformed numerous domains by providing advanced capabilities in natural language understanding, generation, and reasoning. Despite their groundbreaking applications across industries such as research, healthcare, and creative media, their rapid adoption raises critical concerns regarding sustainability. This survey paper comprehensively examines the environmental, economic, and computational challenges associated with LLMs, focusing on energy consumption, carbon emissions, and resource utilization in data centers. By synthesizing insights from existing literature, this work explores strategies such as resource-efficient training, sustainable deployment practices, and lifecycle assessments to mitigate the environmental impacts of LLMs. Key areas of emphasis include energy optimization, renewable energy integration, and balancing performance with sustainability. The findings aim to guide researchers, practitioners, and policymakers in developing actionable strategies for sustainable AI systems, fostering a responsible and environmentally conscious future for artificial intelligence.","A Survey of Sustainability in Large Language Models: Applications, Economics, and Challenges Large Language Models (LLMs) have transformed numerous domains by providing advanced capabilities in natural language understanding, generation, and reasoning. Despite their groundbreaking applications across industries such as research, healthcare, and creative media, their rapid adoption raises critical concerns regarding sustainability. This survey paper comprehensively examines the environmental, economic, and computational challenges associated with LLMs, focusing on energy consumption, carbon emissions, and resource utilization in data centers. By synthesizing insights from existing literature, this work explores strategies such as resource-efficient training, sustainable deployment practices, and lifecycle assessments to mitigate the environmental impacts of LLMs. Key areas of emphasis include energy optimization, renewable energy integration, and balancing performance with sustainability. The findings aim to guide researchers, practitioners, and policymakers in developing actionable strategies for sustainable AI systems, fostering a responsible and environmentally conscious future for artificial intelligence.",Environment
How Market Ecology Explains Market Malfunction,"Standard approaches to the theory of financial markets are based on equilibrium and efficiency. Here we develop an alternative based on concepts and methods developed by biologists, in which the wealth invested in a financial strategy is like the abundance of a species. We study a toy model of a market consisting of value investors, trend followers and noise traders. We show that the average returns of strategies are strongly density dependent, i.e. they depend on the wealth invested in each strategy at any given time. In the absence of noise the market would slowly evolve toward an efficient equilibrium, but the statistical uncertainty in profitability (which is adjusted to match real markets) makes this noisy and uncertain. Even in the long term, the market spends extended periods of time away from perfect efficiency. We show how core concepts from ecology, such as the community matrix and food webs, give insight into market behavior. The wealth dynamics of the market ecology explain how market inefficiencies spontaneously occur and gives insight into the origins of excess price volatility and deviations of prices from fundamental values.","How Market Ecology Explains Market Malfunction Standard approaches to the theory of financial markets are based on equilibrium and efficiency. Here we develop an alternative based on concepts and methods developed by biologists, in which the wealth invested in a financial strategy is like the abundance of a species. We study a toy model of a market consisting of value investors, trend followers and noise traders. We show that the average returns of strategies are strongly density dependent, i.e. they depend on the wealth invested in each strategy at any given time. In the absence of noise the market would slowly evolve toward an efficient equilibrium, but the statistical uncertainty in profitability (which is adjusted to match real markets) makes this noisy and uncertain. Even in the long term, the market spends extended periods of time away from perfect efficiency. We show how core concepts from ecology, such as the community matrix and food webs, give insight into market behavior. The wealth dynamics of the market ecology explain how market inefficiencies spontaneously occur and gives insight into the origins of excess price volatility and deviations of prices from fundamental values.",Finance
Time Delay and Investment Decisions: Evidence from an Experiment in Tanzania,"Attitudes toward risk underlie virtually every important economic decision an individual makes. In this experimental study, I examine how introducing a time delay into the execution of an investment plan influences individuals risk preferences. The field experiment proceeded in three stages: a decision stage, an execution stage and a payout stage. At the outset, in the Decision Stage (Stage 1), each subject was asked to make an investment plan by splitting a monetary investment amount between a risky asset and a safe asset. Subjects were informed that the investment plans they made in the Decision Stage are binding and will be executed during the Execution Stage (Stage 2). The Payout Stage (Stage 3) was the payout date. The timing of the Decision Stage and Payout Stage was the same for each subject, but the timing of the Execution Stage varied experimentally. I find that individuals who were assigned to execute their investment plans later (i.e., for whom there was a greater delay prior to the Execution Stage) invested a greater amount in the risky asset during the Decision Stage.","Time Delay and Investment Decisions: Evidence from an Experiment in Tanzania Attitudes toward risk underlie virtually every important economic decision an individual makes. In this experimental study, I examine how introducing a time delay into the execution of an investment plan influences individuals risk preferences. The field experiment proceeded in three stages: a decision stage, an execution stage and a payout stage. At the outset, in the Decision Stage (Stage 1), each subject was asked to make an investment plan by splitting a monetary investment amount between a risky asset and a safe asset. Subjects were informed that the investment plans they made in the Decision Stage are binding and will be executed during the Execution Stage (Stage 2). The Payout Stage (Stage 3) was the payout date. The timing of the Decision Stage and Payout Stage was the same for each subject, but the timing of the Execution Stage varied experimentally. I find that individuals who were assigned to execute their investment plans later (i.e., for whom there was a greater delay prior to the Execution Stage) invested a greater amount in the risky asset during the Decision Stage.",Finance
On the Financing of Climate Change Adaptation in Developing Countries,"I offer reflections on adaptation to climate change, with emphasis on developing areas.","On the Financing of Climate Change Adaptation in Developing Countries I offer reflections on adaptation to climate change, with emphasis on developing areas.",Environment
Extreme prices in electricity balancing markets from an approach of statistical physics,"An increase in energy production from renewable energy sources is viewed as a crucial achievement in most industrialized countries. The higher variability of power production via renewables leads to a rise in ancillary service costs over the power system, in particular costs within the electricity balancing markets, mainly due to an increased number of extreme price spikes. This study focuses on forecasting the behavior of price and volumes of the Italian balancing market in the presence of an increased share of renewable energy sources. Starting from configurations of load and power production, which guarantee a stable performance, we implement fluctuations in the load and in renewables; in particular we artificially increase the contribution of renewables as compared to conventional power sources to cover the total load. We then forecast the amount of provided energy in the balancing market and its fluctuations, which are induced by production and consumption. Within an approach of agent based modeling we estimate the resulting energy prices and costs. While their average values turn out to be only slightly affected by an increased contribution from renewables, the probability for extreme price events is shown to increase along with undesired peaks in the costs.","Extreme prices in electricity balancing markets from an approach of statistical physics An increase in energy production from renewable energy sources is viewed as a crucial achievement in most industrialized countries. The higher variability of power production via renewables leads to a rise in ancillary service costs over the power system, in particular costs within the electricity balancing markets, mainly due to an increased number of extreme price spikes. This study focuses on forecasting the behavior of price and volumes of the Italian balancing market in the presence of an increased share of renewable energy sources. Starting from configurations of load and power production, which guarantee a stable performance, we implement fluctuations in the load and in renewables; in particular we artificially increase the contribution of renewables as compared to conventional power sources to cover the total load. We then forecast the amount of provided energy in the balancing market and its fluctuations, which are induced by production and consumption. Within an approach of agent based modeling we estimate the resulting energy prices and costs. While their average values turn out to be only slightly affected by an increased contribution from renewables, the probability for extreme price events is shown to increase along with undesired peaks in the costs.",Environment
The dependence of relative dispersion on turbulence scales in Lagrangian Stochastic Models,"The aim of the article is to investigate the relative dispersion properties of the Well Mixed class of Lagrangian Stochastic Models. Dimensional analysis shows that given a model in the class, its properties depend solely on a non-dimensional parameter, which measures the relative weight of Lagrangian-to-Eulerian scales. This parameter is formulated in terms of Kolmogorov constants, and model properties are then studied by modifying its value in a range that contains the experimental variability. Large variations are found for the quantity g2gC_0-1, where g is the Richardson constant, and for the duration of the t3 regime. Asymptotic analysis of model behaviour clarifies some inconsistencies in the literature and excludes the Ornstein-Uhlenbeck process from being considered a reliable model for relative dispersion.","The dependence of relative dispersion on turbulence scales in Lagrangian Stochastic Models The aim of the article is to investigate the relative dispersion properties of the Well Mixed class of Lagrangian Stochastic Models. Dimensional analysis shows that given a model in the class, its properties depend solely on a non-dimensional parameter, which measures the relative weight of Lagrangian-to-Eulerian scales. This parameter is formulated in terms of Kolmogorov constants, and model properties are then studied by modifying its value in a range that contains the experimental variability. Large variations are found for the quantity g2gC_0-1, where g is the Richardson constant, and for the duration of the t3 regime. Asymptotic analysis of model behaviour clarifies some inconsistencies in the literature and excludes the Ornstein-Uhlenbeck process from being considered a reliable model for relative dispersion.",Environment
Human not in the loop: objective sample difficulty measures for Curriculum Learning,"Curriculum learning is a learning method that trains models in a meaningful order from easier to harder samples. A key here is to devise automatic and objective difficulty measures of samples. In the medical domain, previous work applied domain knowledge from human experts to qualitatively assess classification difficulty of medical images to guide curriculum learning, which requires extra annotation efforts, relies on subjective human experience, and may introduce bias. In this work, we propose a new automated curriculum learning technique using the variance of gradients (VoG) to compute an objective difficulty measure of samples and evaluated its effects on elbow fracture classification from X-ray images. Specifically, we used VoG as a metric to rank each sample in terms of the classification difficulty, where high VoG scores indicate more difficult cases for classification, to guide the curriculum training process We compared the proposed technique to a baseline (without curriculum learning), a previous method that used human annotations on classification difficulty, and anti-curriculum learning. Our experiment results showed comparable and higher performance for the binary and multi-class bone fracture classification tasks.","Human not in the loop: objective sample difficulty measures for Curriculum Learning Curriculum learning is a learning method that trains models in a meaningful order from easier to harder samples. A key here is to devise automatic and objective difficulty measures of samples. In the medical domain, previous work applied domain knowledge from human experts to qualitatively assess classification difficulty of medical images to guide curriculum learning, which requires extra annotation efforts, relies on subjective human experience, and may introduce bias. In this work, we propose a new automated curriculum learning technique using the variance of gradients (VoG) to compute an objective difficulty measure of samples and evaluated its effects on elbow fracture classification from X-ray images. Specifically, we used VoG as a metric to rank each sample in terms of the classification difficulty, where high VoG scores indicate more difficult cases for classification, to guide the curriculum training process We compared the proposed technique to a baseline (without curriculum learning), a previous method that used human annotations on classification difficulty, and anti-curriculum learning. Our experiment results showed comparable and higher performance for the binary and multi-class bone fracture classification tasks.",Education
Finite Width Model Sequence Comparison,"Sequence comparison is a widely used computational technique in modern molecular biology. In spite of the frequent use of sequence comparisons the important problem of assigning statistical significance to a given degree of similarity is still outstanding. Analytical approaches to filling this gap usually make use of an approximation that neglects certain correlations in the disorder underlying the sequence comparison algorithm. Here, we use the longest common subsequence problem, a prototype sequence comparison problem, to analytically establish that this approximation does make a difference to certain sequence comparison statistics. In the course of establishing this difference we develop a method that can systematically deal with these disorder correlations.","Finite Width Model Sequence Comparison Sequence comparison is a widely used computational technique in modern molecular biology. In spite of the frequent use of sequence comparisons the important problem of assigning statistical significance to a given degree of similarity is still outstanding. Analytical approaches to filling this gap usually make use of an approximation that neglects certain correlations in the disorder underlying the sequence comparison algorithm. Here, we use the longest common subsequence problem, a prototype sequence comparison problem, to analytically establish that this approximation does make a difference to certain sequence comparison statistics. In the course of establishing this difference we develop a method that can systematically deal with these disorder correlations.",Healthcare
A simplified model of the source channel of the Leksell Gamma Knife(R): testing multisource configurations with PENELOPE,"A simplification of the source channel geometry of the Leksell Gamma KnifecircledR, recently proposed by the authors and checked for a single source configuration (Al-Dweri et al 2004), has been used to calculate the dose distributions along the x, y and z axes in a water phantom with a diameter of 160mm, for different configurations of the Gamma Knife including 201, 150 and 102 unplugged sources. The code PENELOPE (v. 2001) has been used to perform the Monte Carlo simulations. In addition, the output factors for the 14, 8 and 4mm helmets have been calculated. The results found for the dose profiles show a qualitatively good agreement with previous ones obtained with EGS4 and PENELOPE (v. 2000) codes and with the predictions of GammaPlancircledR. The output factors obtained with our model agree within the statistical uncertainties with those calculated with the same Monte Carlo codes and with those measured with different techniques. Owing to the accuracy of the results obtained and to the reduction in the computational time with respect to full geometry simulations (larger than a factor 15), this simplified model opens the possibility to use Monte Carlo tools for planning purposes in the Gamma KnifecircledR.","A simplified model of the source channel of the Leksell Gamma Knife(R): testing multisource configurations with PENELOPE A simplification of the source channel geometry of the Leksell Gamma KnifecircledR, recently proposed by the authors and checked for a single source configuration (Al-Dweri et al 2004), has been used to calculate the dose distributions along the x, y and z axes in a water phantom with a diameter of 160mm, for different configurations of the Gamma Knife including 201, 150 and 102 unplugged sources. The code PENELOPE (v. 2001) has been used to perform the Monte Carlo simulations. In addition, the output factors for the 14, 8 and 4mm helmets have been calculated. The results found for the dose profiles show a qualitatively good agreement with previous ones obtained with EGS4 and PENELOPE (v. 2000) codes and with the predictions of GammaPlancircledR. The output factors obtained with our model agree within the statistical uncertainties with those calculated with the same Monte Carlo codes and with those measured with different techniques. Owing to the accuracy of the results obtained and to the reduction in the computational time with respect to full geometry simulations (larger than a factor 15), this simplified model opens the possibility to use Monte Carlo tools for planning purposes in the Gamma KnifecircledR.",Healthcare
A System for Induction of Oblique Decision Trees,"This article describes a new system for induction of oblique decision trees. This system, OC1, combines deterministic hill-climbing with two forms of randomization to find a good oblique split (in the form of a hyperplane) at each node of a decision tree. Oblique decision tree methods are tuned especially for domains in which the attributes are numeric, although they can be adapted to symbolic or mixed symbolicnumeric attributes. We present extensive empirical studies, using both real and artificial data, that analyze OC1s ability to construct oblique trees that are smaller and more accurate than their axis-parallel counterparts. We also examine the benefits of randomization for the construction of oblique decision trees.","A System for Induction of Oblique Decision Trees This article describes a new system for induction of oblique decision trees. This system, OC1, combines deterministic hill-climbing with two forms of randomization to find a good oblique split (in the form of a hyperplane) at each node of a decision tree. Oblique decision tree methods are tuned especially for domains in which the attributes are numeric, although they can be adapted to symbolic or mixed symbolicnumeric attributes. We present extensive empirical studies, using both real and artificial data, that analyze OC1s ability to construct oblique trees that are smaller and more accurate than their axis-parallel counterparts. We also examine the benefits of randomization for the construction of oblique decision trees.",Technology
Curriculum Design of Competitive Programming: a Contest-based Approach,"Competitive programming (CP) has been increasingly integrated into computer science curricula worldwide due to its efficacy in enhancing students algorithmic reasoning and problem-solving skills. However, existing CP curriculum designs predominantly employ a problem-based approach, lacking the critical dimension of time pressure of real competitive programming contests. Such constraints are prevalent not only in programming contests but also in various real-world scenarios, including technical interviews, software development sprints, and hackathons. To bridge this gap, we introduce a contest-based approach to curriculum design that explicitly incorporates realistic contest scenarios into formative assessments, simulating authentic competitive programming experiences. This paper details the design and implementation of such a course at Purdue University, structured to systematically develop students observational skills, algorithmic techniques, and efficient coding and debugging practices. We outline a pedagogical framework comprising cooperative learning strategies, contest-based assessments, and supplemental activities to boost students problem-solving capabilities.","Curriculum Design of Competitive Programming: a Contest-based Approach Competitive programming (CP) has been increasingly integrated into computer science curricula worldwide due to its efficacy in enhancing students algorithmic reasoning and problem-solving skills. However, existing CP curriculum designs predominantly employ a problem-based approach, lacking the critical dimension of time pressure of real competitive programming contests. Such constraints are prevalent not only in programming contests but also in various real-world scenarios, including technical interviews, software development sprints, and hackathons. To bridge this gap, we introduce a contest-based approach to curriculum design that explicitly incorporates realistic contest scenarios into formative assessments, simulating authentic competitive programming experiences. This paper details the design and implementation of such a course at Purdue University, structured to systematically develop students observational skills, algorithmic techniques, and efficient coding and debugging practices. We outline a pedagogical framework comprising cooperative learning strategies, contest-based assessments, and supplemental activities to boost students problem-solving capabilities.",Education
"EIT Reconstruction Algorithms: Pitfalls, Challenges and Recent Developments","We review developments, issues and challenges in Electrical Impedance Tomography (EIT), for the 4th Workshop on Biomedical Applications of EIT, Manchester 2003. We focus on the necessity for three dimensional data collection and reconstruction, efficient solution of the forward problem and present and future reconstruction algorithms. We also suggest common pitfalls or inverse crimes to avoid.","EIT Reconstruction Algorithms: Pitfalls, Challenges and Recent Developments We review developments, issues and challenges in Electrical Impedance Tomography (EIT), for the 4th Workshop on Biomedical Applications of EIT, Manchester 2003. We focus on the necessity for three dimensional data collection and reconstruction, efficient solution of the forward problem and present and future reconstruction algorithms. We also suggest common pitfalls or inverse crimes to avoid.",Healthcare
Monte Carlo approximation to optimal investment,"This paper sets up a methodology for approximately solving optimal investment problems using duality methods combined with Monte Carlo simulations. In particular, we show how to tackle high dimensional problems in incomplete markets, where traditional methods fail due to the curse of dimensionality.","Monte Carlo approximation to optimal investment This paper sets up a methodology for approximately solving optimal investment problems using duality methods combined with Monte Carlo simulations. In particular, we show how to tackle high dimensional problems in incomplete markets, where traditional methods fail due to the curse of dimensionality.",Finance
The Human Effect Requires Affect: Addressing Social-Psychological Factors of Climate Change with Machine Learning,"Machine learning has the potential to aid in mitigating the human effects of climate change. Previous applications of machine learning to tackle the human effects in climate change include approaches like informing individuals of their carbon footprint and strategies to reduce it. For these methods to be the most effective they must consider relevant social-psychological factors for each individual. Of social-psychological factors at play in climate change, affect has been previously identified as a key element in perceptions and willingness to engage in mitigative behaviours. In this work, we propose an investigation into how affect could be incorporated to enhance machine learning based interventions for climate change. We propose using affective agent-based modelling for climate change as well as the use of a simulated climate change social dilemma to explore the potential benefits of affective machine learning interventions. Behavioural and informational interventions can be a powerful tool in helping humans adopt mitigative behaviours. We expect that utilizing affective ML can make interventions an even more powerful tool and help mitigative behaviours become widely adopted.","The Human Effect Requires Affect: Addressing Social-Psychological Factors of Climate Change with Machine Learning Machine learning has the potential to aid in mitigating the human effects of climate change. Previous applications of machine learning to tackle the human effects in climate change include approaches like informing individuals of their carbon footprint and strategies to reduce it. For these methods to be the most effective they must consider relevant social-psychological factors for each individual. Of social-psychological factors at play in climate change, affect has been previously identified as a key element in perceptions and willingness to engage in mitigative behaviours. In this work, we propose an investigation into how affect could be incorporated to enhance machine learning based interventions for climate change. We propose using affective agent-based modelling for climate change as well as the use of a simulated climate change social dilemma to explore the potential benefits of affective machine learning interventions. Behavioural and informational interventions can be a powerful tool in helping humans adopt mitigative behaviours. We expect that utilizing affective ML can make interventions an even more powerful tool and help mitigative behaviours become widely adopted.",Environment
A Comparative Assessment of Technology Acceptance and Learning Outcomes in Computer-based versus VR-based Pedagogical Agents,"As educational technology evolves, the potential of Pedagogical Agents (PAs) in supporting education is extensively explored. Typically, research on PAs has primarily focused on computer-based learning environments, but their use in VR-based environments and integration into education is still in its infancy. To address this gap, this paper presents a mixed method comparative study that has been conducted to evaluate and examine how these computer-based PAs and VR-based PAs compare, towards their learning efficacy and technology acceptance. 92 Computing and Engineering undergraduate students were recruited and participated in an educational experience focusing on computing machinery education. The findings of this study revealed that both approaches can effectively facilitate learning acquisition, and both technologies have been positively perceived by participants toward acceptance, without any significant differences. The findings of this study shed light on the potential of utilizing intelligent PAs to support education, contributing towards the advancement of our understanding of how to integrate such technologies to develop learning interventions, and establishing the foundation for future investigations that aim to successfully integrate and use PAs in education.","A Comparative Assessment of Technology Acceptance and Learning Outcomes in Computer-based versus VR-based Pedagogical Agents As educational technology evolves, the potential of Pedagogical Agents (PAs) in supporting education is extensively explored. Typically, research on PAs has primarily focused on computer-based learning environments, but their use in VR-based environments and integration into education is still in its infancy. To address this gap, this paper presents a mixed method comparative study that has been conducted to evaluate and examine how these computer-based PAs and VR-based PAs compare, towards their learning efficacy and technology acceptance. 92 Computing and Engineering undergraduate students were recruited and participated in an educational experience focusing on computing machinery education. The findings of this study revealed that both approaches can effectively facilitate learning acquisition, and both technologies have been positively perceived by participants toward acceptance, without any significant differences. The findings of this study shed light on the potential of utilizing intelligent PAs to support education, contributing towards the advancement of our understanding of how to integrate such technologies to develop learning interventions, and establishing the foundation for future investigations that aim to successfully integrate and use PAs in education.",Education
Discrete Network Dynamics. Part 1: Operator Theory,"An operator algebra implementation of Markov chain Monte Carlo algorithms for simulating Markov random fields is proposed. It allows the dynamics of networks whose nodes have discrete state spaces to be specified by the action of an update operator that is composed of creation and annihilation operators. This formulation of discrete network dynamics has properties that are similar to those of a quantum field theory of bosons, which allows reuse of many conceptual and theoretical structures from QFT. The equilibrium behaviour of one of these generalised MRFs and of the adaptive cluster expansion network (ACEnet) are shown to be equivalent, which provides a way of unifying these two theories.","Discrete Network Dynamics. Part 1: Operator Theory An operator algebra implementation of Markov chain Monte Carlo algorithms for simulating Markov random fields is proposed. It allows the dynamics of networks whose nodes have discrete state spaces to be specified by the action of an update operator that is composed of creation and annihilation operators. This formulation of discrete network dynamics has properties that are similar to those of a quantum field theory of bosons, which allows reuse of many conceptual and theoretical structures from QFT. The equilibrium behaviour of one of these generalised MRFs and of the adaptive cluster expansion network (ACEnet) are shown to be equivalent, which provides a way of unifying these two theories.",Technology
Extraction of cartographic objects in high resolution satellite images for object model generation,"The aim of this study is to detect man-made cartographic objects in high-resolution satellite images. New generation satellites offer a sub-metric spatial resolution, in which it is possible (and necessary) to develop methods at object level rather than at pixel level, and to exploit structural features of objects. With this aim, a method to generate structural object models from manually segmented images has been developed. To generate the model from non-segmented images, extraction of the objects from the sample images is required. A hybrid method of extraction (both in terms of input sources and segmentation algorithms) is proposed: A region based segmentation is applied on a 10 meter resolution multi-spectral image. The result is used as marker in a marker-controlled watershed method using edges on a 2.5 meter resolution panchromatic image. Very promising results have been obtained even on images where the limits of the target objects are not apparent.","Extraction of cartographic objects in high resolution satellite images for object model generation The aim of this study is to detect man-made cartographic objects in high-resolution satellite images. New generation satellites offer a sub-metric spatial resolution, in which it is possible (and necessary) to develop methods at object level rather than at pixel level, and to exploit structural features of objects. With this aim, a method to generate structural object models from manually segmented images has been developed. To generate the model from non-segmented images, extraction of the objects from the sample images is required. A hybrid method of extraction (both in terms of input sources and segmentation algorithms) is proposed: A region based segmentation is applied on a 10 meter resolution multi-spectral image. The result is used as marker in a marker-controlled watershed method using edges on a 2.5 meter resolution panchromatic image. Very promising results have been obtained even on images where the limits of the target objects are not apparent.",Technology
Predictions as statements and decisions,"Prediction is a complex notion, and different predictors (such as people, computer programs, and probabilistic theories) can pursue very different goals. In this paper I will review some popular kinds of prediction and argue that the theory of competitive on-line learning can benefit from the kinds of prediction that are now foreign to it.","Predictions as statements and decisions Prediction is a complex notion, and different predictors (such as people, computer programs, and probabilistic theories) can pursue very different goals. In this paper I will review some popular kinds of prediction and argue that the theory of competitive on-line learning can benefit from the kinds of prediction that are now foreign to it.",Technology
From Assistive Technologies to Metaverse: Technologies in Inclusive Higher Education for Students with Specific Learning Difficulties,"The development of new technologies and their expanding use in a wide range of educational environments are driving the transformation of higher education. Assistive technologies are a subset of cutting-edge technology that can help students learn more effectively and make education accessible to everyone. Assistive technology can enhance, maintain, or improve the capacities of students with learning difficulties. Students with learning difficulties will be greatly benefited from the use of assistive technologies. If these technologies are used effectively, students with learning difficulties can compete with their peers and complete their academic tasks. We aim to conduct this review to better understand the role of assistive technologies in providing inclusive higher education for students with learning difficulties. The review begins with the introduction of learning difficulties and their causes; inclusive education and the need for assistive technologies; the reasoning for conducting this review; and a summary of related reviews on assistive technologies for students with learning difficulties in inclusive higher education. Then, we discuss the preliminaries for the learning difficulties type and assistive technology. Later, we discuss the effects of assistive technology on inclusive higher education for students with learning difficulties. Additionally, we discuss related projects and support tools available in inclusive higher education for students with learning difficulties. We also explore the challenges and possible solutions related to using assistive technology in higher education to provide inclusive education for students with learning difficulties. We conclude the review with a discussion of potential promising future directions.","From Assistive Technologies to Metaverse: Technologies in Inclusive Higher Education for Students with Specific Learning Difficulties The development of new technologies and their expanding use in a wide range of educational environments are driving the transformation of higher education. Assistive technologies are a subset of cutting-edge technology that can help students learn more effectively and make education accessible to everyone. Assistive technology can enhance, maintain, or improve the capacities of students with learning difficulties. Students with learning difficulties will be greatly benefited from the use of assistive technologies. If these technologies are used effectively, students with learning difficulties can compete with their peers and complete their academic tasks. We aim to conduct this review to better understand the role of assistive technologies in providing inclusive higher education for students with learning difficulties. The review begins with the introduction of learning difficulties and their causes; inclusive education and the need for assistive technologies; the reasoning for conducting this review; and a summary of related reviews on assistive technologies for students with learning difficulties in inclusive higher education. Then, we discuss the preliminaries for the learning difficulties type and assistive technology. Later, we discuss the effects of assistive technology on inclusive higher education for students with learning difficulties. Additionally, we discuss related projects and support tools available in inclusive higher education for students with learning difficulties. We also explore the challenges and possible solutions related to using assistive technology in higher education to provide inclusive education for students with learning difficulties. We conclude the review with a discussion of potential promising future directions.",Education
Heterogeneous Beliefs with Finite-Lived Agents,"This paper will examine a model with many agents, each of whom has a different belief about the dynamics of a risky asset. The agents are Bayesian and so learn about the asset over time. All agents are assumed to have a finite (but random) lifetime. When an agent dies, he passes his wealth (but not his knowledge) onto his heir. As a result, the agents never become sure of the dynamics of the risky asset. We derive expressions for the stock price and riskless rate. We then use numerical examples to exhibit their behaviour.","Heterogeneous Beliefs with Finite-Lived Agents This paper will examine a model with many agents, each of whom has a different belief about the dynamics of a risky asset. The agents are Bayesian and so learn about the asset over time. All agents are assumed to have a finite (but random) lifetime. When an agent dies, he passes his wealth (but not his knowledge) onto his heir. As a result, the agents never become sure of the dynamics of the risky asset. We derive expressions for the stock price and riskless rate. We then use numerical examples to exhibit their behaviour.",Finance
Truncating Temporal Differences: On the Efficient Implementation of TD(lambda) for Reinforcement Learning,"Temporal difference (TD) methods constitute a class of methods for learning predictions in multi-step prediction problems, parameterized by a recency factor lambda. Currently the most important application of these methods is to temporal credit assignment in reinforcement learning. Well known reinforcement learning algorithms, such as AHC or Q-learning, may be viewed as instances of TD learning. This paper examines the issues of the efficient and general implementation of TD(lambda) for arbitrary lambda, for use with reinforcement learning algorithms optimizing the discounted sum of rewards. The traditional approach, based on eligibility traces, is argued to suffer from both inefficiency and lack of generality. The TTD (Truncated Temporal Differences) procedure is proposed as an alternative, that indeed only approximates TD(lambda), but requires very little computation per action and can be used with arbitrary function representation methods. The idea from which it is derived is fairly simple and not new, but probably unexplored so far. Encouraging experimental results are presented, suggesting that using lambda gt 0 with the TTD procedure allows one to obtain a significant learning speedup at essentially the same cost as usual TD(0) learning.","Truncating Temporal Differences: On the Efficient Implementation of TD(lambda) for Reinforcement Learning Temporal difference (TD) methods constitute a class of methods for learning predictions in multi-step prediction problems, parameterized by a recency factor lambda. Currently the most important application of these methods is to temporal credit assignment in reinforcement learning. Well known reinforcement learning algorithms, such as AHC or Q-learning, may be viewed as instances of TD learning. This paper examines the issues of the efficient and general implementation of TD(lambda) for arbitrary lambda, for use with reinforcement learning algorithms optimizing the discounted sum of rewards. The traditional approach, based on eligibility traces, is argued to suffer from both inefficiency and lack of generality. The TTD (Truncated Temporal Differences) procedure is proposed as an alternative, that indeed only approximates TD(lambda), but requires very little computation per action and can be used with arbitrary function representation methods. The idea from which it is derived is fairly simple and not new, but probably unexplored so far. Encouraging experimental results are presented, suggesting that using lambda gt 0 with the TTD procedure allows one to obtain a significant learning speedup at essentially the same cost as usual TD(0) learning.",Technology
Niederhausers model for epilepsy and wavelet methods,"Wavelets and wavelet transforms (WT) could be a very useful tool to analyze electroencephalogram (EEG) signals. To illustrate the WT method we make use of a simple electric circuit model introduced by Niederhauser, which is used to produce EEG-like signals, particularly during an epileptic seizure. The original model is modified to resemble the 10-20 derivation of the EEG measurements. WT is used to study the main features of these signals","Niederhausers model for epilepsy and wavelet methods Wavelets and wavelet transforms (WT) could be a very useful tool to analyze electroencephalogram (EEG) signals. To illustrate the WT method we make use of a simple electric circuit model introduced by Niederhauser, which is used to produce EEG-like signals, particularly during an epileptic seizure. The original model is modified to resemble the 10-20 derivation of the EEG measurements. WT is used to study the main features of these signals",Healthcare
The Causal Effect of Transport Infrastructure: Evidence from a New Historical Database,"In this paper, we analyze the effect of transport infrastructure investments in railways. As a testing ground, we use data from a new historical database that includes annual panel data on approximately 2,400 Swedish rural geographical areas during the period 1860-1917. We use a staggered event study design that is robust to treatment effect heterogeneity. Importantly, we find extremely large reduced-form effects of having access to railways. For real nonagricultural income, the cumulative treatment effect is approximately 120 after 30 years. Equally important, we also show that our reduced-form effect is likely to reflect growth rather than a reorganization of existing economic activity since we find no spillover effects between treated and untreated regions. Specifically, our results are consistent with the big push hypothesis, which argues that simultaneouscoordinated investment, such as large infrastructure investment in railways, can generate economic growth if there are strong aggregate demand externalities (e.g., Murphy et al. 1989). We used plant-level data to further corroborate this mechanism. Indeed, we find that investments in local railways dramatically, and independent of initial conditions, increase local industrial production and employment on the order of 100-300 across almost all industrial sectors.","The Causal Effect of Transport Infrastructure: Evidence from a New Historical Database In this paper, we analyze the effect of transport infrastructure investments in railways. As a testing ground, we use data from a new historical database that includes annual panel data on approximately 2,400 Swedish rural geographical areas during the period 1860-1917. We use a staggered event study design that is robust to treatment effect heterogeneity. Importantly, we find extremely large reduced-form effects of having access to railways. For real nonagricultural income, the cumulative treatment effect is approximately 120 after 30 years. Equally important, we also show that our reduced-form effect is likely to reflect growth rather than a reorganization of existing economic activity since we find no spillover effects between treated and untreated regions. Specifically, our results are consistent with the big push hypothesis, which argues that simultaneouscoordinated investment, such as large infrastructure investment in railways, can generate economic growth if there are strong aggregate demand externalities (e.g., Murphy et al. 1989). We used plant-level data to further corroborate this mechanism. Indeed, we find that investments in local railways dramatically, and independent of initial conditions, increase local industrial production and employment on the order of 100-300 across almost all industrial sectors.",Finance
Evaluating the quality of ground-based microwave radiometer measurements and retrievals using detrended fluctuation and spectral analysis methods,"Time series both of microwave radiometer brightness temperature measurements at 23.8 and 31.4 GHz and of retrievals of water vapor and liquid water path from these brightness temperatures are evaluated using the detrended fluctuation analysis method. As quantified by the parameter alpha, this method (i) enables identification of the time scales over which noise dominates the time series and (ii) characterizes the temporal range of correlations in the time series. The more common spectral analysis method is also used to assess the data and its results are compared with those from detrended fluctuation analysis method. The assumption that measurements should have certain scaling properties allows the quality of the measurements to be characterized. The additional assumption that the scaling properties of the measurements of an atmospheric quantity are preserved in a useful retrieval provides a means for evaluating the retrieval itself. Applying these two assumptions to microwave radiometer measurements and retrievals demonstrates three points. First, the retrieved water vapor path during cloudy-sky periods can be dominated by noise on shorter than 30min time scales (alpha-exponent  0.1) and exhibits no scaling behavior at longer time scales. However, correlations in the brightness temperatures and liquid water path retrievals are found to be consistent with a power-law behavior for time scales up to 3 hr with an alpha-exponent equal to approximately 0.3, as in other geophysical phenomena. Second, clear-sky, moist atmospheres show the expected scaling for both measurements and retrievals of the water vapor path. Third, during clear-sky, dry atmospheric days, instrument noise from the 31.4 GHz channel compromises the quality of the water vapor path retrieval.","Evaluating the quality of ground-based microwave radiometer measurements and retrievals using detrended fluctuation and spectral analysis methods Time series both of microwave radiometer brightness temperature measurements at 23.8 and 31.4 GHz and of retrievals of water vapor and liquid water path from these brightness temperatures are evaluated using the detrended fluctuation analysis method. As quantified by the parameter alpha, this method (i) enables identification of the time scales over which noise dominates the time series and (ii) characterizes the temporal range of correlations in the time series. The more common spectral analysis method is also used to assess the data and its results are compared with those from detrended fluctuation analysis method. The assumption that measurements should have certain scaling properties allows the quality of the measurements to be characterized. The additional assumption that the scaling properties of the measurements of an atmospheric quantity are preserved in a useful retrieval provides a means for evaluating the retrieval itself. Applying these two assumptions to microwave radiometer measurements and retrievals demonstrates three points. First, the retrieved water vapor path during cloudy-sky periods can be dominated by noise on shorter than 30min time scales (alpha-exponent  0.1) and exhibits no scaling behavior at longer time scales. However, correlations in the brightness temperatures and liquid water path retrievals are found to be consistent with a power-law behavior for time scales up to 3 hr with an alpha-exponent equal to approximately 0.3, as in other geophysical phenomena. Second, clear-sky, moist atmospheres show the expected scaling for both measurements and retrievals of the water vapor path. Third, during clear-sky, dry atmospheric days, instrument noise from the 31.4 GHz channel compromises the quality of the water vapor path retrieval.",Environment
Enhancing Agile Software Development Sustainability through the Integration of User Experience and Gamification,"This article provides a rich discussion on how the sustainability of agile development processes can be enhanced. In particular, we focus on a recently developed framework, named GLUX, that integrates Lean UX into Scrum. GLUXs main goal is to facilitate a seamless integration between agile and user experience (UX) by using gamification to motivate agile teams to adopt a user-centered mindset and carry out UX activities collaboratively throughout the development process. Our role as software researchers is to contribute towards improving software sustainability and provide the software engineering community with the tools and techniques that will improve the human, economic, and environmental sustainability of software development. We found that GLUX addresses human sustainability by empowering self-sufficient, problem-focused teams, building a motivating and engaging environment, and developing team cooperation. Economic sustainability is addressed by minimizing UX debt and using gamification techniques to direct the focus of the behavior and mindset of agile teams towards value creation. Finally, environmental sustainability is promoted by encouraging agile teams to build a minimum viable product (MVP).","Enhancing Agile Software Development Sustainability through the Integration of User Experience and Gamification This article provides a rich discussion on how the sustainability of agile development processes can be enhanced. In particular, we focus on a recently developed framework, named GLUX, that integrates Lean UX into Scrum. GLUXs main goal is to facilitate a seamless integration between agile and user experience (UX) by using gamification to motivate agile teams to adopt a user-centered mindset and carry out UX activities collaboratively throughout the development process. Our role as software researchers is to contribute towards improving software sustainability and provide the software engineering community with the tools and techniques that will improve the human, economic, and environmental sustainability of software development. We found that GLUX addresses human sustainability by empowering self-sufficient, problem-focused teams, building a motivating and engaging environment, and developing team cooperation. Economic sustainability is addressed by minimizing UX debt and using gamification techniques to direct the focus of the behavior and mindset of agile teams towards value creation. Finally, environmental sustainability is promoted by encouraging agile teams to build a minimum viable product (MVP).",Environment
A note on Keens model: The limits of Schumpeters Creative Destruction,"This paper presents a general solution for a recent model by Keen for endogenous money creation. The solution provides an analytic framework that explains all significant dynamical features of Keens model and their parametric dependence, including an exact result for both the period and subsidence rate of the Great Moderation. It emerges that Keens model has just two possible long term solutions: stable growth or terminal collapse. While collapse can come about immediately from economies that are nonviable by virtue of unsuitable parameters or initial conditions, in general the collapse is preceded by an interval of exponential growth. In first approximation, the duration of that exponential growth is half a period of a sinusoidal oscillation. The period is determined by reciprocal of the imaginary part of one root of a certain quintic polynomial. The real part of the same root determines the rate of growth of the economy. The coefficients of that polynomial depend in a complicated way upon the numerous parameters in the problem and so, therefore, the pattern of roots. For a favorable choice of parameters, the salient root is purely real. This is the circumstance that admits the second possible long term solution, that of indefinite stable growth, i.e. an infinite period.","A note on Keens model: The limits of Schumpeters Creative Destruction This paper presents a general solution for a recent model by Keen for endogenous money creation. The solution provides an analytic framework that explains all significant dynamical features of Keens model and their parametric dependence, including an exact result for both the period and subsidence rate of the Great Moderation. It emerges that Keens model has just two possible long term solutions: stable growth or terminal collapse. While collapse can come about immediately from economies that are nonviable by virtue of unsuitable parameters or initial conditions, in general the collapse is preceded by an interval of exponential growth. In first approximation, the duration of that exponential growth is half a period of a sinusoidal oscillation. The period is determined by reciprocal of the imaginary part of one root of a certain quintic polynomial. The real part of the same root determines the rate of growth of the economy. The coefficients of that polynomial depend in a complicated way upon the numerous parameters in the problem and so, therefore, the pattern of roots. For a favorable choice of parameters, the salient root is purely real. This is the circumstance that admits the second possible long term solution, that of indefinite stable growth, i.e. an infinite period.",Finance
"Climate Science and Control Engineering: Insights, Parallels, and Connections","Climate science is the multidisciplinary field that studies the Earths climate and its evolution. At the very core of climate science are indispensable climate models that predict future climate scenarios, inform policy decisions, and dictate how a countrys economy should change in light of the changing climate. Climate models capture a wide range of interacting dynamic processes via extremely complex ordinary and partial differential equations. To model these large-scale complex processes, climate science leverages supercomputers, advanced simulations, and statistical methods to predict future climate. An area of engineering that is rarely studied in climate science is control engineering. Given that climate systems are inherently dynamic, it is intuitive to analyze them within the framework of dynamic system science. This perspective has been underexplored in the literature. In this manuscript, we provide a tutorial that: (i) introduces the control engineering community to climate dynamics and modeling, including spatiotemporal scales and challenges in climate modeling; (ii) offers a fresh perspective on climate models from a control systems viewpoint; and (iii) explores the relevance and applicability of various advanced graph and network control-based approaches in building a physics-informed framework for learning, control and estimation in climate systems. We also present simple and then more complex climate models, depicting fundamental ideas and processes that are instrumental in building climate change projections. This tutorial also builds parallels and observes connections between various contemporary problems at the forefront of climate science and their control theoretic counterparts. We specifically observe that an abundance of climate science problems can be linguistically reworded and mathematically framed as control theoretic ones.","Climate Science and Control Engineering: Insights, Parallels, and Connections Climate science is the multidisciplinary field that studies the Earths climate and its evolution. At the very core of climate science are indispensable climate models that predict future climate scenarios, inform policy decisions, and dictate how a countrys economy should change in light of the changing climate. Climate models capture a wide range of interacting dynamic processes via extremely complex ordinary and partial differential equations. To model these large-scale complex processes, climate science leverages supercomputers, advanced simulations, and statistical methods to predict future climate. An area of engineering that is rarely studied in climate science is control engineering. Given that climate systems are inherently dynamic, it is intuitive to analyze them within the framework of dynamic system science. This perspective has been underexplored in the literature. In this manuscript, we provide a tutorial that: (i) introduces the control engineering community to climate dynamics and modeling, including spatiotemporal scales and challenges in climate modeling; (ii) offers a fresh perspective on climate models from a control systems viewpoint; and (iii) explores the relevance and applicability of various advanced graph and network control-based approaches in building a physics-informed framework for learning, control and estimation in climate systems. We also present simple and then more complex climate models, depicting fundamental ideas and processes that are instrumental in building climate change projections. This tutorial also builds parallels and observes connections between various contemporary problems at the forefront of climate science and their control theoretic counterparts. We specifically observe that an abundance of climate science problems can be linguistically reworded and mathematically framed as control theoretic ones.",Environment
Effect of noise and modeling errors on the reliability of fully 3D Monte Carlo reconstruction in SPECT,"We recently demonstrated the value of reconstructing SPECT data with fully 3D Monte Carlo reconstruction (F3DMC), in terms of spatial resolution and quantification. This was shown on a small cubic phantom (64 projections 10 x 10) in some idealistic configurations. The goals of the present study were to assess the effect of noise and modeling errors on the reliability of F3DMC, to propose and evaluate strategies for reducing the noise in the projector, and to demonstrate the feasibility of F3DMC for a dataset with realistic dimensions. A small cubic phantom and a realistic Jaszczak phantom dataset were considered. Projections and projectors for both phantoms were calculated using the Monte Carlo simulation code GATE. Projectors with different statistics were considered and two methods for reducing noise in the projector were investigated: one based on principal component analysis (PCA) and the other consisting in setting small probability values to zero. Energy and spatial shifts in projection sampling with respect to projector sampling were also introduced to test F3DMC in realistic conditions. Experiments with the cubic phantom showed the importance of using simulations with high statistics for calculating the projector, and the value of filtering the projector using a PCA approach. F3DMC was shown to be robust with respect to energy shift and small spatial sampling off-set between the projector and the projections. Images of the Jaszczak phantom were successfully reconstructed and also showed promising results in terms of spatial resolution recovery and quantitative accuracy in small structures. It is concluded that the promising results of F3DMC hold on realistic data sets","Effect of noise and modeling errors on the reliability of fully 3D Monte Carlo reconstruction in SPECT We recently demonstrated the value of reconstructing SPECT data with fully 3D Monte Carlo reconstruction (F3DMC), in terms of spatial resolution and quantification. This was shown on a small cubic phantom (64 projections 10 x 10) in some idealistic configurations. The goals of the present study were to assess the effect of noise and modeling errors on the reliability of F3DMC, to propose and evaluate strategies for reducing the noise in the projector, and to demonstrate the feasibility of F3DMC for a dataset with realistic dimensions. A small cubic phantom and a realistic Jaszczak phantom dataset were considered. Projections and projectors for both phantoms were calculated using the Monte Carlo simulation code GATE. Projectors with different statistics were considered and two methods for reducing noise in the projector were investigated: one based on principal component analysis (PCA) and the other consisting in setting small probability values to zero. Energy and spatial shifts in projection sampling with respect to projector sampling were also introduced to test F3DMC in realistic conditions. Experiments with the cubic phantom showed the importance of using simulations with high statistics for calculating the projector, and the value of filtering the projector using a PCA approach. F3DMC was shown to be robust with respect to energy shift and small spatial sampling off-set between the projector and the projections. Images of the Jaszczak phantom were successfully reconstructed and also showed promising results in terms of spatial resolution recovery and quantitative accuracy in small structures. It is concluded that the promising results of F3DMC hold on realistic data sets",Healthcare
The Odyssey Journey: Top-Tier Medical Resource Seeking for Specialized Disorder in China,"It is pivotal for patients to receive accurate health information, diagnoses, and timely treatments. However, in China, the significant imbalanced doctor-to-patient ratio intensifies the information and power asymmetries in doctor-patient relationships. Health information-seeking, which enables patients to collect information from sources beyond doctors, is a potential approach to mitigate these asymmetries. While HCI research predominantly focuses on common chronic conditions, our study focuses on specialized disorders, which are often familiar to specialists but not to general practitioners and the public. With Hemifacial Spasm (HFS) as an example, we aim to understand patients health information and top-tier medical resource seeking journeys in China. Through interviews with three neurosurgeons and 12 HFS patients from rural and urban areas, and applying Actor-Network Theory, we provide empirical insights into the roles, interactions, and workflows of various actors in the health information-seeking network. We also identified five strategies patients adopted to mitigate asymmetries and access top-tier medical resources, illustrating these strategies as subnetworks within the broader health information-seeking network and outlining their advantages and challenges.","The Odyssey Journey: Top-Tier Medical Resource Seeking for Specialized Disorder in China It is pivotal for patients to receive accurate health information, diagnoses, and timely treatments. However, in China, the significant imbalanced doctor-to-patient ratio intensifies the information and power asymmetries in doctor-patient relationships. Health information-seeking, which enables patients to collect information from sources beyond doctors, is a potential approach to mitigate these asymmetries. While HCI research predominantly focuses on common chronic conditions, our study focuses on specialized disorders, which are often familiar to specialists but not to general practitioners and the public. With Hemifacial Spasm (HFS) as an example, we aim to understand patients health information and top-tier medical resource seeking journeys in China. Through interviews with three neurosurgeons and 12 HFS patients from rural and urban areas, and applying Actor-Network Theory, we provide empirical insights into the roles, interactions, and workflows of various actors in the health information-seeking network. We also identified five strategies patients adopted to mitigate asymmetries and access top-tier medical resources, illustrating these strategies as subnetworks within the broader health information-seeking network and outlining their advantages and challenges.",Healthcare
Emerging technologies in physics education,"Three emerging technologies in physics education are evaluated from the interdisciplinary perspective of cognitive science and physics education research. The technologies - Physlet Physics, the Andes Intelligent Tutoring System (ITS), and Microcomputer-Based Laboratory (MBL) Tools - are assessed particularly in terms of their potential at promoting conceptual change, developing expert-like problem-solving skills, and achieving the goals of the traditional physics laboratory. Pedagogical methods to maximize the potential of each educational technology are suggested.","Emerging technologies in physics education Three emerging technologies in physics education are evaluated from the interdisciplinary perspective of cognitive science and physics education research. The technologies - Physlet Physics, the Andes Intelligent Tutoring System (ITS), and Microcomputer-Based Laboratory (MBL) Tools - are assessed particularly in terms of their potential at promoting conceptual change, developing expert-like problem-solving skills, and achieving the goals of the traditional physics laboratory. Pedagogical methods to maximize the potential of each educational technology are suggested.",Education
Handling equality constraints by adaptive relaxing rule for swarm algorithms,"The adaptive constraints relaxing rule for swarm algorithms to handle with the problems with equality constraints is presented. The feasible space of such problems may be similiar to ridge function class, which is hard for applying swarm algorithms. To enter the solution space more easily, the relaxed quasi feasible space is introduced and shrinked adaptively. The experimental results on benchmark functions are compared with the performance of other algorithms, which show its efficiency.","Handling equality constraints by adaptive relaxing rule for swarm algorithms The adaptive constraints relaxing rule for swarm algorithms to handle with the problems with equality constraints is presented. The feasible space of such problems may be similiar to ridge function class, which is hard for applying swarm algorithms. To enter the solution space more easily, the relaxed quasi feasible space is introduced and shrinked adaptively. The experimental results on benchmark functions are compared with the performance of other algorithms, which show its efficiency.",Technology
Genetic Algorithms and Quantum Computation,"Recently, researchers have applied genetic algorithms (GAs) to address some problems in quantum computation. Also, there has been some works in the designing of genetic algorithms based on quantum theoretical concepts and techniques. The so called Quantum Evolutionary Programming has two major sub-areas: Quantum Inspired Genetic Algorithms (QIGAs) and Quantum Genetic Algorithms (QGAs). The former adopts qubit chromosomes as representations and employs quantum gates for the search of the best solution. The later tries to solve a key question in this field: what GAs will look like as an implementation on quantum hardware? As we shall see, there is not a complete answer for this question. An important point for QGAs is to build a quantum algorithm that takes advantage of both the GA and quantum computing parallelism as well as true randomness provided by quantum computers. In the first part of this paper we present a survey of the main works in GAs plus quantum computing including also our works in this area. Henceforth, we review some basic concepts in quantum computation and GAs and emphasize their inherent parallelism. Next, we review the application of GAs for learning quantum operators and circuit design. Then, quantum evolutionary programming is considered. Finally, we present our current research in this field and some perspectives.","Genetic Algorithms and Quantum Computation Recently, researchers have applied genetic algorithms (GAs) to address some problems in quantum computation. Also, there has been some works in the designing of genetic algorithms based on quantum theoretical concepts and techniques. The so called Quantum Evolutionary Programming has two major sub-areas: Quantum Inspired Genetic Algorithms (QIGAs) and Quantum Genetic Algorithms (QGAs). The former adopts qubit chromosomes as representations and employs quantum gates for the search of the best solution. The later tries to solve a key question in this field: what GAs will look like as an implementation on quantum hardware? As we shall see, there is not a complete answer for this question. An important point for QGAs is to build a quantum algorithm that takes advantage of both the GA and quantum computing parallelism as well as true randomness provided by quantum computers. In the first part of this paper we present a survey of the main works in GAs plus quantum computing including also our works in this area. Henceforth, we review some basic concepts in quantum computation and GAs and emphasize their inherent parallelism. Next, we review the application of GAs for learning quantum operators and circuit design. Then, quantum evolutionary programming is considered. Finally, we present our current research in this field and some perspectives.",Technology
Beyond Modality Limitations: A Unified MLLM Approach to Automated Speaking Assessment with Effective Curriculum Learning,"Traditional Automated Speaking Assessment (ASA) systems exhibit inherent modality limitations: text-based approaches lack acoustic information while audio-based methods miss semantic context. Multimodal Large Language Models (MLLM) offer unprecedented opportunities for comprehensive ASA by simultaneously processing audio and text within unified frameworks. This paper presents a very first systematic study of MLLM for comprehensive ASA, demonstrating the superior performance of MLLM across the aspects of content and language use . However, assessment on the delivery aspect reveals unique challenges, which is deemed to require specialized training strategies. We thus propose Speech-First Multimodal Training (SFMT), leveraging a curriculum learning principle to establish more robust modeling foundations of speech before cross-modal synergetic fusion. A series of experiments on a benchmark dataset show MLLM-based systems can elevate the holistic assessment performance from a PCC value of 0.783 to 0.846. In particular, SFMT excels in the evaluation of the delivery aspect, achieving an absolute accuracy improvement of 4 over conventional training approaches, which also paves a new avenue for ASA.","Beyond Modality Limitations: A Unified MLLM Approach to Automated Speaking Assessment with Effective Curriculum Learning Traditional Automated Speaking Assessment (ASA) systems exhibit inherent modality limitations: text-based approaches lack acoustic information while audio-based methods miss semantic context. Multimodal Large Language Models (MLLM) offer unprecedented opportunities for comprehensive ASA by simultaneously processing audio and text within unified frameworks. This paper presents a very first systematic study of MLLM for comprehensive ASA, demonstrating the superior performance of MLLM across the aspects of content and language use . However, assessment on the delivery aspect reveals unique challenges, which is deemed to require specialized training strategies. We thus propose Speech-First Multimodal Training (SFMT), leveraging a curriculum learning principle to establish more robust modeling foundations of speech before cross-modal synergetic fusion. A series of experiments on a benchmark dataset show MLLM-based systems can elevate the holistic assessment performance from a PCC value of 0.783 to 0.846. In particular, SFMT excels in the evaluation of the delivery aspect, achieving an absolute accuracy improvement of 4 over conventional training approaches, which also paves a new avenue for ASA.",Education
Climate Impact Assessment Requires Weighting: Introducing the Weighted Climate Dataset,"High-resolution gridded climate data are readily available from multiple sources, yet climate research and decision-making increasingly require country and region-specific climate information weighted by socio-economic factors. Moreover, the current landscape of disparate data sources and inconsistent weighting methodologies exacerbates the reproducibility crisis and undermines scientific integrity. To address these issues, we have developed a globally comprehensive dataset at both country (GADM0) and region (GADM1) levels, encompassing various climate indicators (precipitation, temperature, SPEI, wind gust). Our methodology involves weighting gridded climate data by population density, night-time light intensity, cropland area, and concurrent population count -- all proxies for socio-economic activity -- before aggregation. We process data from multiple sources, offering daily, monthly, and annual climate variables spanning from 1900 to 2023. A unified framework streamlines our preprocessing steps, and rigorous validation against leading climate impact studies ensures data reliability. The resulting Weighted Climate Dataset is publicly accessible through an online dashboard at https:weightedclimatedata.streamlit.app.","Climate Impact Assessment Requires Weighting: Introducing the Weighted Climate Dataset High-resolution gridded climate data are readily available from multiple sources, yet climate research and decision-making increasingly require country and region-specific climate information weighted by socio-economic factors. Moreover, the current landscape of disparate data sources and inconsistent weighting methodologies exacerbates the reproducibility crisis and undermines scientific integrity. To address these issues, we have developed a globally comprehensive dataset at both country (GADM0) and region (GADM1) levels, encompassing various climate indicators (precipitation, temperature, SPEI, wind gust). Our methodology involves weighting gridded climate data by population density, night-time light intensity, cropland area, and concurrent population count -- all proxies for socio-economic activity -- before aggregation. We process data from multiple sources, offering daily, monthly, and annual climate variables spanning from 1900 to 2023. A unified framework streamlines our preprocessing steps, and rigorous validation against leading climate impact studies ensures data reliability. The resulting Weighted Climate Dataset is publicly accessible through an online dashboard at https:weightedclimatedata.streamlit.app.",Environment
Competing with wild prediction rules,"We consider the problem of on-line prediction competitive with a benchmark class of continuous but highly irregular prediction rules. It is known that if the benchmark class is a reproducing kernel Hilbert space, there exists a prediction algorithm whose average loss over the first N examples does not exceed the average loss of any prediction rule in the class plus a regret term of O(N(-12)). The elements of some natural benchmark classes, however, are so irregular that these classes are not Hilbert spaces. In this paper we develop Banach-space methods to construct a prediction algorithm with a regret term of O(N(-1p)), where p is in 2,infty) and p-2 reflects the degree to which the benchmark class fails to be a Hilbert space.","Competing with wild prediction rules We consider the problem of on-line prediction competitive with a benchmark class of continuous but highly irregular prediction rules. It is known that if the benchmark class is a reproducing kernel Hilbert space, there exists a prediction algorithm whose average loss over the first N examples does not exceed the average loss of any prediction rule in the class plus a regret term of O(N(-12)). The elements of some natural benchmark classes, however, are so irregular that these classes are not Hilbert spaces. In this paper we develop Banach-space methods to construct a prediction algorithm with a regret term of O(N(-1p)), where p is in 2,infty) and p-2 reflects the degree to which the benchmark class fails to be a Hilbert space.",Technology
Impact of IT on Higher education Through Continuing Education,"Information Technology is emerging to be the technology of 21st century. The paradigm shift from industrial society to information society had already become a reality! It is indeed high time to think about integrating IT in all facets of education -- may it be in secondary level, or be it in reskilling the employed ones. This paper discusses various issues in incorporating IT in various levels of education, and the need to think about a task force to counter the so-called slow down and recession in IT industry. The opportunities for aspiring IT professionals were also discussed. The importance of reskilling as a continuing education programme to make the people aware of the changing trends in IT was also discussed.","Impact of IT on Higher education Through Continuing Education Information Technology is emerging to be the technology of 21st century. The paradigm shift from industrial society to information society had already become a reality! It is indeed high time to think about integrating IT in all facets of education -- may it be in secondary level, or be it in reskilling the employed ones. This paper discusses various issues in incorporating IT in various levels of education, and the need to think about a task force to counter the so-called slow down and recession in IT industry. The opportunities for aspiring IT professionals were also discussed. The importance of reskilling as a continuing education programme to make the people aware of the changing trends in IT was also discussed.",Education
Top-down induction of clustering trees,"An approach to clustering is presented that adapts the basic top-down induction of decision trees method towards clustering. To this aim, it employs the principles of instance based learning. The resulting methodology is implemented in the TIC (Top down Induction of Clustering trees) system for first order clustering. The TIC system employs the first order logical decision tree representation of the inductive logic programming system Tilde. Various experiments with TIC are presented, in both propositional and relational domains.","Top-down induction of clustering trees An approach to clustering is presented that adapts the basic top-down induction of decision trees method towards clustering. To this aim, it employs the principles of instance based learning. The resulting methodology is implemented in the TIC (Top down Induction of Clustering trees) system for first order clustering. The TIC system employs the first order logical decision tree representation of the inductive logic programming system Tilde. Various experiments with TIC are presented, in both propositional and relational domains.",Technology
Automatic Curriculum Learning with Gradient Reward Signals,"This paper investigates the impact of using gradient norm reward signals in the context of Automatic Curriculum Learning (ACL) for deep reinforcement learning (DRL). We introduce a framework where the teacher model, utilizing the gradient norm information of a student model, dynamically adapts the learning curriculum. This approach is based on the hypothesis that gradient norms can provide a nuanced and effective measure of learning progress. Our experimental setup involves several reinforcement learning environments (PointMaze, AntMaze, and AdroitHandRelocate), to assess the efficacy of our method. We analyze how gradient norm rewards influence the teachers ability to craft challenging yet achievable learning sequences, ultimately enhancing the students performance. Our results show that this approach not only accelerates the learning process but also leads to improved generalization and adaptability in complex tasks. The findings underscore the potential of gradient norm signals in creating more efficient and robust ACL systems, opening new avenues for research in curriculum learning and reinforcement learning.","Automatic Curriculum Learning with Gradient Reward Signals This paper investigates the impact of using gradient norm reward signals in the context of Automatic Curriculum Learning (ACL) for deep reinforcement learning (DRL). We introduce a framework where the teacher model, utilizing the gradient norm information of a student model, dynamically adapts the learning curriculum. This approach is based on the hypothesis that gradient norms can provide a nuanced and effective measure of learning progress. Our experimental setup involves several reinforcement learning environments (PointMaze, AntMaze, and AdroitHandRelocate), to assess the efficacy of our method. We analyze how gradient norm rewards influence the teachers ability to craft challenging yet achievable learning sequences, ultimately enhancing the students performance. Our results show that this approach not only accelerates the learning process but also leads to improved generalization and adaptability in complex tasks. The findings underscore the potential of gradient norm signals in creating more efficient and robust ACL systems, opening new avenues for research in curriculum learning and reinforcement learning.",Education
Form follows function -- how PufX increases the efficiency of the light-harvesting complexes of Rhodobacter sphaeroides,"Some species of purple bacteria as, e.g., Rhodobacter sphaeroides contain the protein PufX. Concurrently, the light harvesting complexes 1 (LH1) form dimers of open rings. In mutants without PufX, the LH1s are closed rings and photosynthesis breaks down, because the ubiquinone exchange at the reaction center is blocked. Thus, PufX is regarded essential for quinone exchange. In contrast to this view, which implicitly treats the LH1s as obstacles to photosynthesis, we propose that the primary purpose of PufX is to improve the efficiency of light harvesting by inducing the LH1 dimerization. Calculations with a dipole model, which compare the photosynthetic efficiency of various configurations of monomeric and dimeric core complexes, show that the dimer can absorb photons directly into the RC about 30 more efficient, when related to the number of bacteriochlorophylls, but that the performance of the more sophisticated dimeric LH1 antenna degrades faster with structural perturbations. The calculations predict an optimal orientation of the reaction centers relative to the LH1 dimer, which agrees well with the experimentally found configuration. For the increased required rigidity of the dimer additional modifications of the LH1 subunits are necessary, which would lead to the observed ubiquinone blockage, when PufX is missing.","Form follows function -- how PufX increases the efficiency of the light-harvesting complexes of Rhodobacter sphaeroides Some species of purple bacteria as, e.g., Rhodobacter sphaeroides contain the protein PufX. Concurrently, the light harvesting complexes 1 (LH1) form dimers of open rings. In mutants without PufX, the LH1s are closed rings and photosynthesis breaks down, because the ubiquinone exchange at the reaction center is blocked. Thus, PufX is regarded essential for quinone exchange. In contrast to this view, which implicitly treats the LH1s as obstacles to photosynthesis, we propose that the primary purpose of PufX is to improve the efficiency of light harvesting by inducing the LH1 dimerization. Calculations with a dipole model, which compare the photosynthetic efficiency of various configurations of monomeric and dimeric core complexes, show that the dimer can absorb photons directly into the RC about 30 more efficient, when related to the number of bacteriochlorophylls, but that the performance of the more sophisticated dimeric LH1 antenna degrades faster with structural perturbations. The calculations predict an optimal orientation of the reaction centers relative to the LH1 dimer, which agrees well with the experimentally found configuration. For the increased required rigidity of the dimer additional modifications of the LH1 subunits are necessary, which would lead to the observed ubiquinone blockage, when PufX is missing.",Healthcare
Econophysics of Macroeconomics: Action-at-a-Distance and Waves,"We present macroeconomic model that describes evolution of macroeconomic variables and macroeconomic waves on economic space. Risk ratings of economic agents play role of their coordinates on economic space. Aggregation of economic variables like Assets and Investment, Credits and Loans of economic agents at point x define corresponding macroeconomic variables as functions of time t and coordinates x on economic space. Evolution of macroeconomic variables is determined by economic and financial transactions between economic agents. Such transactions can occur between economic agents with any coordinates x and y and that reflect non-local action-at-a-distance character of internal macroeconomic interactions. For instance, Buy-Sell transactions between points x and y on economic space define dynamics of Assets at point x and Investment at point y. Aggregates of transactions between economic agents at point x and y on economic space define economic fields as functions of two coordinates. To describe dynamics of economic fields on economic space we derive hydrodynamic-like equations. For simple models of interactions between economic fields we derive hydrodynamic-like equations in a closed form and obtain wave equations for their perturbations. Economic field waves propagate on economic space and their amplitudes can grow up as exponent in time and may disturb economic stability. Diversities of macroeconomic and financial waves on economic space in simple models uncover importance of wave processes for macroeconomic modeling and forecasting.","Econophysics of Macroeconomics: Action-at-a-Distance and Waves We present macroeconomic model that describes evolution of macroeconomic variables and macroeconomic waves on economic space. Risk ratings of economic agents play role of their coordinates on economic space. Aggregation of economic variables like Assets and Investment, Credits and Loans of economic agents at point x define corresponding macroeconomic variables as functions of time t and coordinates x on economic space. Evolution of macroeconomic variables is determined by economic and financial transactions between economic agents. Such transactions can occur between economic agents with any coordinates x and y and that reflect non-local action-at-a-distance character of internal macroeconomic interactions. For instance, Buy-Sell transactions between points x and y on economic space define dynamics of Assets at point x and Investment at point y. Aggregates of transactions between economic agents at point x and y on economic space define economic fields as functions of two coordinates. To describe dynamics of economic fields on economic space we derive hydrodynamic-like equations. For simple models of interactions between economic fields we derive hydrodynamic-like equations in a closed form and obtain wave equations for their perturbations. Economic field waves propagate on economic space and their amplitudes can grow up as exponent in time and may disturb economic stability. Diversities of macroeconomic and financial waves on economic space in simple models uncover importance of wave processes for macroeconomic modeling and forecasting.",Finance
Multiresolution Kernels,"We present in this work a new methodology to design kernels on data which is structured with smaller components, such as text, images or sequences. This methodology is a template procedure which can be applied on most kernels on measures and takes advantage of a more detailed bag of components representation of the objects. To obtain such a detailed description, we consider possible decompositions of the original bag into a collection of nested bags, following a prior knowledge on the objects structure. We then consider these smaller bags to compare two objects both in a detailed perspective, stressing local matches between the smaller bags, and in a global or coarse perspective, by considering the entire bag. This multiresolution approach is likely to be best suited for tasks where the coarse approach is not precise enough, and where a more subtle mixture of both local and global similarities is necessary to compare objects. The approach presented here would not be computationally tractable without a factorization trick that we introduce before presenting promising results on an image retrieval task.","Multiresolution Kernels We present in this work a new methodology to design kernels on data which is structured with smaller components, such as text, images or sequences. This methodology is a template procedure which can be applied on most kernels on measures and takes advantage of a more detailed bag of components representation of the objects. To obtain such a detailed description, we consider possible decompositions of the original bag into a collection of nested bags, following a prior knowledge on the objects structure. We then consider these smaller bags to compare two objects both in a detailed perspective, stressing local matches between the smaller bags, and in a global or coarse perspective, by considering the entire bag. This multiresolution approach is likely to be best suited for tasks where the coarse approach is not precise enough, and where a more subtle mixture of both local and global similarities is necessary to compare objects. The approach presented here would not be computationally tractable without a factorization trick that we introduce before presenting promising results on an image retrieval task.",Technology
Using Decision Lists to Construct Interpretable and Parsimonious Treatment Regimes,"A treatment regime formalizes personalized medicine as a function from individual patient characteristics to a recommended treatment. A high-quality treatment regime can improve patient outcomes while reducing cost, resource consumption, and treatment burden. Thus, there is tremendous interest in estimating treatment regimes from observational and randomized studies. However, the development of treatment regimes for application in clinical practice requires the long-term, joint effort of statisticians and clinical scientists. In this collaborative process, the statistician must integrate clinical science into the statistical models underlying a treatment regime and the clinician must scrutinize the estimated treatment regime for scientific validity. To facilitate meaningful information exchange, it is important that estimated treatment regimes be interpretable in a subject-matter context. We propose a simple, yet flexible class of treatment regimes whose members are representable as a short list of if-then statements. Regimes in this class are immediately interpretable and are therefore an appealing choice for broad application in practice. We derive a robust estimator of the optimal regime within this class and demonstrate its finite sample performance using simulation experiments. The proposed method is illustrated with data from two clinical trials.","Using Decision Lists to Construct Interpretable and Parsimonious Treatment Regimes A treatment regime formalizes personalized medicine as a function from individual patient characteristics to a recommended treatment. A high-quality treatment regime can improve patient outcomes while reducing cost, resource consumption, and treatment burden. Thus, there is tremendous interest in estimating treatment regimes from observational and randomized studies. However, the development of treatment regimes for application in clinical practice requires the long-term, joint effort of statisticians and clinical scientists. In this collaborative process, the statistician must integrate clinical science into the statistical models underlying a treatment regime and the clinician must scrutinize the estimated treatment regime for scientific validity. To facilitate meaningful information exchange, it is important that estimated treatment regimes be interpretable in a subject-matter context. We propose a simple, yet flexible class of treatment regimes whose members are representable as a short list of if-then statements. Regimes in this class are immediately interpretable and are therefore an appealing choice for broad application in practice. We derive a robust estimator of the optimal regime within this class and demonstrate its finite sample performance using simulation experiments. The proposed method is illustrated with data from two clinical trials.",Healthcare
"Integrated Science, Technology, Engineering and Mathematics (STEM) Education through Active Experience of Designing Technical Toys in Vietnamese Schools","STEM has attracted great consideration. The purpose of research is: (1) study STEM education, (2) explore STEM education with the creative and experiential activity, (3) suggest applying STEM education by designing technical toys for the middle school student. This study used a qualitative approach to carry out teaching integration for STEM education. The study applied to teaching the technological field in Vietnamese middle schools. The design performed at the Faculty of Technology Education, Hanoi National University of Education, Vietnam in April 2015. This study used the integrated approach to design subjects for STEM education. Two procedures for integration undertook with analysis. A sample of producing technical toy was consistent with developing students competencies. Integrated approach to STEM education through designing technical toys is possible. Recently, there has been a booming interest in Integrated Science, Technology, Engineering and Mathematics (STEM) education, but the approaches to STEM still remains controversial in diverse educational contexts. This study addressed this issue by exploring STEM education with the use of creative and experiential activities in a Vietnamese educational context. It also proposed a practical model for integrating STEM into teaching technology in secondary schools by designing technical toys. The implementation of the practical model suggests the possibility in using the integrated approach to STEM education through designing technical toys for middle school students in Vietnam. By applying the subject knowledge domains to solve real world problems and settings, the students can experience the benefits of a concrete and active learning in a meaningful and practical context. The multidisciplinary and interdisciplinary integration approaches are consistent with the development of the students competencies.","Integrated Science, Technology, Engineering and Mathematics (STEM) Education through Active Experience of Designing Technical Toys in Vietnamese Schools STEM has attracted great consideration. The purpose of research is: (1) study STEM education, (2) explore STEM education with the creative and experiential activity, (3) suggest applying STEM education by designing technical toys for the middle school student. This study used a qualitative approach to carry out teaching integration for STEM education. The study applied to teaching the technological field in Vietnamese middle schools. The design performed at the Faculty of Technology Education, Hanoi National University of Education, Vietnam in April 2015. This study used the integrated approach to design subjects for STEM education. Two procedures for integration undertook with analysis. A sample of producing technical toy was consistent with developing students competencies. Integrated approach to STEM education through designing technical toys is possible. Recently, there has been a booming interest in Integrated Science, Technology, Engineering and Mathematics (STEM) education, but the approaches to STEM still remains controversial in diverse educational contexts. This study addressed this issue by exploring STEM education with the use of creative and experiential activities in a Vietnamese educational context. It also proposed a practical model for integrating STEM into teaching technology in secondary schools by designing technical toys. The implementation of the practical model suggests the possibility in using the integrated approach to STEM education through designing technical toys for middle school students in Vietnam. By applying the subject knowledge domains to solve real world problems and settings, the students can experience the benefits of a concrete and active learning in a meaningful and practical context. The multidisciplinary and interdisciplinary integration approaches are consistent with the development of the students competencies.",Education
"Searching for image information content, its discovery, extraction, and representation","Image information content is known to be a complicated and controvercial problem. This paper posits a new image information content definition. Following the theory of Solomonoff-Kolmogorov-Chaitins complexity, we define image information content as a set of descriptions of imafe data structures. Three levels of such description can be generally distinguished: 1)the global level, where the coarse structure of the entire scene is initially outlined; 2) the intermediate level, where structures of separate, non-overlapping image regions usually associated with individual scene objects are deliniated; and 3) the low-level description, where local image structures observed in a limited and restricted field of view are resolved. A technique for creating such image information content descriptors is developed. Its algorithm is presented and elucidated with some examples, which demonstrate the effectiveness of the proposed approach.","Searching for image information content, its discovery, extraction, and representation Image information content is known to be a complicated and controvercial problem. This paper posits a new image information content definition. Following the theory of Solomonoff-Kolmogorov-Chaitins complexity, we define image information content as a set of descriptions of imafe data structures. Three levels of such description can be generally distinguished: 1)the global level, where the coarse structure of the entire scene is initially outlined; 2) the intermediate level, where structures of separate, non-overlapping image regions usually associated with individual scene objects are deliniated; and 3) the low-level description, where local image structures observed in a limited and restricted field of view are resolved. A technique for creating such image information content descriptors is developed. Its algorithm is presented and elucidated with some examples, which demonstrate the effectiveness of the proposed approach.",Technology
Toward Attribute Efficient Learning Algorithms,"We make progress on two important problems regarding attribute efficient learnability. First, we give an algorithm for learning decision lists of length k over n variables using 2tildeO(k13) log n examples and time ntildeO(k13). This is the first algorithm for learning decision lists that has both subexponential sample complexity and subexponential running time in the relevant parameters. Our approach establishes a relationship between attribute efficient learning and polynomial threshold functions and is based on a new construction of low degree, low weight polynomial threshold functions for decision lists. For a wide range of parameters our construction matches a 1994 lower bound due to Beigel for the ODDMAXBIT predicate and gives an essentially optimal tradeoff between polynomial threshold function degree and weight. Second, we give an algorithm for learning an unknown parity function on k out of n variables using O(n1-1k) examples in time polynomial in n. For ko(log n) this yields a polynomial time algorithm with sample complexity o(n). This is the first polynomial time algorithm for learning parity on a superconstant number of variables with sublinear sample complexity.","Toward Attribute Efficient Learning Algorithms We make progress on two important problems regarding attribute efficient learnability. First, we give an algorithm for learning decision lists of length k over n variables using 2tildeO(k13) log n examples and time ntildeO(k13). This is the first algorithm for learning decision lists that has both subexponential sample complexity and subexponential running time in the relevant parameters. Our approach establishes a relationship between attribute efficient learning and polynomial threshold functions and is based on a new construction of low degree, low weight polynomial threshold functions for decision lists. For a wide range of parameters our construction matches a 1994 lower bound due to Beigel for the ODDMAXBIT predicate and gives an essentially optimal tradeoff between polynomial threshold function degree and weight. Second, we give an algorithm for learning an unknown parity function on k out of n variables using O(n1-1k) examples in time polynomial in n. For ko(log n) this yields a polynomial time algorithm with sample complexity o(n). This is the first polynomial time algorithm for learning parity on a superconstant number of variables with sublinear sample complexity.",Technology
Future Climate Change Projections over the Indian Region,"Assessments of impacts of climate change and future projections over the Indian region, have so far relied on a single regional climate model (RCM) - eg., the PRECIS RCM of the Hadley Centre, UK. While these assessments have provided inputs to various reports (e.g., INCCA 2010; NATCOMM2 2012), it is important to have an ensemble of climate projections drawn from multiple RCMs due to large uncertainties in regional-scale climate projections. Ensembles of multi-RCM projections driven under different perceivable socio-economic scenarios are required to capture the probable path of growth, and provide the behavior of future climate and impacts on various biophysical systems and economic sectors dependent on such systems. The Centre for Climate Change Research, Indian Institute of Tropical Meteorology (CCCR-IITM) has generated an ensemble of high resolution downscaled projections of regional climate and monsoon over South Asia until 2100 for the Intergovernmental Panel for Climate Change (IPCC)using a RCM (ICTP-RegCM4) at 50 km horizontal resolution, by driving the regional model with lateral and lower boundary conditions from multiple global atmosphere-ocean coupled models from the Coupled Model Intercomparison Project Phase 5 (CMIP5). The future projections are based on three Representation Concentration Pathway (RCP) scenarios (viz., RCP2.6, RCP4.5, RCP8.5) of the IPCC.","Future Climate Change Projections over the Indian Region Assessments of impacts of climate change and future projections over the Indian region, have so far relied on a single regional climate model (RCM) - eg., the PRECIS RCM of the Hadley Centre, UK. While these assessments have provided inputs to various reports (e.g., INCCA 2010; NATCOMM2 2012), it is important to have an ensemble of climate projections drawn from multiple RCMs due to large uncertainties in regional-scale climate projections. Ensembles of multi-RCM projections driven under different perceivable socio-economic scenarios are required to capture the probable path of growth, and provide the behavior of future climate and impacts on various biophysical systems and economic sectors dependent on such systems. The Centre for Climate Change Research, Indian Institute of Tropical Meteorology (CCCR-IITM) has generated an ensemble of high resolution downscaled projections of regional climate and monsoon over South Asia until 2100 for the Intergovernmental Panel for Climate Change (IPCC)using a RCM (ICTP-RegCM4) at 50 km horizontal resolution, by driving the regional model with lateral and lower boundary conditions from multiple global atmosphere-ocean coupled models from the Coupled Model Intercomparison Project Phase 5 (CMIP5). The future projections are based on three Representation Concentration Pathway (RCP) scenarios (viz., RCP2.6, RCP4.5, RCP8.5) of the IPCC.",Environment
Bayesian Model Choice of Grouped t-copula,"One of the most popular copulas for modeling dependence structures is t-copula. Recently the grouped t-copula was generalized to allow each group to have one member only, so that a priori grouping is not required and the dependence modeling is more flexible. This paper describes a Markov chain Monte Carlo (MCMC) method under the Bayesian inference framework for estimating and choosing t-copula models. Using historical data of foreign exchange (FX) rates as a case study, we found that Bayesian model choice criteria overwhelmingly favor the generalized t-copula. In addition, all the criteria also agree on the second most likely model and these inferences are all consistent with classical likelihood ratio tests. Finally, we demonstrate the impact of model choice on the conditional Value-at-Risk for portfolios of six major FX rates.","Bayesian Model Choice of Grouped t-copula One of the most popular copulas for modeling dependence structures is t-copula. Recently the grouped t-copula was generalized to allow each group to have one member only, so that a priori grouping is not required and the dependence modeling is more flexible. This paper describes a Markov chain Monte Carlo (MCMC) method under the Bayesian inference framework for estimating and choosing t-copula models. Using historical data of foreign exchange (FX) rates as a case study, we found that Bayesian model choice criteria overwhelmingly favor the generalized t-copula. In addition, all the criteria also agree on the second most likely model and these inferences are all consistent with classical likelihood ratio tests. Finally, we demonstrate the impact of model choice on the conditional Value-at-Risk for portfolios of six major FX rates.",Finance
A survey of open questions in adaptive therapy: bridging mathematics and clinical translation,"Adaptive therapy is a dynamic cancer treatment protocol that updates (or adapts) treatment decisions in anticipation of evolving tumor dynamics. This broad term encompasses many possible dynamic treatment protocols of patient-specific dose modulation or dose timing. Adaptive therapy maintains high levels of tumor burden to benefit from the competitive suppression of treatment-sensitive subpopulations on treatment-resistant subpopulations. This evolution-based approach to cancer treatment has been integrated into several ongoing or planned clinical trials, including treatment of metastatic castrate resistant prostate cancer, ovarian cancer, and BRAF-mutant melanoma. In the previous few decades, experimental and clinical investigation of adaptive therapy has progressed synergistically with mathematical and computational modeling. In this work, we discuss 11 open questions in cancer adaptive therapy mathematical modeling. The questions are split into three sections: 1) the necessary components of mathematical models of adaptive therapy 2) design and validation of dosing protocols, and 3) challenges and opportunities in clinical translation.","A survey of open questions in adaptive therapy: bridging mathematics and clinical translation Adaptive therapy is a dynamic cancer treatment protocol that updates (or adapts) treatment decisions in anticipation of evolving tumor dynamics. This broad term encompasses many possible dynamic treatment protocols of patient-specific dose modulation or dose timing. Adaptive therapy maintains high levels of tumor burden to benefit from the competitive suppression of treatment-sensitive subpopulations on treatment-resistant subpopulations. This evolution-based approach to cancer treatment has been integrated into several ongoing or planned clinical trials, including treatment of metastatic castrate resistant prostate cancer, ovarian cancer, and BRAF-mutant melanoma. In the previous few decades, experimental and clinical investigation of adaptive therapy has progressed synergistically with mathematical and computational modeling. In this work, we discuss 11 open questions in cancer adaptive therapy mathematical modeling. The questions are split into three sections: 1) the necessary components of mathematical models of adaptive therapy 2) design and validation of dosing protocols, and 3) challenges and opportunities in clinical translation.",Healthcare
Information Geometry of Risks and Returns,We reveal a geometric structure underlying both hedging and investment products. The structure follows from a simple formula expressing investment risks in terms of returns. This informs optimal product designs. Optimal pure hedging (including cost-optimal products) and hybrid hedging (where a partial hedge is built into an optimal investment product) are considered. Duality between hedging and investment is demonstrated with applications to optimal risk recycling. A geometric interpretation of rationality is presented.,Information Geometry of Risks and Returns We reveal a geometric structure underlying both hedging and investment products. The structure follows from a simple formula expressing investment risks in terms of returns. This informs optimal product designs. Optimal pure hedging (including cost-optimal products) and hybrid hedging (where a partial hedge is built into an optimal investment product) are considered. Duality between hedging and investment is demonstrated with applications to optimal risk recycling. A geometric interpretation of rationality is presented.,Finance
An Efficient Mean Field Approach to the Set Covering Problem,"A mean field feedback artificial neural network algorithm is developed and explored for the set covering problem. A convenient encoding of the inequality constraints is achieved by means of a multilinear penalty function. An approximate energy minimum is obtained by iterating a set of mean field equations, in combination with annealing. The approach is numerically tested against a set of publicly available test problems with sizes ranging up to 5x103 rows and 106 columns. When comparing the performance with exact results for sizes where these are available, the approach yields results within a few percent from the optimal solutions. Comparisons with other approximate methods also come out well, in particular given the very low CPU consumption required -- typically a few seconds. Arbitrary problems can be processed using the algorithm via a public domain server.","An Efficient Mean Field Approach to the Set Covering Problem A mean field feedback artificial neural network algorithm is developed and explored for the set covering problem. A convenient encoding of the inequality constraints is achieved by means of a multilinear penalty function. An approximate energy minimum is obtained by iterating a set of mean field equations, in combination with annealing. The approach is numerically tested against a set of publicly available test problems with sizes ranging up to 5x103 rows and 106 columns. When comparing the performance with exact results for sizes where these are available, the approach yields results within a few percent from the optimal solutions. Comparisons with other approximate methods also come out well, in particular given the very low CPU consumption required -- typically a few seconds. Arbitrary problems can be processed using the algorithm via a public domain server.",Technology
A Modified Iterative IOM Approach for Optimization of Biochemical Systems,"The presented previously indirect optimization method (IOM) developed within biochemical systems theory (BST) provides a versatile and mathematically tractable optimization strategy for biochemical systems. However, due to the local approximations nature of the BST formalism, the iterative version of this technique possibly does not yield the true optimum solution. In this work, an algorithm is proposed to obtain the correct and consistent optimum steady-state operating point of biochemical systems. The existing linear optimization problem of the direct IOM approach is modified by adding an equality constraint of describing the consistency of solutions between the S-system and the original model. Lagrangian analysis is employed to derive the first order necessary optimality conditions for the above modified optimization problem. This leads to a procedure that may be regarded as a modified iterative IOM approach in which the optimization objective function includes an extra linear term. The extra term contains a comparison of metabolite concentration derivatives with respect to the enzyme activities between the S-system and the original model and ensures that the new algorithm is still carried out within linear programming techniques. The presented framework is applied to several biochemical systems and shown to the tractability and effectiveness of the method. The simulation is also studied to investigate the convergence properties of the algorithm and to give a performance comparison of standard and modified iterative IOM approach.","A Modified Iterative IOM Approach for Optimization of Biochemical Systems The presented previously indirect optimization method (IOM) developed within biochemical systems theory (BST) provides a versatile and mathematically tractable optimization strategy for biochemical systems. However, due to the local approximations nature of the BST formalism, the iterative version of this technique possibly does not yield the true optimum solution. In this work, an algorithm is proposed to obtain the correct and consistent optimum steady-state operating point of biochemical systems. The existing linear optimization problem of the direct IOM approach is modified by adding an equality constraint of describing the consistency of solutions between the S-system and the original model. Lagrangian analysis is employed to derive the first order necessary optimality conditions for the above modified optimization problem. This leads to a procedure that may be regarded as a modified iterative IOM approach in which the optimization objective function includes an extra linear term. The extra term contains a comparison of metabolite concentration derivatives with respect to the enzyme activities between the S-system and the original model and ensures that the new algorithm is still carried out within linear programming techniques. The presented framework is applied to several biochemical systems and shown to the tractability and effectiveness of the method. The simulation is also studied to investigate the convergence properties of the algorithm and to give a performance comparison of standard and modified iterative IOM approach.",Healthcare
On the possibility of making the complete computer model of a human brain,The development of the algorithm of a neural network building by the corresponding parts of a DNA code is discussed.,On the possibility of making the complete computer model of a human brain The development of the algorithm of a neural network building by the corresponding parts of a DNA code is discussed.,Technology
Routine Outcome Monitoring in Psychotherapy Treatment using Sentiment-Topic Modelling Approach,"Despite the importance of emphasizing the right psychotherapy treatment for an individual patient, assessing the outcome of the therapy session is equally crucial. Evidence showed that continuous monitoring patients progress can significantly improve the therapy outcomes to an expected change. By monitoring the outcome, the patients progress can be tracked closely to help clinicians identify patients who are not progressing in the treatment. These monitoring can help the clinician to consider any necessary actions for the patients treatment as early as possible, e.g., recommend different types of treatment, or adjust the style of approach. Currently, the evaluation system is based on the clinical-rated and self-report questionnaires that measure patients progress pre- and post-treatment. While outcome monitoring tends to improve the therapy outcomes, however, there are many challenges in the current method, e.g. time and financial burden for administering questionnaires, scoring and analysing the results. Therefore, a computational method for measuring and monitoring patient progress over the course of treatment is needed, in order to enhance the likelihood of positive treatment outcome. Moreover, this computational method could potentially lead to an inexpensive monitoring tool to evaluate patients progress in clinical care that could be administered by a wider range of health-care professionals.","Routine Outcome Monitoring in Psychotherapy Treatment using Sentiment-Topic Modelling Approach Despite the importance of emphasizing the right psychotherapy treatment for an individual patient, assessing the outcome of the therapy session is equally crucial. Evidence showed that continuous monitoring patients progress can significantly improve the therapy outcomes to an expected change. By monitoring the outcome, the patients progress can be tracked closely to help clinicians identify patients who are not progressing in the treatment. These monitoring can help the clinician to consider any necessary actions for the patients treatment as early as possible, e.g., recommend different types of treatment, or adjust the style of approach. Currently, the evaluation system is based on the clinical-rated and self-report questionnaires that measure patients progress pre- and post-treatment. While outcome monitoring tends to improve the therapy outcomes, however, there are many challenges in the current method, e.g. time and financial burden for administering questionnaires, scoring and analysing the results. Therefore, a computational method for measuring and monitoring patient progress over the course of treatment is needed, in order to enhance the likelihood of positive treatment outcome. Moreover, this computational method could potentially lead to an inexpensive monitoring tool to evaluate patients progress in clinical care that could be administered by a wider range of health-care professionals.",Healthcare
Quantifying the multi-scale and multi-resource impacts of large-scale adoption of renewable energy sources,"The variability and intermittency of renewable energy sources pose several challenges for power systems operations, including energy curtailment and price volatility. In power systems with considerable renewable sources, co-variability in renewable energy supply and electricity load can intensify these outcomes. In this study, we examine the impacts of renewable co-variability across multiple spatial and temporal scales on the New York State power system, which is undergoing a major transition toward increased renewable generation. We characterize the spatiotemporal co-variability of renewable energy-generating resources and electricity load and investigate the impact of climatic variability on electricity price volatility. We use an accurate, reduced-form representation of the New York power system, which integrates additional wind and solar power resources to meet the states energy targets through 2030. Our results demonstrate that renewable energy resources can vary up to 17 from the annual average, though combining different resources reduces the overall variation to about 8. On an hourly basis, renewable volatility is substantially greater and may vary up to 100 above and below average. This results in a 9 variation in annual average electricity prices and up to a 56 variation in the frequency of price spikes. While yearly average price volatility is influenced mainly by hydropower availability, daily and hourly price volatility is influenced by solar and wind availability.","Quantifying the multi-scale and multi-resource impacts of large-scale adoption of renewable energy sources The variability and intermittency of renewable energy sources pose several challenges for power systems operations, including energy curtailment and price volatility. In power systems with considerable renewable sources, co-variability in renewable energy supply and electricity load can intensify these outcomes. In this study, we examine the impacts of renewable co-variability across multiple spatial and temporal scales on the New York State power system, which is undergoing a major transition toward increased renewable generation. We characterize the spatiotemporal co-variability of renewable energy-generating resources and electricity load and investigate the impact of climatic variability on electricity price volatility. We use an accurate, reduced-form representation of the New York power system, which integrates additional wind and solar power resources to meet the states energy targets through 2030. Our results demonstrate that renewable energy resources can vary up to 17 from the annual average, though combining different resources reduces the overall variation to about 8. On an hourly basis, renewable volatility is substantially greater and may vary up to 100 above and below average. This results in a 9 variation in annual average electricity prices and up to a 56 variation in the frequency of price spikes. While yearly average price volatility is influenced mainly by hydropower availability, daily and hourly price volatility is influenced by solar and wind availability.",Environment
A method of moments approach to pricing double barrier contracts driven by a general class of jump diffusions,"We present the method of moments approach to pricing barrier-type options when the underlying is modelled by a general class of jump diffusions. By general principles the option prices are linked to certain infinite dimensional linear programming problems. Subsequently approximating those systems by finite dimensional linear programming problems, upper and lower bounds for the prices of such options are found. As numerical illustration we apply the method to the valuation of several barrier-type options (double barrier knockout option, American corridor and double no touch) under a number of different models, including a case with deterministic interest rates, and compare with Monte Carlo simulation results. In all cases we find tight bounds with short execution times. Theoretical convergence results are also provided.","A method of moments approach to pricing double barrier contracts driven by a general class of jump diffusions We present the method of moments approach to pricing barrier-type options when the underlying is modelled by a general class of jump diffusions. By general principles the option prices are linked to certain infinite dimensional linear programming problems. Subsequently approximating those systems by finite dimensional linear programming problems, upper and lower bounds for the prices of such options are found. As numerical illustration we apply the method to the valuation of several barrier-type options (double barrier knockout option, American corridor and double no touch) under a number of different models, including a case with deterministic interest rates, and compare with Monte Carlo simulation results. In all cases we find tight bounds with short execution times. Theoretical convergence results are also provided.",Finance
Scale-sensitive Psi-dimensions: the Capacity Measures for Classifiers Taking Values in RQ,"Bounds on the risk play a crucial role in statistical learning theory. They usually involve as capacity measure of the model studied the VC dimension or one of its extensions. In classification, such VC dimensions exist for models taking values in 0, 1, 1,..., Q and R. We introduce the generalizations appropriate for the missing case, the one of models with values in RQ. This provides us with a new guaranteed risk for M-SVMs which appears superior to the existing one.","Scale-sensitive Psi-dimensions: the Capacity Measures for Classifiers Taking Values in RQ Bounds on the risk play a crucial role in statistical learning theory. They usually involve as capacity measure of the model studied the VC dimension or one of its extensions. In classification, such VC dimensions exist for models taking values in 0, 1, 1,..., Q and R. We introduce the generalizations appropriate for the missing case, the one of models with values in RQ. This provides us with a new guaranteed risk for M-SVMs which appears superior to the existing one.",Technology
Cross-correlation in financial dynamics,"To investigate the universal structure of interactions in financial dynamics, we analyze the cross-correlation matrix C of price returns of the Chinese stock market, in comparison with those of the American and Indian stock markets. As an important emerging market, the Chinese market exhibits much stronger correlations than the developed markets. In the Chinese market, the interactions between the stocks in a same business sector are weak, while extra interactions in unusual sectors are detected. Using a variation of the two-factor model, we simulate the interactions in financial markets.","Cross-correlation in financial dynamics To investigate the universal structure of interactions in financial dynamics, we analyze the cross-correlation matrix C of price returns of the Chinese stock market, in comparison with those of the American and Indian stock markets. As an important emerging market, the Chinese market exhibits much stronger correlations than the developed markets. In the Chinese market, the interactions between the stocks in a same business sector are weak, while extra interactions in unusual sectors are detected. Using a variation of the two-factor model, we simulate the interactions in financial markets.",Finance
Climate Change Valuation Adjustment (CCVA) using parameterized climate change impacts,"We introduce Climate Change Valuation Adjustment (CCVA) to capture climate change impacts on CVAFVA that are currently invisible assuming typical market practice. To discuss such impacts on CVAFVA from changes to instantaneous hazard rates we introduce a flexible and expressive parameterization to capture the path of this impact to climate change endpoints, and transient transition effects. Finally we provide quantification of examples of typical interest where there is risk of economic stress from sea level change up to 2101, and from transformations of business models. We find that even with the slowest possible uniform approach to a climate change impact in 2101 there can still be significant CVAFVA impacts on interest rate swaps of 20 years or more maturity. Transformation effects on CVAFVA are strongly dependent on timing and duration of business model transformation. Using a parameterized approach enables discussion with stakeholders of economic impacts on CVAFVA, whatever the details behind the climate impact.","Climate Change Valuation Adjustment (CCVA) using parameterized climate change impacts We introduce Climate Change Valuation Adjustment (CCVA) to capture climate change impacts on CVAFVA that are currently invisible assuming typical market practice. To discuss such impacts on CVAFVA from changes to instantaneous hazard rates we introduce a flexible and expressive parameterization to capture the path of this impact to climate change endpoints, and transient transition effects. Finally we provide quantification of examples of typical interest where there is risk of economic stress from sea level change up to 2101, and from transformations of business models. We find that even with the slowest possible uniform approach to a climate change impact in 2101 there can still be significant CVAFVA impacts on interest rate swaps of 20 years or more maturity. Transformation effects on CVAFVA are strongly dependent on timing and duration of business model transformation. Using a parameterized approach enables discussion with stakeholders of economic impacts on CVAFVA, whatever the details behind the climate impact.",Environment
A review of clustering models in educational data science towards fairness-aware learning,"Ensuring fairness is essential for every education system. Machine learning is increasingly supporting the education system and educational data science (EDS) domain, from decision support to educational activities and learning analytics. However, the machine learning-based decisions can be biased because the algorithms may generate the results based on students protected attributes such as race or gender. Clustering is an important machine learning technique to explore student data in order to support the decision-maker, as well as support educational activities, such as group assignments. Therefore, ensuring high-quality clustering models along with satisfying fairness constraints are important requirements. This chapter comprehensively surveys clustering models and their fairness in EDS. We especially focus on investigating the fair clustering models applied in educational activities. These models are believed to be practical tools for analyzing students data and ensuring fairness in EDS.","A review of clustering models in educational data science towards fairness-aware learning Ensuring fairness is essential for every education system. Machine learning is increasingly supporting the education system and educational data science (EDS) domain, from decision support to educational activities and learning analytics. However, the machine learning-based decisions can be biased because the algorithms may generate the results based on students protected attributes such as race or gender. Clustering is an important machine learning technique to explore student data in order to support the decision-maker, as well as support educational activities, such as group assignments. Therefore, ensuring high-quality clustering models along with satisfying fairness constraints are important requirements. This chapter comprehensively surveys clustering models and their fairness in EDS. We especially focus on investigating the fair clustering models applied in educational activities. These models are believed to be practical tools for analyzing students data and ensuring fairness in EDS.",Education
A Markovian Model Market - Akerlofs Lemmons and the Asymmetry of Information,"In this work we study an economic agent based model under different asymmetric information degrees. This model is quite simple and can be treated analytically since the buyers evaluate the quality of a certain good taking into account only the quality of the last good purchased plus her perceptive capacity beta . As a consequence the system evolves according to a stationary Markovian stochastic process. The value of a product offered by the seller increases with quality according to the exponent alpha, which is a measure of technology. It incorporates all the technological capacity of production systems such as education, scientific development and techniques that change the productivity growth. The technological level plays an important role to explain how the asymmetry of information may affect the market evolution in this model. We observe that, for high technological levels, the market can control adverse selection. The model allows us to compute the maximum asymmetric information degree before market collapse. Below this critical point the market evolves during a very limited time and then dies out completely. When beta is closer to 1(symmetric information), the market becomes more profitable for high quality goods, although high and low quality markets coexist. All the results we obtained from the model are analytical and the maximum asymmetric information level is a consequence of an ergodicity breakdown in the process of quality evaluation.","A Markovian Model Market - Akerlofs Lemmons and the Asymmetry of Information In this work we study an economic agent based model under different asymmetric information degrees. This model is quite simple and can be treated analytically since the buyers evaluate the quality of a certain good taking into account only the quality of the last good purchased plus her perceptive capacity beta . As a consequence the system evolves according to a stationary Markovian stochastic process. The value of a product offered by the seller increases with quality according to the exponent alpha, which is a measure of technology. It incorporates all the technological capacity of production systems such as education, scientific development and techniques that change the productivity growth. The technological level plays an important role to explain how the asymmetry of information may affect the market evolution in this model. We observe that, for high technological levels, the market can control adverse selection. The model allows us to compute the maximum asymmetric information degree before market collapse. Below this critical point the market evolves during a very limited time and then dies out completely. When beta is closer to 1(symmetric information), the market becomes more profitable for high quality goods, although high and low quality markets coexist. All the results we obtained from the model are analytical and the maximum asymmetric information level is a consequence of an ergodicity breakdown in the process of quality evaluation.",Finance
Design of an Experiment to Test Quantum Probabilistic Behavior of the Financial market,"The recent crash demonstrated (once again) that the description of the financial market by present financial mathematics cannot be considered as totally satisfactory. We remind that nowadays financial mathematics is heavily based on the use of random variables and stochastic processes which are described by Kolmogorovs measure-theoretic model for probability (classical probabilistic model). I speculate that the present financial crises is a sign (a kind of experiment to test validity of classical probability theory at the financial market) that the use of this model in finances should be either totally rejected or at least completed. One of the best candidates for a new probabilistic financial model is quantum probability or its generalizations, so to say quantum-like (QL) models. Speculations that the financial market may be nonclassical have been present in scientific literature for many years. The aim of this note is to move from the domain of speculation to rigorous statistical arguments in favor of probabilistic nonclassicality of the financial market. I design a corresponding statistical test which is based on violation of the formula of total probability (FTP). The latter is the basic in classical probability and its violation would be a strong sign in favor of QL behavior at the market.","Design of an Experiment to Test Quantum Probabilistic Behavior of the Financial market The recent crash demonstrated (once again) that the description of the financial market by present financial mathematics cannot be considered as totally satisfactory. We remind that nowadays financial mathematics is heavily based on the use of random variables and stochastic processes which are described by Kolmogorovs measure-theoretic model for probability (classical probabilistic model). I speculate that the present financial crises is a sign (a kind of experiment to test validity of classical probability theory at the financial market) that the use of this model in finances should be either totally rejected or at least completed. One of the best candidates for a new probabilistic financial model is quantum probability or its generalizations, so to say quantum-like (QL) models. Speculations that the financial market may be nonclassical have been present in scientific literature for many years. The aim of this note is to move from the domain of speculation to rigorous statistical arguments in favor of probabilistic nonclassicality of the financial market. I design a corresponding statistical test which is based on violation of the formula of total probability (FTP). The latter is the basic in classical probability and its violation would be a strong sign in favor of QL behavior at the market.",Finance
Embedding Culture and Grit in the Technology Acceptance Model (TAM) for Higher Education,"The implementors of learning technologies within education environments often follow strategies that assume the educational environment within which they are being introduced is culturally neutral. A comprehensive literature review including 150 papers on educational technology challenges was undertaken. The purpose of this review is explore different contextual challenges to the adoption of educational technology in the higher education sector. The cultural factors that define the key stakeholders (e.g., teachers, lectures, students and support staff) are often ignored when the implementation processes are undertaken. Furthermore, it is often assumed that the personnel responsible for the implementation are also culturally neutral and do not possess any attributes unique to their culture. It has been shown that cultural factors may significantly influence the implementation of learning technologies and to design strategies that fail to consider factors may limit their efficiency and effectiveness. The challenges are interrelated and based on the findings, this review proposes a conceptual framework by integrating culture and grit into the Technology Acceptance Model(TAM) for implementing educational technology in higher education. The framework will be useful to guide both practice and research.","Embedding Culture and Grit in the Technology Acceptance Model (TAM) for Higher Education The implementors of learning technologies within education environments often follow strategies that assume the educational environment within which they are being introduced is culturally neutral. A comprehensive literature review including 150 papers on educational technology challenges was undertaken. The purpose of this review is explore different contextual challenges to the adoption of educational technology in the higher education sector. The cultural factors that define the key stakeholders (e.g., teachers, lectures, students and support staff) are often ignored when the implementation processes are undertaken. Furthermore, it is often assumed that the personnel responsible for the implementation are also culturally neutral and do not possess any attributes unique to their culture. It has been shown that cultural factors may significantly influence the implementation of learning technologies and to design strategies that fail to consider factors may limit their efficiency and effectiveness. The challenges are interrelated and based on the findings, this review proposes a conceptual framework by integrating culture and grit into the Technology Acceptance Model(TAM) for implementing educational technology in higher education. The framework will be useful to guide both practice and research.",Education
Cosmic Rays and Solar Insolation as the Main Control Parameters of the Catastrophe Theory of Climatic Response to Orbital Variations,"The energy-balance model of global climate, which is taking into account a nontrivial role of solar and galactic protons, is presented. The model is described by the equation of fold catastrophe relative to increment of temperature, where the variation of a solar insolation and cosmic rays are control parameters. It is shown that the bifurcation equation of the model describes one of two stable states of the climate system. The solution of this equation exhibits the property of the determined bistable behavior of climate at the global level and the possibility of appearance of the determined chaos of the weathers at the local levels. The results of the comparative analysis of the computer simulation of the time-dependent solution of energy-balance model of global climate and the oxygen isotope records for deep-sea core V28-238 over the past 730 kyr are presented, and the evolution of climate on 100 kyr forward is also predicted. It is shown that the proposed model successfully explains the nicety of the paleoclimatic records. The model is clear of all known difficulties of the Milankovich theory for the analysis and the interpretation of physical mechanism, by which the climate system responds to orbital forcing.","Cosmic Rays and Solar Insolation as the Main Control Parameters of the Catastrophe Theory of Climatic Response to Orbital Variations The energy-balance model of global climate, which is taking into account a nontrivial role of solar and galactic protons, is presented. The model is described by the equation of fold catastrophe relative to increment of temperature, where the variation of a solar insolation and cosmic rays are control parameters. It is shown that the bifurcation equation of the model describes one of two stable states of the climate system. The solution of this equation exhibits the property of the determined bistable behavior of climate at the global level and the possibility of appearance of the determined chaos of the weathers at the local levels. The results of the comparative analysis of the computer simulation of the time-dependent solution of energy-balance model of global climate and the oxygen isotope records for deep-sea core V28-238 over the past 730 kyr are presented, and the evolution of climate on 100 kyr forward is also predicted. It is shown that the proposed model successfully explains the nicety of the paleoclimatic records. The model is clear of all known difficulties of the Milankovich theory for the analysis and the interpretation of physical mechanism, by which the climate system responds to orbital forcing.",Environment
A project-based course about outreach in a physics curriculum,"We describe an undergraduate course where physics students are asked to conceive an outreach project of their own. The course alternates between the project conception and teachings about pedagogy and outreach, and ends in a public show. We describe its practical implementation and benefits. Through a student survey and an analysis of their projects, we discuss the merits and flaws of this learning-by-doing teaching approach for physics.","A project-based course about outreach in a physics curriculum We describe an undergraduate course where physics students are asked to conceive an outreach project of their own. The course alternates between the project conception and teachings about pedagogy and outreach, and ends in a public show. We describe its practical implementation and benefits. Through a student survey and an analysis of their projects, we discuss the merits and flaws of this learning-by-doing teaching approach for physics.",Education
A Differential Invariant for Zooming,This paper presents an invariant under scaling and linear brightness change. The invariant is based on differentials and therefore is a local feature. Rotationally invariant 2-d differential Gaussian operators up to third order are proposed for the implementation of the invariant. The performance is analyzed by simulating a camera zoom-out.,A Differential Invariant for Zooming This paper presents an invariant under scaling and linear brightness change. The invariant is based on differentials and therefore is a local feature. Rotationally invariant 2-d differential Gaussian operators up to third order are proposed for the implementation of the invariant. The performance is analyzed by simulating a camera zoom-out.,Technology
The FuturICT Education Accelerator,"Education is a major force for economic and social wellbeing. Despite high aspirations, education at all levels can be expensive and ineffective. Three Grand Challenges are identified: (1) enable people to learn orders of magnitude more effectively, (2) enable people to learn at orders of magnitude less cost, and (3) demonstrate success by exemplary interdisciplinary education in complex systems science. A ten year man-on-the-moon project is proposed in which FuturICTs unique combination of Complexity, Social and Computing Sciences could provide an urgently needed transdisciplinary language for making sense of educational systems. In close dialogue with educational theory and practice, and grounded in the emerging data science and learning analytics paradigms, this will translate into practical tools (both analytical and computational) for researchers, practitioners and leaders; generative principles for resilient educational ecosystems; and innovation for radically scalable, yet personalised, learner engagement and assessment. The proposed em Education Accelerator will serve as a wind tunnel for testing these ideas in the context of real educational programmes, with an international virtual campus delivering complex systems education exploiting the new understanding of complex, social, computationally enhanced organisational structure developed within FuturICT.","The FuturICT Education Accelerator Education is a major force for economic and social wellbeing. Despite high aspirations, education at all levels can be expensive and ineffective. Three Grand Challenges are identified: (1) enable people to learn orders of magnitude more effectively, (2) enable people to learn at orders of magnitude less cost, and (3) demonstrate success by exemplary interdisciplinary education in complex systems science. A ten year man-on-the-moon project is proposed in which FuturICTs unique combination of Complexity, Social and Computing Sciences could provide an urgently needed transdisciplinary language for making sense of educational systems. In close dialogue with educational theory and practice, and grounded in the emerging data science and learning analytics paradigms, this will translate into practical tools (both analytical and computational) for researchers, practitioners and leaders; generative principles for resilient educational ecosystems; and innovation for radically scalable, yet personalised, learner engagement and assessment. The proposed em Education Accelerator will serve as a wind tunnel for testing these ideas in the context of real educational programmes, with an international virtual campus delivering complex systems education exploiting the new understanding of complex, social, computationally enhanced organisational structure developed within FuturICT.",Education
Supervised Feature Selection via Dependence Estimation,"We introduce a framework for filtering features that employs the Hilbert-Schmidt Independence Criterion (HSIC) as a measure of dependence between the features and the labels. The key idea is that good features should maximise such dependence. Feature selection for various supervised learning problems (including classification and regression) is unified under this framework, and the solutions can be approximated using a backward-elimination algorithm. We demonstrate the usefulness of our method on both artificial and real world datasets.","Supervised Feature Selection via Dependence Estimation We introduce a framework for filtering features that employs the Hilbert-Schmidt Independence Criterion (HSIC) as a measure of dependence between the features and the labels. The key idea is that good features should maximise such dependence. Feature selection for various supervised learning problems (including classification and regression) is unified under this framework, and the solutions can be approximated using a backward-elimination algorithm. We demonstrate the usefulness of our method on both artificial and real world datasets.",Technology
Implementation of Tripartite Estimands Using Adherence Causal Estimators Under the Causal Inference Framework,"Intercurrent events (ICEs) and missing values are inevitable in clinical trials of any size and duration, making it difficult to assess the treatment effect for all patients in randomized clinical trials. Defining the appropriate estimand that is relevant to the clinical research question is the first step in analyzing data. The tripartite estimands, which evaluate the treatment differences in the proportion of patients with ICEs due to adverse events, the proportion of patients with ICEs due to lack of efficacy, and the primary efficacy outcome for those who can adhere to study treatment under the causal inference framework, are of interest to many stakeholders in understanding the totality of treatment effects. In this manuscript, we discuss the details of how to estimate tripartite estimands based on a causal inference framework and how to interpret tripartite estimates through a phase 3 clinical study evaluating a basal insulin treatment for patients with type 1 diabetes.","Implementation of Tripartite Estimands Using Adherence Causal Estimators Under the Causal Inference Framework Intercurrent events (ICEs) and missing values are inevitable in clinical trials of any size and duration, making it difficult to assess the treatment effect for all patients in randomized clinical trials. Defining the appropriate estimand that is relevant to the clinical research question is the first step in analyzing data. The tripartite estimands, which evaluate the treatment differences in the proportion of patients with ICEs due to adverse events, the proportion of patients with ICEs due to lack of efficacy, and the primary efficacy outcome for those who can adhere to study treatment under the causal inference framework, are of interest to many stakeholders in understanding the totality of treatment effects. In this manuscript, we discuss the details of how to estimate tripartite estimands based on a causal inference framework and how to interpret tripartite estimates through a phase 3 clinical study evaluating a basal insulin treatment for patients with type 1 diabetes.",Healthcare
Relationship between Remittances and Macroeconomic Variables in Times of Political and Social Upheaval: Evidence from Tunisias Arab Spring,"If Tunisia was hailed as a success story with its high rankings on economic, educational, and other indicators compared to other Arab countries, the 2011 popular uprisings demonstrate the need for political reforms but also major economic reforms. The Arab spring highlights the fragility of its main economic pillars including the tourism and the foreign direct investment. In such turbulent times, the paper examines the economic impact of migrant remittances, expected to have a countercyclical behavior. Our results reveal that prior to the Arab Spring, the impacts of remittances on growth and consumption seem negative and positive respectively, while they varyingly influence local investment. These three relationships held in the short-run. By considering the period surrounding the 2011 uprisings, the investment effect of remittances becomes negative and weak in the short-and medium-run, whereas positive and strong remittances impacts on growth and consumption are found in the long term.","Relationship between Remittances and Macroeconomic Variables in Times of Political and Social Upheaval: Evidence from Tunisias Arab Spring If Tunisia was hailed as a success story with its high rankings on economic, educational, and other indicators compared to other Arab countries, the 2011 popular uprisings demonstrate the need for political reforms but also major economic reforms. The Arab spring highlights the fragility of its main economic pillars including the tourism and the foreign direct investment. In such turbulent times, the paper examines the economic impact of migrant remittances, expected to have a countercyclical behavior. Our results reveal that prior to the Arab Spring, the impacts of remittances on growth and consumption seem negative and positive respectively, while they varyingly influence local investment. These three relationships held in the short-run. By considering the period surrounding the 2011 uprisings, the investment effect of remittances becomes negative and weak in the short-and medium-run, whereas positive and strong remittances impacts on growth and consumption are found in the long term.",Finance
On the Formulation of Lagrangian Stochastic Models for Geophysical Turbulent Flows,"This paper has been withdrawn by the author(s), due to a crucial error in eq. 6.","On the Formulation of Lagrangian Stochastic Models for Geophysical Turbulent Flows This paper has been withdrawn by the author(s), due to a crucial error in eq. 6.",Environment
Least Generalizations and Greatest Specializations of Sets of Clauses,"The main operations in Inductive Logic Programming (ILP) are generalization and specialization, which only make sense in a generality order. In ILP, the three most important generality orders are subsumption, implication and implication relative to background knowledge. The two languages used most often are languages of clauses and languages of only Horn clauses. This gives a total of six different ordered languages. In this paper, we give a systematic treatment of the existence or non-existence of least generalizations and greatest specializations of finite sets of clauses in each of these six ordered sets. We survey results already obtained by others and also contribute some answers of our own. Our main new results are, firstly, the existence of a computable least generalization under implication of every finite set of clauses containing at least one non-tautologous function-free clause (among other, not necessarily function-free clauses). Secondly, we show that such a least generalization need not exist under relative implication, not even if both the set that is to be generalized and the background knowledge are function-free. Thirdly, we give a complete discussion of existence and non-existence of greatest specializations in each of the six ordered languages.","Least Generalizations and Greatest Specializations of Sets of Clauses The main operations in Inductive Logic Programming (ILP) are generalization and specialization, which only make sense in a generality order. In ILP, the three most important generality orders are subsumption, implication and implication relative to background knowledge. The two languages used most often are languages of clauses and languages of only Horn clauses. This gives a total of six different ordered languages. In this paper, we give a systematic treatment of the existence or non-existence of least generalizations and greatest specializations of finite sets of clauses in each of these six ordered sets. We survey results already obtained by others and also contribute some answers of our own. Our main new results are, firstly, the existence of a computable least generalization under implication of every finite set of clauses containing at least one non-tautologous function-free clause (among other, not necessarily function-free clauses). Secondly, we show that such a least generalization need not exist under relative implication, not even if both the set that is to be generalized and the background knowledge are function-free. Thirdly, we give a complete discussion of existence and non-existence of greatest specializations in each of the six ordered languages.",Technology
Pricing American options via multi-level approximation methods,"In this article we propose a novel approach to reduce the computational complexity of various approximation methods for pricing discrete time American options. Given a sequence of continuation values estimates corresponding to different levels of spatial approximation and time discretization, we propose a multi-level low biased estimate for the price of an American option. It turns out that the resulting complexity gain can be rather high and can even reach the order (varepsilon-1) with (varepsilon) denoting the desired precision. The performance of the proposed multilevel algorithm is illustrated by a numerical example of pricing Bermudan max-call options.","Pricing American options via multi-level approximation methods In this article we propose a novel approach to reduce the computational complexity of various approximation methods for pricing discrete time American options. Given a sequence of continuation values estimates corresponding to different levels of spatial approximation and time discretization, we propose a multi-level low biased estimate for the price of an American option. It turns out that the resulting complexity gain can be rather high and can even reach the order (varepsilon-1) with (varepsilon) denoting the desired precision. The performance of the proposed multilevel algorithm is illustrated by a numerical example of pricing Bermudan max-call options.",Finance
"Integrated approaches in physics education: A graduate level course in physics, pedagogy, and education research","We describe a course designed to help future educators build an integrated understanding of the different elements of physics education research (PER), including: research into student learning, content knowledge from the perspective of how it is learned, and reform-based curricula together with evidence of their effectiveness. Course elements include equal parts of studying physics through proven curricula and discussion of research results in the context of the PER literature. We provide examples of the course content and structure as well as representative examples of student learning in the class.","Integrated approaches in physics education: A graduate level course in physics, pedagogy, and education research We describe a course designed to help future educators build an integrated understanding of the different elements of physics education research (PER), including: research into student learning, content knowledge from the perspective of how it is learned, and reform-based curricula together with evidence of their effectiveness. Course elements include equal parts of studying physics through proven curricula and discussion of research results in the context of the PER literature. We provide examples of the course content and structure as well as representative examples of student learning in the class.",Education
Use of the likelihood for measuring the skill of probabilistic forecasts,"We define the likelihood and give a number of justifications for its use as a skill measure for probabilistic forecasts. We describe a number of different scores based on the likelihood, and briefly investigate the relationships between the likelihood, the mean square error and the ignorance.","Use of the likelihood for measuring the skill of probabilistic forecasts We define the likelihood and give a number of justifications for its use as a skill measure for probabilistic forecasts. We describe a number of different scores based on the likelihood, and briefly investigate the relationships between the likelihood, the mean square error and the ignorance.",Environment
Price dynamics in financial markets: a kinetic approach,The use of kinetic modelling based on partial differential equations for the dynamics of stock price formation in financial markets is briefly reviewed. The importance of behavioral aspects in market booms and crashes and the role of agents heterogeneity in emerging power laws for price distributions is emphasized and discussed.,Price dynamics in financial markets: a kinetic approach The use of kinetic modelling based on partial differential equations for the dynamics of stock price formation in financial markets is briefly reviewed. The importance of behavioral aspects in market booms and crashes and the role of agents heterogeneity in emerging power laws for price distributions is emphasized and discussed.,Finance
Automated Generation of Curriculum-Aligned Multiple-Choice Questions for Malaysian Secondary Mathematics Using Generative AI,"This paper addresses the critical need for scalable and high-quality educational assessment tools within the Malaysian education system. It highlights the potential of Generative AI (GenAI) while acknowledging the significant challenges of ensuring factual accuracy and curriculum alignment, especially for low-resource languages like Bahasa Melayu. This research introduces and compares four incremental pipelines for generating Form 1 Mathematics multiple-choice questions (MCQs) in Bahasa Melayu using OpenAIs GPT-4o. The methods range from non-grounded prompting (structured and basic) to Retrieval-Augmented Generation (RAG) approaches (one using the LangChain framework, one implemented manually). The system is grounded in official curriculum documents, including teacher-prepared notes and the yearly teaching plan (RPT). A dual-pronged automated evaluation framework is employed to assess the generated questions. Curriculum alignment is measured using Semantic Textual Similarity (STS) against the RPT, while contextual validity is verified through a novel RAG-based Question-Answering (RAG-QA) method. The results demonstrate that RAG-based pipelines significantly outperform non-grounded prompting methods, producing questions with higher curriculum alignment and factual validity. The study further analyzes the trade-offs between the ease of implementation of framework-based RAG and the fine-grained control offered by a manual pipeline. This work presents a validated methodology for generating curriculum-specific educational content in a low-resource language, introduces a symbiotic RAG-QA evaluation technique, and provides actionable insights for the development and deployment of practical EdTech solutions in Malaysia and similar regions.","Automated Generation of Curriculum-Aligned Multiple-Choice Questions for Malaysian Secondary Mathematics Using Generative AI This paper addresses the critical need for scalable and high-quality educational assessment tools within the Malaysian education system. It highlights the potential of Generative AI (GenAI) while acknowledging the significant challenges of ensuring factual accuracy and curriculum alignment, especially for low-resource languages like Bahasa Melayu. This research introduces and compares four incremental pipelines for generating Form 1 Mathematics multiple-choice questions (MCQs) in Bahasa Melayu using OpenAIs GPT-4o. The methods range from non-grounded prompting (structured and basic) to Retrieval-Augmented Generation (RAG) approaches (one using the LangChain framework, one implemented manually). The system is grounded in official curriculum documents, including teacher-prepared notes and the yearly teaching plan (RPT). A dual-pronged automated evaluation framework is employed to assess the generated questions. Curriculum alignment is measured using Semantic Textual Similarity (STS) against the RPT, while contextual validity is verified through a novel RAG-based Question-Answering (RAG-QA) method. The results demonstrate that RAG-based pipelines significantly outperform non-grounded prompting methods, producing questions with higher curriculum alignment and factual validity. The study further analyzes the trade-offs between the ease of implementation of framework-based RAG and the fine-grained control offered by a manual pipeline. This work presents a validated methodology for generating curriculum-specific educational content in a low-resource language, introduces a symbiotic RAG-QA evaluation technique, and provides actionable insights for the development and deployment of practical EdTech solutions in Malaysia and similar regions.",Education
Training the trainer: Professional Development for High School Physics Teachers with Low Physics Background,"The shortage of highly qualified high school physics teachers is a national problem. The Mitchell Institute Physics Enhancement Program (MIPEP) is a two-week professional development program for in-service high school physics teachers with a limited background in the subject area. MIPEP, which started in 2012, includes intense training in both subject matter and research-based instructional strategies. Content and materials used in the program fulfill state curriculum requirements. The MIPEP curriculum is taught by Texas AM University faculty from the Department of Physics  Astronomy along with two master high school physics teachers. In this paper we present the design and implementation of MIPEP. We report on assessment of knowledge and confidence of 2014-2018 MIPEP cohorts. We also present the results of the 2020 program that was delivered remotely due to the pandemic. Analysis of these assessments showed that the majority of MIPEP participants increased their physics knowledge and their confidence in that knowledge during both traditional and virtual program deliveries.","Training the trainer: Professional Development for High School Physics Teachers with Low Physics Background The shortage of highly qualified high school physics teachers is a national problem. The Mitchell Institute Physics Enhancement Program (MIPEP) is a two-week professional development program for in-service high school physics teachers with a limited background in the subject area. MIPEP, which started in 2012, includes intense training in both subject matter and research-based instructional strategies. Content and materials used in the program fulfill state curriculum requirements. The MIPEP curriculum is taught by Texas AM University faculty from the Department of Physics  Astronomy along with two master high school physics teachers. In this paper we present the design and implementation of MIPEP. We report on assessment of knowledge and confidence of 2014-2018 MIPEP cohorts. We also present the results of the 2020 program that was delivered remotely due to the pandemic. Analysis of these assessments showed that the majority of MIPEP participants increased their physics knowledge and their confidence in that knowledge during both traditional and virtual program deliveries.",Education
Leading strategies in competitive on-line prediction,"We start from a simple asymptotic result for the problem of on-line regression with the quadratic loss function: the class of continuous limited-memory prediction strategies admits a leading prediction strategy, which not only asymptotically performs at least as well as any continuous limited-memory strategy but also satisfies the property that the excess loss of any continuous limited-memory strategy is determined by how closely it imitates the leading strategy. More specifically, for any class of prediction strategies constituting a reproducing kernel Hilbert space we construct a leading strategy, in the sense that the loss of any prediction strategy whose norm is not too large is determined by how closely it imitates the leading strategy. This result is extended to the loss functions given by Bregman divergences and by strictly proper scoring rules.","Leading strategies in competitive on-line prediction We start from a simple asymptotic result for the problem of on-line regression with the quadratic loss function: the class of continuous limited-memory prediction strategies admits a leading prediction strategy, which not only asymptotically performs at least as well as any continuous limited-memory strategy but also satisfies the property that the excess loss of any continuous limited-memory strategy is determined by how closely it imitates the leading strategy. More specifically, for any class of prediction strategies constituting a reproducing kernel Hilbert space we construct a leading strategy, in the sense that the loss of any prediction strategy whose norm is not too large is determined by how closely it imitates the leading strategy. This result is extended to the loss functions given by Bregman divergences and by strictly proper scoring rules.",Technology
Complexity-based Financial Stress Evaluation,"Financial markets typically exhibit dynamically complex properties as they undergo continuous interactions with economic and environmental factors. The Efficient Market Hypothesis indicates a rich difference in the structural complexity of security prices between normal (stable markets) and abnormal (financial crises) situations. Considering the analogy between market undulation of price time series and physical stress of bio-signals, we investigate whether stress indices in bio-systems can be adopted and modified so as to measure standard stress in financial markets. This is achieved by employing structural complexity analysis, based on variants of univariate and multivariate sample entropy, to estimate the stress level of both financial markets on the whole and the performance of the individual financial indices. Further, we propose a novel graphical framework to establish the sensitivity of individual assets and stock markets to financial crises. This is achieved through Catastrophe Theory and entropy-based stress evaluations indicating the unique performance of each indexindividual stock in response to different crises. Four major indices and four individual equities with gold prices are considered over the past 32 years from 1991-2021. Our findings based on nonlinear analyses and the proposed framework support the Efficient Market Hypothesis and reveal the relations among economic indices and within each price time series.","Complexity-based Financial Stress Evaluation Financial markets typically exhibit dynamically complex properties as they undergo continuous interactions with economic and environmental factors. The Efficient Market Hypothesis indicates a rich difference in the structural complexity of security prices between normal (stable markets) and abnormal (financial crises) situations. Considering the analogy between market undulation of price time series and physical stress of bio-signals, we investigate whether stress indices in bio-systems can be adopted and modified so as to measure standard stress in financial markets. This is achieved by employing structural complexity analysis, based on variants of univariate and multivariate sample entropy, to estimate the stress level of both financial markets on the whole and the performance of the individual financial indices. Further, we propose a novel graphical framework to establish the sensitivity of individual assets and stock markets to financial crises. This is achieved through Catastrophe Theory and entropy-based stress evaluations indicating the unique performance of each indexindividual stock in response to different crises. Four major indices and four individual equities with gold prices are considered over the past 32 years from 1991-2021. Our findings based on nonlinear analyses and the proposed framework support the Efficient Market Hypothesis and reveal the relations among economic indices and within each price time series.",Finance
Comparison of linear and non-linear soft tissue models with post-operative CT scan in maxillofacial surgery,"A Finite Element model of the face soft tissue is proposed to simulate the morphological outcomes of maxillofacial surgery. Three modelling options are implemented: a linear elastic model with small and large deformation hypothesis, and an hyperelastic Mooney-Rivlin model. An evaluation procedure based on a qualitative and quantitative comparison of the simulations with a post-operative CT scan is detailed. It is then applied to one clinical case to evaluate the differences between the three models, and with the actual patient morphology. First results shows in particular that for a simple clinical procedure where stress is less than 20, a linear model seams sufficient for a correct modelling.","Comparison of linear and non-linear soft tissue models with post-operative CT scan in maxillofacial surgery A Finite Element model of the face soft tissue is proposed to simulate the morphological outcomes of maxillofacial surgery. Three modelling options are implemented: a linear elastic model with small and large deformation hypothesis, and an hyperelastic Mooney-Rivlin model. An evaluation procedure based on a qualitative and quantitative comparison of the simulations with a post-operative CT scan is detailed. It is then applied to one clinical case to evaluate the differences between the three models, and with the actual patient morphology. First results shows in particular that for a simple clinical procedure where stress is less than 20, a linear model seams sufficient for a correct modelling.",Healthcare
Modeling multi-cellular systems using sub-cellular elements,"We introduce a model for describing the dynamics of large numbers of interacting cells. The fundamental dynamical variables in the model are sub-cellular elements, which interact with each other through phenomenological intra- and inter-cellular potentials. Advantages of the model include i) adaptive cell-shape dynamics, ii) flexible accommodation of additional intra-cellular biology, and iii) the absence of an underlying grid. We present here a detailed description of the model, and use successive mean-field approximations to connect it to more coarse-grained approaches, such as discrete cell-based algorithms and coupled partial differential equations. We also discuss efficient algorithms for encoding the model, and give an example of a simulation of an epithelial sheet. Given the biological flexibility of the model, we propose that it can be used effectively for modeling a range of multi-cellular processes, such as tumor dynamics and embryogenesis.","Modeling multi-cellular systems using sub-cellular elements We introduce a model for describing the dynamics of large numbers of interacting cells. The fundamental dynamical variables in the model are sub-cellular elements, which interact with each other through phenomenological intra- and inter-cellular potentials. Advantages of the model include i) adaptive cell-shape dynamics, ii) flexible accommodation of additional intra-cellular biology, and iii) the absence of an underlying grid. We present here a detailed description of the model, and use successive mean-field approximations to connect it to more coarse-grained approaches, such as discrete cell-based algorithms and coupled partial differential equations. We also discuss efficient algorithms for encoding the model, and give an example of a simulation of an epithelial sheet. Given the biological flexibility of the model, we propose that it can be used effectively for modeling a range of multi-cellular processes, such as tumor dynamics and embryogenesis.",Healthcare
Why Financial Markets Will Remain Marginally Inefficient?,"I summarize the recent work on market (in)efficiency, highlighting key elements why financial markets will never be made efficient. My approach is not by adding more empirical evidence, but giving plausible reasons as to where inefficiency arises and why its not rational to arbitrage it away.","Why Financial Markets Will Remain Marginally Inefficient? I summarize the recent work on market (in)efficiency, highlighting key elements why financial markets will never be made efficient. My approach is not by adding more empirical evidence, but giving plausible reasons as to where inefficiency arises and why its not rational to arbitrage it away.",Finance
The application of Augmented Reality (AR) in Remote Work and Education,"With the rapid advancement of technology, Augmented Reality (AR) technology, known for its ability to deeply integrate virtual information with the real world, is gradually transforming traditional work modes and teaching methods. Particularly in the realms of remote work and online education, AR technology demonstrates a broad spectrum of application prospects. This paper delves into the application potential and actual effects of AR technology in remote work and education. Through a systematic literature review, this study outlines the key features, advantages, and challenges of AR technology. Based on theoretical analysis, it discusses the scientific basis and technical support that AR technology provides for enhancing remote work efficiency and promoting innovation in educational teaching models. Additionally, by designing an empirical research plan and analyzing experimental data, this article reveals the specific performance and influencing factors of AR technology in practical applications. Finally, based on the results of the experiments, this research summarizes the application value of AR technology in remote work and education, looks forward to its future development trends, and proposes forward-looking research directions and strategic suggestions, offering empirical foundation and theoretical guidance for further promoting the in-depth application of AR technology in related fields.","The application of Augmented Reality (AR) in Remote Work and Education With the rapid advancement of technology, Augmented Reality (AR) technology, known for its ability to deeply integrate virtual information with the real world, is gradually transforming traditional work modes and teaching methods. Particularly in the realms of remote work and online education, AR technology demonstrates a broad spectrum of application prospects. This paper delves into the application potential and actual effects of AR technology in remote work and education. Through a systematic literature review, this study outlines the key features, advantages, and challenges of AR technology. Based on theoretical analysis, it discusses the scientific basis and technical support that AR technology provides for enhancing remote work efficiency and promoting innovation in educational teaching models. Additionally, by designing an empirical research plan and analyzing experimental data, this article reveals the specific performance and influencing factors of AR technology in practical applications. Finally, based on the results of the experiments, this research summarizes the application value of AR technology in remote work and education, looks forward to its future development trends, and proposes forward-looking research directions and strategic suggestions, offering empirical foundation and theoretical guidance for further promoting the in-depth application of AR technology in related fields.",Education
The Design and Experimental Analysis of Algorithms for Temporal Reasoning,"Many applications -- from planning and scheduling to problems in molecular biology -- rely heavily on a temporal reasoning component. In this paper, we discuss the design and empirical analysis of algorithms for a temporal reasoning system based on Allens influential interval-based framework for representing temporal information. At the core of the system are algorithms for determining whether the temporal information is consistent, and, if so, finding one or more scenarios that are consistent with the temporal information. Two important algorithms for these tasks are a path consistency algorithm and a backtracking algorithm. For the path consistency algorithm, we develop techniques that can result in up to a ten-fold speedup over an already highly optimized implementation. For the backtracking algorithm, we develop variable and value ordering heuristics that are shown empirically to dramatically improve the performance of the algorithm. As well, we show that a previously suggested reformulation of the backtracking search problem can reduce the time and space requirements of the backtracking search. Taken together, the techniques we develop allow a temporal reasoning component to solve problems that are of practical size.","The Design and Experimental Analysis of Algorithms for Temporal Reasoning Many applications -- from planning and scheduling to problems in molecular biology -- rely heavily on a temporal reasoning component. In this paper, we discuss the design and empirical analysis of algorithms for a temporal reasoning system based on Allens influential interval-based framework for representing temporal information. At the core of the system are algorithms for determining whether the temporal information is consistent, and, if so, finding one or more scenarios that are consistent with the temporal information. Two important algorithms for these tasks are a path consistency algorithm and a backtracking algorithm. For the path consistency algorithm, we develop techniques that can result in up to a ten-fold speedup over an already highly optimized implementation. For the backtracking algorithm, we develop variable and value ordering heuristics that are shown empirically to dramatically improve the performance of the algorithm. As well, we show that a previously suggested reformulation of the backtracking search problem can reduce the time and space requirements of the backtracking search. Taken together, the techniques we develop allow a temporal reasoning component to solve problems that are of practical size.",Technology
Pre-processing methods for nodule detection in lung CT,"The use of automatic systems in the analysis of medical images has proven to be very useful to radiologists, especially in the framework of screening programs, in which radiologists make their first diagnosis on the basis of images only, most of those corresponding to healthy patients, and have to distinguish pathological findings from non-pathological ones at an early stage. In particular, we are developing preprocessing methods to be applied for pulmonary nodule Computer Aided Detection in low-dose lung Multi Slice CT (Computed Tomography) images.","Pre-processing methods for nodule detection in lung CT The use of automatic systems in the analysis of medical images has proven to be very useful to radiologists, especially in the framework of screening programs, in which radiologists make their first diagnosis on the basis of images only, most of those corresponding to healthy patients, and have to distinguish pathological findings from non-pathological ones at an early stage. In particular, we are developing preprocessing methods to be applied for pulmonary nodule Computer Aided Detection in low-dose lung Multi Slice CT (Computed Tomography) images.",Healthcare
Interconnectedness in Education Systems,"Underlying complex systems, there is a rich web of interconnected components that determine the relational properties of the system. Yet, traditional methods used in education sciences often disregard the underlying complexity of the educational system and, consequently, its emergence phenomena. Here, we argue that an interconnected vision of educational systems -- from classrooms to an organizational level -- is key to improving learning, social integration, well-being, and decision making, all fundamental aspects of the educational experience. Understanding the education system as an interconnected network of people, degree programs, and institutions requires methods and concepts from computational social sciences. Thus, we can leverage institutional records and (quasi) experimental designs to elicit the relational maps of key players in education and derive their implications in their functioning at all scales. In different settings, from elementary classrooms to higher education programs, we show how mapping the network relationships between entities can lead to the inference of novel insights about education systems and the development of solutions with societal implications.","Interconnectedness in Education Systems Underlying complex systems, there is a rich web of interconnected components that determine the relational properties of the system. Yet, traditional methods used in education sciences often disregard the underlying complexity of the educational system and, consequently, its emergence phenomena. Here, we argue that an interconnected vision of educational systems -- from classrooms to an organizational level -- is key to improving learning, social integration, well-being, and decision making, all fundamental aspects of the educational experience. Understanding the education system as an interconnected network of people, degree programs, and institutions requires methods and concepts from computational social sciences. Thus, we can leverage institutional records and (quasi) experimental designs to elicit the relational maps of key players in education and derive their implications in their functioning at all scales. In different settings, from elementary classrooms to higher education programs, we show how mapping the network relationships between entities can lead to the inference of novel insights about education systems and the development of solutions with societal implications.",Education
A novel evolutionary formulation of the maximum independent set problem,"We introduce a novel evolutionary formulation of the problem of finding a maximum independent set of a graph. The new formulation is based on the relationship that exists between a graphs independence number and its acyclic orientations. It views such orientations as individuals and evolves them with the aid of evolutionary operators that are very heavily based on the structure of the graph and its acyclic orientations. The resulting heuristic has been tested on some of the Second DIMACS Implementation Challenge benchmark graphs, and has been found to be competitive when compared to several of the other heuristics that have also been tested on those graphs.","A novel evolutionary formulation of the maximum independent set problem We introduce a novel evolutionary formulation of the problem of finding a maximum independent set of a graph. The new formulation is based on the relationship that exists between a graphs independence number and its acyclic orientations. It views such orientations as individuals and evolves them with the aid of evolutionary operators that are very heavily based on the structure of the graph and its acyclic orientations. The resulting heuristic has been tested on some of the Second DIMACS Implementation Challenge benchmark graphs, and has been found to be competitive when compared to several of the other heuristics that have also been tested on those graphs.",Technology
Using Pseudo-Stochastic Rational Languages in Probabilistic Grammatical Inference,"In probabilistic grammatical inference, a usual goal is to infer a good approximation of an unknown distribution P called a stochastic language. The estimate of P stands in some class of probabilistic models such as probabilistic automata (PA). In this paper, we focus on probabilistic models based on multiplicity automata (MA). The stochastic languages generated by MA are called rational stochastic languages; they strictly include stochastic languages generated by PA; they also admit a very concise canonical representation. Despite the fact that this class is not recursively enumerable, it is efficiently identifiable in the limit by using the algorithm DEES, introduced by the authors in a previous paper. However, the identification is not proper and before the convergence of the algorithm, DEES can produce MA that do not define stochastic languages. Nevertheless, it is possible to use these MA to define stochastic languages. We show that they belong to a broader class of rational series, that we call pseudo-stochastic rational languages. The aim of this paper is twofold. First we provide a theoretical study of pseudo-stochastic rational languages, the languages output by DEES, showing for example that this class is decidable within polynomial time. Second, we have carried out a lot of experiments in order to compare DEES to classical inference algorithms such as ALERGIA and MDI. They show that DEES outperforms them in most cases.","Using Pseudo-Stochastic Rational Languages in Probabilistic Grammatical Inference In probabilistic grammatical inference, a usual goal is to infer a good approximation of an unknown distribution P called a stochastic language. The estimate of P stands in some class of probabilistic models such as probabilistic automata (PA). In this paper, we focus on probabilistic models based on multiplicity automata (MA). The stochastic languages generated by MA are called rational stochastic languages; they strictly include stochastic languages generated by PA; they also admit a very concise canonical representation. Despite the fact that this class is not recursively enumerable, it is efficiently identifiable in the limit by using the algorithm DEES, introduced by the authors in a previous paper. However, the identification is not proper and before the convergence of the algorithm, DEES can produce MA that do not define stochastic languages. Nevertheless, it is possible to use these MA to define stochastic languages. We show that they belong to a broader class of rational series, that we call pseudo-stochastic rational languages. The aim of this paper is twofold. First we provide a theoretical study of pseudo-stochastic rational languages, the languages output by DEES, showing for example that this class is decidable within polynomial time. Second, we have carried out a lot of experiments in order to compare DEES to classical inference algorithms such as ALERGIA and MDI. They show that DEES outperforms them in most cases.",Technology
Software Agents: Completing Patterns and Constructing User Interfaces,"To support the goal of allowing users to record and retrieve information, this paper describes an interactive note-taking system for pen-based computers with two distinctive features. First, it actively predicts what the user is going to write. Second, it automatically constructs a custom, button-box user interface on request. The system is an example of a learning-apprentice software- agent. A machine learning component characterizes the syntax and semantics of the users information. A performance system uses this learned information to generate completion strings and construct a user interface. Description of Online Appendix: People like to record information. Doing this on paper is initially efficient, but lacks flexibility. Recording information on a computer is less efficient but more powerful. In our new note taking softwre, the user records information directly on a computer. Behind the interface, an agent acts for the user. To help, it provides defaults and constructs a custom user interface. The demonstration is a QuickTime movie of the note taking agent in action. The file is a binhexed self-extracting archive. Macintosh utilities for binhex are available from mac.archive.umich.edu. QuickTime is available from ftp.apple.com in the dtsmacsys.softquicktime.","Software Agents: Completing Patterns and Constructing User Interfaces To support the goal of allowing users to record and retrieve information, this paper describes an interactive note-taking system for pen-based computers with two distinctive features. First, it actively predicts what the user is going to write. Second, it automatically constructs a custom, button-box user interface on request. The system is an example of a learning-apprentice software- agent. A machine learning component characterizes the syntax and semantics of the users information. A performance system uses this learned information to generate completion strings and construct a user interface. Description of Online Appendix: People like to record information. Doing this on paper is initially efficient, but lacks flexibility. Recording information on a computer is less efficient but more powerful. In our new note taking softwre, the user records information directly on a computer. Behind the interface, an agent acts for the user. To help, it provides defaults and constructs a custom user interface. The demonstration is a QuickTime movie of the note taking agent in action. The file is a binhexed self-extracting archive. Macintosh utilities for binhex are available from mac.archive.umich.edu. QuickTime is available from ftp.apple.com in the dtsmacsys.softquicktime.",Technology
Pricing of vanilla and first generation exotic options in the local stochastic volatility framework: survey and new results,"Stochastic volatility (SV) and local stochastic volatility (LSV) processes can be used to model the evolution of various financial variables such as FX rates, stock prices, and so on. Considerable efforts have been devoted to pricing derivatives written on underliers governed by such processes. Many issues remain, though, including the efficacy of the standard alternating direction implicit (ADI) numerical methods for solving SV and LSV pricing problems. In general, the amount of required computations for these methods is very substantial. In this paper we address some of these issues and propose a viable alternative to the standard ADI methods based on Galerkin-Ritz ideas. We also discuss various approaches to solving the corresponding pricing problems in a semi-analytical fashion. We use the fact that in the zero correlation case some of the pricing problems can be solved analytically, and develop a closed-form series expansion in powers of correlation. We perform a thorough benchmarking of various numerical solutions by using analytical and semi-analytical solutions derived in the paper.","Pricing of vanilla and first generation exotic options in the local stochastic volatility framework: survey and new results Stochastic volatility (SV) and local stochastic volatility (LSV) processes can be used to model the evolution of various financial variables such as FX rates, stock prices, and so on. Considerable efforts have been devoted to pricing derivatives written on underliers governed by such processes. Many issues remain, though, including the efficacy of the standard alternating direction implicit (ADI) numerical methods for solving SV and LSV pricing problems. In general, the amount of required computations for these methods is very substantial. In this paper we address some of these issues and propose a viable alternative to the standard ADI methods based on Galerkin-Ritz ideas. We also discuss various approaches to solving the corresponding pricing problems in a semi-analytical fashion. We use the fact that in the zero correlation case some of the pricing problems can be solved analytically, and develop a closed-form series expansion in powers of correlation. We perform a thorough benchmarking of various numerical solutions by using analytical and semi-analytical solutions derived in the paper.",Finance
GPM Ground Validation Basic Radar Products and Implications for Observation Strategies,"Recommendations are made for NASAJAXA Global Precipitation Measurement (GPM) satellite Ground Validation (GV) program. This report details recommended GV site local radar products based on data from surface-based scanning radars including S-band, C-band, and X-band polarimetric and non-polarimetric radars. Three general categories of products are described: text products summarizing information on the statistical characteristics of the radar data and derived parameters, 2D products providing maps of the horizontal variability of near surface radar observed and derived parameters, and 3D products describing volumetric echo structure. Regional composites could include products based on several of the 2D and 3D single radar products. Several types of time-integrated 2D and 3D products are also recommended. A brief discussion of useful ancillary data from other sources and remaining challenges concludes the report.","GPM Ground Validation Basic Radar Products and Implications for Observation Strategies Recommendations are made for NASAJAXA Global Precipitation Measurement (GPM) satellite Ground Validation (GV) program. This report details recommended GV site local radar products based on data from surface-based scanning radars including S-band, C-band, and X-band polarimetric and non-polarimetric radars. Three general categories of products are described: text products summarizing information on the statistical characteristics of the radar data and derived parameters, 2D products providing maps of the horizontal variability of near surface radar observed and derived parameters, and 3D products describing volumetric echo structure. Regional composites could include products based on several of the 2D and 3D single radar products. Several types of time-integrated 2D and 3D products are also recommended. A brief discussion of useful ancillary data from other sources and remaining challenges concludes the report.",Environment
Active Learning with Statistical Models,"For many types of machine learning algorithms, one can compute the statistically optimal way to select training data. In this paper, we review how optimal data selection techniques have been used with feedforward neural networks. We then show how the same principles may be used to select data for two alternative, statistically-based learning architectures: mixtures of Gaussians and locally weighted regression. While the techniques for neural networks are computationally expensive and approximate, the techniques for mixtures of Gaussians and locally weighted regression are both efficient and accurate. Empirically, we observe that the optimality criterion sharply decreases the number of training examples the learner needs in order to achieve good performance.","Active Learning with Statistical Models For many types of machine learning algorithms, one can compute the statistically optimal way to select training data. In this paper, we review how optimal data selection techniques have been used with feedforward neural networks. We then show how the same principles may be used to select data for two alternative, statistically-based learning architectures: mixtures of Gaussians and locally weighted regression. While the techniques for neural networks are computationally expensive and approximate, the techniques for mixtures of Gaussians and locally weighted regression are both efficient and accurate. Empirically, we observe that the optimality criterion sharply decreases the number of training examples the learner needs in order to achieve good performance.",Technology
Predicting Genetic Regulatory Response Using Classification,"We present a novel classification-based method for learning to predict gene regulatory response. Our approach is motivated by the hypothesis that in simple organisms such as Saccharomyces cerevisiae, we can learn a decision rule for predicting whether a gene is up- or down-regulated in a particular experiment based on (1) the presence of binding site subsequences (motifs) in the genes regulatory region and (2) the expression levels of regulators such as transcription factors in the experiment (parents). Thus our learning task integrates two qualitatively different data sources: genome-wide cDNA microarray data across multiple perturbation and mutant experiments along with motif profile data from regulatory sequences. We convert the regression task of predicting real-valued gene expression measurement to a classification task of predicting 1 and -1 labels, corresponding to up- and down-regulation beyond the levels of biological and measurement noise in microarray measurements. The learning algorithm employed is boosting with a margin-based generalization of decision trees, alternating decision trees. This large-margin classifier is sufficiently flexible to allow complex logical functions, yet sufficiently simple to give insight into the combinatorial mechanisms of gene regulation. We observe encouraging prediction accuracy on experiments based on the Gasch S. cerevisiae dataset, and we show that we can accurately predict up- and down-regulation on held-out experiments. Our method thus provides predictive hypotheses, suggests biological experiments, and provides interpretable insight into the structure of genetic regulatory networks.","Predicting Genetic Regulatory Response Using Classification We present a novel classification-based method for learning to predict gene regulatory response. Our approach is motivated by the hypothesis that in simple organisms such as Saccharomyces cerevisiae, we can learn a decision rule for predicting whether a gene is up- or down-regulated in a particular experiment based on (1) the presence of binding site subsequences (motifs) in the genes regulatory region and (2) the expression levels of regulators such as transcription factors in the experiment (parents). Thus our learning task integrates two qualitatively different data sources: genome-wide cDNA microarray data across multiple perturbation and mutant experiments along with motif profile data from regulatory sequences. We convert the regression task of predicting real-valued gene expression measurement to a classification task of predicting 1 and -1 labels, corresponding to up- and down-regulation beyond the levels of biological and measurement noise in microarray measurements. The learning algorithm employed is boosting with a margin-based generalization of decision trees, alternating decision trees. This large-margin classifier is sufficiently flexible to allow complex logical functions, yet sufficiently simple to give insight into the combinatorial mechanisms of gene regulation. We observe encouraging prediction accuracy on experiments based on the Gasch S. cerevisiae dataset, and we show that we can accurately predict up- and down-regulation on held-out experiments. Our method thus provides predictive hypotheses, suggests biological experiments, and provides interpretable insight into the structure of genetic regulatory networks.",Healthcare
Youthful perspectives on sustainability: Examining pro-environmental behaviors in tourism through latent class cluster analysis,"Tourism has emerged as a significant driver of the global economy. As its economic impact grows, concerns regarding environmental sustainability have intensified. This paper explores the dual dimensions of sustainable tourism: the relationship between tourism supply and sustainability, and tourist demand characteristics. It highlights the critical role of young tourists, who exhibit a heightened awareness of environmental issues and advocate for sustainable practices. By conducting a survey among young Italian university students, the study identifies distinct segments based on family background, political orientation, and travel habits. Utilizing latent class cluster analysis, the findings aim to enhance understanding of pro-environmental behaviors among youth, offering insights for policymakers to foster sustainable tourism practices.","Youthful perspectives on sustainability: Examining pro-environmental behaviors in tourism through latent class cluster analysis Tourism has emerged as a significant driver of the global economy. As its economic impact grows, concerns regarding environmental sustainability have intensified. This paper explores the dual dimensions of sustainable tourism: the relationship between tourism supply and sustainability, and tourist demand characteristics. It highlights the critical role of young tourists, who exhibit a heightened awareness of environmental issues and advocate for sustainable practices. By conducting a survey among young Italian university students, the study identifies distinct segments based on family background, political orientation, and travel habits. Utilizing latent class cluster analysis, the findings aim to enhance understanding of pro-environmental behaviors among youth, offering insights for policymakers to foster sustainable tourism practices.",Environment
Accurate collection of reasons for treatment discontinuation to better define estimands in clinical trials,"Background: Reasons for treatment discontinuation are important not only to understand the benefit and risk profile of experimental treatments, but also to help choose appropriate strategies to handle intercurrent events in defining estimands. The current case report form (CRF) commonly in use mixes the underlying reasons for treatment discontinuation and who makes the decision for treatment discontinuation, often resulting in an inaccurate collection of reasons for treatment discontinuation. Methods and results: We systematically reviewed and analyzed treatment discontinuation data from nine phase 2 and phase 3 studies for insulin peglispro. A total of 857 participants with treatment discontinuation were included in the analysis. Our review suggested that, due to the vague multiple-choice options for treatment discontinuation present in the CRF, different reasons were sometimes recorded for the same underlying reason for treatment discontinuation. Based on our review and analysis, we suggest an intermediate solution and a more systematic way to improve the current CRF for treatment discontinuations. Conclusion: This research provides insight and directions on how to optimize the CRF for recording treatment discontinuation. Further work needs to be done to build the learning into Clinical Data Interchange Standards Consortium standards.","Accurate collection of reasons for treatment discontinuation to better define estimands in clinical trials Background: Reasons for treatment discontinuation are important not only to understand the benefit and risk profile of experimental treatments, but also to help choose appropriate strategies to handle intercurrent events in defining estimands. The current case report form (CRF) commonly in use mixes the underlying reasons for treatment discontinuation and who makes the decision for treatment discontinuation, often resulting in an inaccurate collection of reasons for treatment discontinuation. Methods and results: We systematically reviewed and analyzed treatment discontinuation data from nine phase 2 and phase 3 studies for insulin peglispro. A total of 857 participants with treatment discontinuation were included in the analysis. Our review suggested that, due to the vague multiple-choice options for treatment discontinuation present in the CRF, different reasons were sometimes recorded for the same underlying reason for treatment discontinuation. Based on our review and analysis, we suggest an intermediate solution and a more systematic way to improve the current CRF for treatment discontinuations. Conclusion: This research provides insight and directions on how to optimize the CRF for recording treatment discontinuation. Further work needs to be done to build the learning into Clinical Data Interchange Standards Consortium standards.",Healthcare
A New Framework for the Assessment and Calibration of Medium Range Ensemble Temperature Forecasts,"We present a new framework for the assessment and calibration of medium range ensemble temperature forecasts. The method is based on maximising the likelihood of a simple parametric model for the temperature distribution, and leads to some new insights into the predictability of uncertainty.","A New Framework for the Assessment and Calibration of Medium Range Ensemble Temperature Forecasts We present a new framework for the assessment and calibration of medium range ensemble temperature forecasts. The method is based on maximising the likelihood of a simple parametric model for the temperature distribution, and leads to some new insights into the predictability of uncertainty.",Environment
Preparing graduate students to be educators,"We present two programs that address needs to better prepare graduate students for their roles as professional physicists, particularly in the areas of teaching and education research. The two programs, Preparing Future Physicists (PFP) and a course, Teaching and Learning Physics, are designed to be mutually supportive, address these broader graduate roles, and support the development of the field of physics education research. While voluntary, PFP has attracted the participation of roughly half the physics graduate students at each of two large research institutions. Compared to the national rate, these students are roughly twice as likely to report an interest in pursuing future roles as educators. While less than one in five of participants surveyed reported education being valued by the research community in physics, more than 90 reported intentions to incorporate the results of research in physics education in their future teaching. Experience with the synergistic program, Teaching and Learning Physics, demonstrates that it is possible to replicate earlier successes of the program initiated at a different institution, including increasing student mastery of physics, developing student interest in education and teaching, and engaging students in research projects in physics education. In addition to introducing these programs, we identify some of the critical features that contribute to their successes.","Preparing graduate students to be educators We present two programs that address needs to better prepare graduate students for their roles as professional physicists, particularly in the areas of teaching and education research. The two programs, Preparing Future Physicists (PFP) and a course, Teaching and Learning Physics, are designed to be mutually supportive, address these broader graduate roles, and support the development of the field of physics education research. While voluntary, PFP has attracted the participation of roughly half the physics graduate students at each of two large research institutions. Compared to the national rate, these students are roughly twice as likely to report an interest in pursuing future roles as educators. While less than one in five of participants surveyed reported education being valued by the research community in physics, more than 90 reported intentions to incorporate the results of research in physics education in their future teaching. Experience with the synergistic program, Teaching and Learning Physics, demonstrates that it is possible to replicate earlier successes of the program initiated at a different institution, including increasing student mastery of physics, developing student interest in education and teaching, and engaging students in research projects in physics education. In addition to introducing these programs, we identify some of the critical features that contribute to their successes.",Education
Strategize Before Teaching: A Conversational Tutoring System with Pedagogy Self-Distillation,"Conversational tutoring systems (CTSs) aim to help students master educational material with natural language interaction in the form of a dialog. CTSs have become a key pillar in educational data mining research. A key challenge in CTSs is to engage the student in the conversation while exposing them to a diverse set of teaching strategies, akin to a human teacher, thereby, helping them learn in the process. Different from previous work that generates responses given the strategies as input, we propose to jointly predict teaching strategies and generate tutor responses accordingly, which fits a more realistic application scenario. We benchmark several competitive models on three dialog tutoring datasets and propose a unified framework that combines teaching response generation and pedagogical strategy prediction, where a self-distillation mechanism is adopted to guide the teaching strategy learning and facilitate tutor response generation. Our experiments and analyses shed light on how teaching strategies affect dialog tutoring.","Strategize Before Teaching: A Conversational Tutoring System with Pedagogy Self-Distillation Conversational tutoring systems (CTSs) aim to help students master educational material with natural language interaction in the form of a dialog. CTSs have become a key pillar in educational data mining research. A key challenge in CTSs is to engage the student in the conversation while exposing them to a diverse set of teaching strategies, akin to a human teacher, thereby, helping them learn in the process. Different from previous work that generates responses given the strategies as input, we propose to jointly predict teaching strategies and generate tutor responses accordingly, which fits a more realistic application scenario. We benchmark several competitive models on three dialog tutoring datasets and propose a unified framework that combines teaching response generation and pedagogical strategy prediction, where a self-distillation mechanism is adopted to guide the teaching strategy learning and facilitate tutor response generation. Our experiments and analyses shed light on how teaching strategies affect dialog tutoring.",Education
Sovereign wealth funds: main activity trends,"Sovereign wealth funds are created in those countries whose budget is highly dependent on market factors, usually world commodity prices. At the same time, these funds are large institutional investors. An analysis of the nature of investments by the State Pension Fund Global of Norway showed that investments of the Fund are based on a seven-level model of diversifying its investments. This model can also be applied to the investments of the National Wealth Fund of Russia to increase its profitability.","Sovereign wealth funds: main activity trends Sovereign wealth funds are created in those countries whose budget is highly dependent on market factors, usually world commodity prices. At the same time, these funds are large institutional investors. An analysis of the nature of investments by the State Pension Fund Global of Norway showed that investments of the Fund are based on a seven-level model of diversifying its investments. This model can also be applied to the investments of the National Wealth Fund of Russia to increase its profitability.",Finance
Genetic code on the dyadic plane,We introduce the simple parametrization for the space of codons (triples of nucleotides) by 8times 8 table. This table (which we call the dyadic plane) possesses the natural 2-adic ultrametric. We show that after this parametrization the genetic code will be a locally constant map of the simple form. The local constancy of this map will describe degeneracy of the genetic code. The map of the genetic code defines 2-adic ultrametric on the space of amino acids. We show that hydrophobic amino acids will be clustered in two balls with respect to this ultrametric. Therefore the introduced parametrization of space of codons exhibits the hidden regularity of the genetic code.,Genetic code on the dyadic plane We introduce the simple parametrization for the space of codons (triples of nucleotides) by 8times 8 table. This table (which we call the dyadic plane) possesses the natural 2-adic ultrametric. We show that after this parametrization the genetic code will be a locally constant map of the simple form. The local constancy of this map will describe degeneracy of the genetic code. The map of the genetic code defines 2-adic ultrametric on the space of amino acids. We show that hydrophobic amino acids will be clustered in two balls with respect to this ultrametric. Therefore the introduced parametrization of space of codons exhibits the hidden regularity of the genetic code.,Healthcare
The effect of the primary collimator and flattening filter on asymmetric fields for a Siemens PRIMUS linear accelerator,"Homogeneity for highly asymmetric fields has been studied for a Siemens PRIMUS linear accelerator. The flattening filter has a radius smaller than the primary collimator one, creating inhomogeneities that affect large fields in areas far from the collimator axis, and asymmetric fields with large offset. Profiles and absolute dose have been measured in fields with two jaws at maximal position (20 cm) and the other two at maximal overtravel (10 cm.), corresponding to 10 x 10 fields with extreme offset. Profiles have a remarkable gradient decreasing towards the beam edge, making these fields unsuitable for treatments. Results show that the design of the primary collimator and flattening filter assembly has direct consequences in homogeneity. This can have clinical consequences for treatments involving fields that include these inhomogeneous areas. Comparisons with the treatment planning system (Philips Pinnacle) calculations, that computes under the hypotheses of a uniformly flattened beam, result in severe discrepancies.","The effect of the primary collimator and flattening filter on asymmetric fields for a Siemens PRIMUS linear accelerator Homogeneity for highly asymmetric fields has been studied for a Siemens PRIMUS linear accelerator. The flattening filter has a radius smaller than the primary collimator one, creating inhomogeneities that affect large fields in areas far from the collimator axis, and asymmetric fields with large offset. Profiles and absolute dose have been measured in fields with two jaws at maximal position (20 cm) and the other two at maximal overtravel (10 cm.), corresponding to 10 x 10 fields with extreme offset. Profiles have a remarkable gradient decreasing towards the beam edge, making these fields unsuitable for treatments. Results show that the design of the primary collimator and flattening filter assembly has direct consequences in homogeneity. This can have clinical consequences for treatments involving fields that include these inhomogeneous areas. Comparisons with the treatment planning system (Philips Pinnacle) calculations, that computes under the hypotheses of a uniformly flattened beam, result in severe discrepancies.",Healthcare
Toward an Effective Pedagogy of Climate Change: Lessons From a Physics Classroom,"A major roadblock to effective climate education is the lack of radical visions (Kwauk, 2020) for engaging with climate change and related social-environmental crises in the classroom. Key aspects of the climate system: its inherent complexity and transdisciplinarity, the entanglement of social and natural systems, and the fact that it spans large scales of space and time - all present serious challenges to our modern, compartmentalized formal education systems. I present an argument for a justice-centered, inter-to-transdisciplinary conceptualization of climate change at the nexus of science, society and justice, which is designed to engage with the very features of the climate system that make it challenging. Based on pedagogical experiments in an undergraduate physics classroom for non-science majors, I identify five general barriers to teaching climate change, and formulate four dimensions of an effective climate pedagogy: the scientific-technological, the transdisciplinary, the onto-epistemological, and the psychosocial action dimensions. Within the context of a collaborative classroom culture informed by transformational learning, I describe the use of a meta-conceptual framework that, along with the issue of climate justice, seeks to realize all four dimensions of an effective climate pedagogy. This broad holistic framework is potentially adaptable to contexts and disciplines beyond undergraduate physics.","Toward an Effective Pedagogy of Climate Change: Lessons From a Physics Classroom A major roadblock to effective climate education is the lack of radical visions (Kwauk, 2020) for engaging with climate change and related social-environmental crises in the classroom. Key aspects of the climate system: its inherent complexity and transdisciplinarity, the entanglement of social and natural systems, and the fact that it spans large scales of space and time - all present serious challenges to our modern, compartmentalized formal education systems. I present an argument for a justice-centered, inter-to-transdisciplinary conceptualization of climate change at the nexus of science, society and justice, which is designed to engage with the very features of the climate system that make it challenging. Based on pedagogical experiments in an undergraduate physics classroom for non-science majors, I identify five general barriers to teaching climate change, and formulate four dimensions of an effective climate pedagogy: the scientific-technological, the transdisciplinary, the onto-epistemological, and the psychosocial action dimensions. Within the context of a collaborative classroom culture informed by transformational learning, I describe the use of a meta-conceptual framework that, along with the issue of climate justice, seeks to realize all four dimensions of an effective climate pedagogy. This broad holistic framework is potentially adaptable to contexts and disciplines beyond undergraduate physics.",Education
Generalized Optimal Current Patterns and Electrical Safety in EIT,"There are a number of constraints which limit the current and voltages which can be applied on a multiple drive electrical imaging system. One obvious constraint is to limit the maximum Ohmic power dissipated in the body. Current patterns optimising distinguishability with respect to this constraint are singular functions of the difference of transconductance matrices with respect to the power norm. (the optimal currents of Isaacson). If one constrains the total current (L1 norm) the optimal patterns are pair drives. On the other hand if one constrains the maximum current on each drive electrode (an Linfty norm), the optimal patterns have each drive channel set to the maximum source or sink current value. In this paper we consider appropriate safety constraints and discuss how to find the optimal current patterns with those constraints.","Generalized Optimal Current Patterns and Electrical Safety in EIT There are a number of constraints which limit the current and voltages which can be applied on a multiple drive electrical imaging system. One obvious constraint is to limit the maximum Ohmic power dissipated in the body. Current patterns optimising distinguishability with respect to this constraint are singular functions of the difference of transconductance matrices with respect to the power norm. (the optimal currents of Isaacson). If one constrains the total current (L1 norm) the optimal patterns are pair drives. On the other hand if one constrains the maximum current on each drive electrode (an Linfty norm), the optimal patterns have each drive channel set to the maximum source or sink current value. In this paper we consider appropriate safety constraints and discuss how to find the optimal current patterns with those constraints.",Healthcare
Computerized Face Detection and Recognition,"This publication presents methods for face detection, analysis and recognition: fast normalized cross-correlation (fast correlation coefficient) between multiple templates based face pre-detection method, method for detection of exact face contour based on snakes and Generalized Gradient Vector Flow field, method for combining recognition algorithms based on Cumulative Match Characteristics in order to increase recognition speed and accuracy, and face recognition method based on Principal Component Analysis of the Wavelet Packet Decomposition allowing to use PCA - based recognition method with large number of training images. For all the methods are presented experimental results and comparisons of speed and accuracy with large face databases.","Computerized Face Detection and Recognition This publication presents methods for face detection, analysis and recognition: fast normalized cross-correlation (fast correlation coefficient) between multiple templates based face pre-detection method, method for detection of exact face contour based on snakes and Generalized Gradient Vector Flow field, method for combining recognition algorithms based on Cumulative Match Characteristics in order to increase recognition speed and accuracy, and face recognition method based on Principal Component Analysis of the Wavelet Packet Decomposition allowing to use PCA - based recognition method with large number of training images. For all the methods are presented experimental results and comparisons of speed and accuracy with large face databases.",Technology
A Linear Shift Invariant Multiscale Transform,"This paper presents a multiscale decomposition algorithm. Unlike standard wavelet transforms, the proposed operator is both linear and shift invariant. The central idea is to obtain shift invariance by averaging the aligned wavelet transform projections over all circular shifts of the signal. It is shown how the same transform can be obtained by a linear filter bank.","A Linear Shift Invariant Multiscale Transform This paper presents a multiscale decomposition algorithm. Unlike standard wavelet transforms, the proposed operator is both linear and shift invariant. The central idea is to obtain shift invariance by averaging the aligned wavelet transform projections over all circular shifts of the signal. It is shown how the same transform can be obtained by a linear filter bank.",Technology
Using the past to constrain the future: how the palaeorecord can improve estimates of global warming,"Climate sensitivity is defined as the change in global mean equilibrium temperature after a doubling of atmospheric CO2 concentration and provides a simple measure of global warming. An early estimate of climate sensitivity, 1.5-4.5degC, has changed little subsequently, including the latest assessment by the Intergovernmental Panel on Climate Change. The persistence of such large uncertainties in this simple measure casts doubt on our understanding of the mechanisms of climate change and our ability to predict the response of the climate system to future perturbations. This has motivated continued attempts to constrain the range with climate data, alone or in conjunction with models. The majority of studies use data from the instrumental period (post-1850) but recent work has made use of information about the large climate changes experienced in the geological past. In this review, we first outline approaches that estimate climate sensitivity using instrumental climate observations and then summarise attempts to use the record of climate change on geological timescales. We examine the limitations of these studies and suggest ways in which the power of the palaeoclimate record could be better used to reduce uncertainties in our predictions of climate sensitivity.","Using the past to constrain the future: how the palaeorecord can improve estimates of global warming Climate sensitivity is defined as the change in global mean equilibrium temperature after a doubling of atmospheric CO2 concentration and provides a simple measure of global warming. An early estimate of climate sensitivity, 1.5-4.5degC, has changed little subsequently, including the latest assessment by the Intergovernmental Panel on Climate Change. The persistence of such large uncertainties in this simple measure casts doubt on our understanding of the mechanisms of climate change and our ability to predict the response of the climate system to future perturbations. This has motivated continued attempts to constrain the range with climate data, alone or in conjunction with models. The majority of studies use data from the instrumental period (post-1850) but recent work has made use of information about the large climate changes experienced in the geological past. In this review, we first outline approaches that estimate climate sensitivity using instrumental climate observations and then summarise attempts to use the record of climate change on geological timescales. We examine the limitations of these studies and suggest ways in which the power of the palaeoclimate record could be better used to reduce uncertainties in our predictions of climate sensitivity.",Environment
Analysis of the sensitivity to discrete dividends : A new approach for pricing vanillas,"The incorporation of a dividend yield in the classical option pricing model of Black- Scholes results in a minor modification of the Black-Scholes formula, since the lognormal dynamic of the underlying asset is preserved. However, market makers prefer to work with cash dividends with fixed value instead of a dividend yield. Since there is no closed-form solution for the price of a European Call in this case, many methods have been proposed in the literature to approximate it. Here, we present a new approach. We derive an exact analytic formula for the sensitivity to dividends of an European option. We use this result to elaborate a proxy which possesses the same Taylor expansion around 0 with respect to the dividends as the exact price. The obtained approximation is very fast to compute (the same complexity than the usual Black-Scholes formula) and numerical tests show the extreme accuracy of the method for all practical cases.","Analysis of the sensitivity to discrete dividends : A new approach for pricing vanillas The incorporation of a dividend yield in the classical option pricing model of Black- Scholes results in a minor modification of the Black-Scholes formula, since the lognormal dynamic of the underlying asset is preserved. However, market makers prefer to work with cash dividends with fixed value instead of a dividend yield. Since there is no closed-form solution for the price of a European Call in this case, many methods have been proposed in the literature to approximate it. Here, we present a new approach. We derive an exact analytic formula for the sensitivity to dividends of an European option. We use this result to elaborate a proxy which possesses the same Taylor expansion around 0 with respect to the dividends as the exact price. The obtained approximation is very fast to compute (the same complexity than the usual Black-Scholes formula) and numerical tests show the extreme accuracy of the method for all practical cases.",Finance
Foreign Portfolio Investment and Economy: The Network Perspective,"The European Union and Eurozone present an inquisitive case of strongly interconnected network with high degree of dependence among nodes. This research focused on investment network of European Union and its major trading partners for specific time period 2001 to 2014. The changing investment patterns within Eurozone suggest strong financial and trade links with central and large economies. This study is about the association between portfolio investment and economic indicators with respect to financial networks. The analysis used the strongly connected investment network of Eurozone and its large trading partners. A strong correlation between, increasing or decreasing investment patterns with economic indicators of particular economy was found. Interestingly correlation patterns for network members other than Eurozone states were not as strong and depicted mild behavior. This as well, explains the significance of interconnectedness level among nodes of one network with varying centrality measures. Investment network visualization techniques helped to validate the results based on networks statistical measures.","Foreign Portfolio Investment and Economy: The Network Perspective The European Union and Eurozone present an inquisitive case of strongly interconnected network with high degree of dependence among nodes. This research focused on investment network of European Union and its major trading partners for specific time period 2001 to 2014. The changing investment patterns within Eurozone suggest strong financial and trade links with central and large economies. This study is about the association between portfolio investment and economic indicators with respect to financial networks. The analysis used the strongly connected investment network of Eurozone and its large trading partners. A strong correlation between, increasing or decreasing investment patterns with economic indicators of particular economy was found. Interestingly correlation patterns for network members other than Eurozone states were not as strong and depicted mild behavior. This as well, explains the significance of interconnectedness level among nodes of one network with varying centrality measures. Investment network visualization techniques helped to validate the results based on networks statistical measures.",Finance
The Volatility in a Multi-share Financial Market Model,"Single index financial market models cannot account for the empirically observed complex interactions between shares in a market. We describe a multi-share financial market model and compare characteristics of the volatility, that is the standard deviation of the price fluctuations, with empirical characteristics. In particular we find its probability distribution is similar to a log normal distribution but with a long power-law tail for the large fluctuations, and that the time development shows superdiffusion. Both these results are in good quantitative agreement with observations.","The Volatility in a Multi-share Financial Market Model Single index financial market models cannot account for the empirically observed complex interactions between shares in a market. We describe a multi-share financial market model and compare characteristics of the volatility, that is the standard deviation of the price fluctuations, with empirical characteristics. In particular we find its probability distribution is similar to a log normal distribution but with a long power-law tail for the large fluctuations, and that the time development shows superdiffusion. Both these results are in good quantitative agreement with observations.",Finance
Mean Field Theory for Sigmoid Belief Networks,We develop a mean field theory for sigmoid belief networks based on ideas from statistical mechanics. Our mean field theory provides a tractable approximation to the true probability distribution in these networks; it also yields a lower bound on the likelihood of evidence. We demonstrate the utility of this framework on a benchmark problem in statistical pattern recognition---the classification of handwritten digits.,Mean Field Theory for Sigmoid Belief Networks We develop a mean field theory for sigmoid belief networks based on ideas from statistical mechanics. Our mean field theory provides a tractable approximation to the true probability distribution in these networks; it also yields a lower bound on the likelihood of evidence. We demonstrate the utility of this framework on a benchmark problem in statistical pattern recognition---the classification of handwritten digits.,Technology
Does Financial Literacy Impact Investment Participation and Retirement Planning in Japan?,"By employing causal discovery method, the Fast Causal Inference (FCI) model to analyze data from the 2022 Financial Literacy Survey, we explore the causal relationships between financial literacy and financial activities, specifically investment participation and retirement planning. Our findings indicate that increasing financial literacy may not directly boost engagement in financial investments or retirement planning in Japan, which underscores the necessity for alternative strategies to motivate financial activities among Japanese households. This research offers valuable insights for policymakers focused on improving financial well-being by advancing the use of causal discovery algorithms in understanding financial behaviors.","Does Financial Literacy Impact Investment Participation and Retirement Planning in Japan? By employing causal discovery method, the Fast Causal Inference (FCI) model to analyze data from the 2022 Financial Literacy Survey, we explore the causal relationships between financial literacy and financial activities, specifically investment participation and retirement planning. Our findings indicate that increasing financial literacy may not directly boost engagement in financial investments or retirement planning in Japan, which underscores the necessity for alternative strategies to motivate financial activities among Japanese households. This research offers valuable insights for policymakers focused on improving financial well-being by advancing the use of causal discovery algorithms in understanding financial behaviors.",Finance
Remote Teaching and Learning in Applied Engineering: A Post-Pandemic Perspective,"The COVID-19 pandemic significantly disrupted the educational sector. Faced with this life-threatening pandemic, educators had to swiftly pivot to an alternate form of course delivery without severely impacting the quality of the educational experience. Following the transition to online learning, educators had to grapple with a host of challenges. With interrupted face-to-face delivery, limited access to state-of-the-art labs, barriers with educational technologies, challenges of academic integrity, and obstacles with remote teamwork and student participation, creative solutions were urgently needed. In this chapter, we provide a rationale for a variety of course delivery models at different stages of the pandemic and highlight the approaches we took to overcome some of the pressing challenges of remote education. We also discuss how we ensured that hands-on learning remains an integral part of engineering curricula, and we argue that some of the applied changes during the pandemic will likely serve as a catalyst for modernizing education.","Remote Teaching and Learning in Applied Engineering: A Post-Pandemic Perspective The COVID-19 pandemic significantly disrupted the educational sector. Faced with this life-threatening pandemic, educators had to swiftly pivot to an alternate form of course delivery without severely impacting the quality of the educational experience. Following the transition to online learning, educators had to grapple with a host of challenges. With interrupted face-to-face delivery, limited access to state-of-the-art labs, barriers with educational technologies, challenges of academic integrity, and obstacles with remote teamwork and student participation, creative solutions were urgently needed. In this chapter, we provide a rationale for a variety of course delivery models at different stages of the pandemic and highlight the approaches we took to overcome some of the pressing challenges of remote education. We also discuss how we ensured that hands-on learning remains an integral part of engineering curricula, and we argue that some of the applied changes during the pandemic will likely serve as a catalyst for modernizing education.",Education
VR Accessibility in Distance Adult Education,"As virtual reality (VR) technology becomes more pervasive, it continues to find multiple new uses beyond research laboratories. One of them is distance adult education -- the potential of VR to provide valuable education experiences is massive, despite the current barriers to its widespread application. Nevertheless, recent trends demonstrate clearly that VR is on the rise in education settings, and VR-only courses are becoming more popular across the globe. This trend will continue as more affordable VR solutions are released commercially, increasing the number of education institutions that benefit from the technology. No accessibility guidelines exist at present that are created specifically for the design, development, and use of VR hardware and software in distance education. The purpose of this workshop is to address this niche. It gathers researchers and practitioners who are interested in education and intend to work together to formulate a set of practical guidelines for the use of VR in distance adult education to make it accessible to a wider range of people.","VR Accessibility in Distance Adult Education As virtual reality (VR) technology becomes more pervasive, it continues to find multiple new uses beyond research laboratories. One of them is distance adult education -- the potential of VR to provide valuable education experiences is massive, despite the current barriers to its widespread application. Nevertheless, recent trends demonstrate clearly that VR is on the rise in education settings, and VR-only courses are becoming more popular across the globe. This trend will continue as more affordable VR solutions are released commercially, increasing the number of education institutions that benefit from the technology. No accessibility guidelines exist at present that are created specifically for the design, development, and use of VR hardware and software in distance education. The purpose of this workshop is to address this niche. It gathers researchers and practitioners who are interested in education and intend to work together to formulate a set of practical guidelines for the use of VR in distance adult education to make it accessible to a wider range of people.",Education
Experimental study of a liquid Xenon PET prototype module,"A detector using liquid Xenon in the scintillation mode is studied for Positron Emission Tomography (PET). The specific design aims at taking full advantage of the liquid Xenon properties. It does feature a promising insensitive to any parallax effect. This work reports on the performances of the first LXe prototype module, equipped with a position sensitive PMT operating in the VUV range (178 nm).","Experimental study of a liquid Xenon PET prototype module A detector using liquid Xenon in the scintillation mode is studied for Positron Emission Tomography (PET). The specific design aims at taking full advantage of the liquid Xenon properties. It does feature a promising insensitive to any parallax effect. This work reports on the performances of the first LXe prototype module, equipped with a position sensitive PMT operating in the VUV range (178 nm).",Healthcare
"Integrating Renewable Energy Sources as Reserve Providers: Modeling, Pricing, and Properties","In pursuit of carbon neutrality, many countries have adopted renewable portfolio standards to facilitate the integration of renewable energy. However, increasing penetration of renewable energy resources will also pose higher requirements on system flexibility. Allowing renewable themselves to participate in the reserve market could be a viable solution. To this end, this paper proposes an optimal dispatch model for joint energy-reserve procurement that incorporates renewable portfolio standards and RES serve as reserve providers. Potential generator outages and deviations in renewable and load power are modelled through a given number of probability-weighted scenarios. In particular, reserve resources are initially booked in the base case and then activated in non-base scenarios through the re-dispatch process. Marginal pricing is used to derive energy, reserve, and power deviation prices. Next, we develop the associated settlement process and establish several market properties. The proposed pricing scheme establishes equivalence between thermal generators and renewable units by accounting for their uncertainties, including thermal generator outages and renewable power deviations, and their flexibility, namely reserve and re-dispatch. We have shown that for renewable resources, supplying reserve according to the dispatch results compared to generating as much as possible leads to better profits. Simulations validate the effectiveness of the proposed method and properties established.","Integrating Renewable Energy Sources as Reserve Providers: Modeling, Pricing, and Properties In pursuit of carbon neutrality, many countries have adopted renewable portfolio standards to facilitate the integration of renewable energy. However, increasing penetration of renewable energy resources will also pose higher requirements on system flexibility. Allowing renewable themselves to participate in the reserve market could be a viable solution. To this end, this paper proposes an optimal dispatch model for joint energy-reserve procurement that incorporates renewable portfolio standards and RES serve as reserve providers. Potential generator outages and deviations in renewable and load power are modelled through a given number of probability-weighted scenarios. In particular, reserve resources are initially booked in the base case and then activated in non-base scenarios through the re-dispatch process. Marginal pricing is used to derive energy, reserve, and power deviation prices. Next, we develop the associated settlement process and establish several market properties. The proposed pricing scheme establishes equivalence between thermal generators and renewable units by accounting for their uncertainties, including thermal generator outages and renewable power deviations, and their flexibility, namely reserve and re-dispatch. We have shown that for renewable resources, supplying reserve according to the dispatch results compared to generating as much as possible leads to better profits. Simulations validate the effectiveness of the proposed method and properties established.",Environment
Camera Calibration: a USU Implementation,"The task of camera calibration is to estimate the intrinsic and extrinsic parameters of a camera model. Though there are some restricted techniques to infer the 3-D information about the scene from uncalibrated cameras, effective camera calibration procedures will open up the possibility of using a wide range of existing algorithms for 3-D reconstruction and recognition. The applications of camera calibration include vision-based metrology, robust visual platooning and visual docking of mobile robots where the depth information is important.","Camera Calibration: a USU Implementation The task of camera calibration is to estimate the intrinsic and extrinsic parameters of a camera model. Though there are some restricted techniques to infer the 3-D information about the scene from uncalibrated cameras, effective camera calibration procedures will open up the possibility of using a wide range of existing algorithms for 3-D reconstruction and recognition. The applications of camera calibration include vision-based metrology, robust visual platooning and visual docking of mobile robots where the depth information is important.",Technology
A Bayesian Treatment Selection Design for Phase II Randomised Cancer Clinical Trials,"It is crucial to design Phase II cancer clinical trials that balance the efficiency of treatment selection with clinical practicality. Sargent and Goldberg proposed a frequentist design that allow decision-making even when the primary endpoint is ambiguous. However, frequentist approaches rely on fixed thresholds and long-run frequency properties, which can limit flexibility in practical applications. In contrast, the Bayesian decision rule, based on posterior probabilities, enables transparent decision-making by incorporating prior knowledge and updating beliefs with new data, addressing some of the inherent limitations of frequentist designs. In this study, we propose a novel Bayesian design, allowing selection of a best-performing treatment. Specifically, concerning phase II clinical trials with a binary outcome, our decision rule employs posterior interval probability by integrating the joint distribution over all values, for which the success rate of the bester-performing treatment is greater than that of the other(s). This design can then determine which a treatment should proceed to the next phase, given predefined decision thresholds. Furthermore, we propose two sample size determination methods to empower such treatment selection designs implemented in a Bayesian framework. Through simulation studies and real-data applications, we demonstrate how this approach can overcome challenges related to sample size constraints in randomised trials. In addition, we present a user-friendly R Shiny application, enabling clinicians to Bayesian designs. Both our methodology and the software application can advance the design and analysis of clinical trials for evaluating cancer treatments.","A Bayesian Treatment Selection Design for Phase II Randomised Cancer Clinical Trials It is crucial to design Phase II cancer clinical trials that balance the efficiency of treatment selection with clinical practicality. Sargent and Goldberg proposed a frequentist design that allow decision-making even when the primary endpoint is ambiguous. However, frequentist approaches rely on fixed thresholds and long-run frequency properties, which can limit flexibility in practical applications. In contrast, the Bayesian decision rule, based on posterior probabilities, enables transparent decision-making by incorporating prior knowledge and updating beliefs with new data, addressing some of the inherent limitations of frequentist designs. In this study, we propose a novel Bayesian design, allowing selection of a best-performing treatment. Specifically, concerning phase II clinical trials with a binary outcome, our decision rule employs posterior interval probability by integrating the joint distribution over all values, for which the success rate of the bester-performing treatment is greater than that of the other(s). This design can then determine which a treatment should proceed to the next phase, given predefined decision thresholds. Furthermore, we propose two sample size determination methods to empower such treatment selection designs implemented in a Bayesian framework. Through simulation studies and real-data applications, we demonstrate how this approach can overcome challenges related to sample size constraints in randomised trials. In addition, we present a user-friendly R Shiny application, enabling clinicians to Bayesian designs. Both our methodology and the software application can advance the design and analysis of clinical trials for evaluating cancer treatments.",Healthcare
Characterizing Financial Market Coverage using Artificial Intelligence,"This paper scrutinizes a database of over 4900 YouTube videos to characterize financial market coverage. Financial market coverage generates a large number of videos. Therefore, watching these videos to derive actionable insights could be challenging and complex. In this paper, we leverage Whisper, a speech-to-text model from OpenAI, to generate a text corpus of market coverage videos from Bloomberg and Yahoo Finance. We employ natural language processing to extract insights regarding language use from the market coverage. Moreover, we examine the prominent presence of trending topics and their evolution over time, and the impacts that some individuals and organizations have on the financial market. Our characterization highlights the dynamics of the financial market coverage and provides valuable insights reflecting broad discussions regarding recent financial events and the world economy.","Characterizing Financial Market Coverage using Artificial Intelligence This paper scrutinizes a database of over 4900 YouTube videos to characterize financial market coverage. Financial market coverage generates a large number of videos. Therefore, watching these videos to derive actionable insights could be challenging and complex. In this paper, we leverage Whisper, a speech-to-text model from OpenAI, to generate a text corpus of market coverage videos from Bloomberg and Yahoo Finance. We employ natural language processing to extract insights regarding language use from the market coverage. Moreover, we examine the prominent presence of trending topics and their evolution over time, and the impacts that some individuals and organizations have on the financial market. Our characterization highlights the dynamics of the financial market coverage and provides valuable insights reflecting broad discussions regarding recent financial events and the world economy.",Finance
Understanding racial bias in health using the Medical Expenditure Panel Survey data,"Over the years, several studies have demonstrated that there exist significant disparities in health indicators in the United States population across various groups. Healthcare expense is used as a proxy for health in algorithms that drive healthcare systems and this exacerbates the existing bias. In this work, we focus on the presence of racial bias in health indicators in the publicly available, and nationally representative Medical Expenditure Panel Survey (MEPS) data. We show that predictive models for care management trained using this data inherit this bias. Finally, we demonstrate that this inherited bias can be reduced significantly using simple mitigation techniques.","Understanding racial bias in health using the Medical Expenditure Panel Survey data Over the years, several studies have demonstrated that there exist significant disparities in health indicators in the United States population across various groups. Healthcare expense is used as a proxy for health in algorithms that drive healthcare systems and this exacerbates the existing bias. In this work, we focus on the presence of racial bias in health indicators in the publicly available, and nationally representative Medical Expenditure Panel Survey (MEPS) data. We show that predictive models for care management trained using this data inherit this bias. Finally, we demonstrate that this inherited bias can be reduced significantly using simple mitigation techniques.",Healthcare
Assessing Evolutionary Terrain Generation Methods for Curriculum Reinforcement Learning,"Curriculum learning allows complex tasks to be mastered via incremental progression over stepping stone goals towards a final desired behaviour. Typical implementations learn locomotion policies for challenging environments through gradual complexification of a terrain mesh generated through a parameterised noise function. To date, researchers have predominantly generated terrains from a limited range of noise functions, and the effect of the generator on the learning process is underrepresented in the literature. We compare popular noise-based terrain generators to two indirect encodings, CPPN and GAN. To allow direct comparison between both direct and indirect representations, we assess the impact of a range of representation-agnostic MAP-Elites feature descriptors that compute metrics directly from the generated terrain meshes. Next, performance and coverage are assessed when training a humanoid robot in a physics simulator using the PPO algorithm. Results describe key differences between the generators that inform their use in curriculum learning, and present a range of useful feature descriptors for uptake by the community.","Assessing Evolutionary Terrain Generation Methods for Curriculum Reinforcement Learning Curriculum learning allows complex tasks to be mastered via incremental progression over stepping stone goals towards a final desired behaviour. Typical implementations learn locomotion policies for challenging environments through gradual complexification of a terrain mesh generated through a parameterised noise function. To date, researchers have predominantly generated terrains from a limited range of noise functions, and the effect of the generator on the learning process is underrepresented in the literature. We compare popular noise-based terrain generators to two indirect encodings, CPPN and GAN. To allow direct comparison between both direct and indirect representations, we assess the impact of a range of representation-agnostic MAP-Elites feature descriptors that compute metrics directly from the generated terrain meshes. Next, performance and coverage are assessed when training a humanoid robot in a physics simulator using the PPO algorithm. Results describe key differences between the generators that inform their use in curriculum learning, and present a range of useful feature descriptors for uptake by the community.",Education
Glocalizing Generative AI in Education for the Global South: The Design Case of 21st Century Teacher Educator AI for Ghana,"This study presents the design and development of the 21st Century Teacher Educator for Ghana GPT, a customized Generative AI (GenAI) tool created using OpenAIs Retrieval-Augmented Generation (RAG) and Interactive Semi-Automated Prompting Strategy (ISA). Anchored in a Glocalized design approach, this tool supports pre-service teachers (PSTs) in Ghana by embedding localized linguistic, cultural, and curricular content within globally aligned principles of ethical and responsible AI use. The model utilizes structured, preloaded datasets-including Ghanas National Teacher Education Curriculum Framework (NTECF), UNESCOs (2023) AI guidelines, and culturally responsive pedagogies-to offer curriculum-aligned, linguistically adaptive, and pedagogically grounded learning support. The ISA enables users to input their institution, year, and semester, generating tailored academic content such as lecture notes, assessment practice, practicum resources, and action research guidance. The design incorporates the Culture and Context-Aware Framework, GenAI-CRSciA, and frameworks addressing GenAI neocolonialism to ensure equity, curriculum fidelity, and local relevance. Pilot implementation revealed notable strengths in language adaptation and localization, delivering bilingual support in English and Ghanaian languages like Twi, Dagbani, Mampruli, and Dagaare, with contextualized examples for deeper understanding. The GPT also generated practice assessments aligned with course objectives, reinforcing learner engagement. Challenges included occasional hallucinations due to limited corpora in some indigenous languages and access barriers tied to premium subscriptions. This design case contributes to discourse on Glocalized GenAI and calls for collaboration with OpenAI NextGen to expand access and empirically assess usage across diverse African educational contexts.","Glocalizing Generative AI in Education for the Global South: The Design Case of 21st Century Teacher Educator AI for Ghana This study presents the design and development of the 21st Century Teacher Educator for Ghana GPT, a customized Generative AI (GenAI) tool created using OpenAIs Retrieval-Augmented Generation (RAG) and Interactive Semi-Automated Prompting Strategy (ISA). Anchored in a Glocalized design approach, this tool supports pre-service teachers (PSTs) in Ghana by embedding localized linguistic, cultural, and curricular content within globally aligned principles of ethical and responsible AI use. The model utilizes structured, preloaded datasets-including Ghanas National Teacher Education Curriculum Framework (NTECF), UNESCOs (2023) AI guidelines, and culturally responsive pedagogies-to offer curriculum-aligned, linguistically adaptive, and pedagogically grounded learning support. The ISA enables users to input their institution, year, and semester, generating tailored academic content such as lecture notes, assessment practice, practicum resources, and action research guidance. The design incorporates the Culture and Context-Aware Framework, GenAI-CRSciA, and frameworks addressing GenAI neocolonialism to ensure equity, curriculum fidelity, and local relevance. Pilot implementation revealed notable strengths in language adaptation and localization, delivering bilingual support in English and Ghanaian languages like Twi, Dagbani, Mampruli, and Dagaare, with contextualized examples for deeper understanding. The GPT also generated practice assessments aligned with course objectives, reinforcing learner engagement. Challenges included occasional hallucinations due to limited corpora in some indigenous languages and access barriers tied to premium subscriptions. This design case contributes to discourse on Glocalized GenAI and calls for collaboration with OpenAI NextGen to expand access and empirically assess usage across diverse African educational contexts.",Education
A Computer Aided Detection system for mammographic images implemented on a GRID infrastructure,"The use of an automatic system for the analysis of mammographic images has proven to be very useful to radiologists in the investigation of breast cancer, especially in the framework of mammographic-screening programs. A breast neoplasia is often marked by the presence of microcalcification clusters and massive lesions in the mammogram: hence the need for tools able to recognize such lesions at an early stage. In the framework of the GPCALMA (GRID Platform for Computer Assisted Library for MAmmography) project, the co-working of italian physicists and radiologists built a large distributed database of digitized mammographic images (about 5500 images corresponding to 1650 patients) and developed a CAD (Computer Aided Detection) system, able to make an automatic search of massive lesions and microcalcification clusters. The CAD is implemented in the GPCALMA integrated station, which can be used also for digitization, as archive and to perform statistical analyses. Some GPCALMA integrated stations have already been implemented and are currently on clinical trial in some italian hospitals. The emerging GRID technology can been used to connect the GPCALMA integrated stations operating in different medical centers. The GRID approach will support an effective tele- and co-working between radiologists, cancer specialists and epidemiology experts by allowing remote image analysis and interactive online diagnosis.","A Computer Aided Detection system for mammographic images implemented on a GRID infrastructure The use of an automatic system for the analysis of mammographic images has proven to be very useful to radiologists in the investigation of breast cancer, especially in the framework of mammographic-screening programs. A breast neoplasia is often marked by the presence of microcalcification clusters and massive lesions in the mammogram: hence the need for tools able to recognize such lesions at an early stage. In the framework of the GPCALMA (GRID Platform for Computer Assisted Library for MAmmography) project, the co-working of italian physicists and radiologists built a large distributed database of digitized mammographic images (about 5500 images corresponding to 1650 patients) and developed a CAD (Computer Aided Detection) system, able to make an automatic search of massive lesions and microcalcification clusters. The CAD is implemented in the GPCALMA integrated station, which can be used also for digitization, as archive and to perform statistical analyses. Some GPCALMA integrated stations have already been implemented and are currently on clinical trial in some italian hospitals. The emerging GRID technology can been used to connect the GPCALMA integrated stations operating in different medical centers. The GRID approach will support an effective tele- and co-working between radiologists, cancer specialists and epidemiology experts by allowing remote image analysis and interactive online diagnosis.",Healthcare
Total-Order and Partial-Order Planning: A Comparative Analysis,"For many years, the intuitions underlying partial-order planning were largely taken for granted. Only in the past few years has there been renewed interest in the fundamental principles underlying this paradigm. In this paper, we present a rigorous comparative analysis of partial-order and total-order planning by focusing on two specific planners that can be directly compared. We show that there are some subtle assumptions that underly the wide-spread intuitions regarding the supposed efficiency of partial-order planning. For instance, the superiority of partial-order planning can depend critically upon the search strategy and the structure of the search space. Understanding the underlying assumptions is crucial for constructing efficient planners.","Total-Order and Partial-Order Planning: A Comparative Analysis For many years, the intuitions underlying partial-order planning were largely taken for granted. Only in the past few years has there been renewed interest in the fundamental principles underlying this paradigm. In this paper, we present a rigorous comparative analysis of partial-order and total-order planning by focusing on two specific planners that can be directly compared. We show that there are some subtle assumptions that underly the wide-spread intuitions regarding the supposed efficiency of partial-order planning. For instance, the superiority of partial-order planning can depend critically upon the search strategy and the structure of the search space. Understanding the underlying assumptions is crucial for constructing efficient planners.",Technology
Inequality in Turkey: Looking Beyond Growth,"This paper investigates the relationships between economic growth, investment in human capital and income equality in Turkey. The conclusion drawn based on the data from the OECD and the World Bank suggests that economic growth can improve income equality depending on the expenditures undertaken by the government. As opposed to the standard view that economic growth and income inequality are positively related, the findings of this paper suggest that other factors such as education and healthcare spending are also driving factors of income inequality in Turkey. The proven positive impact of investment in education and health care on income equality could aid policymakers who aim to achieve fairer income equality and economic growth, in investment decisions.","Inequality in Turkey: Looking Beyond Growth This paper investigates the relationships between economic growth, investment in human capital and income equality in Turkey. The conclusion drawn based on the data from the OECD and the World Bank suggests that economic growth can improve income equality depending on the expenditures undertaken by the government. As opposed to the standard view that economic growth and income inequality are positively related, the findings of this paper suggest that other factors such as education and healthcare spending are also driving factors of income inequality in Turkey. The proven positive impact of investment in education and health care on income equality could aid policymakers who aim to achieve fairer income equality and economic growth, in investment decisions.",Finance
Diffusion of Context and Credit Information in Markovian Models,"This paper studies the problem of ergodicity of transition probability matrices in Markovian models, such as hidden Markov models (HMMs), and how it makes very difficult the task of learning to represent long-term context for sequential data. This phenomenon hurts the forward propagation of long-term context information, as well as learning a hidden state representation to represent long-term context, which depends on propagating credit information backwards in time. Using results from Markov chain theory, we show that this problem of diffusion of context and credit is reduced when the transition probabilities approach 0 or 1, i.e., the transition probability matrices are sparse and the model essentially deterministic. The results found in this paper apply to learning approaches based on continuous optimization, such as gradient descent and the Baum-Welch algorithm.","Diffusion of Context and Credit Information in Markovian Models This paper studies the problem of ergodicity of transition probability matrices in Markovian models, such as hidden Markov models (HMMs), and how it makes very difficult the task of learning to represent long-term context for sequential data. This phenomenon hurts the forward propagation of long-term context information, as well as learning a hidden state representation to represent long-term context, which depends on propagating credit information backwards in time. Using results from Markov chain theory, we show that this problem of diffusion of context and credit is reduced when the transition probabilities approach 0 or 1, i.e., the transition probability matrices are sparse and the model essentially deterministic. The results found in this paper apply to learning approaches based on continuous optimization, such as gradient descent and the Baum-Welch algorithm.",Technology
Rapid Virtual Simulations: Achieving Satisficing Learning Impact with Realistic-Enough Activities in Health Science Education,"This manuscript introduces the concept of Rapid Virtual Simulations, a new techno-pedagogical activity that fosters expert autonomy for creating virtual educational simulations. It is grounded in a Realistic-Enough Philosophy that consists of pursuing the development of the least complex simulation while still ensuring a Satisficing (or good enough) Learning Impact. It also introduces the concept of a Rapid Virtual Simulation Ecosystem as an integrated set of technological modules that facilitates the work of health professional educators while multiplying educational affordances for learners. Finally, this manuscript presents an argument for technological agility and simplicity as key guiding principles for the design of future simulation-based educational systems.","Rapid Virtual Simulations: Achieving Satisficing Learning Impact with Realistic-Enough Activities in Health Science Education This manuscript introduces the concept of Rapid Virtual Simulations, a new techno-pedagogical activity that fosters expert autonomy for creating virtual educational simulations. It is grounded in a Realistic-Enough Philosophy that consists of pursuing the development of the least complex simulation while still ensuring a Satisficing (or good enough) Learning Impact. It also introduces the concept of a Rapid Virtual Simulation Ecosystem as an integrated set of technological modules that facilitates the work of health professional educators while multiplying educational affordances for learners. Finally, this manuscript presents an argument for technological agility and simplicity as key guiding principles for the design of future simulation-based educational systems.",Education
Mapping markets to the statistical mechanics: the derivatives act against the self-regulation of stock market,"Mapping the economy to the some statistical physics models we get strong indications that, in contrary to the pure stock market, the stock market with derivatives could not self-regulate.","Mapping markets to the statistical mechanics: the derivatives act against the self-regulation of stock market Mapping the economy to the some statistical physics models we get strong indications that, in contrary to the pure stock market, the stock market with derivatives could not self-regulate.",Finance
AI and personalized learning: bridging the gap with modern educational goals,"Personalized learning (PL) aspires to provide an alternative to the one-size-fits-all approach in education. Technology-based PL solutions have shown notable effectiveness in enhancing learning performance. However, their alignment with the broader goals of modern education is inconsistent across technologies and research areas. In this paper, we examine the characteristics of AI-driven PL solutions in light of the goals outlined in the OECD Learning Compass 2030. Our analysis indicates a gap between the objectives of modern education and the technological approach to PL. We identify areas where the AI-based PL solutions could embrace essential elements of contemporary education, such as fostering learners agency, cognitive engagement, and general competencies. While the PL solutions that narrowly focus on domain-specific knowledge acquisition are instrumental in aiding learning processes, the PL envisioned by educational experts extends beyond simple technological tools and requires a holistic change in the educational system. Finally, we explore the potential of generative AI, such as ChatGPT, and propose a hybrid model that blends artificial intelligence with a collaborative, teacher-facilitated approach to personalized learning.","AI and personalized learning: bridging the gap with modern educational goals Personalized learning (PL) aspires to provide an alternative to the one-size-fits-all approach in education. Technology-based PL solutions have shown notable effectiveness in enhancing learning performance. However, their alignment with the broader goals of modern education is inconsistent across technologies and research areas. In this paper, we examine the characteristics of AI-driven PL solutions in light of the goals outlined in the OECD Learning Compass 2030. Our analysis indicates a gap between the objectives of modern education and the technological approach to PL. We identify areas where the AI-based PL solutions could embrace essential elements of contemporary education, such as fostering learners agency, cognitive engagement, and general competencies. While the PL solutions that narrowly focus on domain-specific knowledge acquisition are instrumental in aiding learning processes, the PL envisioned by educational experts extends beyond simple technological tools and requires a holistic change in the educational system. Finally, we explore the potential of generative AI, such as ChatGPT, and propose a hybrid model that blends artificial intelligence with a collaborative, teacher-facilitated approach to personalized learning.",Education
Optimal Interventions in Coupled-Activity Network Games: Application to Sustainable Forestry,"We address the challenge of promoting sustainable practices in production forests managed by strategic entities (agents) that harvest agricultural commodities under concession agreements. These entities engage in activities that either follow sustainable production practices or expand into protected forests for agricultural growth, which leads to unsustainable production. Our study uses a network game model to design optimal pricing policies that incentivize sustainability and discourage environmentally harmful practices. Specifically, we model interactions between agents, capturing both intra-activity (within a single production activity) and cross-activity (between sustainable and unsustainable practices) influences on agent behavior. We solve the problem of maximizing welfare while adhering to budgetary and environmental constraints - particularly, limiting the aggregate level of unsustainable effort across all agents. Although this problem is NP-hard in general, we derive closed-form solutions for various realistic scenarios, including cases with regionally uniform pricing and the use of sustainability premiums or penalties. Remarkably, we find that it is possible to achieve both welfare improvement and reduction in unsustainable practices without reducing any agents utility, even when there is no external budget for increasing premiums. We introduce a novel node centrality measure to identify agents whose decisions most influence aggregate unsustainable effort. Empirical validation confirms our theoretical findings, offering actionable insights for policymakers aiming to promote sustainable resource management in agricultural commodity markets. Our work has broader implications for addressing sustainability challenges in the presence of network effects, offering a framework for designing incentive structures that align economic objectives with environmental stewardship.","Optimal Interventions in Coupled-Activity Network Games: Application to Sustainable Forestry We address the challenge of promoting sustainable practices in production forests managed by strategic entities (agents) that harvest agricultural commodities under concession agreements. These entities engage in activities that either follow sustainable production practices or expand into protected forests for agricultural growth, which leads to unsustainable production. Our study uses a network game model to design optimal pricing policies that incentivize sustainability and discourage environmentally harmful practices. Specifically, we model interactions between agents, capturing both intra-activity (within a single production activity) and cross-activity (between sustainable and unsustainable practices) influences on agent behavior. We solve the problem of maximizing welfare while adhering to budgetary and environmental constraints - particularly, limiting the aggregate level of unsustainable effort across all agents. Although this problem is NP-hard in general, we derive closed-form solutions for various realistic scenarios, including cases with regionally uniform pricing and the use of sustainability premiums or penalties. Remarkably, we find that it is possible to achieve both welfare improvement and reduction in unsustainable practices without reducing any agents utility, even when there is no external budget for increasing premiums. We introduce a novel node centrality measure to identify agents whose decisions most influence aggregate unsustainable effort. Empirical validation confirms our theoretical findings, offering actionable insights for policymakers aiming to promote sustainable resource management in agricultural commodity markets. Our work has broader implications for addressing sustainability challenges in the presence of network effects, offering a framework for designing incentive structures that align economic objectives with environmental stewardship.",Environment
ACDNet: Attention-guided Collaborative Decision Network for Effective Medication Recommendation,"Medication recommendation using Electronic Health Records (EHR) is challenging due to complex medical data. Current approaches extract longitudinal information from patient EHR to personalize recommendations. However, existing models often lack sufficient patient representation and overlook the importance of considering the similarity between a patients medication records and specific medicines. Therefore, an Attention-guided Collaborative Decision Network (ACDNet) for medication recommendation is proposed in this paper. Specifically, ACDNet utilizes attention mechanism and Transformer to effectively capture patient health conditions and medication records by modeling their historical visits at both global and local levels. ACDNet also employs a collaborative decision framework, utilizing the similarity between medication records and medicine representation to facilitate the recommendation process. The experimental results on two extensive medical datasets, MIMIC-III and MIMIC-IV, clearly demonstrate that ACDNet outperforms state-of-the-art models in terms of Jaccard, PR-AUC, and F1 score, reaffirming its superiority. Moreover, the ablation experiments provide solid evidence of the effectiveness of each module in ACDNet, validating their contribution to the overall performance. Furthermore, a detailed case study reinforces the effectiveness of ACDNet in medication recommendation based on EHR data, showcasing its practical value in real-world healthcare scenarios.","ACDNet: Attention-guided Collaborative Decision Network for Effective Medication Recommendation Medication recommendation using Electronic Health Records (EHR) is challenging due to complex medical data. Current approaches extract longitudinal information from patient EHR to personalize recommendations. However, existing models often lack sufficient patient representation and overlook the importance of considering the similarity between a patients medication records and specific medicines. Therefore, an Attention-guided Collaborative Decision Network (ACDNet) for medication recommendation is proposed in this paper. Specifically, ACDNet utilizes attention mechanism and Transformer to effectively capture patient health conditions and medication records by modeling their historical visits at both global and local levels. ACDNet also employs a collaborative decision framework, utilizing the similarity between medication records and medicine representation to facilitate the recommendation process. The experimental results on two extensive medical datasets, MIMIC-III and MIMIC-IV, clearly demonstrate that ACDNet outperforms state-of-the-art models in terms of Jaccard, PR-AUC, and F1 score, reaffirming its superiority. Moreover, the ablation experiments provide solid evidence of the effectiveness of each module in ACDNet, validating their contribution to the overall performance. Furthermore, a detailed case study reinforces the effectiveness of ACDNet in medication recommendation based on EHR data, showcasing its practical value in real-world healthcare scenarios.",Healthcare
Developing Augmented Reality based Gaming Model to Teach Ethical Education in Primary Schools,"Education sector is adopting new technologies for both teaching and learning pedagogy. Augmented Reality (AR) is a new technology that can be used in the educational pedagogy to enhance the engagement with students. Students interact with AR-based educational material for more visualization and explanation. Therefore, the use of AR in education is becoming more popular. However, most researches narrate the use of AR technologies in the field of English, Maths, Science, Culture, Arts, and History education but the absence of ethical education is visible. In our paper, we design the system and develop an AR-based mobile game model in the field of Ethical education for pre-primary students. Students from pre-primary require more interactive lessons than theoretical concepts. So, we use AR technology to develop a game which offers interactive procedures where students can learn with fun and engage with the context. Finally, we develop a prototype that works with our research objective. We conclude our paper with future works.","Developing Augmented Reality based Gaming Model to Teach Ethical Education in Primary Schools Education sector is adopting new technologies for both teaching and learning pedagogy. Augmented Reality (AR) is a new technology that can be used in the educational pedagogy to enhance the engagement with students. Students interact with AR-based educational material for more visualization and explanation. Therefore, the use of AR in education is becoming more popular. However, most researches narrate the use of AR technologies in the field of English, Maths, Science, Culture, Arts, and History education but the absence of ethical education is visible. In our paper, we design the system and develop an AR-based mobile game model in the field of Ethical education for pre-primary students. Students from pre-primary require more interactive lessons than theoretical concepts. So, we use AR technology to develop a game which offers interactive procedures where students can learn with fun and engage with the context. Finally, we develop a prototype that works with our research objective. We conclude our paper with future works.",Education
Computing-specific pedagogies and theoretical models: common uses and relationships,"Computing education widely applies general learning theories and pedagogical practices. However, computing also includes specific disciplinary knowledge and skills, e.g., programming and software development methods, for which there has been a long history of development and application of specific pedagogical practices. In recent years, there has also been substantial interest in developing computing-specific theoretical models, which seek to describe and explain the complex interactions within teaching and learning computing in various contexts. In this paper, we explore connections between computing-specific pedagogies and theoretical models as reported in the literature. Our goal is to enrich computing education research and practice by illustrating how explicit use of field-specific theories and pedagogies can further the whole field. We have collected a list of computing-specific pedagogical practices and theoretical models from a literature search, identifying source papers where they have been first introduced or well described. We then searched for papers in the ACM digital library that cite source papers from each list, and analyzed the type of interaction between the model and pedagogy in each paper. We developed a categorization of how theoretical models and pedagogies have supported or discounted each other, have been used together in empirical studies or used to build new artefacts. Our results showed that pair programming and parsons problems have had the most interactions with theoretical models in the explored papers, and we present findings of the analysis of these interactions.","Computing-specific pedagogies and theoretical models: common uses and relationships Computing education widely applies general learning theories and pedagogical practices. However, computing also includes specific disciplinary knowledge and skills, e.g., programming and software development methods, for which there has been a long history of development and application of specific pedagogical practices. In recent years, there has also been substantial interest in developing computing-specific theoretical models, which seek to describe and explain the complex interactions within teaching and learning computing in various contexts. In this paper, we explore connections between computing-specific pedagogies and theoretical models as reported in the literature. Our goal is to enrich computing education research and practice by illustrating how explicit use of field-specific theories and pedagogies can further the whole field. We have collected a list of computing-specific pedagogical practices and theoretical models from a literature search, identifying source papers where they have been first introduced or well described. We then searched for papers in the ACM digital library that cite source papers from each list, and analyzed the type of interaction between the model and pedagogy in each paper. We developed a categorization of how theoretical models and pedagogies have supported or discounted each other, have been used together in empirical studies or used to build new artefacts. Our results showed that pair programming and parsons problems have had the most interactions with theoretical models in the explored papers, and we present findings of the analysis of these interactions.",Education
Leffet de levier de trsorerie,"The effect of leverage on liquidity is a tool for analysing the level of liquidity for a given production process. It measures the sensitivity of the level of liquidity that results from changes in the volume of production and unit operating margin. A commercial activity is liquid at the moment when all costs are covered by revenues. However, not all of the cash flows from production influence liquidity levels. The estimated costs do not directly influence the level of liquidity. Therefore, two indicators are to be taken into consideration: the elasticity of ongoing liquidity - fixed costs include estimated costs, and, the elasticity of immediate liquidity - fixed costs only include costs that are payable. The coefficients of leverage of ongoing liquidity and of leverage of immediate liquidity in relation to the operating margin have a behaviour that is identical to that calculated in relation to production. If the productive capacity remains unchanged, the regulation of the change in elasticity of the costs and of its influence on the unitary operating margin is the sole parameter available to the entrepreneur to maintain the liquidity of the company at the desired level. But, if the productive capacity is variable, the entrepreneur can use the volume of sales to control liquidity but then the transformation of the production process must be analysed so as to adjust the relevant elements to retain in the operating structure the degree of liquidity wished for.","Leffet de levier de trsorerie The effect of leverage on liquidity is a tool for analysing the level of liquidity for a given production process. It measures the sensitivity of the level of liquidity that results from changes in the volume of production and unit operating margin. A commercial activity is liquid at the moment when all costs are covered by revenues. However, not all of the cash flows from production influence liquidity levels. The estimated costs do not directly influence the level of liquidity. Therefore, two indicators are to be taken into consideration: the elasticity of ongoing liquidity - fixed costs include estimated costs, and, the elasticity of immediate liquidity - fixed costs only include costs that are payable. The coefficients of leverage of ongoing liquidity and of leverage of immediate liquidity in relation to the operating margin have a behaviour that is identical to that calculated in relation to production. If the productive capacity remains unchanged, the regulation of the change in elasticity of the costs and of its influence on the unitary operating margin is the sole parameter available to the entrepreneur to maintain the liquidity of the company at the desired level. But, if the productive capacity is variable, the entrepreneur can use the volume of sales to control liquidity but then the transformation of the production process must be analysed so as to adjust the relevant elements to retain in the operating structure the degree of liquidity wished for.",Finance
"Navigating the Future of Education: Educators Insights on AI Integration and Challenges in Greece, Hungary, Latvia, Ireland and Armenia","Understanding teachers perspectives on AI in Education (AIEd) is crucial for its effective integration into the educational framework. This paper aims to explore how teachers currently use AI and how it can enhance the educational process. We conducted a cross-national study spanning Greece, Hungary, Latvia, Ireland, and Armenia, surveying 1754 educators through an online questionnaire, addressing three research questions. Our first research question examines educators understanding of AIEd, their skepticism, and its integration within schools. Most educators report a solid understanding of AI and acknowledge its potential risks. AIEd is primarily used for educator support and engaging students. However, concerns exist about AIs impact on fostering critical thinking and exposing students to biased data. The second research question investigates student engagement with AI tools from educators perspectives. Teachers indicate that students use AI mainly to manage their academic workload, while outside school, AI tools are primarily used for entertainment. The third research question addresses future implications of AI in education. Educators are optimistic about AIs potential to enhance educational processes, particularly through personalized learning experiences. Nonetheless, they express significant concerns about AIs impact on cultivating critical thinking and ethical issues related to potential misuse. There is a strong emphasis on the need for professional development through training seminars, workshops, and online courses to integrate AI effectively into teaching practices. Overall, the findings highlight a cautious optimism among educators regarding AI in education, alongside a clear demand for targeted professional development to address concerns and enhance skills in using AI tools.","Navigating the Future of Education: Educators Insights on AI Integration and Challenges in Greece, Hungary, Latvia, Ireland and Armenia Understanding teachers perspectives on AI in Education (AIEd) is crucial for its effective integration into the educational framework. This paper aims to explore how teachers currently use AI and how it can enhance the educational process. We conducted a cross-national study spanning Greece, Hungary, Latvia, Ireland, and Armenia, surveying 1754 educators through an online questionnaire, addressing three research questions. Our first research question examines educators understanding of AIEd, their skepticism, and its integration within schools. Most educators report a solid understanding of AI and acknowledge its potential risks. AIEd is primarily used for educator support and engaging students. However, concerns exist about AIs impact on fostering critical thinking and exposing students to biased data. The second research question investigates student engagement with AI tools from educators perspectives. Teachers indicate that students use AI mainly to manage their academic workload, while outside school, AI tools are primarily used for entertainment. The third research question addresses future implications of AI in education. Educators are optimistic about AIs potential to enhance educational processes, particularly through personalized learning experiences. Nonetheless, they express significant concerns about AIs impact on cultivating critical thinking and ethical issues related to potential misuse. There is a strong emphasis on the need for professional development through training seminars, workshops, and online courses to integrate AI effectively into teaching practices. Overall, the findings highlight a cautious optimism among educators regarding AI in education, alongside a clear demand for targeted professional development to address concerns and enhance skills in using AI tools.",Education
From Correlation to Causation: Understanding Climate Change through Causal Analysis and LLM Interpretations,"This research presents a three-step causal inference framework that integrates correlation analysis, machine learning-based causality discovery, and LLM-driven interpretations to identify socioeconomic factors influencing carbon emissions and contributing to climate change. The approach begins with identifying correlations, progresses to causal analysis, and enhances decision making through LLM-generated inquiries about the context of climate change. The proposed framework offers adaptable solutions that support data-driven policy-making and strategic decision-making in climate-related contexts, uncovering causal relationships within the climate change domain.","From Correlation to Causation: Understanding Climate Change through Causal Analysis and LLM Interpretations This research presents a three-step causal inference framework that integrates correlation analysis, machine learning-based causality discovery, and LLM-driven interpretations to identify socioeconomic factors influencing carbon emissions and contributing to climate change. The approach begins with identifying correlations, progresses to causal analysis, and enhances decision making through LLM-generated inquiries about the context of climate change. The proposed framework offers adaptable solutions that support data-driven policy-making and strategic decision-making in climate-related contexts, uncovering causal relationships within the climate change domain.",Environment
LXPER Index 2.0: Improving Text Readability Assessment Model for L2 English Students in Korea,"Developing a text readability assessment model specifically for texts in a foreign English Language Training (ELT) curriculum has never had much attention in the field of Natural Language Processing. Hence, most developed models show extremely low accuracy for L2 English texts, up to the point where not many even serve as a fair comparison. In this paper, we investigate a text readability assessment model for L2 English learners in Korea. In accordance, we improve and expand the Text Corpus of the Korean ELT curriculum (CoKEC-text). Each text is labeled with its target grade level. We train our model with CoKEC-text and significantly improve the accuracy of readability assessment for texts in the Korean ELT curriculum.","LXPER Index 2.0: Improving Text Readability Assessment Model for L2 English Students in Korea Developing a text readability assessment model specifically for texts in a foreign English Language Training (ELT) curriculum has never had much attention in the field of Natural Language Processing. Hence, most developed models show extremely low accuracy for L2 English texts, up to the point where not many even serve as a fair comparison. In this paper, we investigate a text readability assessment model for L2 English learners in Korea. In accordance, we improve and expand the Text Corpus of the Korean ELT curriculum (CoKEC-text). Each text is labeled with its target grade level. We train our model with CoKEC-text and significantly improve the accuracy of readability assessment for texts in the Korean ELT curriculum.",Education
Flexibly Instructable Agents,"This paper presents an approach to learning from situated, interactive tutorial instruction within an ongoing agent. Tutorial instruction is a flexible (and thus powerful) paradigm for teaching tasks because it allows an instructor to communicate whatever types of knowledge an agent might need in whatever situations might arise. To support this flexibility, however, the agent must be able to learn multiple kinds of knowledge from a broad range of instructional interactions. Our approach, called situated explanation, achieves such learning through a combination of analytic and inductive techniques. It combines a form of explanation-based learning that is situated for each instruction with a full suite of contextually guided responses to incomplete explanations. The approach is implemented in an agent called Instructo-Soar that learns hierarchies of new tasks and other domain knowledge from interactive natural language instructions. Instructo-Soar meets three key requirements of flexible instructability that distinguish it from previous systems: (1) it can take known or unknown commands at any instruction point; (2) it can handle instructions that apply to either its current situation or to a hypothetical situation specified in language (as in, for instance, conditional instructions); and (3) it can learn, from instructions, each class of knowledge it uses to perform tasks.","Flexibly Instructable Agents This paper presents an approach to learning from situated, interactive tutorial instruction within an ongoing agent. Tutorial instruction is a flexible (and thus powerful) paradigm for teaching tasks because it allows an instructor to communicate whatever types of knowledge an agent might need in whatever situations might arise. To support this flexibility, however, the agent must be able to learn multiple kinds of knowledge from a broad range of instructional interactions. Our approach, called situated explanation, achieves such learning through a combination of analytic and inductive techniques. It combines a form of explanation-based learning that is situated for each instruction with a full suite of contextually guided responses to incomplete explanations. The approach is implemented in an agent called Instructo-Soar that learns hierarchies of new tasks and other domain knowledge from interactive natural language instructions. Instructo-Soar meets three key requirements of flexible instructability that distinguish it from previous systems: (1) it can take known or unknown commands at any instruction point; (2) it can handle instructions that apply to either its current situation or to a hypothetical situation specified in language (as in, for instance, conditional instructions); and (3) it can learn, from instructions, each class of knowledge it uses to perform tasks.",Technology
Conditional Expressions for Blind Deconvolution: Derivative form,We developed novel conditional expressions (CEs) for Lane and Bates blind deconvolution. The CEs are given in term of the derivatives of the zero-values of the z-transform of given images. The CEs make it possible to automatically detect multiple blur convolved in the given images all at once without performing any analysis of the zero-sheets of the given images. We illustrate the multiple blur-detection by the CEs for a model image,Conditional Expressions for Blind Deconvolution: Derivative form We developed novel conditional expressions (CEs) for Lane and Bates blind deconvolution. The CEs are given in term of the derivatives of the zero-values of the z-transform of given images. The CEs make it possible to automatically detect multiple blur convolved in the given images all at once without performing any analysis of the zero-sheets of the given images. We illustrate the multiple blur-detection by the CEs for a model image,Technology
Functional Multi-Layer Perceptron: a Nonlinear Tool for Functional Data Analysis,"In this paper, we study a natural extension of Multi-Layer Perceptrons (MLP) to functional inputs. We show that fundamental results for classical MLP can be extended to functional MLP. We obtain universal approximation results that show the expressive power of functional MLP is comparable to that of numerical MLP. We obtain consistency results which imply that the estimation of optimal parameters for functional MLP is statistically well defined. We finally show on simulated and real world data that the proposed model performs in a very satisfactory way.","Functional Multi-Layer Perceptron: a Nonlinear Tool for Functional Data Analysis In this paper, we study a natural extension of Multi-Layer Perceptrons (MLP) to functional inputs. We show that fundamental results for classical MLP can be extended to functional MLP. We obtain universal approximation results that show the expressive power of functional MLP is comparable to that of numerical MLP. We obtain consistency results which imply that the estimation of optimal parameters for functional MLP is statistically well defined. We finally show on simulated and real world data that the proposed model performs in a very satisfactory way.",Technology
Emerging trends on the topic of Information Technology in the field of Educational Sciences: a bibliometric exploration,"The paper presents a bibliometric analysis on the topic of Information Technology (IT) in the field of Educational Sciences, aimed at envisioning the research emerging trends. The ERIC data base is used as a consultation source; the results were subjected to productivity by authors, journals, and term co-occurrence analysis indicators for the period 2009-2013. The productivity of Computers  Education, and Turkish Online Journal of Educational Technology-TOJET, as well as the preceding authors from Canada, have been emphasized. The more used terms are the following: Information technology, foreign countries, educational technology, technology integration, and student attitudes. Researches performed here seem to have a largely qualitative character, highlighting computers and internet as the mostly explored technological objects. The largest subject matter trend refers to the integration of IT in the higher education learning context, and its incidence over the teaching methods.","Emerging trends on the topic of Information Technology in the field of Educational Sciences: a bibliometric exploration The paper presents a bibliometric analysis on the topic of Information Technology (IT) in the field of Educational Sciences, aimed at envisioning the research emerging trends. The ERIC data base is used as a consultation source; the results were subjected to productivity by authors, journals, and term co-occurrence analysis indicators for the period 2009-2013. The productivity of Computers  Education, and Turkish Online Journal of Educational Technology-TOJET, as well as the preceding authors from Canada, have been emphasized. The more used terms are the following: Information technology, foreign countries, educational technology, technology integration, and student attitudes. Researches performed here seem to have a largely qualitative character, highlighting computers and internet as the mostly explored technological objects. The largest subject matter trend refers to the integration of IT in the higher education learning context, and its incidence over the teaching methods.",Education
Aspects of Sustainable Test Processes,"Testing is a core software development activity that has huge potential to make software development more sustainable. In this paper, we discuss how environmental, social, economic, and technical sustainability map onto the activities of test planning, design, execution, and evaluation.","Aspects of Sustainable Test Processes Testing is a core software development activity that has huge potential to make software development more sustainable. In this paper, we discuss how environmental, social, economic, and technical sustainability map onto the activities of test planning, design, execution, and evaluation.",Environment
Universality and Scale Invariance in Hourly Rainfall,We show that the hourly rainfall rate distribution can be described by a simple power law to a good approximation. We show that the exponent of the distribution in tropics is universal and is equal to 1.13pm 0.11. At higher latitudes the exponent increases and is found to lie in the range 1.3-1.6.,Universality and Scale Invariance in Hourly Rainfall We show that the hourly rainfall rate distribution can be described by a simple power law to a good approximation. We show that the exponent of the distribution in tropics is universal and is equal to 1.13pm 0.11. At higher latitudes the exponent increases and is found to lie in the range 1.3-1.6.,Environment
Large Language Models in Education: Vision and Opportunities,"With the rapid development of artificial intelligence technology, large language models (LLMs) have become a hot research topic. Education plays an important role in human social development and progress. Traditional education faces challenges such as individual student differences, insufficient allocation of teaching resources, and assessment of teaching effectiveness. Therefore, the applications of LLMs in the field of digitalsmart education have broad prospects. The research on educational large models (EduLLMs) is constantly evolving, providing new methods and approaches to achieve personalized learning, intelligent tutoring, and educational assessment goals, thereby improving the quality of education and the learning experience. This article aims to investigate and summarize the application of LLMs in smart education. It first introduces the research background and motivation of LLMs and explains the essence of LLMs. It then discusses the relationship between digital education and EduLLMs and summarizes the current research status of educational large models. The main contributions are the systematic summary and vision of the research background, motivation, and application of large models for education (LLM4Edu). By reviewing existing research, this article provides guidance and insights for educators, researchers, and policy-makers to gain a deep understanding of the potential and challenges of LLM4Edu. It further provides guidance for further advancing the development and application of LLM4Edu, while still facing technical, ethical, and practical challenges requiring further research and exploration.","Large Language Models in Education: Vision and Opportunities With the rapid development of artificial intelligence technology, large language models (LLMs) have become a hot research topic. Education plays an important role in human social development and progress. Traditional education faces challenges such as individual student differences, insufficient allocation of teaching resources, and assessment of teaching effectiveness. Therefore, the applications of LLMs in the field of digitalsmart education have broad prospects. The research on educational large models (EduLLMs) is constantly evolving, providing new methods and approaches to achieve personalized learning, intelligent tutoring, and educational assessment goals, thereby improving the quality of education and the learning experience. This article aims to investigate and summarize the application of LLMs in smart education. It first introduces the research background and motivation of LLMs and explains the essence of LLMs. It then discusses the relationship between digital education and EduLLMs and summarizes the current research status of educational large models. The main contributions are the systematic summary and vision of the research background, motivation, and application of large models for education (LLM4Edu). By reviewing existing research, this article provides guidance and insights for educators, researchers, and policy-makers to gain a deep understanding of the potential and challenges of LLM4Edu. It further provides guidance for further advancing the development and application of LLM4Edu, while still facing technical, ethical, and practical challenges requiring further research and exploration.",Education
Scenario Forecast of Cross-border Electric Interconnection towards Renewables in South America,"Cross-border Electric Interconnection towards renewables is a promising solution for electric sector under the UN 2030 sustainable development goals which is widely promoted in emerging economies. This paper comprehensively investigates state of art in renewable resources and cross-border electric interconnection in South America. Based on the raw data collected from typical countries, a long-term scenario forecast methodology is applied to estimate key indicators of electric sector in target years, comparing the prospects of active promoting cross-border Interconnections Towards Renewables (ITR) scenario with Business as Usual (BAU) scenario in South America region. Key indicators including peak load, installed capacity, investment, and generation cost are forecasted and comparative analyzed by year 2035 and 2050. The comparative data analysis shows that by promoting cross-border interconnection towards renewables in South America, renewable resources can be highly utilized for energy supply, energy matrix can be optimized balanced, economics can be obviously driven and generation cost can be greatly reduced.","Scenario Forecast of Cross-border Electric Interconnection towards Renewables in South America Cross-border Electric Interconnection towards renewables is a promising solution for electric sector under the UN 2030 sustainable development goals which is widely promoted in emerging economies. This paper comprehensively investigates state of art in renewable resources and cross-border electric interconnection in South America. Based on the raw data collected from typical countries, a long-term scenario forecast methodology is applied to estimate key indicators of electric sector in target years, comparing the prospects of active promoting cross-border Interconnections Towards Renewables (ITR) scenario with Business as Usual (BAU) scenario in South America region. Key indicators including peak load, installed capacity, investment, and generation cost are forecasted and comparative analyzed by year 2035 and 2050. The comparative data analysis shows that by promoting cross-border interconnection towards renewables in South America, renewable resources can be highly utilized for energy supply, energy matrix can be optimized balanced, economics can be obviously driven and generation cost can be greatly reduced.",Environment
Identifying treatment response subgroups in observational time-to-event data,"Identifying patient subgroups with different treatment responses is an important task to inform medical recommendations, guidelines, and the design of future clinical trials. Existing approaches for treatment effect estimation primarily rely on Randomised Controlled Trials (RCTs), which are often limited by insufficient power, multiple comparisons, and unbalanced covariates. In addition, RCTs tend to feature more homogeneous patient groups, making them less relevant for uncovering subgroups in the population encountered in real-world clinical practice. Subgroup analyses established for RCTs suffer from significant statistical biases when applied to observational studies, which benefit from larger and more representative populations. Our work introduces a novel, outcome-guided, subgroup analysis strategy for identifying subgroups of treatment response in both RCTs and observational studies alike. It hence positions itself in-between individualised and average treatment effect estimation to uncover patient subgroups with distinct treatment responses, critical for actionable insights that may influence treatment guidelines. In experiments, our approach significantly outperforms the current state-of-the-art method for subgroup analysis in both randomised and observational treatment regimes.","Identifying treatment response subgroups in observational time-to-event data Identifying patient subgroups with different treatment responses is an important task to inform medical recommendations, guidelines, and the design of future clinical trials. Existing approaches for treatment effect estimation primarily rely on Randomised Controlled Trials (RCTs), which are often limited by insufficient power, multiple comparisons, and unbalanced covariates. In addition, RCTs tend to feature more homogeneous patient groups, making them less relevant for uncovering subgroups in the population encountered in real-world clinical practice. Subgroup analyses established for RCTs suffer from significant statistical biases when applied to observational studies, which benefit from larger and more representative populations. Our work introduces a novel, outcome-guided, subgroup analysis strategy for identifying subgroups of treatment response in both RCTs and observational studies alike. It hence positions itself in-between individualised and average treatment effect estimation to uncover patient subgroups with distinct treatment responses, critical for actionable insights that may influence treatment guidelines. In experiments, our approach significantly outperforms the current state-of-the-art method for subgroup analysis in both randomised and observational treatment regimes.",Healthcare
Paving the Way for Image Understanding: A New Kind of Image Decomposition is Desired,"In this paper we present an unconventional image segmentation approach which is devised to meet the requirements of image understanding and pattern recognition tasks. Generally image understanding assumes interplay of two sub-processes: image information content discovery and image information content interpretation. Despite of its widespread use, the notion of image information content is still ill defined, intuitive, and ambiguous. Most often, it is used in the Shannons sense, which means information content assessment averaged over the whole signal ensemble. Humans, however,rarely resort to such estimates. They are very effective in decomposing images into their meaningful constituents and focusing attention to the perceptually relevant image parts. We posit that following the latest findings in human attention vision studies and the concepts of Kolmogorovs complexity theory an unorthodox segmentation approach can be proposed that provides effective image decomposition to information preserving image fragments well suited for subsequent image interpretation. We provide some illustrative examples, demonstrating effectiveness of this approach.","Paving the Way for Image Understanding: A New Kind of Image Decomposition is Desired In this paper we present an unconventional image segmentation approach which is devised to meet the requirements of image understanding and pattern recognition tasks. Generally image understanding assumes interplay of two sub-processes: image information content discovery and image information content interpretation. Despite of its widespread use, the notion of image information content is still ill defined, intuitive, and ambiguous. Most often, it is used in the Shannons sense, which means information content assessment averaged over the whole signal ensemble. Humans, however,rarely resort to such estimates. They are very effective in decomposing images into their meaningful constituents and focusing attention to the perceptually relevant image parts. We posit that following the latest findings in human attention vision studies and the concepts of Kolmogorovs complexity theory an unorthodox segmentation approach can be proposed that provides effective image decomposition to information preserving image fragments well suited for subsequent image interpretation. We provide some illustrative examples, demonstrating effectiveness of this approach.",Technology
Impact of Financial Literacy on Investment Decisions and Stock Market Participation using Extreme Learning Machines,"The stock market has become an increasingly popular investment option among new generations, with individuals exploring more complex assets. This rise in retail investors participation necessitates a deeper understanding of the driving factors behind this trend and the role of financial literacy in enhancing investment decisions. This study aims to investigate how financial literacy influences financial decision-making and stock market participation. By identifying key barriers and motivators, the findings can provide valuable insights for individuals and policymakers to promote informed investing practices. Our research is qualitative in nature, utilizing data collected from social media platforms to analyze real-time investor behavior and attitudes. This approach allows us to capture the nuanced ways in which financial literacy impacts investment choices and participation in the stock market. The findings indicate that financial literacy plays a critical role in stock market participation and financial decision-making. Key barriers to participation include low financial literacy, while increased financial knowledge enhances investment confidence and decision-making. Additionally, behavioral finance factors and susceptibility to financial scams are significantly influenced by levels of financial literacy. These results underscore the importance of targeted financial education programs to improve financial literacy and empower individuals to participate effectively in the stock market.","Impact of Financial Literacy on Investment Decisions and Stock Market Participation using Extreme Learning Machines The stock market has become an increasingly popular investment option among new generations, with individuals exploring more complex assets. This rise in retail investors participation necessitates a deeper understanding of the driving factors behind this trend and the role of financial literacy in enhancing investment decisions. This study aims to investigate how financial literacy influences financial decision-making and stock market participation. By identifying key barriers and motivators, the findings can provide valuable insights for individuals and policymakers to promote informed investing practices. Our research is qualitative in nature, utilizing data collected from social media platforms to analyze real-time investor behavior and attitudes. This approach allows us to capture the nuanced ways in which financial literacy impacts investment choices and participation in the stock market. The findings indicate that financial literacy plays a critical role in stock market participation and financial decision-making. Key barriers to participation include low financial literacy, while increased financial knowledge enhances investment confidence and decision-making. Additionally, behavioral finance factors and susceptibility to financial scams are significantly influenced by levels of financial literacy. These results underscore the importance of targeted financial education programs to improve financial literacy and empower individuals to participate effectively in the stock market.",Finance
"Uncertainty, volatility and the persistence norms of financial time series","Norms of Persistent Homology introduced in topological data analysis are seen as indicators of system instability, analogous to the changing predictability that is captured in financial market uncertainty indexes. This paper demonstrates norms from the financial markets are significant in explaining financial uncertainty, whilst macroeconomic uncertainty is only explainable by market volatility. Meanwhile, volatility is insignificant in the determination of norms when uncertainty enters the regression. Persistence norms therefore have potential as a further tool in asset pricing, and also as a means of capturing signals from financial time series beyond volatility.","Uncertainty, volatility and the persistence norms of financial time series Norms of Persistent Homology introduced in topological data analysis are seen as indicators of system instability, analogous to the changing predictability that is captured in financial market uncertainty indexes. This paper demonstrates norms from the financial markets are significant in explaining financial uncertainty, whilst macroeconomic uncertainty is only explainable by market volatility. Meanwhile, volatility is insignificant in the determination of norms when uncertainty enters the regression. Persistence norms therefore have potential as a further tool in asset pricing, and also as a means of capturing signals from financial time series beyond volatility.",Finance
Surveying Instructors Attitudes and Approaches to Teaching Quantum Mechanics,"Understanding instructor attitudes and approaches to teaching quantum mechanics can be helpful in developing research-based learning tools. Here we discuss the findings from a survey in which 13 instructors reflected on issues related to quantum mechanics teaching. Topics included opinions about the goals of a quantum mechanics course, general challenges in teaching the subject, student preparation for the course, comparison between their own learning of quantum mechanics vs. how they teach it and the extent to which contemporary topics are incorporated into the syllabus.","Surveying Instructors Attitudes and Approaches to Teaching Quantum Mechanics Understanding instructor attitudes and approaches to teaching quantum mechanics can be helpful in developing research-based learning tools. Here we discuss the findings from a survey in which 13 instructors reflected on issues related to quantum mechanics teaching. Topics included opinions about the goals of a quantum mechanics course, general challenges in teaching the subject, student preparation for the course, comparison between their own learning of quantum mechanics vs. how they teach it and the extent to which contemporary topics are incorporated into the syllabus.",Education
Does Logarithm Transformation of Microarray Data Affect Ranking Order of Differentially Expressed Genes?,"A common practice in microarray analysis is to transform the microarray raw data (light intensity) by a logarithmic transformation, and the justification for this transformation is to make the distribution more symmetric and Gaussian-like. Since this transformation is not universally practiced in all microarray analysis, we examined whether the discrepancy of this treatment of raw data affect the high level analysis result. In particular, whether the differentially expressed genes as obtained by t-test, regularized t-test, or logistic regression have altered rank orders due to presence or absence of the transformation. We show that as much as 20--40 of significant genes are discordant (significant only in one form of the data and not in both), depending on the test being used and the threshold value for claiming significance. The t-test is more likely to be affected by logarithmic transformation than logistic regression, and regularized t-test more affected than t-test. On the other hand, the very top ranking genes (e.g. up to top 20--50 genes, depending on the test) are not affected by the logarithmic transformation.","Does Logarithm Transformation of Microarray Data Affect Ranking Order of Differentially Expressed Genes? A common practice in microarray analysis is to transform the microarray raw data (light intensity) by a logarithmic transformation, and the justification for this transformation is to make the distribution more symmetric and Gaussian-like. Since this transformation is not universally practiced in all microarray analysis, we examined whether the discrepancy of this treatment of raw data affect the high level analysis result. In particular, whether the differentially expressed genes as obtained by t-test, regularized t-test, or logistic regression have altered rank orders due to presence or absence of the transformation. We show that as much as 20--40 of significant genes are discordant (significant only in one form of the data and not in both), depending on the test being used and the threshold value for claiming significance. The t-test is more likely to be affected by logarithmic transformation than logistic regression, and regularized t-test more affected than t-test. On the other hand, the very top ranking genes (e.g. up to top 20--50 genes, depending on the test) are not affected by the logarithmic transformation.",Healthcare
Pricing Derivatives in Hermite Markets,"We introduce Hermite fractional financial markets, where market uncertainties are described by multidimensional Hermite motions. Hermite markets include as particular cases financial markets driven by multivariate fractional Brownian motion and multivariate Rosenblatt motion. Conditions for no-arbitrage and market completeness for Hermite markets are derived. Perpetual derivatives, bonds forwards, and futures are priced. The corresponding partial and partial-differential equations are derived.","Pricing Derivatives in Hermite Markets We introduce Hermite fractional financial markets, where market uncertainties are described by multidimensional Hermite motions. Hermite markets include as particular cases financial markets driven by multivariate fractional Brownian motion and multivariate Rosenblatt motion. Conditions for no-arbitrage and market completeness for Hermite markets are derived. Perpetual derivatives, bonds forwards, and futures are priced. The corresponding partial and partial-differential equations are derived.",Finance
Multivariate Modeling for Sustainable and Resilient Infrastructure Systems and Communities,"Sustainability and resilience of urban systems are multifaceted concepts, requiring information about multiple system attributes to adequately evaluate and characterize. However, despite the scientific consensus on the multivariate nature of these concepts, many of the existing techniques to model urban sustainability and resilience are unidimensional in nature, focusing on a characterizing a single element of highly interconnected urban systems. We champion a paradigm shift in modeling urban sustainability and resilience, using an integrated approach to simultaneously estimate multiple interconnected (correlated) system attributes of sustainability and resilience as a function of key environmental factors. We present a novel case study and review a few recent studies to illustrate the applicability and benefits of the multivariate approach to modeling urban sustainability and resilience. Our proposed framework can be utilized by infrastructure managers, urban planners, and researchers to conceptualize and assess urban sustainability and resilience more holistically, and to better understand the key factors in advancing the sustainability and resilience of infrastructure systems.","Multivariate Modeling for Sustainable and Resilient Infrastructure Systems and Communities Sustainability and resilience of urban systems are multifaceted concepts, requiring information about multiple system attributes to adequately evaluate and characterize. However, despite the scientific consensus on the multivariate nature of these concepts, many of the existing techniques to model urban sustainability and resilience are unidimensional in nature, focusing on a characterizing a single element of highly interconnected urban systems. We champion a paradigm shift in modeling urban sustainability and resilience, using an integrated approach to simultaneously estimate multiple interconnected (correlated) system attributes of sustainability and resilience as a function of key environmental factors. We present a novel case study and review a few recent studies to illustrate the applicability and benefits of the multivariate approach to modeling urban sustainability and resilience. Our proposed framework can be utilized by infrastructure managers, urban planners, and researchers to conceptualize and assess urban sustainability and resilience more holistically, and to better understand the key factors in advancing the sustainability and resilience of infrastructure systems.",Environment
Econophysics Macroeconomic Model,"This paper presents macroeconomic model that is based on parallels between macroeconomic multi-agent systems and multi-particle systems. We use risk ratings of economic agents as their coordinates on economic space. Aggregates of economic or financial variables like Investment, Assets, Demand, Credits and etc. of economic agents near point x define corresponding macroeconomic variables as functions of time t and coordinates x on economic space. Parallels between multi-agent and multi-particle systems on economic space allow describe transition from economic kinetic-like to economic hydrodynamic-like approximation and derive macroeconomic hydrodynamic-like equations on economic space. Economic or financial transactions between economic agents determine evolution of macroeconomic variables This paper describes local macroeconomic approximation that takes into account transactions between economic agents with coordinates near same point x on economic space only and describes interaction between macroeconomic variables by linear differential operators. For simple model of interaction between macroeconomic variables as Demand on Investment and Interest Rate we derive hydrodynamic-like equations in a closed form. For perturbations of these macroeconomic variables we derive macroeconomic wave equations. Macroeconomic waves on economic space can propagate with exponential growth of amplitude and cause irregular time fluctuations of macroeconomic variables or induce economic crises.","Econophysics Macroeconomic Model This paper presents macroeconomic model that is based on parallels between macroeconomic multi-agent systems and multi-particle systems. We use risk ratings of economic agents as their coordinates on economic space. Aggregates of economic or financial variables like Investment, Assets, Demand, Credits and etc. of economic agents near point x define corresponding macroeconomic variables as functions of time t and coordinates x on economic space. Parallels between multi-agent and multi-particle systems on economic space allow describe transition from economic kinetic-like to economic hydrodynamic-like approximation and derive macroeconomic hydrodynamic-like equations on economic space. Economic or financial transactions between economic agents determine evolution of macroeconomic variables This paper describes local macroeconomic approximation that takes into account transactions between economic agents with coordinates near same point x on economic space only and describes interaction between macroeconomic variables by linear differential operators. For simple model of interaction between macroeconomic variables as Demand on Investment and Interest Rate we derive hydrodynamic-like equations in a closed form. For perturbations of these macroeconomic variables we derive macroeconomic wave equations. Macroeconomic waves on economic space can propagate with exponential growth of amplitude and cause irregular time fluctuations of macroeconomic variables or induce economic crises.",Finance
Probabilistic temperature forecasting: a summary of our recent research results,We summarise the main results from a number of our recent articles on the subject of probabilistic temperature forecasting.,Probabilistic temperature forecasting: a summary of our recent research results We summarise the main results from a number of our recent articles on the subject of probabilistic temperature forecasting.,Environment
Development of a Virtual Reality Application for Oculomotor Examination Education Based on Student-Centered Pedagogy,"This work-in-progress paper discusses the use of student-centered pedagogy to teach clinical oculomotor examination via Virtual Reality (VR). Traditional methods, such as PowerPoint slides and lab activities, are often insufficient for providing hands-on experience due to the high cost of clinical equipment. To address this, a VR-based application was developed using Unity and the HTC Vive Pro headset, offering a cost-effective solution for practical learning. The VR app allows students to engage in oculomotor examinations at their own pace, accommodating diverse backgrounds and learning preferences. This application enables students to collect and analyze data, providing a realistic simulation of clinical practice. The user study results from Doctor of Physical Therapy students indicate a high preference for the flexibility offered by the VR app, suggesting its potential as a valuable educational tool. Additionally, the paper explores the broader implications of using VR in engineering and computing education, highlighting the benefits of immersive, interactive learning environments.","Development of a Virtual Reality Application for Oculomotor Examination Education Based on Student-Centered Pedagogy This work-in-progress paper discusses the use of student-centered pedagogy to teach clinical oculomotor examination via Virtual Reality (VR). Traditional methods, such as PowerPoint slides and lab activities, are often insufficient for providing hands-on experience due to the high cost of clinical equipment. To address this, a VR-based application was developed using Unity and the HTC Vive Pro headset, offering a cost-effective solution for practical learning. The VR app allows students to engage in oculomotor examinations at their own pace, accommodating diverse backgrounds and learning preferences. This application enables students to collect and analyze data, providing a realistic simulation of clinical practice. The user study results from Doctor of Physical Therapy students indicate a high preference for the flexibility offered by the VR app, suggesting its potential as a valuable educational tool. Additionally, the paper explores the broader implications of using VR in engineering and computing education, highlighting the benefits of immersive, interactive learning environments.",Education
Computational LPPL Fit to Financial Bubbles,"The log-periodic power law (LPPL) is a model of asset prices during endogenous bubbles. If the on-going development of a bubble is suspected, asset prices can be fit numerically to the LPPL law. The best solutions can then indicate whether a bubble is in progress and, if so, the bubble critical time (i.e., when the bubble is expected to burst). Consequently, the LPPL model is useful only if the data can be fit to the model with algorithms that are accurate and computationally efficient. In this paper, we address primarily the computational efficiency and secondarily the precision of the LPPL non-linear least-square fit. Specifically, we present a parallel Levenberg-Marquardt algorithm (LMA) for LPPL least-square fit that sped up computation of more than a factor of four over a sequential LMA on historical and synthetic price series. Additionally, we isolate a linear sub-structure of the LPPL least-square fit that can be paired with an exact computation of the Jacobian, give new settings for the Levenberg-Marquardt damping factor, and describe a heuristic method to choose initial solutions.","Computational LPPL Fit to Financial Bubbles The log-periodic power law (LPPL) is a model of asset prices during endogenous bubbles. If the on-going development of a bubble is suspected, asset prices can be fit numerically to the LPPL law. The best solutions can then indicate whether a bubble is in progress and, if so, the bubble critical time (i.e., when the bubble is expected to burst). Consequently, the LPPL model is useful only if the data can be fit to the model with algorithms that are accurate and computationally efficient. In this paper, we address primarily the computational efficiency and secondarily the precision of the LPPL non-linear least-square fit. Specifically, we present a parallel Levenberg-Marquardt algorithm (LMA) for LPPL least-square fit that sped up computation of more than a factor of four over a sequential LMA on historical and synthetic price series. Additionally, we isolate a linear sub-structure of the LPPL least-square fit that can be paired with an exact computation of the Jacobian, give new settings for the Levenberg-Marquardt damping factor, and describe a heuristic method to choose initial solutions.",Finance
Communication Systems for Grid Integration of Renewable Energy Resources,"There is growing interest in renewable energy around the world. Since most renewable sources are intermittent in nature, it is a challenging task to integrate renewable energy resources into the power grid infrastructure. In this grid integration, communication systems are crucial technologies, which enable the accommodation of distributed renewable energy generation and play extremely important role in monitoring, operating, and protecting both renewable energy generators and power systems. In this paper, we review some communication technologies available for grid integration of renewable energy resources. Then, we present the communication systems used in a real renewable energy project, Bear Mountain Wind Farm (BMW) in British Columbia, Canada. In addition, we present the communication systems used in Photovoltaic Power Systems (PPS). Finally, we outline some research challenges and possible solutions about the communication systems for grid integration of renewable energy resources.","Communication Systems for Grid Integration of Renewable Energy Resources There is growing interest in renewable energy around the world. Since most renewable sources are intermittent in nature, it is a challenging task to integrate renewable energy resources into the power grid infrastructure. In this grid integration, communication systems are crucial technologies, which enable the accommodation of distributed renewable energy generation and play extremely important role in monitoring, operating, and protecting both renewable energy generators and power systems. In this paper, we review some communication technologies available for grid integration of renewable energy resources. Then, we present the communication systems used in a real renewable energy project, Bear Mountain Wind Farm (BMW) in British Columbia, Canada. In addition, we present the communication systems used in Photovoltaic Power Systems (PPS). Finally, we outline some research challenges and possible solutions about the communication systems for grid integration of renewable energy resources.",Environment
The Sustainable Future is now: a dynamic model to advance investments in PV and Energy Storage,"We examine the relationship among photovoltaic (PV) investments, energy production, and environmental impact using a dynamic optimization model. Our findings show that increasing investment in renewables supports both energy generation and ecological sustainability, with the optimal path depending on policy priorities. Our analysis demonstrates that the economic and technological conditions for a transition to PV energy are already in place, challenging the idea that renewables will only become competitive in the future. We also account for the fact that PV optimality conditions improve over time as storage technology efficiency increases and production costs decrease. In this perspective we find that energy storage may be a more effective policy tool than carbon taxation for cutting emissions, as it faces less political resistance and further strengthens the long-term viability of renewable energy. Policy insights of the paper capture the evolving competitiveness of PV and its role in accelerating the energy transition. They also provide policymakers with strategies to align economic growth with long-term sustainability through renewable energy investments.","The Sustainable Future is now: a dynamic model to advance investments in PV and Energy Storage We examine the relationship among photovoltaic (PV) investments, energy production, and environmental impact using a dynamic optimization model. Our findings show that increasing investment in renewables supports both energy generation and ecological sustainability, with the optimal path depending on policy priorities. Our analysis demonstrates that the economic and technological conditions for a transition to PV energy are already in place, challenging the idea that renewables will only become competitive in the future. We also account for the fact that PV optimality conditions improve over time as storage technology efficiency increases and production costs decrease. In this perspective we find that energy storage may be a more effective policy tool than carbon taxation for cutting emissions, as it faces less political resistance and further strengthens the long-term viability of renewable energy. Policy insights of the paper capture the evolving competitiveness of PV and its role in accelerating the energy transition. They also provide policymakers with strategies to align economic growth with long-term sustainability through renewable energy investments.",Finance
Engineering Educators Perspectives on the Impact of Generative AI in Higher Education,"The introduction of generative artificial intelligence (GenAI) has been met with a mix of reactions by higher education institutions, ranging from consternation and resistance to wholehearted acceptance. Previous work has looked at the discourse and policies adopted by universities across the U.S. as well as educators, along with the inclusion of GenAI-related content and topics in higher education. Building on previous research, this study reports findings from a survey of engineering educators on their use of and perspectives toward generative AI. Specifically, we surveyed 98 educators from engineering, computer science, and education who participated in a workshop on GenAI in Engineering Education to learn about their perspectives on using these tools for teaching and research. We asked them about their use of and comfort with GenAI, their overall perspectives on GenAI, the challenges and potential harms of using it for teaching, learning, and research, and examined whether their approach to using and integrating GenAI in their classroom influenced their experiences with GenAI and perceptions of it. Consistent with other research in GenAI education, we found that while the majority of participants were somewhat familiar with GenAI, reported use varied considerably. We found that educators harbored mostly hopeful and positive views about the potential of GenAI. We also found that those who engaged more with their students on the topic of GenAI, tend to be more positive about its contribution to learning, while also being more attuned to its potential abuses. These findings suggest that integrating and engaging with generative AI is essential to foster productive interactions between instructors and students around this technology.","Engineering Educators Perspectives on the Impact of Generative AI in Higher Education The introduction of generative artificial intelligence (GenAI) has been met with a mix of reactions by higher education institutions, ranging from consternation and resistance to wholehearted acceptance. Previous work has looked at the discourse and policies adopted by universities across the U.S. as well as educators, along with the inclusion of GenAI-related content and topics in higher education. Building on previous research, this study reports findings from a survey of engineering educators on their use of and perspectives toward generative AI. Specifically, we surveyed 98 educators from engineering, computer science, and education who participated in a workshop on GenAI in Engineering Education to learn about their perspectives on using these tools for teaching and research. We asked them about their use of and comfort with GenAI, their overall perspectives on GenAI, the challenges and potential harms of using it for teaching, learning, and research, and examined whether their approach to using and integrating GenAI in their classroom influenced their experiences with GenAI and perceptions of it. Consistent with other research in GenAI education, we found that while the majority of participants were somewhat familiar with GenAI, reported use varied considerably. We found that educators harbored mostly hopeful and positive views about the potential of GenAI. We also found that those who engaged more with their students on the topic of GenAI, tend to be more positive about its contribution to learning, while also being more attuned to its potential abuses. These findings suggest that integrating and engaging with generative AI is essential to foster productive interactions between instructors and students around this technology.",Education
Wrap-Up: a Trainable Discourse Module for Information Extraction,"The vast amounts of on-line text now available have led to renewed interest in information extraction (IE) systems that analyze unrestricted text, producing a structured representation of selected information from the text. This paper presents a novel approach that uses machine learning to acquire knowledge for some of the higher level IE processing. Wrap-Up is a trainable IE discourse component that makes intersentential inferences and identifies logical relations among information extracted from the text. Previous corpus-based approaches were limited to lower level processing such as part-of-speech tagging, lexical disambiguation, and dictionary construction. Wrap-Up is fully trainable, and not only automatically decides what classifiers are needed, but even derives the feature set for each classifier automatically. Performance equals that of a partially trainable discourse module requiring manual customization for each domain.","Wrap-Up: a Trainable Discourse Module for Information Extraction The vast amounts of on-line text now available have led to renewed interest in information extraction (IE) systems that analyze unrestricted text, producing a structured representation of selected information from the text. This paper presents a novel approach that uses machine learning to acquire knowledge for some of the higher level IE processing. Wrap-Up is a trainable IE discourse component that makes intersentential inferences and identifies logical relations among information extracted from the text. Previous corpus-based approaches were limited to lower level processing such as part-of-speech tagging, lexical disambiguation, and dictionary construction. Wrap-Up is fully trainable, and not only automatically decides what classifiers are needed, but even derives the feature set for each classifier automatically. Performance equals that of a partially trainable discourse module requiring manual customization for each domain.",Technology
A Parallel Algorithm for Dilated Contour Extraction from Bilevel Images,"We describe a simple, but efficient algorithm for the generation of dilated contours from bilevel images. The initial part of the contour extraction is explained to be a good candidate for parallel computer code generation. The remainder of the algorithm is of linear nature.","A Parallel Algorithm for Dilated Contour Extraction from Bilevel Images We describe a simple, but efficient algorithm for the generation of dilated contours from bilevel images. The initial part of the contour extraction is explained to be a good candidate for parallel computer code generation. The remainder of the algorithm is of linear nature.",Technology
The CALMA system: an artificial neural network method for detecting masses and microcalcifications in digitized mammograms,"The CALMA (Computer Assisted Library for MAmmography) project is a five years plan developed in a physics research frame in collaboration between INFN (Istituto Nazionale di Fisica Nucleare) and many Italian hospitals. At present a large database of digitized mammographic images (more than 6000) was collected and a software based on neural network algorithms for the search of suspicious breast lesions was developed. Two tools are available: a microcalcification clusters hunter, based on supervised and unsupervised feedforward neural network, and a massive lesions searcher, based on a hibrid approach. Both the algorithms analyzed preprocessed digitized images by high frequency filters. Clinical tests were performed to evaluate sensitivity and specificity of the system, considering the system as alone and as secon reader. Results show that the system is ready to be implemented by medical industry. The CALMA project, just ended, has its natural development in the GPCALMA (Grid Platform for CALMA) project, where distributed users join common resources (images, tools, statistical analysis).","The CALMA system: an artificial neural network method for detecting masses and microcalcifications in digitized mammograms The CALMA (Computer Assisted Library for MAmmography) project is a five years plan developed in a physics research frame in collaboration between INFN (Istituto Nazionale di Fisica Nucleare) and many Italian hospitals. At present a large database of digitized mammographic images (more than 6000) was collected and a software based on neural network algorithms for the search of suspicious breast lesions was developed. Two tools are available: a microcalcification clusters hunter, based on supervised and unsupervised feedforward neural network, and a massive lesions searcher, based on a hibrid approach. Both the algorithms analyzed preprocessed digitized images by high frequency filters. Clinical tests were performed to evaluate sensitivity and specificity of the system, considering the system as alone and as secon reader. Results show that the system is ready to be implemented by medical industry. The CALMA project, just ended, has its natural development in the GPCALMA (Grid Platform for CALMA) project, where distributed users join common resources (images, tools, statistical analysis).",Healthcare
A Barrier Penetration Model for DNA Double Strand Separation,A barrier penetration model has been proposed to explain the spontaneous melting of the DNA oligomers into two separate single strands whereas the partially melted intermediate states are shown to be the bound state solution of the same effective potential that generates the barrier.,A Barrier Penetration Model for DNA Double Strand Separation A barrier penetration model has been proposed to explain the spontaneous melting of the DNA oligomers into two separate single strands whereas the partially melted intermediate states are shown to be the bound state solution of the same effective potential that generates the barrier.,Healthcare
A comparison of two operational wave assimilation methods,"A comparison is carried out between two operational wave forecastingassimilation models for the North Sea, with the emphasis on the assimilation schemes. One model is the WAM model, in combination with an optimal interpolation method (OIP). The other model, DASWAM, consists of the third generation wave model PHIDIAS in combination with an approximate implementation of the adjoint method. In an experiment over the period February 19 - March 30, 1993, the models are driven by the same wind field (HIRLAM analysis winds), and the same observation data set is assimilated. This set consists of a) spectra from three pitch-and-roll buoys and b) Synthetic Aperture Radar (SAR) spectra from the ERS-1 satellite. Three analysisforecast runs are performed: one without assimilation, one with assimilation of buoy measurements only, and one with all data assimilated. For validation, observations from four buoys, altimeter data from ERS-1 and Topex-Poseidon, and scatterometer data from ERS-1 are used. A detailed analysis of the Wadden Storm (February 20-22) shows the very different nature of the two assimilation schemes: the wave and wind field corrections of the WAMOIP scheme are all in the vicinity of the observations, whereas the DASWAM adjustments are more of a global nature. The impact of some individual buoy and SAR observations is visualized. A comparison of the performance of the two schemes is somewhat obscured by the very different behaviour of the two first-guess runs. A statistical analysis over the whole 39-day period gives the following results. In a comparison with buoy observations it is shown that a positive impact of wave data assimilation remains until about 12 hours in forecast in","A comparison of two operational wave assimilation methods A comparison is carried out between two operational wave forecastingassimilation models for the North Sea, with the emphasis on the assimilation schemes. One model is the WAM model, in combination with an optimal interpolation method (OIP). The other model, DASWAM, consists of the third generation wave model PHIDIAS in combination with an approximate implementation of the adjoint method. In an experiment over the period February 19 - March 30, 1993, the models are driven by the same wind field (HIRLAM analysis winds), and the same observation data set is assimilated. This set consists of a) spectra from three pitch-and-roll buoys and b) Synthetic Aperture Radar (SAR) spectra from the ERS-1 satellite. Three analysisforecast runs are performed: one without assimilation, one with assimilation of buoy measurements only, and one with all data assimilated. For validation, observations from four buoys, altimeter data from ERS-1 and Topex-Poseidon, and scatterometer data from ERS-1 are used. A detailed analysis of the Wadden Storm (February 20-22) shows the very different nature of the two assimilation schemes: the wave and wind field corrections of the WAMOIP scheme are all in the vicinity of the observations, whereas the DASWAM adjustments are more of a global nature. The impact of some individual buoy and SAR observations is visualized. A comparison of the performance of the two schemes is somewhat obscured by the very different behaviour of the two first-guess runs. A statistical analysis over the whole 39-day period gives the following results. In a comparison with buoy observations it is shown that a positive impact of wave data assimilation remains until about 12 hours in forecast in",Environment
Cooperative Planning of Renewable Generations for Interconnected Microgrids,"We study the renewable energy generations in Hong Kong based on realistic meteorological data, and find that different renewable sources exhibit diverse time-varying and location-dependent profiles. To efficiently explore and utilize the diverse renewable energy generations, we propose a theoretical framework for the cooperative planning of renewable generations in a system of interconnected microgrids. The cooperative framework considers the self-interested behaviors of microgrids, and incorporates both their long-term investment costs and short-term operational costs over the planning horizon. Specifically, interconnected microgrids jointly decide where and how much to deploy renewable energy generations, and how to split the associated investment cost. We show that the cooperative framework minimizes the overall system cost. We also design a fair cost sharing method based on Nash bargaining to incentivize cooperative planning, such that all microgrids will benefit from cooperative planning. Using realistic data obtained from the Hong Kong observatory, we validate the cooperative planning framework, and demonstrate that all microgrids benefit through the cooperation, and the overall system cost is reduced by 35.9 compared to the noncooperative planning benchmark.","Cooperative Planning of Renewable Generations for Interconnected Microgrids We study the renewable energy generations in Hong Kong based on realistic meteorological data, and find that different renewable sources exhibit diverse time-varying and location-dependent profiles. To efficiently explore and utilize the diverse renewable energy generations, we propose a theoretical framework for the cooperative planning of renewable generations in a system of interconnected microgrids. The cooperative framework considers the self-interested behaviors of microgrids, and incorporates both their long-term investment costs and short-term operational costs over the planning horizon. Specifically, interconnected microgrids jointly decide where and how much to deploy renewable energy generations, and how to split the associated investment cost. We show that the cooperative framework minimizes the overall system cost. We also design a fair cost sharing method based on Nash bargaining to incentivize cooperative planning, such that all microgrids will benefit from cooperative planning. Using realistic data obtained from the Hong Kong observatory, we validate the cooperative planning framework, and demonstrate that all microgrids benefit through the cooperation, and the overall system cost is reduced by 35.9 compared to the noncooperative planning benchmark.",Environment
Design of statistical quality control procedures using genetic algorithms,"In general, we can not use algebraic or enumerative methods to optimize a quality control (QC) procedure so as to detect the critical random and systematic analytical errors with stated probabilities, while the probability for false rejection is minimum. Genetic algorithms (GAs) offer an alternative, as they do not require knowledge of the objective function to be optimized and search through large parameter spaces quickly. To explore the application of GAs in statistical QC, we have developed an interactive GAs based computer program that designs a novel near optimal QC procedure, given an analytical process. The program uses the deterministic crowding algorithm. An illustrative application of the program suggests that it has the potential to design QC procedures that are significantly better than 45 alternative ones that are used in the clinical laboratories.","Design of statistical quality control procedures using genetic algorithms In general, we can not use algebraic or enumerative methods to optimize a quality control (QC) procedure so as to detect the critical random and systematic analytical errors with stated probabilities, while the probability for false rejection is minimum. Genetic algorithms (GAs) offer an alternative, as they do not require knowledge of the objective function to be optimized and search through large parameter spaces quickly. To explore the application of GAs in statistical QC, we have developed an interactive GAs based computer program that designs a novel near optimal QC procedure, given an analytical process. The program uses the deterministic crowding algorithm. An illustrative application of the program suggests that it has the potential to design QC procedures that are significantly better than 45 alternative ones that are used in the clinical laboratories.",Technology
Parametric Learning and Monte Carlo Optimization,"This paper uncovers and explores the close relationship between Monte Carlo Optimization of a parametrized integral (MCO), Parametric machine-Learning (PL), and blackbox or oracle-based optimization (BO). We make four contributions. First, we prove that MCO is mathematically identical to a broad class of PL problems. This identity potentially provides a new application domain for all broadly applicable PL techniques: MCO. Second, we introduce immediate sampling, a new version of the Probability Collectives (PC) algorithm for blackbox optimization. Immediate sampling transforms the original BO problem into an MCO problem. Accordingly, by combining these first two contributions, we can apply all PL techniques to BO. In our third contribution we validate this way of improving BO by demonstrating that cross-validation and bagging improve immediate sampling. Finally, conventional MC and MCO procedures ignore the relationship between the sample point locations and the associated values of the integrand; only the values of the integrand at those locations are considered. We demonstrate that one can exploit the sample location information using PL techniques, for example by forming a fit of the sample locations to the associated values of the integrand. This provides an additional way to apply PL techniques to improve MCO.","Parametric Learning and Monte Carlo Optimization This paper uncovers and explores the close relationship between Monte Carlo Optimization of a parametrized integral (MCO), Parametric machine-Learning (PL), and blackbox or oracle-based optimization (BO). We make four contributions. First, we prove that MCO is mathematically identical to a broad class of PL problems. This identity potentially provides a new application domain for all broadly applicable PL techniques: MCO. Second, we introduce immediate sampling, a new version of the Probability Collectives (PC) algorithm for blackbox optimization. Immediate sampling transforms the original BO problem into an MCO problem. Accordingly, by combining these first two contributions, we can apply all PL techniques to BO. In our third contribution we validate this way of improving BO by demonstrating that cross-validation and bagging improve immediate sampling. Finally, conventional MC and MCO procedures ignore the relationship between the sample point locations and the associated values of the integrand; only the values of the integrand at those locations are considered. We demonstrate that one can exploit the sample location information using PL techniques, for example by forming a fit of the sample locations to the associated values of the integrand. This provides an additional way to apply PL techniques to improve MCO.",Technology
Smoothing effect for spatially distributed renewable resources and its impact on power grid robustness,"In this paper, we show that spatial correlation of renewable energy outputs greatly influences the robustness of power grids. First, we propose a new index for the spatial correlation among renewable energy outputs. We find that the spatial correlation of renewable energy outputs in a short time-scale is as weak as that caused by independent random variables and that in a long time-scale is as strong as that under perfect synchronization. Then, by employing the topology of the power grid in eastern Japan, we analyze the robustness of the power grid with spatial correlation of renewable energy outputs. The analysis is performed by using a realistic differential-algebraic equations model and the result shows that the spatial correlation of the energy resources strongly degrades the robustness of the power grid. Our result suggests that the spatial correlation of the renewable energy outputs should be taken into account when estimating the stability of power grids.","Smoothing effect for spatially distributed renewable resources and its impact on power grid robustness In this paper, we show that spatial correlation of renewable energy outputs greatly influences the robustness of power grids. First, we propose a new index for the spatial correlation among renewable energy outputs. We find that the spatial correlation of renewable energy outputs in a short time-scale is as weak as that caused by independent random variables and that in a long time-scale is as strong as that under perfect synchronization. Then, by employing the topology of the power grid in eastern Japan, we analyze the robustness of the power grid with spatial correlation of renewable energy outputs. The analysis is performed by using a realistic differential-algebraic equations model and the result shows that the spatial correlation of the energy resources strongly degrades the robustness of the power grid. Our result suggests that the spatial correlation of the renewable energy outputs should be taken into account when estimating the stability of power grids.",Environment
Autonomous Curriculum Design via Relative Entropy Based Task Modifications,"Curriculum learning is a training method in which an agent is first trained on a curriculum of relatively simple tasks related to a target task in an effort to shorten the time required to train on the target task. Autonomous curriculum design involves the design of such curriculum with no reliance on human knowledge andor expertise. Finding an efficient and effective way of autonomously designing curricula remains an open problem. We propose a novel approach for automatically designing curricula by leveraging the learners uncertainty to select curricula tasks. Our approach measures the uncertainty in the learners policy using relative entropy, and guides the agent to states of high uncertainty to facilitate learning. Our algorithm supports the generation of autonomous curricula in a self-assessed manner by leveraging the learners past and current policies but it also allows the use of teacher guided design in an instructive setting. We provide theoretical guarantees for the convergence of our algorithm using two time-scale optimization processes. Results show that our algorithm outperforms randomly generated curriculum, and learning directly on the target task as well as the curriculum-learning criteria existing in literature. We also present two additional heuristic distance measures that could be combined with our relative-entropy approach for further performance improvements.","Autonomous Curriculum Design via Relative Entropy Based Task Modifications Curriculum learning is a training method in which an agent is first trained on a curriculum of relatively simple tasks related to a target task in an effort to shorten the time required to train on the target task. Autonomous curriculum design involves the design of such curriculum with no reliance on human knowledge andor expertise. Finding an efficient and effective way of autonomously designing curricula remains an open problem. We propose a novel approach for automatically designing curricula by leveraging the learners uncertainty to select curricula tasks. Our approach measures the uncertainty in the learners policy using relative entropy, and guides the agent to states of high uncertainty to facilitate learning. Our algorithm supports the generation of autonomous curricula in a self-assessed manner by leveraging the learners past and current policies but it also allows the use of teacher guided design in an instructive setting. We provide theoretical guarantees for the convergence of our algorithm using two time-scale optimization processes. Results show that our algorithm outperforms randomly generated curriculum, and learning directly on the target task as well as the curriculum-learning criteria existing in literature. We also present two additional heuristic distance measures that could be combined with our relative-entropy approach for further performance improvements.",Education
A robust tree method for pricing American options with CIR stochastic interest rate,We propose a robust and stable lattice method which permits to obtain very accurate American option prices in presence of CIR stochastic interest rate without any numerical restriction on its parameters. Numerical results show the reliability and the accuracy of the proposed method.,A robust tree method for pricing American options with CIR stochastic interest rate We propose a robust and stable lattice method which permits to obtain very accurate American option prices in presence of CIR stochastic interest rate without any numerical restriction on its parameters. Numerical results show the reliability and the accuracy of the proposed method.,Finance
Text Line Segmentation of Historical Documents: a Survey,"There is a huge amount of historical documents in libraries and in various National Archives that have not been exploited electronically. Although automatic reading of complete pages remains, in most cases, a long-term objective, tasks such as word spotting, textimage alignment, authentication and extraction of specific fields are in use today. For all these tasks, a major step is document segmentation into text lines. Because of the low quality and the complexity of these documents (background noise, artifacts due to aging, interfering lines),automatic text line segmentation remains an open research field. The objective of this paper is to present a survey of existing methods, developed during the last decade, and dedicated to documents of historical interest.","Text Line Segmentation of Historical Documents: a Survey There is a huge amount of historical documents in libraries and in various National Archives that have not been exploited electronically. Although automatic reading of complete pages remains, in most cases, a long-term objective, tasks such as word spotting, textimage alignment, authentication and extraction of specific fields are in use today. For all these tasks, a major step is document segmentation into text lines. Because of the low quality and the complexity of these documents (background noise, artifacts due to aging, interfering lines),automatic text line segmentation remains an open research field. The objective of this paper is to present a survey of existing methods, developed during the last decade, and dedicated to documents of historical interest.",Technology
Effective Sample Size: Quick Estimation of the Effect of Related Samples in Genetic Case-Control Association Analyses,"Affected relatives are essential for pedigree linkage analysis, however, they cause a violation of the independent sample assumption in case-control association studies. To avoid the correlation between samples, a common practice is to take only one affected sample per pedigree in association analysis. Although several methods exist in handling correlated samples, they are still not widely used in part because these are not easily implemented, or because they are not widely known. We advocate the effective sample size method as a simple and accessible approach for case-control association analysis with correlated samples. This method modifies the chi-square test statistic, p-value, and 95 confidence interval of the odds-ratio by replacing the apparent number of allele or genotype counts with the effective ones in the standard formula, without the need for specialized computer programs. We present a simple formula for calculating effective sample size for many types of relative pairs and relative sets. For allele frequency estimation, the effective sample size method captures the variance inflation exactly. For genotype frequency, simulations showed that effective sample size provides a satisfactory approximation. A gene which is previously identified as a type 1 diabetes susceptibility locus, the interferon-induced helicase gene (IFIH1), is shown to be significantly associated with rheumatoid arthritis when the effective sample size method is applied. This significant association is not established if only one affected sib per pedigree were used in the association analysis. Relationship between the effective sample size method and other methods -- the generalized estimation equation, variance of eigenvalues for correlation matrices, and genomic controls -- are discussed.","Effective Sample Size: Quick Estimation of the Effect of Related Samples in Genetic Case-Control Association Analyses Affected relatives are essential for pedigree linkage analysis, however, they cause a violation of the independent sample assumption in case-control association studies. To avoid the correlation between samples, a common practice is to take only one affected sample per pedigree in association analysis. Although several methods exist in handling correlated samples, they are still not widely used in part because these are not easily implemented, or because they are not widely known. We advocate the effective sample size method as a simple and accessible approach for case-control association analysis with correlated samples. This method modifies the chi-square test statistic, p-value, and 95 confidence interval of the odds-ratio by replacing the apparent number of allele or genotype counts with the effective ones in the standard formula, without the need for specialized computer programs. We present a simple formula for calculating effective sample size for many types of relative pairs and relative sets. For allele frequency estimation, the effective sample size method captures the variance inflation exactly. For genotype frequency, simulations showed that effective sample size provides a satisfactory approximation. A gene which is previously identified as a type 1 diabetes susceptibility locus, the interferon-induced helicase gene (IFIH1), is shown to be significantly associated with rheumatoid arthritis when the effective sample size method is applied. This significant association is not established if only one affected sib per pedigree were used in the association analysis. Relationship between the effective sample size method and other methods -- the generalized estimation equation, variance of eigenvalues for correlation matrices, and genomic controls -- are discussed.",Healthcare
Taxation and Valuation,"I explain the root of persistent failure of efforts to remove tax-induced distortions of economic incentives. It lies in FUNDAMENTAL IMPOSSIBILITY of objectively evaluating tax base. Distortions can be entirely avoided in the sector of publicly traded corporations. Evaluation can be bypassed by taxing it in shares (to be auctioned) rather than cash. Stock capital includes cost basis (B) and unrealized gains (G). Gains are presently tax-deferred until realized in divestment. The deferral is remedied by corporate income tax (rate t). i is the variable interest rate on special constant value cv-bonds. The proposed system replaces (1) corporate income tax - with interest on the deferred Gt, and (2) divestment taxes - with interest on Bt. To collect both, IRS will periodically take to auction a fraction it of privately held publicly traded shares. Note: (2) is a neutral simplification: Investments can be split into B(1-t) stock and Bt in bond portfolios. Bond interest buys back the auctioned shares, and tax-free divestment matches the original yield. The it stock tax matches the income tax on cv-bond portfolios of equal value. The Treasury, too, could match its income (in bond sales) to the rate t tax on the full stock market return (without tempting price manipulation). It can vary i to keep the bond volume at a fraction t of market capitalization; then share auctions supply bond interest. Taxpayers, too, could unilaterally match their burden to such tax by keeping a fraction t of capital in bonds. The main feature is: nothing companies and investors do can change their tax (fraction it of shares), so business decisions would be exactly the same as without taxes. No longer would taxes on dividends and capital gains impede capital flow, companies would forget bewildering mazes of tax laws, regulations, precedents; Congress would still collect the same revenue it now does.","Taxation and Valuation I explain the root of persistent failure of efforts to remove tax-induced distortions of economic incentives. It lies in FUNDAMENTAL IMPOSSIBILITY of objectively evaluating tax base. Distortions can be entirely avoided in the sector of publicly traded corporations. Evaluation can be bypassed by taxing it in shares (to be auctioned) rather than cash. Stock capital includes cost basis (B) and unrealized gains (G). Gains are presently tax-deferred until realized in divestment. The deferral is remedied by corporate income tax (rate t). i is the variable interest rate on special constant value cv-bonds. The proposed system replaces (1) corporate income tax - with interest on the deferred Gt, and (2) divestment taxes - with interest on Bt. To collect both, IRS will periodically take to auction a fraction it of privately held publicly traded shares. Note: (2) is a neutral simplification: Investments can be split into B(1-t) stock and Bt in bond portfolios. Bond interest buys back the auctioned shares, and tax-free divestment matches the original yield. The it stock tax matches the income tax on cv-bond portfolios of equal value. The Treasury, too, could match its income (in bond sales) to the rate t tax on the full stock market return (without tempting price manipulation). It can vary i to keep the bond volume at a fraction t of market capitalization; then share auctions supply bond interest. Taxpayers, too, could unilaterally match their burden to such tax by keeping a fraction t of capital in bonds. The main feature is: nothing companies and investors do can change their tax (fraction it of shares), so business decisions would be exactly the same as without taxes. No longer would taxes on dividends and capital gains impede capital flow, companies would forget bewildering mazes of tax laws, regulations, precedents; Congress would still collect the same revenue it now does.",Finance
A pixel-based approach to massive lesion detection in X-ray mammography,"A system for the automated detection of massive lesions in mammograms is presented. The approach we adopted is a pixel-based and multi-level one. Each pixel in a mammogram is flagged with the appropriate class membership, e.g. massive lesions or normal breast tissue.","A pixel-based approach to massive lesion detection in X-ray mammography A system for the automated detection of massive lesions in mammograms is presented. The approach we adopted is a pixel-based and multi-level one. Each pixel in a mammogram is flagged with the appropriate class membership, e.g. massive lesions or normal breast tissue.",Healthcare
Regional Climate Change Datasets for South Asia,"The Centre for Climate Change Research (CCCR;http:cccr.tropmet.res.in) at the Indian Institute of Tropical Meteorology (IITM; http:www.tropmet.res.in), Pune, launched in 2009 with the support of the Ministry of Earth Sciences (MoES), Government of India, focuses on the development of new climate modelling capabilities in India and South Asia to address issues concerning the science of climate change. CCCR-IITM has the mandate of developing an Earth System Model and to make the regional climate projections. An important achievement was made by developing an Earth System Model at IITM, which is an important step towards understanding global and regional climate response to long-term climate variability and climate change. CCCR-IITM has also generated an ensemble of high resolution dynamically downscaled future projections of regional climate over South Asia and Indian monsoon, which are found useful for impact assessment studies and for quantifying uncertainties in the regional projections. A brief overview of these core climate change modeling activities of CCCR-IITM was presented in an Interim Report on Climate Change over India (available at http:cccr.tropmet.res.inhomereports.jsp)","Regional Climate Change Datasets for South Asia The Centre for Climate Change Research (CCCR;http:cccr.tropmet.res.in) at the Indian Institute of Tropical Meteorology (IITM; http:www.tropmet.res.in), Pune, launched in 2009 with the support of the Ministry of Earth Sciences (MoES), Government of India, focuses on the development of new climate modelling capabilities in India and South Asia to address issues concerning the science of climate change. CCCR-IITM has the mandate of developing an Earth System Model and to make the regional climate projections. An important achievement was made by developing an Earth System Model at IITM, which is an important step towards understanding global and regional climate response to long-term climate variability and climate change. CCCR-IITM has also generated an ensemble of high resolution dynamically downscaled future projections of regional climate over South Asia and Indian monsoon, which are found useful for impact assessment studies and for quantifying uncertainties in the regional projections. A brief overview of these core climate change modeling activities of CCCR-IITM was presented in an Interim Report on Climate Change over India (available at http:cccr.tropmet.res.inhomereports.jsp)",Environment
Universality in snowflake aggregation,"Aggregation of ice crystals is a key process governing precipitation. Individual ice crystals exhibit considerable diversity of shape, and a wide range of physical processes could influence their aggregation; despite this we show that a simple computer model captures key features of aggregate shape and size distribution reported recently from Cirrus clouds. The results prompt a new way to plot the experimental size distributions leading to remarkably good dynamical scaling. That scaling independently confirms that there is a single dominant aggregation mechanism at play, albeit our model (based on undeflected trajectories to contact) does not capture its form exactly.","Universality in snowflake aggregation Aggregation of ice crystals is a key process governing precipitation. Individual ice crystals exhibit considerable diversity of shape, and a wide range of physical processes could influence their aggregation; despite this we show that a simple computer model captures key features of aggregate shape and size distribution reported recently from Cirrus clouds. The results prompt a new way to plot the experimental size distributions leading to remarkably good dynamical scaling. That scaling independently confirms that there is a single dominant aggregation mechanism at play, albeit our model (based on undeflected trajectories to contact) does not capture its form exactly.",Environment
Developing a custom GPT based on Inquiry Based Learning for Physics Teachers,"Generative Artificial Intelligence (GenAI) has emerged as a valuable assistant in many fields such as marketing, finance, project management, and education. In education, many GenAI tools have been developed to aid teachers in preparing proper educational material and offering personalized learning to their students, tailored to their educational needs. In this paper, we present a custom GPT (IBL Educator GPT) that is designed and developed based on Inquiry-based Learning and offers physics teachers a framework in which they can interact with ChatGPT and design educational strategies. The utilization of the IBL Educator GPT has led to an improvement in teachers perspectives regarding the adoption of artificial intelligence-based tools for personalizing teaching.","Developing a custom GPT based on Inquiry Based Learning for Physics Teachers Generative Artificial Intelligence (GenAI) has emerged as a valuable assistant in many fields such as marketing, finance, project management, and education. In education, many GenAI tools have been developed to aid teachers in preparing proper educational material and offering personalized learning to their students, tailored to their educational needs. In this paper, we present a custom GPT (IBL Educator GPT) that is designed and developed based on Inquiry-based Learning and offers physics teachers a framework in which they can interact with ChatGPT and design educational strategies. The utilization of the IBL Educator GPT has led to an improvement in teachers perspectives regarding the adoption of artificial intelligence-based tools for personalizing teaching.",Education
System Support for Environmentally Sustainable Computing in Data Centers,"Modern data centers suffer from a growing carbon footprint due to insufficient support for environmental sustainability. While hardware accelerators and renewable energy have been utilized to enhance sustainability, addressing Quality of Service (QoS) degradation caused by renewable energy supply and hardware recycling remains challenging: (1) prior accelerators exhibit significant carbon footprints due to limited reconfigurability and inability to adapt to renewable energy fluctuations; (2) integrating recycled NAND flash chips in data centers poses challenges due to their short lifetime, increasing energy consumption; (3) the absence of a sustainability estimator impedes data centers and users in evaluating and improving their environmental impact. This study aims to improve system support for environmentally sustainable data centers by proposing a reconfigurable hardware accelerator for intensive computing primitives and developing a fractional NAND flash cell to extend the lifetime of recycled flash chips while supporting graceful capacity degradation. We also introduce a sustainability estimator to evaluate user task energy consumption and promote sustainable practices. We present our preliminary results and recognize this as an ongoing initiative with significant potential to advance environmentally sustainable computing in data centers and stimulate further exploration in this critical research domain.","System Support for Environmentally Sustainable Computing in Data Centers Modern data centers suffer from a growing carbon footprint due to insufficient support for environmental sustainability. While hardware accelerators and renewable energy have been utilized to enhance sustainability, addressing Quality of Service (QoS) degradation caused by renewable energy supply and hardware recycling remains challenging: (1) prior accelerators exhibit significant carbon footprints due to limited reconfigurability and inability to adapt to renewable energy fluctuations; (2) integrating recycled NAND flash chips in data centers poses challenges due to their short lifetime, increasing energy consumption; (3) the absence of a sustainability estimator impedes data centers and users in evaluating and improving their environmental impact. This study aims to improve system support for environmentally sustainable data centers by proposing a reconfigurable hardware accelerator for intensive computing primitives and developing a fractional NAND flash cell to extend the lifetime of recycled flash chips while supporting graceful capacity degradation. We also introduce a sustainability estimator to evaluate user task energy consumption and promote sustainable practices. We present our preliminary results and recognize this as an ongoing initiative with significant potential to advance environmentally sustainable computing in data centers and stimulate further exploration in this critical research domain.",Environment
Asymptotic light field in the presence of a bubble-layer,We report that the submerged microbubbles are an efficient source of diffuse radiance and may contribute to a rapid transition to the diffuse asymptotic regime. In this asymptotic regime an average cosine is easily predictable and measurable.,Asymptotic light field in the presence of a bubble-layer We report that the submerged microbubbles are an efficient source of diffuse radiance and may contribute to a rapid transition to the diffuse asymptotic regime. In this asymptotic regime an average cosine is easily predictable and measurable.,Environment
An example of a stochastic equilibrium with incomplete markets,"We prove existence and uniqueness of stochastic equilibria in a class of incomplete continuous-time financial environments where the market participants are exponential utility maximizers with heterogeneous risk-aversion coefficients and general Markovian random endowments. The incompleteness featured in our setting - the source of which can be thought of as a credit event or a catastrophe - is genuine in the sense that not only the prices, but also the family of replicable claims itself is determined as a part of the equilibrium. Consequently, equilibrium allocations are not necessarily Pareto optimal and the related representative-agent techniques cannot be used. Instead, we follow a novel route based on new stability results for a class of semilinear partial differential equations related to the Hamilton-Jacobi-Bellman equation for the agents utility-maximization problems. This approach leads to a reformulation of the problem where the Banach fixed point theorem can be used not only to show existence and uniqueness, but also to provide a simple and efficient numerical procedure for its computation.","An example of a stochastic equilibrium with incomplete markets We prove existence and uniqueness of stochastic equilibria in a class of incomplete continuous-time financial environments where the market participants are exponential utility maximizers with heterogeneous risk-aversion coefficients and general Markovian random endowments. The incompleteness featured in our setting - the source of which can be thought of as a credit event or a catastrophe - is genuine in the sense that not only the prices, but also the family of replicable claims itself is determined as a part of the equilibrium. Consequently, equilibrium allocations are not necessarily Pareto optimal and the related representative-agent techniques cannot be used. Instead, we follow a novel route based on new stability results for a class of semilinear partial differential equations related to the Hamilton-Jacobi-Bellman equation for the agents utility-maximization problems. This approach leads to a reformulation of the problem where the Banach fixed point theorem can be used not only to show existence and uniqueness, but also to provide a simple and efficient numerical procedure for its computation.",Finance
HealthE: Classifying Entities in Online Textual Health Advice,"The processing of entities in natural language is essential to many medical NLP systems. Unfortunately, existing datasets vastly under-represent the entities required to model public health relevant texts such as health advice often found on sites like WebMD. People rely on such information for personal health management and clinically relevant decision making. In this work, we release a new annotated dataset, HealthE, consisting of 6,756 health advice. HealthE has a more granular label space compared to existing medical NER corpora and contains annotation for diverse health phrases. Additionally, we introduce a new health entity classification model, EP S-BERT, which leverages textual context patterns in the classification of entity classes. EP S-BERT provides a 4-point increase in F1 score over the nearest baseline and a 34-point increase in F1 when compared to off-the-shelf medical NER tools trained to extract disease and medication mentions from clinical texts. All code and data are publicly available on Github.","HealthE: Classifying Entities in Online Textual Health Advice The processing of entities in natural language is essential to many medical NLP systems. Unfortunately, existing datasets vastly under-represent the entities required to model public health relevant texts such as health advice often found on sites like WebMD. People rely on such information for personal health management and clinically relevant decision making. In this work, we release a new annotated dataset, HealthE, consisting of 6,756 health advice. HealthE has a more granular label space compared to existing medical NER corpora and contains annotation for diverse health phrases. Additionally, we introduce a new health entity classification model, EP S-BERT, which leverages textual context patterns in the classification of entity classes. EP S-BERT provides a 4-point increase in F1 score over the nearest baseline and a 34-point increase in F1 when compared to off-the-shelf medical NER tools trained to extract disease and medication mentions from clinical texts. All code and data are publicly available on Github.",Healthcare
Rule-based Machine Learning Methods for Functional Prediction,"We describe a machine learning method for predicting the value of a real-valued function, given the values of multiple input variables. The method induces solutions from samples in the form of ordered disjunctive normal form (DNF) decision rules. A central objective of the method and representation is the induction of compact, easily interpretable solutions. This rule-based decision model can be extended to search efficiently for similar cases prior to approximating function values. Experimental results on real-world data demonstrate that the new techniques are competitive with existing machine learning and statistical methods and can sometimes yield superior regression performance.","Rule-based Machine Learning Methods for Functional Prediction We describe a machine learning method for predicting the value of a real-valued function, given the values of multiple input variables. The method induces solutions from samples in the form of ordered disjunctive normal form (DNF) decision rules. A central objective of the method and representation is the induction of compact, easily interpretable solutions. This rule-based decision model can be extended to search efficiently for similar cases prior to approximating function values. Experimental results on real-world data demonstrate that the new techniques are competitive with existing machine learning and statistical methods and can sometimes yield superior regression performance.",Technology
From Bench to Bedside: A Review of Clinical Trials in Drug Discovery and Development,"Clinical trials are an indispensable part of the drug development process, bridging the gap between basic research and clinical application. During the development of new drugs, clinical trials are used not only to evaluate the safety and efficacy of the drug but also to explore its dosage, treatment regimens, and potential side effects. This review discusses the various stages of clinical trials, including Phase I (safety assessment), Phase II (preliminary efficacy evaluation), Phase III (large-scale validation), and Phase IV (post-marketing surveillance), highlighting the characteristics of each phase and their interrelationships. Additionally, the paper addresses the major challenges encountered in clinical trials, such as ethical issues, subject recruitment difficulties, diversity and representativeness concerns, and proposes strategies for overcoming these challenges. With the advancement of technology, innovative technologies such as artificial intelligence, big data, and digitalization are gradually transforming clinical trial design and implementation, improving trial efficiency and data quality. The article also looks forward to the future of clinical trials, particularly the impact of emerging therapies such as gene therapy and immunotherapy on trial design, as well as the importance of regulatory reforms and global collaboration. In conclusion, the core role of clinical trials in drug development will continue to drive the progress of innovative drug development and clinical treatment.","From Bench to Bedside: A Review of Clinical Trials in Drug Discovery and Development Clinical trials are an indispensable part of the drug development process, bridging the gap between basic research and clinical application. During the development of new drugs, clinical trials are used not only to evaluate the safety and efficacy of the drug but also to explore its dosage, treatment regimens, and potential side effects. This review discusses the various stages of clinical trials, including Phase I (safety assessment), Phase II (preliminary efficacy evaluation), Phase III (large-scale validation), and Phase IV (post-marketing surveillance), highlighting the characteristics of each phase and their interrelationships. Additionally, the paper addresses the major challenges encountered in clinical trials, such as ethical issues, subject recruitment difficulties, diversity and representativeness concerns, and proposes strategies for overcoming these challenges. With the advancement of technology, innovative technologies such as artificial intelligence, big data, and digitalization are gradually transforming clinical trial design and implementation, improving trial efficiency and data quality. The article also looks forward to the future of clinical trials, particularly the impact of emerging therapies such as gene therapy and immunotherapy on trial design, as well as the importance of regulatory reforms and global collaboration. In conclusion, the core role of clinical trials in drug development will continue to drive the progress of innovative drug development and clinical treatment.",Healthcare
Face Verification in Polar Frequency Domain: a Biologically Motivated Approach,"We present a novel local-based face verification system whose components are analogous to those of biological systems. In the proposed system, after global registration and normalization, three eye regions are converted from the spatial to polar frequency domain by a Fourier-Bessel Transform. The resulting representations are embedded in a dissimilarity space, where each image is represented by its distance to all the other images. In this dissimilarity space a Pseudo-Fisher discriminator is built. ROC and equal error rate verification test results on the FERET database showed that the system performed at least as state-of-the-art methods and better than a system based on polar Fourier features. The local-based system is especially robust to facial expression and age variations, but sensitive to registration errors.","Face Verification in Polar Frequency Domain: a Biologically Motivated Approach We present a novel local-based face verification system whose components are analogous to those of biological systems. In the proposed system, after global registration and normalization, three eye regions are converted from the spatial to polar frequency domain by a Fourier-Bessel Transform. The resulting representations are embedded in a dissimilarity space, where each image is represented by its distance to all the other images. In this dissimilarity space a Pseudo-Fisher discriminator is built. ROC and equal error rate verification test results on the FERET database showed that the system performed at least as state-of-the-art methods and better than a system based on polar Fourier features. The local-based system is especially robust to facial expression and age variations, but sensitive to registration errors.",Technology
Social dynamics can delay or prevent climate tipping points by speeding the adoption of climate change mitigation,"Social behaviour models are increasingly integrated into climate change studies, and the significance of climate tipping points for runaway climate change is well recognised. However, there has been insufficient focus on tipping points in social-climate dynamics. We developed a coupled social-climate model consisting of an Earth system model and a social behaviour model, both with tipping elements. The social model explores opinion formation by analysing social learning rates, the net cost of mitigation, and the strength of social norms. Our results indicate that the net cost of mitigation and social norms have minimal impact on tipping points when social norms are weak. As social norms strengthen, the climate tipping point can trigger a tipping element in the social model. However, faster social learning can delay or prevent the climate tipping point: sufficiently fast social learning means growing climate change mitigation can outpace the oncoming climate tipping point, despite social-climate feedback. By comparing high- and low-risk scenarios, we demonstrated high-risk scenarios increase the likelihood of tipping points. We also illustrate the role of a critical temperature anomaly in triggering tipping points. In conclusion, understanding social behaviour dynamics is vital for predicting climate tipping points and mitigating their impacts.","Social dynamics can delay or prevent climate tipping points by speeding the adoption of climate change mitigation Social behaviour models are increasingly integrated into climate change studies, and the significance of climate tipping points for runaway climate change is well recognised. However, there has been insufficient focus on tipping points in social-climate dynamics. We developed a coupled social-climate model consisting of an Earth system model and a social behaviour model, both with tipping elements. The social model explores opinion formation by analysing social learning rates, the net cost of mitigation, and the strength of social norms. Our results indicate that the net cost of mitigation and social norms have minimal impact on tipping points when social norms are weak. As social norms strengthen, the climate tipping point can trigger a tipping element in the social model. However, faster social learning can delay or prevent the climate tipping point: sufficiently fast social learning means growing climate change mitigation can outpace the oncoming climate tipping point, despite social-climate feedback. By comparing high- and low-risk scenarios, we demonstrated high-risk scenarios increase the likelihood of tipping points. We also illustrate the role of a critical temperature anomaly in triggering tipping points. In conclusion, understanding social behaviour dynamics is vital for predicting climate tipping points and mitigating their impacts.",Environment
An effective edge--directed frequency filter for removal of aliasing in upsampled images,"Raster images can have a range of various distortions connected to their raster structure. Upsampling them might in effect substantially yield the raster structure of the original image, known as aliasing. The upsampling itself may introduce aliasing into the upsampled image as well. The presented method attempts to remove the aliasing using frequency filters based on the discrete fast Fourier transform, and applied directionally in certain regions placed along the edges in the image. As opposed to some anisotropic smoothing methods, the presented algorithm aims to selectively reduce only the aliasing, preserving the sharpness of image details. The method can be used as a post--processing filter along with various upsampling algorithms. It was experimentally shown that the method can improve the visual quality of the upsampled images.","An effective edge--directed frequency filter for removal of aliasing in upsampled images Raster images can have a range of various distortions connected to their raster structure. Upsampling them might in effect substantially yield the raster structure of the original image, known as aliasing. The upsampling itself may introduce aliasing into the upsampled image as well. The presented method attempts to remove the aliasing using frequency filters based on the discrete fast Fourier transform, and applied directionally in certain regions placed along the edges in the image. As opposed to some anisotropic smoothing methods, the presented algorithm aims to selectively reduce only the aliasing, preserving the sharpness of image details. The method can be used as a post--processing filter along with various upsampling algorithms. It was experimentally shown that the method can improve the visual quality of the upsampled images.",Technology
On Planning while Learning,"This paper introduces a framework for Planning while Learning where an agent is given a goal to achieve in an environment whose behavior is only partially known to the agent. We discuss the tractability of various plan-design processes. We show that for a large natural class of Planning while Learning systems, a plan can be presented and verified in a reasonable time. However, coming up algorithmically with a plan, even for simple classes of systems is apparently intractable. We emphasize the role of off-line plan-design processes, and show that, in most natural cases, the verification (projection) part can be carried out in an efficient algorithmic manner.","On Planning while Learning This paper introduces a framework for Planning while Learning where an agent is given a goal to achieve in an environment whose behavior is only partially known to the agent. We discuss the tractability of various plan-design processes. We show that for a large natural class of Planning while Learning systems, a plan can be presented and verified in a reasonable time. However, coming up algorithmically with a plan, even for simple classes of systems is apparently intractable. We emphasize the role of off-line plan-design processes, and show that, in most natural cases, the verification (projection) part can be carried out in an efficient algorithmic manner.",Technology
La Loi organique relative aux lois de finances (LOLF) dans les institutions culturelles publiques du spectacle vivant en France,"In a crisis of public finances, France bases all its hopes on the evaluation of performance to moderate the effects of a complex crisis. Under the banner of modernization of the State, a new financial constitution called the Organic Law on finance laws (LOLF) became the main lever of reform of public management. Fully applied to the Cultural Affairs since 2006, the LOLF is based on a set of performance indicators and sets the public performance arts institutions specific targets. This article defines and analyzes the pattern of the design and the course of these indicators and targets. It also examines the controversy generated by this new mode of governance.","La Loi organique relative aux lois de finances (LOLF) dans les institutions culturelles publiques du spectacle vivant en France In a crisis of public finances, France bases all its hopes on the evaluation of performance to moderate the effects of a complex crisis. Under the banner of modernization of the State, a new financial constitution called the Organic Law on finance laws (LOLF) became the main lever of reform of public management. Fully applied to the Cultural Affairs since 2006, the LOLF is based on a set of performance indicators and sets the public performance arts institutions specific targets. This article defines and analyzes the pattern of the design and the course of these indicators and targets. It also examines the controversy generated by this new mode of governance.",Finance
Logarithmic-Time Updates and Queries in Probabilistic Networks,"Traditional databases commonly support efficient query and update procedures that operate in time which is sublinear in the size of the database. Our goal in this paper is to take a first step toward dynamic reasoning in probabilistic databases with comparable efficiency. We propose a dynamic data structure that supports efficient algorithms for updating and querying singly connected Bayesian networks. In the conventional algorithm, new evidence is absorbed in O(1) time and queries are processed in time O(N), where N is the size of the network. We propose an algorithm which, after a preprocessing phase, allows us to answer queries in time O(log N) at the expense of O(log N) time per evidence absorption. The usefulness of sub-linear processing time manifests itself in applications requiring (near) real-time response over large probabilistic databases. We briefly discuss a potential application of dynamic probabilistic reasoning in computational biology.","Logarithmic-Time Updates and Queries in Probabilistic Networks Traditional databases commonly support efficient query and update procedures that operate in time which is sublinear in the size of the database. Our goal in this paper is to take a first step toward dynamic reasoning in probabilistic databases with comparable efficiency. We propose a dynamic data structure that supports efficient algorithms for updating and querying singly connected Bayesian networks. In the conventional algorithm, new evidence is absorbed in O(1) time and queries are processed in time O(N), where N is the size of the network. We propose an algorithm which, after a preprocessing phase, allows us to answer queries in time O(log N) at the expense of O(log N) time per evidence absorption. The usefulness of sub-linear processing time manifests itself in applications requiring (near) real-time response over large probabilistic databases. We briefly discuss a potential application of dynamic probabilistic reasoning in computational biology.",Technology
Selection of future events from a time series in relation to estimations of forecasting uncertainty,"A new general procedure for a priori selection of more predictable events from a time series of observed variable is proposed. The procedure is applicable to time series which contains different types of events that feature significantly different predictability, or, in other words, to heteroskedastic time series. A priori selection of future events in accordance to expected uncertainty of their forecasts may be helpful for making practical decisions. The procedure first implies creation of two neural network based forecasting models, one of which is aimed at prediction of conditional mean and other - conditional dispersion, and then elaboration of the rule for future event selection into groups of more and less predictable events. The method is demonstrated and tested by the example of the computer generated time series, and then applied to the real world time series, Dow Jones Industrial Average index.","Selection of future events from a time series in relation to estimations of forecasting uncertainty A new general procedure for a priori selection of more predictable events from a time series of observed variable is proposed. The procedure is applicable to time series which contains different types of events that feature significantly different predictability, or, in other words, to heteroskedastic time series. A priori selection of future events in accordance to expected uncertainty of their forecasts may be helpful for making practical decisions. The procedure first implies creation of two neural network based forecasting models, one of which is aimed at prediction of conditional mean and other - conditional dispersion, and then elaboration of the rule for future event selection into groups of more and less predictable events. The method is demonstrated and tested by the example of the computer generated time series, and then applied to the real world time series, Dow Jones Industrial Average index.",Technology
Econophysics: A new discipline,"This paper debates the contribution of Econophysics to the economic or financial domains. Since the traditional approach performed by Economics or Finance has revealed to be insufficient in fully characterizing and explaining the correspondingly phenomena, we discuss whether Econophysics can provide a new insight onto these matters. Thus, an assessment is presented in order to weight its potential opportunities and limitations. This is particularly relevant as it is widely recognized that during its yet short existence Econophysics has experienced a growing interest not only by physicists but also by economists in searching for new approaches that could help explaining existing questions. In fact, many papers have been submitted, some books have been released, new journals have been published, several conferences have been held, a site is maintained -- http:www.unifr.checonophysics where news, events, book reviews, papers and a blog are exhibited; a 3-year licentiate studies (University of Silesia 1) and a B.Sc. course (University of Wroclaw 2) have been created and also some Ph.D. thesis have been written. Therefore, a fundamental question arises: Is this just a fad or is it something much more consistent that will prevail? This is what this paper addresses.","Econophysics: A new discipline This paper debates the contribution of Econophysics to the economic or financial domains. Since the traditional approach performed by Economics or Finance has revealed to be insufficient in fully characterizing and explaining the correspondingly phenomena, we discuss whether Econophysics can provide a new insight onto these matters. Thus, an assessment is presented in order to weight its potential opportunities and limitations. This is particularly relevant as it is widely recognized that during its yet short existence Econophysics has experienced a growing interest not only by physicists but also by economists in searching for new approaches that could help explaining existing questions. In fact, many papers have been submitted, some books have been released, new journals have been published, several conferences have been held, a site is maintained -- http:www.unifr.checonophysics where news, events, book reviews, papers and a blog are exhibited; a 3-year licentiate studies (University of Silesia 1) and a B.Sc. course (University of Wroclaw 2) have been created and also some Ph.D. thesis have been written. Therefore, a fundamental question arises: Is this just a fad or is it something much more consistent that will prevail? This is what this paper addresses.",Finance
Structural Breaks in the Mexicos Integration into the World Stock Market,"This article investigates the evolution of the Mexican stock market integration into the world market. First, we estimate the time-varying Mexican degree of market integration using an international conditional version of the CAPM with segmentation effects. Second, we study the structural breaks in this series. Finally, we relate the obtained results to important facts and economic events","Structural Breaks in the Mexicos Integration into the World Stock Market This article investigates the evolution of the Mexican stock market integration into the world market. First, we estimate the time-varying Mexican degree of market integration using an international conditional version of the CAPM with segmentation effects. Second, we study the structural breaks in this series. Finally, we relate the obtained results to important facts and economic events",Finance
Five guidelines for the evaluation of site-specific medium range probabilistic temperature forecasts,"Probabilistic temperature forecasts are potentially useful to the energy and weather derivatives industries. However, at present, they are little used. There are a number of reasons for this, but we believe this is in part due to inadequacies in the methodologies that have been used to evaluate such forecasts, leading to uncertainty as to whether the forecasts are really useful or not and making it hard to work out which forecasts are best. To remedy this situation we describe a set of guidelines that we recommend should be followed when evaluating the skill of site-specific probabilistic medium range temperature forecasts. If these guidelines are followed then the results of validation can be used directly by forecast users to make decisions about which forecasts to use. If they are not followed then the results of validation may be interesting, but will not be practically useful for users. We find that none of the published studies that evaluate such forecasts fall within our guidelines, and that, as a result, none convey the information that the users need to make appropriate decisions about which forecasts are best.","Five guidelines for the evaluation of site-specific medium range probabilistic temperature forecasts Probabilistic temperature forecasts are potentially useful to the energy and weather derivatives industries. However, at present, they are little used. There are a number of reasons for this, but we believe this is in part due to inadequacies in the methodologies that have been used to evaluate such forecasts, leading to uncertainty as to whether the forecasts are really useful or not and making it hard to work out which forecasts are best. To remedy this situation we describe a set of guidelines that we recommend should be followed when evaluating the skill of site-specific probabilistic medium range temperature forecasts. If these guidelines are followed then the results of validation can be used directly by forecast users to make decisions about which forecasts to use. If they are not followed then the results of validation may be interesting, but will not be practically useful for users. We find that none of the published studies that evaluate such forecasts fall within our guidelines, and that, as a result, none convey the information that the users need to make appropriate decisions about which forecasts are best.",Environment
PLATYPUS: Progressive Local Surface Estimator for Arbitrary-Scale Point Cloud Upsampling,"3D point clouds are increasingly vital for applications like autonomous driving and robotics, yet the raw data captured by sensors often suffer from noise and sparsity, creating challenges for downstream tasks. Consequently, point cloud upsampling becomes essential for improving density and uniformity, with recent approaches showing promise by projecting randomly generated query points onto the underlying surface of sparse point clouds. However, these methods often result in outliers, non-uniformity, and difficulties in handling regions with high curvature and intricate structures. In this work, we address these challenges by introducing the Progressive Local Surface Estimator (PLSE), which more effectively captures local features in complex regions through a curvature-based sampling technique that selectively targets high-curvature areas. Additionally, we incorporate a curriculum learning strategy that leverages the curvature distribution within the point cloud to naturally assess the sample difficulty, enabling curriculum learning on point cloud data for the first time. The experimental results demonstrate that our approach significantly outperforms existing methods, achieving high-quality, dense point clouds with superior accuracy and detail.","PLATYPUS: Progressive Local Surface Estimator for Arbitrary-Scale Point Cloud Upsampling 3D point clouds are increasingly vital for applications like autonomous driving and robotics, yet the raw data captured by sensors often suffer from noise and sparsity, creating challenges for downstream tasks. Consequently, point cloud upsampling becomes essential for improving density and uniformity, with recent approaches showing promise by projecting randomly generated query points onto the underlying surface of sparse point clouds. However, these methods often result in outliers, non-uniformity, and difficulties in handling regions with high curvature and intricate structures. In this work, we address these challenges by introducing the Progressive Local Surface Estimator (PLSE), which more effectively captures local features in complex regions through a curvature-based sampling technique that selectively targets high-curvature areas. Additionally, we incorporate a curriculum learning strategy that leverages the curvature distribution within the point cloud to naturally assess the sample difficulty, enabling curriculum learning on point cloud data for the first time. The experimental results demonstrate that our approach significantly outperforms existing methods, achieving high-quality, dense point clouds with superior accuracy and detail.",Education
"Its not approved, but many, like myself, ignore the rule: Investigating the Landscape and Consequences of Unsanctioned Technology Use in Educational Institutes","Educators regularly use unsanctioned technologies (apps not formally approved by their institutions) for teaching, grading, and other academic tasks. While these tools often support instructional needs, they raise significant privacy, security, and regulatory compliance concerns. Despite its importance, understanding the adoptions and risks from the perspective of educators, who serve as de facto decision makers behind unsanctioned technology use, is largely understudied in existing literature.To address this gap, we conducted two surveys: one with 375 educators who listed 1,373 unsanctioned apps, and another with 21 administrators who either often help educators to set up educational technologies (EdTechs) or observe their security or privacy incidents. Our study identified 494 unique applications used by educators, primarily for pedagogical utility (n213) and functional convenience (n155), and the associated risks were often ignored. In fact, despite security and privacy concerns, many educators continued using the same apps (n  62), citing a lack of alternatives or heavy dependence as barriers to discontinuation. We also found that fewer than a third of educators were aware of any institutional policy on unsanctioned technology use (K12: 30.3, HEI: 24.8), and 22 knowingly violated such policies. While 107 received formal warnings, only 33 adjusted their behavior. Finally, we conclude by discussing the implications of our findings and future recommendations to minimize the risks.","Its not approved, but many, like myself, ignore the rule: Investigating the Landscape and Consequences of Unsanctioned Technology Use in Educational Institutes Educators regularly use unsanctioned technologies (apps not formally approved by their institutions) for teaching, grading, and other academic tasks. While these tools often support instructional needs, they raise significant privacy, security, and regulatory compliance concerns. Despite its importance, understanding the adoptions and risks from the perspective of educators, who serve as de facto decision makers behind unsanctioned technology use, is largely understudied in existing literature.To address this gap, we conducted two surveys: one with 375 educators who listed 1,373 unsanctioned apps, and another with 21 administrators who either often help educators to set up educational technologies (EdTechs) or observe their security or privacy incidents. Our study identified 494 unique applications used by educators, primarily for pedagogical utility (n213) and functional convenience (n155), and the associated risks were often ignored. In fact, despite security and privacy concerns, many educators continued using the same apps (n  62), citing a lack of alternatives or heavy dependence as barriers to discontinuation. We also found that fewer than a third of educators were aware of any institutional policy on unsanctioned technology use (K12: 30.3, HEI: 24.8), and 22 knowingly violated such policies. While 107 received formal warnings, only 33 adjusted their behavior. Finally, we conclude by discussing the implications of our findings and future recommendations to minimize the risks.",Education
Stability of leap-frog constant-coefficients semi-implicit schemes for the fully elastic system of Euler equations. Flat-terrain case,"The aim of this paper is to investigate the response of this systemscheme in terms of stability in presence of explicitly treated residual terms, as it inevitably occurs in the reality of NWP. This sudy is restricted to the impact of thermal and baric residual terms (metric residual terms linked to the orography are not considered here). It is shown that conversely to what occurs with Hydrostatic Primitive Equations, the choice of the prognostic variables used to solve the system in time is of primary importance for the robustness with Euler Equations. For an optimal choice of prognostic variables, unconditionnally stable schemes can be obtained (with respect to the length of the time-step), but only for a smaller range of reference states than in the case of Hydrostatic Primitive Equations. This study also indicates that: (i) vertical coordinates based on geometrical height and on mass behave similarly in terms of stability for the problems examined here, and (ii) hybrid coordinates induce an intrinsic instability, the practical importance of which is however not completely elucidated in the theoretical context of this paper.","Stability of leap-frog constant-coefficients semi-implicit schemes for the fully elastic system of Euler equations. Flat-terrain case The aim of this paper is to investigate the response of this systemscheme in terms of stability in presence of explicitly treated residual terms, as it inevitably occurs in the reality of NWP. This sudy is restricted to the impact of thermal and baric residual terms (metric residual terms linked to the orography are not considered here). It is shown that conversely to what occurs with Hydrostatic Primitive Equations, the choice of the prognostic variables used to solve the system in time is of primary importance for the robustness with Euler Equations. For an optimal choice of prognostic variables, unconditionnally stable schemes can be obtained (with respect to the length of the time-step), but only for a smaller range of reference states than in the case of Hydrostatic Primitive Equations. This study also indicates that: (i) vertical coordinates based on geometrical height and on mass behave similarly in terms of stability for the problems examined here, and (ii) hybrid coordinates induce an intrinsic instability, the practical importance of which is however not completely elucidated in the theoretical context of this paper.",Environment
Statistical Inference for Heterogeneous Treatment Effect with Right-censored Data from Synthesizing Randomized Clinical Trials and Real-world Data,"The heterogeneous treatment effect plays a crucial role in precision medicine. There is evidence that real-world data, even subject to biases, can be employed as supplementary evidence for randomized clinical trials to improve the statistical efficiency of the heterogeneous treatment effect estimation. In this paper, for survival data with right censoring, we consider estimating the heterogeneous treatment effect, defined as the difference of the treatment-specific conditional restricted mean survival times given covariates, by synthesizing evidence from randomized clinical trials and the real-world data with possible biases. We define an omnibus confounding function to characterize the effect of biases caused by unmeasured confounders, censoring, outcome heterogeneity, and measurement error, and further, identify it by combining the trial and real-world data. We propose a penalized sieve method to estimate the heterogeneous treatment effect and the confounding function and further study the theoretical properties of the proposed integrative estimators based on the theory of reproducing kernel Hilbert space and empirical process. The proposed methodology is shown to outperform the approach solely based on the trial data through simulation studies and an integrative analysis of the data from a randomized trial and a real-world registry on early-stage non-small-cell lung cancer.","Statistical Inference for Heterogeneous Treatment Effect with Right-censored Data from Synthesizing Randomized Clinical Trials and Real-world Data The heterogeneous treatment effect plays a crucial role in precision medicine. There is evidence that real-world data, even subject to biases, can be employed as supplementary evidence for randomized clinical trials to improve the statistical efficiency of the heterogeneous treatment effect estimation. In this paper, for survival data with right censoring, we consider estimating the heterogeneous treatment effect, defined as the difference of the treatment-specific conditional restricted mean survival times given covariates, by synthesizing evidence from randomized clinical trials and the real-world data with possible biases. We define an omnibus confounding function to characterize the effect of biases caused by unmeasured confounders, censoring, outcome heterogeneity, and measurement error, and further, identify it by combining the trial and real-world data. We propose a penalized sieve method to estimate the heterogeneous treatment effect and the confounding function and further study the theoretical properties of the proposed integrative estimators based on the theory of reproducing kernel Hilbert space and empirical process. The proposed methodology is shown to outperform the approach solely based on the trial data through simulation studies and an integrative analysis of the data from a randomized trial and a real-world registry on early-stage non-small-cell lung cancer.",Healthcare
Enhancing Tourism Recommender Systems for Sustainable City Trips Using Retrieval-Augmented Generation,"Tourism Recommender Systems (TRS) have traditionally focused on providing personalized travel suggestions, often prioritizing user preferences without considering broader sustainability goals. Integrating sustainability into TRS has become essential with the increasing need to balance environmental impact, local community interests, and visitor satisfaction. This paper proposes a novel approach to enhancing TRS for sustainable city trips using Large Language Models (LLMs) and a modified Retrieval-Augmented Generation (RAG) pipeline. We enhance the traditional RAG system by incorporating a sustainability metric based on a citys popularity and seasonal demand during the prompt augmentation phase. This modification, called Sustainability Augmented Reranking (SAR), ensures the systems recommendations align with sustainability goals. Evaluations using popular open-source LLMs, such as Llama-3.1-Instruct-8B and Mistral-Instruct-7B, demonstrate that the SAR-enhanced approach consistently matches or outperforms the baseline (without SAR) across most metrics, highlighting the benefits of incorporating sustainability into TRS.","Enhancing Tourism Recommender Systems for Sustainable City Trips Using Retrieval-Augmented Generation Tourism Recommender Systems (TRS) have traditionally focused on providing personalized travel suggestions, often prioritizing user preferences without considering broader sustainability goals. Integrating sustainability into TRS has become essential with the increasing need to balance environmental impact, local community interests, and visitor satisfaction. This paper proposes a novel approach to enhancing TRS for sustainable city trips using Large Language Models (LLMs) and a modified Retrieval-Augmented Generation (RAG) pipeline. We enhance the traditional RAG system by incorporating a sustainability metric based on a citys popularity and seasonal demand during the prompt augmentation phase. This modification, called Sustainability Augmented Reranking (SAR), ensures the systems recommendations align with sustainability goals. Evaluations using popular open-source LLMs, such as Llama-3.1-Instruct-8B and Mistral-Instruct-7B, demonstrate that the SAR-enhanced approach consistently matches or outperforms the baseline (without SAR) across most metrics, highlighting the benefits of incorporating sustainability into TRS.",Environment
Spectral analysis of gene expression profiles using gene networks,"Microarrays have become extremely useful for analysing genetic phenomena, but establishing a relation between microarray analysis results (typically a list of genes) and their biological significance is often difficult. Currently, the standard approach is to map a posteriori the results onto gene networks to elucidate the functions perturbed at the level of pathways. However, integrating a priori knowledge of the gene networks could help in the statistical analysis of gene expression data and in their biological interpretation. Here we propose a method to integrate a priori the knowledge of a gene network in the analysis of gene expression data. The approach is based on the spectral decomposition of gene expression profiles with respect to the eigenfunctions of the graph, resulting in an attenuation of the high-frequency components of the expression profiles with respect to the topology of the graph. We show how to derive unsupervised and supervised classification algorithms of expression profiles, resulting in classifiers with biological relevance. We applied the method to the analysis of a set of expression profiles from irradiated and non-irradiated yeast strains. It performed at least as well as the usual classification but provides much more biologically relevant results and allows a direct biological interpretation.","Spectral analysis of gene expression profiles using gene networks Microarrays have become extremely useful for analysing genetic phenomena, but establishing a relation between microarray analysis results (typically a list of genes) and their biological significance is often difficult. Currently, the standard approach is to map a posteriori the results onto gene networks to elucidate the functions perturbed at the level of pathways. However, integrating a priori knowledge of the gene networks could help in the statistical analysis of gene expression data and in their biological interpretation. Here we propose a method to integrate a priori the knowledge of a gene network in the analysis of gene expression data. The approach is based on the spectral decomposition of gene expression profiles with respect to the eigenfunctions of the graph, resulting in an attenuation of the high-frequency components of the expression profiles with respect to the topology of the graph. We show how to derive unsupervised and supervised classification algorithms of expression profiles, resulting in classifiers with biological relevance. We applied the method to the analysis of a set of expression profiles from irradiated and non-irradiated yeast strains. It performed at least as well as the usual classification but provides much more biologically relevant results and allows a direct biological interpretation.",Healthcare
"Detection, attribution, and modeling of climate change: key open issues","The CMIP global climate models (GCMs) assess that nearly 100 of global surface warming observed between 1850-1900 and 2011-2020 is attributable to anthropogenic drivers like greenhouse gas emissions. These models also generate future climate projections based on shared socioeconomic pathways (SSPs), aiding in risk assessment and the development of costly Net-Zero climate mitigation strategies. Yet, the CMIP GCMs face significant scientific challenges in attributing and modeling climate change, particularly in capturing natural climate variability over multiple timescales throughout the Holocene. Other key concerns include the reliability of global surface temperature records, the accuracy of solar irradiance models, and the robustness of climate sensitivity estimates. Global warming estimates may be overstated due to uncorrected non-climatic biases, and the GCMs may significantly underestimate solar and astronomical influences on climate variations. The equilibrium climate sensitivity (ECS) to radiative forcing could be lower than commonly assumed; empirical findings suggest ECS values lower than 3 K and possibly even closer to 1.1 - 0.4 K. Empirical models incorporating natural variability suggest that the 21st-century global warming may remain moderate, even under SSP scenarios that do not necessitate Net-Zero emission policies. These findings raise important questions regarding the necessity and urgency of implementing aggressive climate mitigation strategies. While GCMs remain essential tools for climate research and policymaking, their scientific limitations underscore the need for more refined modeling approaches to ensure accurate future climate assessments. Addressing uncertainties related to climate change detection, natural variability, solar influences, and climate sensitivity to radiative forcing will enhance predictions and better inform sustainable climate strategies.","Detection, attribution, and modeling of climate change: key open issues The CMIP global climate models (GCMs) assess that nearly 100 of global surface warming observed between 1850-1900 and 2011-2020 is attributable to anthropogenic drivers like greenhouse gas emissions. These models also generate future climate projections based on shared socioeconomic pathways (SSPs), aiding in risk assessment and the development of costly Net-Zero climate mitigation strategies. Yet, the CMIP GCMs face significant scientific challenges in attributing and modeling climate change, particularly in capturing natural climate variability over multiple timescales throughout the Holocene. Other key concerns include the reliability of global surface temperature records, the accuracy of solar irradiance models, and the robustness of climate sensitivity estimates. Global warming estimates may be overstated due to uncorrected non-climatic biases, and the GCMs may significantly underestimate solar and astronomical influences on climate variations. The equilibrium climate sensitivity (ECS) to radiative forcing could be lower than commonly assumed; empirical findings suggest ECS values lower than 3 K and possibly even closer to 1.1 - 0.4 K. Empirical models incorporating natural variability suggest that the 21st-century global warming may remain moderate, even under SSP scenarios that do not necessitate Net-Zero emission policies. These findings raise important questions regarding the necessity and urgency of implementing aggressive climate mitigation strategies. While GCMs remain essential tools for climate research and policymaking, their scientific limitations underscore the need for more refined modeling approaches to ensure accurate future climate assessments. Addressing uncertainties related to climate change detection, natural variability, solar influences, and climate sensitivity to radiative forcing will enhance predictions and better inform sustainable climate strategies.",Environment
History Of Rigor: A Review Of 20th Century Science Education,"Rigor is an often sought after but ill-defined concept in education. This work reviews several models of rigor from current literature before proposing a tool which is used to analyze science education throughout history. The 20textsuperscriptth century science education in the United States was subject to changing sociopolitical motivations about the use of science both in general and for students. These factors as well as developments in theory of learning and broad education reforms had changing affects on the level of rigor in science education. This work analyzes the theoretical level of rigor of science education in the US based on two main motivating factors for science education; science as a social endeavor and science as a discipline, throughout the 20textsuperscriptth century.","History Of Rigor: A Review Of 20th Century Science Education Rigor is an often sought after but ill-defined concept in education. This work reviews several models of rigor from current literature before proposing a tool which is used to analyze science education throughout history. The 20textsuperscriptth century science education in the United States was subject to changing sociopolitical motivations about the use of science both in general and for students. These factors as well as developments in theory of learning and broad education reforms had changing affects on the level of rigor in science education. This work analyzes the theoretical level of rigor of science education in the US based on two main motivating factors for science education; science as a social endeavor and science as a discipline, throughout the 20textsuperscriptth century.",Education
From chalkboards to chatbots: SELAR assists teachers in embracing AI in the curriculum,"This paper introduces SELAR, a framework designed to effectively help teachers integrate artificial intelligence (AI) into their curriculum. The framework was designed by running workshops organized to gather lecturers feedback. In this paper, we assess the effectiveness of the framework through additional workshops organized with lecturers from the Hague University of Applied Sciences. The workshops tested the application of the framework to adapt existing courses to leverage generative AI technology. Each participant was tasked to apply SELAR to one of their learning goals in order to evaluate AI integration potential and, if successful, to update the teaching methods accordingly. Findings show that teachers were able to effectively use the SELAR to integrate generative AI into their courses. Future work will focus on providing additional guidance and examples to use the framework more effectively.","From chalkboards to chatbots: SELAR assists teachers in embracing AI in the curriculum This paper introduces SELAR, a framework designed to effectively help teachers integrate artificial intelligence (AI) into their curriculum. The framework was designed by running workshops organized to gather lecturers feedback. In this paper, we assess the effectiveness of the framework through additional workshops organized with lecturers from the Hague University of Applied Sciences. The workshops tested the application of the framework to adapt existing courses to leverage generative AI technology. Each participant was tasked to apply SELAR to one of their learning goals in order to evaluate AI integration potential and, if successful, to update the teaching methods accordingly. Findings show that teachers were able to effectively use the SELAR to integrate generative AI into their courses. Future work will focus on providing additional guidance and examples to use the framework more effectively.",Education
On the utility of the multimodal problem generator for assessing the performance of Evolutionary Algorithms,"This paper looks in detail at how an evolutionary algorithm attempts to solve instances from the multimodal problem generator. The paper shows that in order to consistently reach the global optimum, an evolutionary algorithm requires a population size that should grow at least linearly with the number of peaks. It is also shown a close relationship between the supply and decision making issues that have been identified previously in the context of population sizing models for additively decomposable problems. The most important result of the paper, however, is that solving an instance of the multimodal problem generator is like solving a peak-in-a-haystack, and it is argued that evolutionary algorithms are not the best algorithms for such a task. Finally, and as opposed to what several researchers have been doing, it is our strong belief that the multimodal problem generator is not adequate for assessing the performance of evolutionary algorithms.","On the utility of the multimodal problem generator for assessing the performance of Evolutionary Algorithms This paper looks in detail at how an evolutionary algorithm attempts to solve instances from the multimodal problem generator. The paper shows that in order to consistently reach the global optimum, an evolutionary algorithm requires a population size that should grow at least linearly with the number of peaks. It is also shown a close relationship between the supply and decision making issues that have been identified previously in the context of population sizing models for additively decomposable problems. The most important result of the paper, however, is that solving an instance of the multimodal problem generator is like solving a peak-in-a-haystack, and it is argued that evolutionary algorithms are not the best algorithms for such a task. Finally, and as opposed to what several researchers have been doing, it is our strong belief that the multimodal problem generator is not adequate for assessing the performance of evolutionary algorithms.",Technology
Adaptive Load Balancing: A Study in Multi-Agent Learning,"We study the process of multi-agent reinforcement learning in the context of load balancing in a distributed system, without use of either central coordination or explicit communication. We first define a precise framework in which to study adaptive load balancing, important features of which are its stochastic nature and the purely local information available to individual agents. Given this framework, we show illuminating results on the interplay between basic adaptive behavior parameters and their effect on system efficiency. We then investigate the properties of adaptive load balancing in heterogeneous populations, and address the issue of exploration vs. exploitation in that context. Finally, we show that naive use of communication may not improve, and might even harm system efficiency.","Adaptive Load Balancing: A Study in Multi-Agent Learning We study the process of multi-agent reinforcement learning in the context of load balancing in a distributed system, without use of either central coordination or explicit communication. We first define a precise framework in which to study adaptive load balancing, important features of which are its stochastic nature and the purely local information available to individual agents. Given this framework, we show illuminating results on the interplay between basic adaptive behavior parameters and their effect on system efficiency. We then investigate the properties of adaptive load balancing in heterogeneous populations, and address the issue of exploration vs. exploitation in that context. Finally, we show that naive use of communication may not improve, and might even harm system efficiency.",Technology
The pharmacophore kernel for virtual screening with support vector machines,"We introduce a family of positive definite kernels specifically optimized for the manipulation of 3D structures of molecules with kernel methods. The kernels are based on the comparison of the three-points pharmacophores present in the 3D structures of molecul es, a set of molecular features known to be particularly relevant for virtual screening applications. We present a computationally demanding exact implementation of these kernels, as well as fast approximations related to the classical fingerprint-based approa ches. Experimental results suggest that this new approach outperforms state-of-the-art algorithms based on the 2D structure of mol ecules for the detection of inhibitors of several drug targets.","The pharmacophore kernel for virtual screening with support vector machines We introduce a family of positive definite kernels specifically optimized for the manipulation of 3D structures of molecules with kernel methods. The kernels are based on the comparison of the three-points pharmacophores present in the 3D structures of molecul es, a set of molecular features known to be particularly relevant for virtual screening applications. We present a computationally demanding exact implementation of these kernels, as well as fast approximations related to the classical fingerprint-based approa ches. Experimental results suggest that this new approach outperforms state-of-the-art algorithms based on the 2D structure of mol ecules for the detection of inhibitors of several drug targets.",Healthcare
Energy Allocation Policy for Cellular Networks Powered by Renewable Energy,"The explosive wireless data service requirement accompanied with carbon dioxide emission and consumption of traditional energy has put pressure on both industry and academia. Wireless networks powered with the uneven and intermittent generated renewable energy have been widely researched and lead to a new research paradigm called green communication. In this paper, we comprehensively consider the total generated renewable energy, QoS requirement and channel quality, then propose a utility based renewable energy allocation policy. The utility here means the satisfaction degree of users with a certain amount allocated renewable energy. The energy allocation problem is formulated as a constraint optimization problem and a heuristic algorithm with low complexity is derived to solve the raised problem. Numerical results show that the renewable energy allocation policy is applicable for any situation. When the renewable energy is very scarce, only users with good channel quality can achieve allocated energy.","Energy Allocation Policy for Cellular Networks Powered by Renewable Energy The explosive wireless data service requirement accompanied with carbon dioxide emission and consumption of traditional energy has put pressure on both industry and academia. Wireless networks powered with the uneven and intermittent generated renewable energy have been widely researched and lead to a new research paradigm called green communication. In this paper, we comprehensively consider the total generated renewable energy, QoS requirement and channel quality, then propose a utility based renewable energy allocation policy. The utility here means the satisfaction degree of users with a certain amount allocated renewable energy. The energy allocation problem is formulated as a constraint optimization problem and a heuristic algorithm with low complexity is derived to solve the raised problem. Numerical results show that the renewable energy allocation policy is applicable for any situation. When the renewable energy is very scarce, only users with good channel quality can achieve allocated energy.",Environment
Nonparametric estimation of an optimal treatment rule with fused randomized trials and missing effect modifiers,"A fundamental principle of clinical medicine is that a treatment should only be administered to those patients who would benefit from it. Treatment strategies that assign treatment to patients as a function of their individual characteristics are known as dynamic treatment rules. The dynamic treatment rule that optimizes the outcome in the population is called the optimal dynamic treatment rule. Randomized clinical trials are considered the gold standard for estimating the marginal causal effect of a treatment on an outcome; they are often not powered to detect heterogeneous treatment effects, and thus, may rarely inform more personalized treatment decisions. The availability of multiple trials studying a common set of treatments presents an opportunity for combining data, often called data-fusion, to better estimate dynamic treatment rules. However, there may be a mismatch in the set of patient covariates measured across trials. We address this problem here; we propose a nonparametric estimator for the optimal dynamic treatment rule that leverages information across the set of randomized trials. We apply the estimator to fused randomized trials of medications for the treatment of opioid use disorder to estimate a treatment rule that would match patient subgroups with the medication that would minimize risk of return to regular opioid use.","Nonparametric estimation of an optimal treatment rule with fused randomized trials and missing effect modifiers A fundamental principle of clinical medicine is that a treatment should only be administered to those patients who would benefit from it. Treatment strategies that assign treatment to patients as a function of their individual characteristics are known as dynamic treatment rules. The dynamic treatment rule that optimizes the outcome in the population is called the optimal dynamic treatment rule. Randomized clinical trials are considered the gold standard for estimating the marginal causal effect of a treatment on an outcome; they are often not powered to detect heterogeneous treatment effects, and thus, may rarely inform more personalized treatment decisions. The availability of multiple trials studying a common set of treatments presents an opportunity for combining data, often called data-fusion, to better estimate dynamic treatment rules. However, there may be a mismatch in the set of patient covariates measured across trials. We address this problem here; we propose a nonparametric estimator for the optimal dynamic treatment rule that leverages information across the set of randomized trials. We apply the estimator to fused randomized trials of medications for the treatment of opioid use disorder to estimate a treatment rule that would match patient subgroups with the medication that would minimize risk of return to regular opioid use.",Healthcare
Investigating the Role of Renewable Energies in Integrated Energy-water Nexus Planning under Uncertainty Using Fuzzy Logic,"Energy and water systems are highly interconnected. Energy is required to extract, transmit, and treat water and wastewater, and water is needed for cooling energy systems. There is a rapid increase in demand for energy and water due to factors such as population and economic growth. In less than 30 years, the need for energy and water will nearly double globally. As the energy and water resources are limited, it is critical to have a sustainable energy-water nexus framework to meet these growing demands. Renewable energies provide substantial opportunities in energy-water nexuses by boosting energy and water reliability and sustainability and can be less water-intensive than conventional technologies. These resources, such as wind and solar power, do not need water inputs. As a result, they can be used as a supplement to the energy-water nexus portfolio. In this paper, renewable energies in energy-water nexus have been investigated for a range of possible scenarios. As renewable energy resources are not deterministic, fuzzy logic is used to model the uncertainty. The results show that renewable energies can significantly improve the energy-water nexus planning; however, the power grid reliability on renewable energy should be aligned with the level of systems uncertainty. The gap between the decisions extracted from the Fuzzy model and the deterministic model amplifies the importance of considering uncertainty to generate reliable decisions. Keywords: Energy-water Nexus, Renewable Energies, Optimization under Uncertainty, Fuzzy Logic.","Investigating the Role of Renewable Energies in Integrated Energy-water Nexus Planning under Uncertainty Using Fuzzy Logic Energy and water systems are highly interconnected. Energy is required to extract, transmit, and treat water and wastewater, and water is needed for cooling energy systems. There is a rapid increase in demand for energy and water due to factors such as population and economic growth. In less than 30 years, the need for energy and water will nearly double globally. As the energy and water resources are limited, it is critical to have a sustainable energy-water nexus framework to meet these growing demands. Renewable energies provide substantial opportunities in energy-water nexuses by boosting energy and water reliability and sustainability and can be less water-intensive than conventional technologies. These resources, such as wind and solar power, do not need water inputs. As a result, they can be used as a supplement to the energy-water nexus portfolio. In this paper, renewable energies in energy-water nexus have been investigated for a range of possible scenarios. As renewable energy resources are not deterministic, fuzzy logic is used to model the uncertainty. The results show that renewable energies can significantly improve the energy-water nexus planning; however, the power grid reliability on renewable energy should be aligned with the level of systems uncertainty. The gap between the decisions extracted from the Fuzzy model and the deterministic model amplifies the importance of considering uncertainty to generate reliable decisions. Keywords: Energy-water Nexus, Renewable Energies, Optimization under Uncertainty, Fuzzy Logic.",Environment
Computer-aided hepatic tumour ablation : requirements and preliminary results,"Surgical resection of hepatic tumours is not always possible, since it depends on different factors, among which their location inside the liver functional segments. Alternative techniques consist in local use of chemical or physical agents to destroy the tumour. Radio frequency and cryosurgical ablations are examples of such alternative techniques that may be performed percutaneously. This requires a precise localisation of the tumour placement during ablation. Computer-assisted surgery tools may be used in conjunction with these new ablation techniques to improve the therapeutic efficiency, whilst they benefit from minimal invasiveness. This paper introduces the principles of a system for computer-assisted hepatic tumour ablation and describes preliminary experiments focusing on data registration evaluation. To keep close to conventional protocols, we consider registration of pre-operative CT or MRI data to intra-operative echographic data.","Computer-aided hepatic tumour ablation : requirements and preliminary results Surgical resection of hepatic tumours is not always possible, since it depends on different factors, among which their location inside the liver functional segments. Alternative techniques consist in local use of chemical or physical agents to destroy the tumour. Radio frequency and cryosurgical ablations are examples of such alternative techniques that may be performed percutaneously. This requires a precise localisation of the tumour placement during ablation. Computer-assisted surgery tools may be used in conjunction with these new ablation techniques to improve the therapeutic efficiency, whilst they benefit from minimal invasiveness. This paper introduces the principles of a system for computer-assisted hepatic tumour ablation and describes preliminary experiments focusing on data registration evaluation. To keep close to conventional protocols, we consider registration of pre-operative CT or MRI data to intra-operative echographic data.",Healthcare
The NoN Approach to Autonomic Face Recognition,"A method of autonomic face recognition based on the biologically plausible network of networks (NoN) model of information processing is presented. The NoN model is based on locally parallel and globally coordinated transformations in which the neurons or computational units form distributed networks, which themselves link to form larger networks. This models the structures in the cerebral cortex described by Mountcastle and the architecture based on that proposed for information processing by Sutton. In the proposed implementation, face images are processed by a nested family of locally operating networks along with a hierarchically superior network that classifies the information from each of the local networks. The results of the experiments yielded a maximum of 98.5 recognition accuracy and an average of 97.4 recognition accuracy on a benchmark database.","The NoN Approach to Autonomic Face Recognition A method of autonomic face recognition based on the biologically plausible network of networks (NoN) model of information processing is presented. The NoN model is based on locally parallel and globally coordinated transformations in which the neurons or computational units form distributed networks, which themselves link to form larger networks. This models the structures in the cerebral cortex described by Mountcastle and the architecture based on that proposed for information processing by Sutton. In the proposed implementation, face images are processed by a nested family of locally operating networks along with a hierarchically superior network that classifies the information from each of the local networks. The results of the experiments yielded a maximum of 98.5 recognition accuracy and an average of 97.4 recognition accuracy on a benchmark database.",Technology
Learning Membership Functions in a Function-Based Object Recognition System,"Functionality-based recognition systems recognize objects at the category level by reasoning about how well the objects support the expected function. Such systems naturally associate a measure of goodness or membership value with a recognized object. This measure of goodness is the result of combining individual measures, or membership values, from potentially many primitive evaluations of different properties of the objects shape. A membership function is used to compute the membership value when evaluating a primitive of a particular physical property of an object. In previous versions of a recognition system known as Gruff, the membership function for each of the primitive evaluations was hand-crafted by the system designer. In this paper, we provide a learning component for the Gruff system, called Omlet, that automatically learns membership functions given a set of example objects labeled with their desired category measure. The learning algorithm is generally applicable to any problem in which low-level membership values are combined through an and-or tree structure to give a final overall membership value.","Learning Membership Functions in a Function-Based Object Recognition System Functionality-based recognition systems recognize objects at the category level by reasoning about how well the objects support the expected function. Such systems naturally associate a measure of goodness or membership value with a recognized object. This measure of goodness is the result of combining individual measures, or membership values, from potentially many primitive evaluations of different properties of the objects shape. A membership function is used to compute the membership value when evaluating a primitive of a particular physical property of an object. In previous versions of a recognition system known as Gruff, the membership function for each of the primitive evaluations was hand-crafted by the system designer. In this paper, we provide a learning component for the Gruff system, called Omlet, that automatically learns membership functions given a set of example objects labeled with their desired category measure. The learning algorithm is generally applicable to any problem in which low-level membership values are combined through an and-or tree structure to give a final overall membership value.",Technology
Effects of the globalization in the Korean financial markets,"We study the effect of globalization on the Korean market, one of the emerging markets. Some characteristics of the Korean market are different from those of the mature market according to the latest market data, and this is due to the influence of foreign markets or investors. We concentrate on the market network structures over the past two decades with knowledge of the history of the market, and determine the globalization effect and market integration as a function of time.","Effects of the globalization in the Korean financial markets We study the effect of globalization on the Korean market, one of the emerging markets. Some characteristics of the Korean market are different from those of the mature market according to the latest market data, and this is due to the influence of foreign markets or investors. We concentrate on the market network structures over the past two decades with knowledge of the history of the market, and determine the globalization effect and market integration as a function of time.",Finance
The mobile information and educational environment of higher educational institution,"In the modern world in the conditions of informatization of society and high level of competition at the labor-market the problem of preparation of specialists appears to the use of modern information and of communication technologies. Modern higher educational establishment must become the core of innovative education with problem preparation of specialists of new generation. Especially it touches preparation professionally of competent teachers, capable easily to adapt oneself in a modern educational environment, be competitive in the conditions of modern labor-market. Purpose. To highlight the definition of mobile information and educational environment of higher educational institution. Results. Define the concept of mobile information and educational environment of a higher educational institution aiming to meet the educational and research needs of all users in providing the necessary e-resources anytime and anywhere. Conclusion. Mobile information and educational environment of a higher educational institution ensures the realization of a few preferences: effective using modern technical learning tools; attracting the best educators; implementation and supporting authors courses; ensuring purposeful development of students. In such environment every student have free access (independent from time and place) to any materials from the academic disciplines, while gaining for them the necessary practical skills, useful implements interaction, knowledge sharing, organizes continuous learning process. The creation and support of mobile information and educational environment of higher educational institution will bring the University activities to a qualitatively new level and enhance its competitiveness in modern conditions.","The mobile information and educational environment of higher educational institution In the modern world in the conditions of informatization of society and high level of competition at the labor-market the problem of preparation of specialists appears to the use of modern information and of communication technologies. Modern higher educational establishment must become the core of innovative education with problem preparation of specialists of new generation. Especially it touches preparation professionally of competent teachers, capable easily to adapt oneself in a modern educational environment, be competitive in the conditions of modern labor-market. Purpose. To highlight the definition of mobile information and educational environment of higher educational institution. Results. Define the concept of mobile information and educational environment of a higher educational institution aiming to meet the educational and research needs of all users in providing the necessary e-resources anytime and anywhere. Conclusion. Mobile information and educational environment of a higher educational institution ensures the realization of a few preferences: effective using modern technical learning tools; attracting the best educators; implementation and supporting authors courses; ensuring purposeful development of students. In such environment every student have free access (independent from time and place) to any materials from the academic disciplines, while gaining for them the necessary practical skills, useful implements interaction, knowledge sharing, organizes continuous learning process. The creation and support of mobile information and educational environment of higher educational institution will bring the University activities to a qualitatively new level and enhance its competitiveness in modern conditions.",Education
BESS Aided Reconfigurable Energy Supply using Deep Reinforcement Learning for 5G and Beyond,"The year of 2020 has witnessed the unprecedented development of 5G networks, along with the widespread deployment of 5G base stations (BSs). Nevertheless, the enormous energy consumption of BSs and the incurred huge energy cost have become significant concerns for the mobile operators. As the continuous decline of the renewable energy cost, equipping the power-hungry BSs with renewable energy generators could be a sustainable solution. In this work, we propose an energy storage aided reconfigurable renewable energy supply solution for the BS, which could supply clean energy to the BS and store surplus energy for backup usage. Specifically, to flexibly reconfigure the batterys dischargingcharging operations, we propose a deep reinforcement learning based reconfiguring policy, which can adapt to the dynamical renewable energy generations as well as the varying power demands. Our experiments using the real-world data on renewable energy generations and power demands demonstrate that, our reconfigurable power supply solution can achieve an energy saving ratio of 74.8, compared to the case with traditional power grid supply.","BESS Aided Reconfigurable Energy Supply using Deep Reinforcement Learning for 5G and Beyond The year of 2020 has witnessed the unprecedented development of 5G networks, along with the widespread deployment of 5G base stations (BSs). Nevertheless, the enormous energy consumption of BSs and the incurred huge energy cost have become significant concerns for the mobile operators. As the continuous decline of the renewable energy cost, equipping the power-hungry BSs with renewable energy generators could be a sustainable solution. In this work, we propose an energy storage aided reconfigurable renewable energy supply solution for the BS, which could supply clean energy to the BS and store surplus energy for backup usage. Specifically, to flexibly reconfigure the batterys dischargingcharging operations, we propose a deep reinforcement learning based reconfiguring policy, which can adapt to the dynamical renewable energy generations as well as the varying power demands. Our experiments using the real-world data on renewable energy generations and power demands demonstrate that, our reconfigurable power supply solution can achieve an energy saving ratio of 74.8, compared to the case with traditional power grid supply.",Environment
Optimally Designing Cybersecurity Insurance Contracts to Encourage the Sharing of Medical Data,"Though the sharing of medical data has the potential to lead to breakthroughs in health care, the sharing process itself exposes patients and health care providers to various risks. Patients face risks due to the possible loss in privacy or livelihood that can occur when medical data is stolen or used in non-permitted ways, whereas health care providers face risks due to the associated liability. For medical data, these risks persist even after anonymizingdeidentifying, according to the standards defined in existing legislation, the data sets prior to sharing, because shared medical data can often be deanonymizedreidentified using advanced artificial intelligence and machine learning methodologies. As a result, health care providers are hesitant to share medical data. One possible solution to encourage health care providers to responsibly share data is through the use of cybersecurity insurance contracts. This paper studies the problem of designing optimal cybersecurity insurance contracts, with the goal of encouraging the sharing of the medical data. We use a principal-agent model with moral hazard to model various scenarios, derive the optimal contract, discuss its implications, and perform numerical case studies. In particular, we consider two scenarios: the first scenario is where a health care provider is selling medical data to a technology firm who is developing an artificial intelligence algorithm using the shared data. The second scenario is where a group of health care providers share health data amongst themselves for the purpose of furthering medical research using the aggregated medical data.","Optimally Designing Cybersecurity Insurance Contracts to Encourage the Sharing of Medical Data Though the sharing of medical data has the potential to lead to breakthroughs in health care, the sharing process itself exposes patients and health care providers to various risks. Patients face risks due to the possible loss in privacy or livelihood that can occur when medical data is stolen or used in non-permitted ways, whereas health care providers face risks due to the associated liability. For medical data, these risks persist even after anonymizingdeidentifying, according to the standards defined in existing legislation, the data sets prior to sharing, because shared medical data can often be deanonymizedreidentified using advanced artificial intelligence and machine learning methodologies. As a result, health care providers are hesitant to share medical data. One possible solution to encourage health care providers to responsibly share data is through the use of cybersecurity insurance contracts. This paper studies the problem of designing optimal cybersecurity insurance contracts, with the goal of encouraging the sharing of the medical data. We use a principal-agent model with moral hazard to model various scenarios, derive the optimal contract, discuss its implications, and perform numerical case studies. In particular, we consider two scenarios: the first scenario is where a health care provider is selling medical data to a technology firm who is developing an artificial intelligence algorithm using the shared data. The second scenario is where a group of health care providers share health data amongst themselves for the purpose of furthering medical research using the aggregated medical data.",Healthcare
Probabilistic Temporal Prediction of Continuous Disease Trajectories and Treatment Effects Using Neural SDEs,"Personalized medicine based on medical images, including predicting future individualized clinical disease progression and treatment response, would have an enormous impact on healthcare and drug development, particularly for diseases (e.g. multiple sclerosis (MS)) with long term, complex, heterogeneous evolutions and no cure. In this work, we present the first stochastic causal temporal framework to model the continuous temporal evolution of disease progression via Neural Stochastic Differential Equations (NSDE). The proposed causal inference model takes as input the patients high dimensional images (MRI) and tabular data, and predicts both factual and counterfactual progression trajectories on different treatments in latent space. The NSDE permits the estimation of high-confidence personalized trajectories and treatment effects. Extensive experiments were performed on a large, multi-centre, proprietary dataset of patient 3D MRI and clinical data acquired during several randomized clinical trials for MS treatments. Our results present the first successful uncertainty-based causal Deep Learning (DL) model to: (a) accurately predict future patient MS disability evolution (e.g. EDSS) and treatment effects leveraging baseline MRI, and (b) permit the discovery of subgroups of patients for which the model has high confidence in their response to treatment even in clinical trials which did not reach their clinical endpoints.","Probabilistic Temporal Prediction of Continuous Disease Trajectories and Treatment Effects Using Neural SDEs Personalized medicine based on medical images, including predicting future individualized clinical disease progression and treatment response, would have an enormous impact on healthcare and drug development, particularly for diseases (e.g. multiple sclerosis (MS)) with long term, complex, heterogeneous evolutions and no cure. In this work, we present the first stochastic causal temporal framework to model the continuous temporal evolution of disease progression via Neural Stochastic Differential Equations (NSDE). The proposed causal inference model takes as input the patients high dimensional images (MRI) and tabular data, and predicts both factual and counterfactual progression trajectories on different treatments in latent space. The NSDE permits the estimation of high-confidence personalized trajectories and treatment effects. Extensive experiments were performed on a large, multi-centre, proprietary dataset of patient 3D MRI and clinical data acquired during several randomized clinical trials for MS treatments. Our results present the first successful uncertainty-based causal Deep Learning (DL) model to: (a) accurately predict future patient MS disability evolution (e.g. EDSS) and treatment effects leveraging baseline MRI, and (b) permit the discovery of subgroups of patients for which the model has high confidence in their response to treatment even in clinical trials which did not reach their clinical endpoints.",Healthcare
Barriers to Active Learning for Computer Science Faculty,"Active learning is a proven pedagogical style that has demonstrated value by improving students performance and classroom experience. In spite of the evidence, adoption of active learning in computer science remains relatively low. To identify what barriers to adoption exist, an electronic survey was sent to 369 computer science faculty in a state in the Upper Midwest and to 78 administrators and support staff. Analysis of the responses revealed that time remained the most commonly reported barrier for faculty that desire to change their teaching style, with 42.8 of faculty respondents disagreeing with the statement that they have the time they need to change their teaching style. Administrators and support staff also indicated that time was a concern but that otherwise faculty were aware of active learning and had the resources they need. Reported use of active learning pedagogy was much higher among faculty that received pedagogical training during their undergraduate or graduate studies. Given the time constraints of faculty, it is recommended that new avenues be explored to provide future faculty with exposure to active learning pedagogy in their undergraduate and graduate training.","Barriers to Active Learning for Computer Science Faculty Active learning is a proven pedagogical style that has demonstrated value by improving students performance and classroom experience. In spite of the evidence, adoption of active learning in computer science remains relatively low. To identify what barriers to adoption exist, an electronic survey was sent to 369 computer science faculty in a state in the Upper Midwest and to 78 administrators and support staff. Analysis of the responses revealed that time remained the most commonly reported barrier for faculty that desire to change their teaching style, with 42.8 of faculty respondents disagreeing with the statement that they have the time they need to change their teaching style. Administrators and support staff also indicated that time was a concern but that otherwise faculty were aware of active learning and had the resources they need. Reported use of active learning pedagogy was much higher among faculty that received pedagogical training during their undergraduate or graduate studies. Given the time constraints of faculty, it is recommended that new avenues be explored to provide future faculty with exposure to active learning pedagogy in their undergraduate and graduate training.",Education
Financial climate risk: a review of recent advances and key challenges,"The document provides an overview of financial climate risks. It delves into how climate change impacts the global financial system, distinguishing between physical risks (such as extreme weather events) and transition risks (stemming from policy changes and economic transitions towards low carbon technologies). The paper underlines the complexity of accurately defining financial climate risk, citing the integration of climate science with financial risk analysis as a significant challenge. The paper highlights the pivotal role of microfinance institutions (MFIs) in addressing financial climate risk, especially for populations vulnerable to climate change. The document emphasizes the importance of updating risk management practices within MFIs to explicitly include climate risk assessments and suggests leveraging technology to improve these practices.","Financial climate risk: a review of recent advances and key challenges The document provides an overview of financial climate risks. It delves into how climate change impacts the global financial system, distinguishing between physical risks (such as extreme weather events) and transition risks (stemming from policy changes and economic transitions towards low carbon technologies). The paper underlines the complexity of accurately defining financial climate risk, citing the integration of climate science with financial risk analysis as a significant challenge. The paper highlights the pivotal role of microfinance institutions (MFIs) in addressing financial climate risk, especially for populations vulnerable to climate change. The document emphasizes the importance of updating risk management practices within MFIs to explicitly include climate risk assessments and suggests leveraging technology to improve these practices.",Environment
What are the real implications for CO_2 as generation from renewables increases?,"Wind and solar electricity generation account for 14 of total electricity generation in the United States and are expected to continue to grow in the next decades. In low carbon systems, generation from renewable energy sources displaces conventional fossil fuel power plants resulting in lower system-level emissions and emissions intensity. However, we find that intermittent generation from renewables changes the way conventional thermal power plants operate, and that the displacement of generation is not 1 to 1 as expected. Our work provides a method that allows policy and decision makers to continue to track the effect of additional renewable capacity and the resulting thermal power plant operational responses.","What are the real implications for CO_2 as generation from renewables increases? Wind and solar electricity generation account for 14 of total electricity generation in the United States and are expected to continue to grow in the next decades. In low carbon systems, generation from renewable energy sources displaces conventional fossil fuel power plants resulting in lower system-level emissions and emissions intensity. However, we find that intermittent generation from renewables changes the way conventional thermal power plants operate, and that the displacement of generation is not 1 to 1 as expected. Our work provides a method that allows policy and decision makers to continue to track the effect of additional renewable capacity and the resulting thermal power plant operational responses.",Environment
Measurement of Investment activity in China based on Natural language processing technology,"The purpose of this study is to propose a new index to measure and reflect Chinas investment activity in time, and to analyze the changes of Chinas investment activity in the past five years. This study first uses the NEZHA model for semantic representation, and expand the indicator system based on semantic similarity. Then we calculate Chinas investment activity index by using the network search data. This study shows that Chinas investment activity began to decline in 2019, rebounded for a period of time after the outbreak of COVID-19 in 2020, and then continued to maintain a downward trend. Private investment activity has declined significantly, while government investment activity has increased. Among the provinces in Chinese Mainland, the investment activity of economically developed provinces has decreased significantly, while the investment activity of some economically less developed provinces in the north and south is higher. After the outbreak of COVID-19, the investment period became shorter. Our research will provide timely investment information for the government, decision makers and managers, as well as provide other researchers who also pay attention to investment with a perspective other than investment in fixed asset.","Measurement of Investment activity in China based on Natural language processing technology The purpose of this study is to propose a new index to measure and reflect Chinas investment activity in time, and to analyze the changes of Chinas investment activity in the past five years. This study first uses the NEZHA model for semantic representation, and expand the indicator system based on semantic similarity. Then we calculate Chinas investment activity index by using the network search data. This study shows that Chinas investment activity began to decline in 2019, rebounded for a period of time after the outbreak of COVID-19 in 2020, and then continued to maintain a downward trend. Private investment activity has declined significantly, while government investment activity has increased. Among the provinces in Chinese Mainland, the investment activity of economically developed provinces has decreased significantly, while the investment activity of some economically less developed provinces in the north and south is higher. After the outbreak of COVID-19, the investment period became shorter. Our research will provide timely investment information for the government, decision makers and managers, as well as provide other researchers who also pay attention to investment with a perspective other than investment in fixed asset.",Finance
Building and Refining Abstract Planning Cases by Change of Representation Language,"ion is one of the most promising approaches to improve the performance of problem solvers. In several domains abstraction by dropping sentences of a domain description -- as used in most hierarchical planners -- has proven useful. In this paper we present examples which illustrate significant drawbacks of abstraction by dropping sentences. To overcome these drawbacks, we propose a more general view of abstraction involving the change of representation language. We have developed a new abstraction methodology and a related sound and complete learning algorithm that allows the complete change of representation language of planning cases from concrete to abstract. However, to achieve a powerful change of the representation language, the abstract language itself as well as rules which describe admissible ways of abstracting states must be provided in the domain model. This new abstraction approach is the core of Paris (Plan Abstraction and Refinement in an Integrated System), a system in which abstract planning cases are automatically learned from given concrete cases. An empirical study in the domain of process planning in mechanical engineering shows significant advantages of the proposed reasoning from abstract cases over classical hierarchical planning.","Building and Refining Abstract Planning Cases by Change of Representation Language ion is one of the most promising approaches to improve the performance of problem solvers. In several domains abstraction by dropping sentences of a domain description -- as used in most hierarchical planners -- has proven useful. In this paper we present examples which illustrate significant drawbacks of abstraction by dropping sentences. To overcome these drawbacks, we propose a more general view of abstraction involving the change of representation language. We have developed a new abstraction methodology and a related sound and complete learning algorithm that allows the complete change of representation language of planning cases from concrete to abstract. However, to achieve a powerful change of the representation language, the abstract language itself as well as rules which describe admissible ways of abstracting states must be provided in the domain model. This new abstraction approach is the core of Paris (Plan Abstraction and Refinement in an Integrated System), a system in which abstract planning cases are automatically learned from given concrete cases. An empirical study in the domain of process planning in mechanical engineering shows significant advantages of the proposed reasoning from abstract cases over classical hierarchical planning.",Technology
Existence of Financial Equilibria in Continuous Time with Potentially Complete Markets,"We prove that in smooth Markovian continuous-time economies with potentially complete asset markets, Radner equilibria with endogenously complete markets exist.","Existence of Financial Equilibria in Continuous Time with Potentially Complete Markets We prove that in smooth Markovian continuous-time economies with potentially complete asset markets, Radner equilibria with endogenously complete markets exist.",Finance
EpiClim: Weekly District-Wise all-India multi-epidemics Climate-Health Dataset for accelerated GeoHealth research,"Climate change significantly impacts public health, driving the emergence and spread of epidemics. Climate health models are essential for assessing and predicting disease outbreaks influenced by climatic variables like temperature and precipitation. For instance, dengue and malaria correlate with temperature changes, while cholera is linked to precipitation anomalies. Advances in AI-enabled weather prediction (AI-NWP) have improved forecasting, but integrating climate models with health systems is hindered by the lack of comprehensive, granular health datasets. This study introduces EpiClim: Indias Epidemic-Climate Dataset, the first weekly district-wise dataset for major epidemics in India from 2009 to the present, sourced from the Integrated Disease Surveillance Programme (IDSP). The dataset, covering diseases like dengue, malaria, and acute-diarrheal disease, bridges the gap between climate and health data, enabling the integration of climate forecasts with epidemic prediction models. This work lays the foundation for coupling predictive climate health models with weather and climate models, advancing efforts to mitigate climate-induced public health crises.","EpiClim: Weekly District-Wise all-India multi-epidemics Climate-Health Dataset for accelerated GeoHealth research Climate change significantly impacts public health, driving the emergence and spread of epidemics. Climate health models are essential for assessing and predicting disease outbreaks influenced by climatic variables like temperature and precipitation. For instance, dengue and malaria correlate with temperature changes, while cholera is linked to precipitation anomalies. Advances in AI-enabled weather prediction (AI-NWP) have improved forecasting, but integrating climate models with health systems is hindered by the lack of comprehensive, granular health datasets. This study introduces EpiClim: Indias Epidemic-Climate Dataset, the first weekly district-wise dataset for major epidemics in India from 2009 to the present, sourced from the Integrated Disease Surveillance Programme (IDSP). The dataset, covering diseases like dengue, malaria, and acute-diarrheal disease, bridges the gap between climate and health data, enabling the integration of climate forecasts with epidemic prediction models. This work lays the foundation for coupling predictive climate health models with weather and climate models, advancing efforts to mitigate climate-induced public health crises.",Environment
Predicting Response-Function Results of ElectricalMechanical Systems Through Artificial Neural Network,"In the present paper a newer application of Artificial Neural Network (ANN) has been developed i.e., predicting response-function results of electrical-mechanical system through ANN. This method is specially useful to complex systems for which it is not possible to find the response-function because of complexity of the system. The proposed approach suggests that how even without knowing the response-function, the response-function results can be predicted with the use of ANN to the system. The steps used are: (i) Depending on the system, the ANN-architecture and the input  output parameters are decided, (ii) Training  test data are generated from simplified circuits and through tactic-superposition of it for complex circuits, (iii) Training the ANN with training data through many cycles and (iv) Test-data are used for predicting the response-function results. It is found that the proposed novel method for response prediction works satisfactorily. Thus this method could be used specially for complex systems where other methods are unable to tackle it. In this paper the application of ANN is particularly demonstrated to electrical-circuit system but can be applied to other systems too.","Predicting Response-Function Results of ElectricalMechanical Systems Through Artificial Neural Network In the present paper a newer application of Artificial Neural Network (ANN) has been developed i.e., predicting response-function results of electrical-mechanical system through ANN. This method is specially useful to complex systems for which it is not possible to find the response-function because of complexity of the system. The proposed approach suggests that how even without knowing the response-function, the response-function results can be predicted with the use of ANN to the system. The steps used are: (i) Depending on the system, the ANN-architecture and the input  output parameters are decided, (ii) Training  test data are generated from simplified circuits and through tactic-superposition of it for complex circuits, (iii) Training the ANN with training data through many cycles and (iv) Test-data are used for predicting the response-function results. It is found that the proposed novel method for response prediction works satisfactorily. Thus this method could be used specially for complex systems where other methods are unable to tackle it. In this paper the application of ANN is particularly demonstrated to electrical-circuit system but can be applied to other systems too.",Technology
Towards Sustainable Development: A Novel Integrated Machine Learning Model for Holistic Environmental Health Monitoring,"Urbanization enables economic growth but also harms the environment through degradation. Traditional methods of detecting environmental issues have proven inefficient. Machine learning has emerged as a promising tool for tracking environmental deterioration by identifying key predictive features. Recent research focused on developing a predictive model using pollutant levels and particulate matter as indicators of environmental state in order to outline challenges. Machine learning was employed to identify patterns linking areas with worse conditions. This research aims to assist governments in identifying intervention points, improving planning and conservation efforts, and ultimately contributing to sustainable development.","Towards Sustainable Development: A Novel Integrated Machine Learning Model for Holistic Environmental Health Monitoring Urbanization enables economic growth but also harms the environment through degradation. Traditional methods of detecting environmental issues have proven inefficient. Machine learning has emerged as a promising tool for tracking environmental deterioration by identifying key predictive features. Recent research focused on developing a predictive model using pollutant levels and particulate matter as indicators of environmental state in order to outline challenges. Machine learning was employed to identify patterns linking areas with worse conditions. This research aims to assist governments in identifying intervention points, improving planning and conservation efforts, and ultimately contributing to sustainable development.",Environment
HouYi: An open-source large language model specially designed for renewable energy and carbon neutrality field,"Renewable energy is important for achieving carbon neutrality goal. With the great success of Large Language Models (LLMs) like ChatGPT in automatic content generation, LLMs are playing an increasingly important role. However, there has not been a specially designed LLM for renewable energy. Meanwhile, there has not been any dataset of renewable energy for training LLMs. Therefore, this paper published the first open-source Renewable Energy Academic Paper (REAP) dataset for non-commercial LLM research of renewable energy. REAP dataset is collected through searching the title and abstract of 1,168,970 academic literatures from Web of Science. Based on REAP dataset, HouYi model, the first LLM for renewable energy, is developed through finetuning general LLMs. HouYi demonstrated powerful academic paper paragraph generation ability in renewable energy field. Experiments show that its ability to generate academic papers on renewable energy is comparable to ChatGPT, slightly outperforms Claude, ERNIE Bot and SparkDesk, and significantly outperforms open-source LLaMA-13B model.","HouYi: An open-source large language model specially designed for renewable energy and carbon neutrality field Renewable energy is important for achieving carbon neutrality goal. With the great success of Large Language Models (LLMs) like ChatGPT in automatic content generation, LLMs are playing an increasingly important role. However, there has not been a specially designed LLM for renewable energy. Meanwhile, there has not been any dataset of renewable energy for training LLMs. Therefore, this paper published the first open-source Renewable Energy Academic Paper (REAP) dataset for non-commercial LLM research of renewable energy. REAP dataset is collected through searching the title and abstract of 1,168,970 academic literatures from Web of Science. Based on REAP dataset, HouYi model, the first LLM for renewable energy, is developed through finetuning general LLMs. HouYi demonstrated powerful academic paper paragraph generation ability in renewable energy field. Experiments show that its ability to generate academic papers on renewable energy is comparable to ChatGPT, slightly outperforms Claude, ERNIE Bot and SparkDesk, and significantly outperforms open-source LLaMA-13B model.",Environment
"A non-linear optimal estimation inverse method for radio occultation measurements of temperature, humidity and surface pressure","An optimal estimation inverse method is presented which can be used to retrieve simultaneously vertical profiles of temperature and specific humidity, in addition to surface pressure, from satellite-to-satellite radio occultation observations of the Earths atmosphere. The method is a non-linear, maximum it a posteriori technique which can accommodate most aspects of the real radio occultation problem and is found to be stable and to converge rapidly in most cases. The optimal estimation inverse method has two distinct advantages over the analytic inverse method in that it accounts for some of the effects of horizontal gradients and is able to retrieve optimally temperature and humidity simultaneously from the observations. It is also able to account for observation noise and other sources of error. Combined, these advantages ensure a realistic retrieval of atmospheric quantities. A complete error analysis emerges naturally from the optimal estimation theory, allowing a full characterisation of the solution. Using this analysis a quality control scheme is implemented which allows anomalous retrieval conditions to be recognised and removed, thus preventing gross retrieval errors. The inverse method presented in this paper has been implemented for bending angle measurements derived from GPSMET radio occultation observations of the Earth. Preliminary results from simulated data suggest that these observations have the potential to improve NWP model analyses significantly throughout their vertical range.","A non-linear optimal estimation inverse method for radio occultation measurements of temperature, humidity and surface pressure An optimal estimation inverse method is presented which can be used to retrieve simultaneously vertical profiles of temperature and specific humidity, in addition to surface pressure, from satellite-to-satellite radio occultation observations of the Earths atmosphere. The method is a non-linear, maximum it a posteriori technique which can accommodate most aspects of the real radio occultation problem and is found to be stable and to converge rapidly in most cases. The optimal estimation inverse method has two distinct advantages over the analytic inverse method in that it accounts for some of the effects of horizontal gradients and is able to retrieve optimally temperature and humidity simultaneously from the observations. It is also able to account for observation noise and other sources of error. Combined, these advantages ensure a realistic retrieval of atmospheric quantities. A complete error analysis emerges naturally from the optimal estimation theory, allowing a full characterisation of the solution. Using this analysis a quality control scheme is implemented which allows anomalous retrieval conditions to be recognised and removed, thus preventing gross retrieval errors. The inverse method presented in this paper has been implemented for bending angle measurements derived from GPSMET radio occultation observations of the Earth. Preliminary results from simulated data suggest that these observations have the potential to improve NWP model analyses significantly throughout their vertical range.",Environment
"A Conceptual Model for Bidirectional Service, Information and Product Quality in an IS Outsourcing Collaboration Environment","This paper advances theory on the process of collaboration between entities and its implications on the quality of services, information, andor products (SIPs) that the collaborating entities provide to each other. It investigates the scenario of outsourced IS projects (such as custom software development) where the extent of collaboration between a client and vendor is high. Using the social exchange theory, the proposed conceptual model tries to establish the bidirectional nature of SIP quality in a collaborative environment, where the SIPs exchanged are possibly dependent on each other, and if any entity wishes to receive high SIP quality then it should make efforts to provide high SIP quality in return too. Furthermore, it advocates increasing efforts to link financial stakes (tangible or intangible monetary benefits or risks) to the quality of SIP being continuously exchanged throughout the project lifecycle.","A Conceptual Model for Bidirectional Service, Information and Product Quality in an IS Outsourcing Collaboration Environment This paper advances theory on the process of collaboration between entities and its implications on the quality of services, information, andor products (SIPs) that the collaborating entities provide to each other. It investigates the scenario of outsourced IS projects (such as custom software development) where the extent of collaboration between a client and vendor is high. Using the social exchange theory, the proposed conceptual model tries to establish the bidirectional nature of SIP quality in a collaborative environment, where the SIPs exchanged are possibly dependent on each other, and if any entity wishes to receive high SIP quality then it should make efforts to provide high SIP quality in return too. Furthermore, it advocates increasing efforts to link financial stakes (tangible or intangible monetary benefits or risks) to the quality of SIP being continuously exchanged throughout the project lifecycle.",Finance
Designing an Interdisciplinary Artificial Intelligence Curriculum for Engineering: Evaluation and Insights from Experts,"As Artificial Intelligence (AI) increasingly impacts professional practice, there is a growing need to AI-related competencies into higher education curricula. However, research on the implementation of AI education within study programs remains limited and requires new forms of collaboration across disciplines. This study addresses this gap and explores perspectives on interdisciplinary curriculum development through the lens of different stakeholders. In particular, we examine the case of curriculum development for a novel undergraduate program in AI in engineering. The research uses a mixed methods approach, combining quantitative curriculum mapping with qualitative focus group interviews. In addition to assessing the alignment of the curriculum with the targeted competencies, the study also examines the perceived quality, consistency, practicality and effectiveness from both academic and industry perspectives, as well as differences in perceptions between educators who were involved in the development and those who were not. The findings provide a practical understanding of the outcomes of interdisciplinary AI curriculum development and contribute to a broader understanding of how educator participation in curriculum development influences perceptions of quality aspects. It also advances the field of AI education by providing a reference point and insights for further interdisciplinary curriculum developments in response to evolving industry needs.","Designing an Interdisciplinary Artificial Intelligence Curriculum for Engineering: Evaluation and Insights from Experts As Artificial Intelligence (AI) increasingly impacts professional practice, there is a growing need to AI-related competencies into higher education curricula. However, research on the implementation of AI education within study programs remains limited and requires new forms of collaboration across disciplines. This study addresses this gap and explores perspectives on interdisciplinary curriculum development through the lens of different stakeholders. In particular, we examine the case of curriculum development for a novel undergraduate program in AI in engineering. The research uses a mixed methods approach, combining quantitative curriculum mapping with qualitative focus group interviews. In addition to assessing the alignment of the curriculum with the targeted competencies, the study also examines the perceived quality, consistency, practicality and effectiveness from both academic and industry perspectives, as well as differences in perceptions between educators who were involved in the development and those who were not. The findings provide a practical understanding of the outcomes of interdisciplinary AI curriculum development and contribute to a broader understanding of how educator participation in curriculum development influences perceptions of quality aspects. It also advances the field of AI education by providing a reference point and insights for further interdisciplinary curriculum developments in response to evolving industry needs.",Education
A note on the use of the word likelihood in statistics and meteorology,"We highlight the different uses of the word likelihood that have arisen in statistics and meteorology, and make the recommendation that one of these uses should be dropped to prevent confusion and misunderstanding.","A note on the use of the word likelihood in statistics and meteorology We highlight the different uses of the word likelihood that have arisen in statistics and meteorology, and make the recommendation that one of these uses should be dropped to prevent confusion and misunderstanding.",Environment
Impacts of Variable-Impedance-Based Power Flow Control on Renewable Energy Integration,"The electric power grid has evolved significantly over the past two decades in response to climate change. Increased levels of renewable energy generation, as a prominent feature of this evolution, have led to new congestion patterns in the transmission network. The transmission system is originally designed for conventional energy sources, with predictable flow patterns. Insufficient transfer capability in congested transmission systems results in commitment of more expensive power plants and higher levels of renewable energy curtailment. One way to mitigate congestion is adoption of power flow control through variable-impedance flexible ac transmission system (FACTS) devices. In this paper the impacts of power flow control on generation cost, carbon emissions and renewable energy curtailment are studied under a wide range of scenarios, including generation mix from major US regional transmission organizations, and different load curves, representing seasonal variations. A two-stage stochastic unit commitment, including FACTS adjustment, is used to evaluate the impacts of FACTS devices on various types and penetration levels of renewable energy. The results show that FACTS installation effectively reduces generation cost, carbon emissions, and renewable energy curtailment. Location of renewable energy resources, peak-hour demand and the systems generation mix are among the influential factors.","Impacts of Variable-Impedance-Based Power Flow Control on Renewable Energy Integration The electric power grid has evolved significantly over the past two decades in response to climate change. Increased levels of renewable energy generation, as a prominent feature of this evolution, have led to new congestion patterns in the transmission network. The transmission system is originally designed for conventional energy sources, with predictable flow patterns. Insufficient transfer capability in congested transmission systems results in commitment of more expensive power plants and higher levels of renewable energy curtailment. One way to mitigate congestion is adoption of power flow control through variable-impedance flexible ac transmission system (FACTS) devices. In this paper the impacts of power flow control on generation cost, carbon emissions and renewable energy curtailment are studied under a wide range of scenarios, including generation mix from major US regional transmission organizations, and different load curves, representing seasonal variations. A two-stage stochastic unit commitment, including FACTS adjustment, is used to evaluate the impacts of FACTS devices on various types and penetration levels of renewable energy. The results show that FACTS installation effectively reduces generation cost, carbon emissions, and renewable energy curtailment. Location of renewable energy resources, peak-hour demand and the systems generation mix are among the influential factors.",Environment
Balancing Innovation and Sustainability: Addressing the Environmental Impact of Bitcoin Mining,"This study explores the intersection of technological innovation and environmental sustainability in the context of Bitcoin mining. With Bitcoins growing adoption, concerns surrounding the energy consumption and environmental impact of mining activities have intensified. The study examines the core process of Bitcoin mining, focusing on its energy-intensive proof-of-work mechanism, and provides a detailed analysis of its ecological footprint, especially in terms of carbon emissions and electronic waste. Various models estimate that Bitcoins energy consumption rivals that of entire nations, highlighting serious sustainability concerns. To address these issues, the paper unearths potential technological innovations, such as energy-efficient mining hardware and the integration of renewable energy sources, as viable strategies to reduce environmental impact. Additionally, the study reviews current sustainability initiatives, including efforts to lower carbon footprints and manage electronic waste effectively. Regulatory developments and market-based approaches are also discussed as possible pathways to mitigate the environmental harm associated with Bitcoin mining. Ultimately, the paper advocates for a balanced approach that fosters technological innovation while promoting environmental responsibility, suggesting that, with appropriate policy and technological interventions, Bitcoin mining can evolve to be both innovative and sustainable.","Balancing Innovation and Sustainability: Addressing the Environmental Impact of Bitcoin Mining This study explores the intersection of technological innovation and environmental sustainability in the context of Bitcoin mining. With Bitcoins growing adoption, concerns surrounding the energy consumption and environmental impact of mining activities have intensified. The study examines the core process of Bitcoin mining, focusing on its energy-intensive proof-of-work mechanism, and provides a detailed analysis of its ecological footprint, especially in terms of carbon emissions and electronic waste. Various models estimate that Bitcoins energy consumption rivals that of entire nations, highlighting serious sustainability concerns. To address these issues, the paper unearths potential technological innovations, such as energy-efficient mining hardware and the integration of renewable energy sources, as viable strategies to reduce environmental impact. Additionally, the study reviews current sustainability initiatives, including efforts to lower carbon footprints and manage electronic waste effectively. Regulatory developments and market-based approaches are also discussed as possible pathways to mitigate the environmental harm associated with Bitcoin mining. Ultimately, the paper advocates for a balanced approach that fosters technological innovation while promoting environmental responsibility, suggesting that, with appropriate policy and technological interventions, Bitcoin mining can evolve to be both innovative and sustainable.",Environment
Generative AI and Its Educational Implications,"We discuss the implications of generative AI on education across four critical sections: the historical development of AI in education, its contemporary applications in learning, societal repercussions, and strategic recommendations for researchers. We propose ways in which generative AI can transform the educational landscape, primarily via its ability to conduct assessment of complex cognitive performances and create personalized content. We also address the challenges of effective educational tool deployment, data bias, design transparency, and accurate output verification. Acknowledging the societal impact, we emphasize the need for updating curricula, redefining communicative trust, and adjusting to transformed social norms. We end by outlining the ways in which educational stakeholders can actively engage with generative AI, develop fluency with its capacities and limitations, and apply these insights to steer educational practices in a rapidly advancing digital landscape.","Generative AI and Its Educational Implications We discuss the implications of generative AI on education across four critical sections: the historical development of AI in education, its contemporary applications in learning, societal repercussions, and strategic recommendations for researchers. We propose ways in which generative AI can transform the educational landscape, primarily via its ability to conduct assessment of complex cognitive performances and create personalized content. We also address the challenges of effective educational tool deployment, data bias, design transparency, and accurate output verification. Acknowledging the societal impact, we emphasize the need for updating curricula, redefining communicative trust, and adjusting to transformed social norms. We end by outlining the ways in which educational stakeholders can actively engage with generative AI, develop fluency with its capacities and limitations, and apply these insights to steer educational practices in a rapidly advancing digital landscape.",Education
The computation of Greeks with multilevel Monte Carlo,We study the use of the multilevel Monte Carlo technique in the context of the calculation of Greeks. The pathwise sensitivity analysis differentiates the path evolution and reduces the payoffs smoothness. This leads to new challenges: the inapplicability of pathwise sensitivities to non-Lipschitz payoffs often makes the use of naive algorithms impossible. These challenges can be addressed in three different ways: payoff smoothing using conditional expectations of the payoff before maturity; approximating the previous technique with path splitting for the final timestep; using of a hybrid combination of pathwise sensitivity and the Likelihood Ratio Method. We investigate the strengths and weaknesses of these alternatives in different multilevel Monte Carlo settings.,The computation of Greeks with multilevel Monte Carlo We study the use of the multilevel Monte Carlo technique in the context of the calculation of Greeks. The pathwise sensitivity analysis differentiates the path evolution and reduces the payoffs smoothness. This leads to new challenges: the inapplicability of pathwise sensitivities to non-Lipschitz payoffs often makes the use of naive algorithms impossible. These challenges can be addressed in three different ways: payoff smoothing using conditional expectations of the payoff before maturity; approximating the previous technique with path splitting for the final timestep; using of a hybrid combination of pathwise sensitivity and the Likelihood Ratio Method. We investigate the strengths and weaknesses of these alternatives in different multilevel Monte Carlo settings.,Finance
Understanding physics from interconnected data,"Metal melting on release after explosion is a physical system far from quilibrium. A complete physical model of this system does not exist, because many interrelated effects have to be considered. General methodology needs to be developed so as to describe and understand physical phenomena involved. The high noise of the data, moving blur of images, the high degree of uncertainty due to the different types of sensors, and the information entangled and hidden inside the noisy images makes reasoning about the physical processes very difficult. Major problems include proper information extraction and the problem of reconstruction, as well as prediction of the missing data. In this paper, several techniques addressing the first problem are given, building the basis for tackling the second problem.","Understanding physics from interconnected data Metal melting on release after explosion is a physical system far from quilibrium. A complete physical model of this system does not exist, because many interrelated effects have to be considered. General methodology needs to be developed so as to describe and understand physical phenomena involved. The high noise of the data, moving blur of images, the high degree of uncertainty due to the different types of sensors, and the information entangled and hidden inside the noisy images makes reasoning about the physical processes very difficult. Major problems include proper information extraction and the problem of reconstruction, as well as prediction of the missing data. In this paper, several techniques addressing the first problem are given, building the basis for tackling the second problem.",Technology
SHIFT: A Highly Realistic Financial Market Simulation Platform,"This paper presents a new financial market simulator that may be used as a tool in both industry and academia for research in market microstructure. It allows multiple automated traders andor researchers to simultaneously connect to an exchange-like environment, where they are able to asynchronously trade several financial assets at the same time. In its current iteration, this order-driven market implements the basic rules of U.S. equity markets, supporting both market and limit orders, and executing them in a first-in-first-out fashion. We overview the system architecture and we present possible use cases. We demonstrate how a set of automated agents is capable of producing a price process with characteristics similar to the statistics of real price from financial markets. Finally, we detail a market stress scenario and we draw, what we believe to be, interesting conclusions about crash events.","SHIFT: A Highly Realistic Financial Market Simulation Platform This paper presents a new financial market simulator that may be used as a tool in both industry and academia for research in market microstructure. It allows multiple automated traders andor researchers to simultaneously connect to an exchange-like environment, where they are able to asynchronously trade several financial assets at the same time. In its current iteration, this order-driven market implements the basic rules of U.S. equity markets, supporting both market and limit orders, and executing them in a first-in-first-out fashion. We overview the system architecture and we present possible use cases. We demonstrate how a set of automated agents is capable of producing a price process with characteristics similar to the statistics of real price from financial markets. Finally, we detail a market stress scenario and we draw, what we believe to be, interesting conclusions about crash events.",Finance
Confidence-Based Curriculum Learning for Multi-Agent Path Finding,"A wide range of real-world applications can be formulated as Multi-Agent Path Finding (MAPF) problem, where the goal is to find collision-free paths for multiple agents with individual start and goal locations. State-of-the-art MAPF solvers are mainly centralized and depend on global information, which limits their scalability and flexibility regarding changes or new maps that would require expensive replanning. Multi-agent reinforcement learning (MARL) offers an alternative way by learning decentralized policies that can generalize over a variety of maps. While there exist some prior works that attempt to connect both areas, the proposed techniques are heavily engineered and very complex due to the integration of many mechanisms that limit generality and are expensive to use. We argue that much simpler and general approaches are needed to bring the areas of MARL and MAPF closer together with significantly lower costs. In this paper, we propose Confidence-based Auto-Curriculum for Team Update Stability (CACTUS) as a lightweight MARL approach to MAPF. CACTUS defines a simple reverse curriculum scheme, where the goal of each agent is randomly placed within an allocation radius around the agents start location. The allocation radius increases gradually as all agents improve, which is assessed by a confidence-based measure. We evaluate CACTUS in various maps of different sizes, obstacle densities, and numbers of agents. Our experiments demonstrate better performance and generalization capabilities than state-of-the-art MARL approaches with less than 600,000 trainable parameters, which is less than 5 of the neural network size of current MARL approaches to MAPF.","Confidence-Based Curriculum Learning for Multi-Agent Path Finding A wide range of real-world applications can be formulated as Multi-Agent Path Finding (MAPF) problem, where the goal is to find collision-free paths for multiple agents with individual start and goal locations. State-of-the-art MAPF solvers are mainly centralized and depend on global information, which limits their scalability and flexibility regarding changes or new maps that would require expensive replanning. Multi-agent reinforcement learning (MARL) offers an alternative way by learning decentralized policies that can generalize over a variety of maps. While there exist some prior works that attempt to connect both areas, the proposed techniques are heavily engineered and very complex due to the integration of many mechanisms that limit generality and are expensive to use. We argue that much simpler and general approaches are needed to bring the areas of MARL and MAPF closer together with significantly lower costs. In this paper, we propose Confidence-based Auto-Curriculum for Team Update Stability (CACTUS) as a lightweight MARL approach to MAPF. CACTUS defines a simple reverse curriculum scheme, where the goal of each agent is randomly placed within an allocation radius around the agents start location. The allocation radius increases gradually as all agents improve, which is assessed by a confidence-based measure. We evaluate CACTUS in various maps of different sizes, obstacle densities, and numbers of agents. Our experiments demonstrate better performance and generalization capabilities than state-of-the-art MARL approaches with less than 600,000 trainable parameters, which is less than 5 of the neural network size of current MARL approaches to MAPF.",Education
Social interactions for a sustainable lifestyle: The design of an experimental case study,"Every day we face numerous lifestyle decisions, some dictated by habits and some more conscious, which may or may not promote sustainable living. Aided by digital technology, sustainable behaviors can diffuse within social groups and inclusive communities. This paper outlines a longitudinal experimental study of social influence in behavioral changes toward sustainability, in the context of smart residential homes. Participants are residing in the housing on campus referred to as KTH Live-In Lab, whose behaviors are observed w.r.t. key lifestyle choices, such as food, resources, mobility, consumption, and environmental citizenship. The focus is on the preparatory phase of the case study and the challenges and limitations encountered during its setup. In particular, this work proposes a definition of sustainability indicators for environmentally significant behaviors, and hypothesizes that, through digitalization of a household into a social network of interacting tenants, sustainable living can be promoted. Preliminary results confirm the feasibility of the proposed experimental methodology.","Social interactions for a sustainable lifestyle: The design of an experimental case study Every day we face numerous lifestyle decisions, some dictated by habits and some more conscious, which may or may not promote sustainable living. Aided by digital technology, sustainable behaviors can diffuse within social groups and inclusive communities. This paper outlines a longitudinal experimental study of social influence in behavioral changes toward sustainability, in the context of smart residential homes. Participants are residing in the housing on campus referred to as KTH Live-In Lab, whose behaviors are observed w.r.t. key lifestyle choices, such as food, resources, mobility, consumption, and environmental citizenship. The focus is on the preparatory phase of the case study and the challenges and limitations encountered during its setup. In particular, this work proposes a definition of sustainability indicators for environmentally significant behaviors, and hypothesizes that, through digitalization of a household into a social network of interacting tenants, sustainable living can be promoted. Preliminary results confirm the feasibility of the proposed experimental methodology.",Environment
"The Face on Mars: a photographic approach for the search of signs of past civilizations from a macroscopic point of view, factoring long-term erosion in image reconstruction","This short article presents an alternative view of high resolution imaging from various sources with the aim of the discovery of potential sites of archaeological importance, or sites that exhibit anomalies such that they may merit closer inspection and analysis. It is conjectured, and to a certain extent demonstrated here, that it is possible for advanced civilizations to factor in erosion by natural processes into a large scale design so that main features be preserved even with the passage of millions of years. Alternatively viewed, even without such intent embedded in a design left for posterity, it is possible that a gigantic construction may naturally decay in such a way that even cataclysmic (massive) events may leave sufficient information intact with the passage of time, provided one changes the point of view from high resolution images to enhanced blurred renderings of the sites in question.","The Face on Mars: a photographic approach for the search of signs of past civilizations from a macroscopic point of view, factoring long-term erosion in image reconstruction This short article presents an alternative view of high resolution imaging from various sources with the aim of the discovery of potential sites of archaeological importance, or sites that exhibit anomalies such that they may merit closer inspection and analysis. It is conjectured, and to a certain extent demonstrated here, that it is possible for advanced civilizations to factor in erosion by natural processes into a large scale design so that main features be preserved even with the passage of millions of years. Alternatively viewed, even without such intent embedded in a design left for posterity, it is possible that a gigantic construction may naturally decay in such a way that even cataclysmic (massive) events may leave sufficient information intact with the passage of time, provided one changes the point of view from high resolution images to enhanced blurred renderings of the sites in question.",Technology
On the Usage of Databases of Educational Materials in Macedonian Education,"Technologies have become important part of our lives. The steps for introducing ICTs in education vary from country to country. The Republic of Macedonia has invested with a lot in installment of hardware and software in education and in teacher training. This research was aiming to determine the situation of usage of databases of digital educational materials and to define recommendation for future improvements. Teachers from urban schools were interviewed with a questionnaire. The findings are several: only part of the interviewed teachers had experience with databases of educational materials; all teachers still need capacity building activities focusing exactly on the use and benefits from databases of educational materials; preferably capacity building materials to be in Macedonian language; technical support and upgrading of software and materials should be performed on a regular basis. Most of the findings can be applied at both national and international level - with all this implemented, application of ICT in education will have much bigger positive impact.","On the Usage of Databases of Educational Materials in Macedonian Education Technologies have become important part of our lives. The steps for introducing ICTs in education vary from country to country. The Republic of Macedonia has invested with a lot in installment of hardware and software in education and in teacher training. This research was aiming to determine the situation of usage of databases of digital educational materials and to define recommendation for future improvements. Teachers from urban schools were interviewed with a questionnaire. The findings are several: only part of the interviewed teachers had experience with databases of educational materials; all teachers still need capacity building activities focusing exactly on the use and benefits from databases of educational materials; preferably capacity building materials to be in Macedonian language; technical support and upgrading of software and materials should be performed on a regular basis. Most of the findings can be applied at both national and international level - with all this implemented, application of ICT in education will have much bigger positive impact.",Education
Institutional reforms and the employment effects of spatially targeted investment grants: The case of Germanys GRW,"Spatially targeted investment grant schemes are a common tool to support firms in lagging regions. We exploit exogenous variations in Germanys main regional policy instrument (GRW) arriving from institutional reforms to analyse local employment effects of investment grants. Findings for reduced-form and IV regressions point to a significant policy channel running from higher funding rates to increased firm-level investments and newly created jobs. When we contrast effects for regions with high but declining funding rates to those with low but rising rates, we find that GRW reforms led to diminishing employment increases. Especially small firms responded to changing funding conditions.","Institutional reforms and the employment effects of spatially targeted investment grants: The case of Germanys GRW Spatially targeted investment grant schemes are a common tool to support firms in lagging regions. We exploit exogenous variations in Germanys main regional policy instrument (GRW) arriving from institutional reforms to analyse local employment effects of investment grants. Findings for reduced-form and IV regressions point to a significant policy channel running from higher funding rates to increased firm-level investments and newly created jobs. When we contrast effects for regions with high but declining funding rates to those with low but rising rates, we find that GRW reforms led to diminishing employment increases. Especially small firms responded to changing funding conditions.",Finance
The structure of the climate debate,"First-best climate policy is a uniform carbon tax which gradually rises over time. Civil servants have complicated climate policy to expand bureaucracies, politicians to create rents. Environmentalists have exaggerated climate change to gain influence, other activists have joined the climate bandwagon. Opponents to climate policy have attacked the weaknesses in climate research. The climate debate is convoluted and polarized as a result, and climate policy complex. Climate policy should become easier and more rational as the Paris Agreement has shifted climate policy back towards national governments. Changing political priorities, austerity, and a maturing bureaucracy should lead to a more constructive climate debate.","The structure of the climate debate First-best climate policy is a uniform carbon tax which gradually rises over time. Civil servants have complicated climate policy to expand bureaucracies, politicians to create rents. Environmentalists have exaggerated climate change to gain influence, other activists have joined the climate bandwagon. Opponents to climate policy have attacked the weaknesses in climate research. The climate debate is convoluted and polarized as a result, and climate policy complex. Climate policy should become easier and more rational as the Paris Agreement has shifted climate policy back towards national governments. Changing political priorities, austerity, and a maturing bureaucracy should lead to a more constructive climate debate.",Environment
From Cbits to Qbits: Teaching computer scientists quantum mechanics,"A strategy is suggested for teaching mathematically literate students, with no background in physics, just enough quantum mechanics for them to understand and develop algorithms in quantum computation and quantum information theory. Although the article as a whole addresses teachers of physics, well versed in quantum mechanics, the central pedagogical development is addressed directly to computer scientists and mathematicians, with only occasional asides to their teacher. Physicists uninterested in quantum pedagogy may be amused (or irritated) by some of the views of standard quantum mechanics that arise naturally from this unorthodox perspective.","From Cbits to Qbits: Teaching computer scientists quantum mechanics A strategy is suggested for teaching mathematically literate students, with no background in physics, just enough quantum mechanics for them to understand and develop algorithms in quantum computation and quantum information theory. Although the article as a whole addresses teachers of physics, well versed in quantum mechanics, the central pedagogical development is addressed directly to computer scientists and mathematicians, with only occasional asides to their teacher. Physicists uninterested in quantum pedagogy may be amused (or irritated) by some of the views of standard quantum mechanics that arise naturally from this unorthodox perspective.",Education
Physics Pedagogy and Assessment in Secondary Schools in the U.S.,"The objective of this project is to compare the effectiveness of teaching styles used in high school physics classes and the methods used to assess them. We would like to determine those approaches to physics at the high schools that work and those that do not work for students from different demographics. We sent out a survey to high school physics teachers in the U.S. Midwest states, inquiring about student preparation, pedagogy in the classroom, assessment and professional development. We found that there are differences in the practices of physics teachers in all of these areas, depending on the school location, be it rural, suburban or urban. Our results enable us to report on the most common successful practices in physics courses for these demographic areas.","Physics Pedagogy and Assessment in Secondary Schools in the U.S. The objective of this project is to compare the effectiveness of teaching styles used in high school physics classes and the methods used to assess them. We would like to determine those approaches to physics at the high schools that work and those that do not work for students from different demographics. We sent out a survey to high school physics teachers in the U.S. Midwest states, inquiring about student preparation, pedagogy in the classroom, assessment and professional development. We found that there are differences in the practices of physics teachers in all of these areas, depending on the school location, be it rural, suburban or urban. Our results enable us to report on the most common successful practices in physics courses for these demographic areas.",Education
Curriculum Learning for Small Code Language Models,"Code language models have emerged as useful tools for various programming tasks, yet they often struggle when it comes to complex ones. In this paper, we explore the potential of curriculum learning in enhancing the performance of these models. While prior research has suggested that curriculum learning does not necessarily help in improving the performance of language models, our results surprisingly show that this may not be the case for code language models. We demonstrate that a well-designed curriculum learning approach significantly improves the accuracy of small decoder-only code language models on the task of code execution, while its effect on code completion is less significant. To explore the potential of curriculum learning, we train multiple GPT models with 1 million parameters each to predict the next token and evaluate them on code completion and execution tasks. Our contributions include proposing a novel code difficulty assessment metric by combining software code measures, investigating the effectiveness of Curriculum Learning for code language models, and introducing a Novel Curriculum Learning schedule that enhances the performance of small decoder-only language models in code execution tasks. The results of this paper open the door for more research on the use of curriculum learning for code language models.","Curriculum Learning for Small Code Language Models Code language models have emerged as useful tools for various programming tasks, yet they often struggle when it comes to complex ones. In this paper, we explore the potential of curriculum learning in enhancing the performance of these models. While prior research has suggested that curriculum learning does not necessarily help in improving the performance of language models, our results surprisingly show that this may not be the case for code language models. We demonstrate that a well-designed curriculum learning approach significantly improves the accuracy of small decoder-only code language models on the task of code execution, while its effect on code completion is less significant. To explore the potential of curriculum learning, we train multiple GPT models with 1 million parameters each to predict the next token and evaluate them on code completion and execution tasks. Our contributions include proposing a novel code difficulty assessment metric by combining software code measures, investigating the effectiveness of Curriculum Learning for code language models, and introducing a Novel Curriculum Learning schedule that enhances the performance of small decoder-only language models in code execution tasks. The results of this paper open the door for more research on the use of curriculum learning for code language models.",Education
Privacy-preserving Spatiotemporal Scenario Generation of Renewable Energies: A Federated Deep Generative Learning Approach,"Scenario generation is a fundamental and crucial tool for decision-making in power systems with high-penetration renewables. Based on big historical data, a novel federated deep generative learning framework, called Fed-LSGAN, is proposed by integrating federated learning and least square generative adversarial networks (LSGANs) for renewable scenario generation. Specifically, federated learning learns a shared global model in a central server from renewable sites at network edges, which enables the Fed-LSGAN to generate scenarios in a privacy-preserving manner without sacrificing the generation quality by transferring model parameters, rather than all data. Meanwhile, the LSGANs-based deep generative model generates scenarios that conform to the distribution of historical data through fully capturing the spatial-temporal characteristics of renewable powers, which leverages the least squares loss function to improve the training stability and generation quality. The simulation results demonstrate that the proposal manages to generate high-quality renewable scenarios and outperforms the state-of-the-art centralized methods. Besides, an experiment with different federated learning settings is designed and conducted to verify the robustness of our method.","Privacy-preserving Spatiotemporal Scenario Generation of Renewable Energies: A Federated Deep Generative Learning Approach Scenario generation is a fundamental and crucial tool for decision-making in power systems with high-penetration renewables. Based on big historical data, a novel federated deep generative learning framework, called Fed-LSGAN, is proposed by integrating federated learning and least square generative adversarial networks (LSGANs) for renewable scenario generation. Specifically, federated learning learns a shared global model in a central server from renewable sites at network edges, which enables the Fed-LSGAN to generate scenarios in a privacy-preserving manner without sacrificing the generation quality by transferring model parameters, rather than all data. Meanwhile, the LSGANs-based deep generative model generates scenarios that conform to the distribution of historical data through fully capturing the spatial-temporal characteristics of renewable powers, which leverages the least squares loss function to improve the training stability and generation quality. The simulation results demonstrate that the proposal manages to generate high-quality renewable scenarios and outperforms the state-of-the-art centralized methods. Besides, an experiment with different federated learning settings is designed and conducted to verify the robustness of our method.",Environment
Gradient Vector Flow Models for Boundary Extraction in 2D Images,"The Gradient Vector Flow (GVF) is a vector diffusion approach based on Partial Differential Equations (PDEs). This method has been applied together with snake models for boundary extraction medical images segmentation. The key idea is to use a diffusion-reaction PDE to generate a new external force field that makes snake models less sensitivity to initialization as well as improves the snakes ability to move into boundary concavities. In this paper, we firstly review basic results about convergence and numerical analysis of usual GVF schemes. We point out that GVF presents numerical problems due to discontinuities image intensity. This point is considered from a practical viewpoint from which the GVF parameters must follow a relationship in order to improve numerical convergence. Besides, we present an analytical analysis of the GVF dependency from the parameters values. Also, we observe that the method can be used for multiply connected domains by just imposing the suitable boundary condition. In the experimental results we verify these theoretical points and demonstrate the utility of GVF on a segmentation approach that we have developed based on snakes.","Gradient Vector Flow Models for Boundary Extraction in 2D Images The Gradient Vector Flow (GVF) is a vector diffusion approach based on Partial Differential Equations (PDEs). This method has been applied together with snake models for boundary extraction medical images segmentation. The key idea is to use a diffusion-reaction PDE to generate a new external force field that makes snake models less sensitivity to initialization as well as improves the snakes ability to move into boundary concavities. In this paper, we firstly review basic results about convergence and numerical analysis of usual GVF schemes. We point out that GVF presents numerical problems due to discontinuities image intensity. This point is considered from a practical viewpoint from which the GVF parameters must follow a relationship in order to improve numerical convergence. Besides, we present an analytical analysis of the GVF dependency from the parameters values. Also, we observe that the method can be used for multiply connected domains by just imposing the suitable boundary condition. In the experimental results we verify these theoretical points and demonstrate the utility of GVF on a segmentation approach that we have developed based on snakes.",Technology
Comparing the ensemble mean and the ensemble standard deviation as inputs for probabilistic medium-range temperature forecasts,"We ask the following question: what are the relative contributions of the ensemble mean and the ensemble standard deviation to the skill of a site-specific probabilistic temperature forecast? Is it the case that most of the benefit of using an ensemble forecast to predict temperatures comes from the ensemble mean, or from the ensemble spread, or is the benefit derived equally from the two? The answer is that one of the two is much more useful than the other.","Comparing the ensemble mean and the ensemble standard deviation as inputs for probabilistic medium-range temperature forecasts We ask the following question: what are the relative contributions of the ensemble mean and the ensemble standard deviation to the skill of a site-specific probabilistic temperature forecast? Is it the case that most of the benefit of using an ensemble forecast to predict temperatures comes from the ensemble mean, or from the ensemble spread, or is the benefit derived equally from the two? The answer is that one of the two is much more useful than the other.",Environment
A link between an ice age era and a rapid polar shift,"The striking asymmetry of the ice cover during the Last Global Maximum suggests that the North Pole was in Greenland and then rapidly shifted to its present position in the Arctic See. A scenario which causes such a rapid geographic polar shift is physically possible. It involves an additional planet, which disappeared by evaporation within the Holocene. This is only possible within such a short period, if the planet was in an extremely eccentric orbit and hot. Then, since this produced an interplanetary gas cloud, the polar shift had to be preceded by a cold period with large global temperature variations during several million years.","A link between an ice age era and a rapid polar shift The striking asymmetry of the ice cover during the Last Global Maximum suggests that the North Pole was in Greenland and then rapidly shifted to its present position in the Arctic See. A scenario which causes such a rapid geographic polar shift is physically possible. It involves an additional planet, which disappeared by evaporation within the Holocene. This is only possible within such a short period, if the planet was in an extremely eccentric orbit and hot. Then, since this produced an interplanetary gas cloud, the polar shift had to be preceded by a cold period with large global temperature variations during several million years.",Environment
An operatorial approach to stock markets,"We propose and discuss some toy models of stock markets using the same operatorial approach adopted in quantum mechanics. Our models are suggested by the discrete nature of the number of shares and of the cash which are exchanged in a real market, and by the existence of conserved quantities, like the total number of shares or some linear combination of cash and shares. The same framework as the one used in the description of a gas of interacting bosons is adopted.","An operatorial approach to stock markets We propose and discuss some toy models of stock markets using the same operatorial approach adopted in quantum mechanics. Our models are suggested by the discrete nature of the number of shares and of the cash which are exchanged in a real market, and by the existence of conserved quantities, like the total number of shares or some linear combination of cash and shares. The same framework as the one used in the description of a gas of interacting bosons is adopted.",Finance
Hedging predictions in machine learning,"Recent advances in machine learning make it possible to design efficient prediction algorithms for data sets with huge numbers of parameters. This paper describes a new technique for hedging the predictions output by many such algorithms, including support vector machines, kernel ridge regression, kernel nearest neighbours, and by many other state-of-the-art methods. The hedged predictions for the labels of new objects include quantitative measures of their own accuracy and reliability. These measures are provably valid under the assumption of randomness, traditional in machine learning: the objects and their labels are assumed to be generated independently from the same probability distribution. In particular, it becomes possible to control (up to statistical fluctuations) the number of erroneous predictions by selecting a suitable confidence level. Validity being achieved automatically, the remaining goal of hedged prediction is efficiency: taking full account of the new objects features and other available information to produce as accurate predictions as possible. This can be done successfully using the powerful machinery of modern machine learning.","Hedging predictions in machine learning Recent advances in machine learning make it possible to design efficient prediction algorithms for data sets with huge numbers of parameters. This paper describes a new technique for hedging the predictions output by many such algorithms, including support vector machines, kernel ridge regression, kernel nearest neighbours, and by many other state-of-the-art methods. The hedged predictions for the labels of new objects include quantitative measures of their own accuracy and reliability. These measures are provably valid under the assumption of randomness, traditional in machine learning: the objects and their labels are assumed to be generated independently from the same probability distribution. In particular, it becomes possible to control (up to statistical fluctuations) the number of erroneous predictions by selecting a suitable confidence level. Validity being achieved automatically, the remaining goal of hedged prediction is efficiency: taking full account of the new objects features and other available information to produce as accurate predictions as possible. This can be done successfully using the powerful machinery of modern machine learning.",Technology
Revisiting Evolutionary Algorithms with On-the-Fly Population Size Adjustment,"In an evolutionary algorithm, the population has a very important role as its size has direct implications regarding solution quality, speed, and reliability. Theoretical studies have been done in the past to investigate the role of population sizing in evolutionary algorithms. In addition to those studies, several self-adjusting population sizing mechanisms have been proposed in the literature. This paper revisits the latter topic and pays special attention to the genetic algorithm with adaptive population size (APGA), for which several researchers have claimed to be very effective at autonomously (re)sizing the population. As opposed to those previous claims, this paper suggests a complete opposite view. Specifically, it shows that APGA is not capable of adapting the population size at all. This claim is supported on theoretical grounds and confirmed by computer simulations.","Revisiting Evolutionary Algorithms with On-the-Fly Population Size Adjustment In an evolutionary algorithm, the population has a very important role as its size has direct implications regarding solution quality, speed, and reliability. Theoretical studies have been done in the past to investigate the role of population sizing in evolutionary algorithms. In addition to those studies, several self-adjusting population sizing mechanisms have been proposed in the literature. This paper revisits the latter topic and pays special attention to the genetic algorithm with adaptive population size (APGA), for which several researchers have claimed to be very effective at autonomously (re)sizing the population. As opposed to those previous claims, this paper suggests a complete opposite view. Specifically, it shows that APGA is not capable of adapting the population size at all. This claim is supported on theoretical grounds and confirmed by computer simulations.",Technology
Sustainable Quantum Computing: Opportunities and Challenges of Benchmarking Carbon in the Quantum Computing Lifecycle,"While researchers in both industry and academia are racing to build Quantum Computing (QC) platforms with viable performance and functionality, the environmental impacts of this endeavor, such as its carbon footprint, e-waste generation, mineral use, and water and energy consumption, remain largely unknown. A similar oversight occurred during the semiconductor revolution and continues to have disastrous consequences for the health of our planet. As we build the quantum computing stack from the ground up, it is crucial to comprehensively assess it through an environmental sustainability lens for its entire life-cycle: production, use, and disposal. In this paper, we highlight the need and challenges in establishing a QC sustainability benchmark that enables researchers to make informed architectural design decisions and celebrate the potential quantum environmental advantage. We propose a carbon-aware quantum computing (CQC) framework that provides the foundational methodology and open research questions for calculating the total life-cycle carbon footprint of a QC platform. Our call to action to the research community is the establishment of a new research direction known as, sustainable quantum computing that promotes both quantum computing for sustainability-oriented applications and the sustainability of quantum computing.","Sustainable Quantum Computing: Opportunities and Challenges of Benchmarking Carbon in the Quantum Computing Lifecycle While researchers in both industry and academia are racing to build Quantum Computing (QC) platforms with viable performance and functionality, the environmental impacts of this endeavor, such as its carbon footprint, e-waste generation, mineral use, and water and energy consumption, remain largely unknown. A similar oversight occurred during the semiconductor revolution and continues to have disastrous consequences for the health of our planet. As we build the quantum computing stack from the ground up, it is crucial to comprehensively assess it through an environmental sustainability lens for its entire life-cycle: production, use, and disposal. In this paper, we highlight the need and challenges in establishing a QC sustainability benchmark that enables researchers to make informed architectural design decisions and celebrate the potential quantum environmental advantage. We propose a carbon-aware quantum computing (CQC) framework that provides the foundational methodology and open research questions for calculating the total life-cycle carbon footprint of a QC platform. Our call to action to the research community is the establishment of a new research direction known as, sustainable quantum computing that promotes both quantum computing for sustainability-oriented applications and the sustainability of quantum computing.",Environment
Designing learning experiences for online teaching and learning,"Teaching is about constantly innovating strategies, ways and means to engage diverse students in active and meaningful learning. In line with this, SUTD adopts various student-centric teaching and learning teaching methods and approaches. This means that our graduateundergraduate instructors have to be ready to teach using these student student-centric teaching and learning pedagogies. In this article, I share my experiences of redesigning this teaching course that is typically conducted face-to-face to a synchronous online course and also invite one of the participant in this course to reflect on his experience as a student.","Designing learning experiences for online teaching and learning Teaching is about constantly innovating strategies, ways and means to engage diverse students in active and meaningful learning. In line with this, SUTD adopts various student-centric teaching and learning teaching methods and approaches. This means that our graduateundergraduate instructors have to be ready to teach using these student student-centric teaching and learning pedagogies. In this article, I share my experiences of redesigning this teaching course that is typically conducted face-to-face to a synchronous online course and also invite one of the participant in this course to reflect on his experience as a student.",Education
Logical settings for concept learning from incomplete examples in First Order Logic,"We investigate here concept learning from incomplete examples. Our first purpose is to discuss to what extent logical learning settings have to be modified in order to cope with data incompleteness. More precisely we are interested in extending the learning from interpretations setting introduced by L. De Raedt that extends to relational representations the classical propositional (or attribute-value) concept learning from examples framework. We are inspired here by ideas presented by H. Hirsh in a work extending the Version space inductive paradigm to incomplete data. H. Hirsh proposes to slightly modify the notion of solution when dealing with incomplete examples: a solution has to be a hypothesis compatible with all pieces of information concerning the examples. We identify two main classes of incompleteness. First, uncertainty deals with our state of knowledge concerning an example. Second, generalization (or abstraction) deals with what part of the description of the example is sufficient for the learning purpose. These two main sources of incompleteness can be mixed up when only part of the useful information is known. We discuss a general learning setting, referred to as learning from possibilities that formalizes these ideas, then we present a more specific learning setting, referred to as assumption-based learning that cope with examples which uncertainty can be reduced when considering contextual information outside of the proper description of the examples. Assumption-based learning is illustrated on a recent work concerning the prediction of a consensus secondary structure common to a set of RNA sequences.","Logical settings for concept learning from incomplete examples in First Order Logic We investigate here concept learning from incomplete examples. Our first purpose is to discuss to what extent logical learning settings have to be modified in order to cope with data incompleteness. More precisely we are interested in extending the learning from interpretations setting introduced by L. De Raedt that extends to relational representations the classical propositional (or attribute-value) concept learning from examples framework. We are inspired here by ideas presented by H. Hirsh in a work extending the Version space inductive paradigm to incomplete data. H. Hirsh proposes to slightly modify the notion of solution when dealing with incomplete examples: a solution has to be a hypothesis compatible with all pieces of information concerning the examples. We identify two main classes of incompleteness. First, uncertainty deals with our state of knowledge concerning an example. Second, generalization (or abstraction) deals with what part of the description of the example is sufficient for the learning purpose. These two main sources of incompleteness can be mixed up when only part of the useful information is known. We discuss a general learning setting, referred to as learning from possibilities that formalizes these ideas, then we present a more specific learning setting, referred to as assumption-based learning that cope with examples which uncertainty can be reduced when considering contextual information outside of the proper description of the examples. Assumption-based learning is illustrated on a recent work concerning the prediction of a consensus secondary structure common to a set of RNA sequences.",Technology
Interpretable Hierarchical Attention Network for Medical Condition Identification,"Accurate prediction of medical conditions with straight past clinical evidence is a long-sought topic in the medical management and health insurance field. Although great progress has been made with machine learning algorithms, the medical community is still skeptical about the model accuracy and interpretability. This paper presents an innovative hierarchical attention deep learning model to achieve better prediction and clear interpretability that can be easily understood by medical professionals. This paper developed an Interpretable Hierarchical Attention Network (IHAN). IHAN uses a hierarchical attention structure that matches naturally with the medical history data structure and reflects patients encounter (date of service) sequence. The model attention structure consists of 3 levels: (1) attention on the medical code types (diagnosis codes, procedure codes, lab test results, and prescription drugs), (2) attention on the sequential medical encounters within a type, (3) attention on the individual medical codes within an encounter and type. This model is applied to predict the occurrence of stage 3 chronic kidney disease (CKD), using three years medical history of Medicare Advantage (MA) members from an American nationwide health insurance company. The model takes members medical events, both claims and Electronic Medical Records (EMR) data, as input, makes a prediction of stage 3 CKD and calculates contribution from individual events to the predicted outcome.","Interpretable Hierarchical Attention Network for Medical Condition Identification Accurate prediction of medical conditions with straight past clinical evidence is a long-sought topic in the medical management and health insurance field. Although great progress has been made with machine learning algorithms, the medical community is still skeptical about the model accuracy and interpretability. This paper presents an innovative hierarchical attention deep learning model to achieve better prediction and clear interpretability that can be easily understood by medical professionals. This paper developed an Interpretable Hierarchical Attention Network (IHAN). IHAN uses a hierarchical attention structure that matches naturally with the medical history data structure and reflects patients encounter (date of service) sequence. The model attention structure consists of 3 levels: (1) attention on the medical code types (diagnosis codes, procedure codes, lab test results, and prescription drugs), (2) attention on the sequential medical encounters within a type, (3) attention on the individual medical codes within an encounter and type. This model is applied to predict the occurrence of stage 3 chronic kidney disease (CKD), using three years medical history of Medicare Advantage (MA) members from an American nationwide health insurance company. The model takes members medical events, both claims and Electronic Medical Records (EMR) data, as input, makes a prediction of stage 3 CKD and calculates contribution from individual events to the predicted outcome.",Healthcare
Distantly supervised end-to-end medical entity extraction from electronic health records with human-level quality,"Medical entity extraction (EE) is a standard procedure used as a first stage in medical texts processing. Usually Medical EE is a two-step process: named entity recognition (NER) and named entity normalization (NEN). We propose a novel method of doing medical EE from electronic health records (EHR) as a single-step multi-label classification task by fine-tuning a transformer model pretrained on a large EHR dataset. Our model is trained end-to-end in an distantly supervised manner using targets automatically extracted from medical knowledge base. We show that our model learns to generalize for entities that are present frequently enough, achieving human-level classification quality for most frequent entities. Our work demonstrates that medical entity extraction can be done end-to-end without human supervision and with human quality given the availability of a large enough amount of unlabeled EHR and a medical knowledge base.","Distantly supervised end-to-end medical entity extraction from electronic health records with human-level quality Medical entity extraction (EE) is a standard procedure used as a first stage in medical texts processing. Usually Medical EE is a two-step process: named entity recognition (NER) and named entity normalization (NEN). We propose a novel method of doing medical EE from electronic health records (EHR) as a single-step multi-label classification task by fine-tuning a transformer model pretrained on a large EHR dataset. Our model is trained end-to-end in an distantly supervised manner using targets automatically extracted from medical knowledge base. We show that our model learns to generalize for entities that are present frequently enough, achieving human-level classification quality for most frequent entities. Our work demonstrates that medical entity extraction can be done end-to-end without human supervision and with human quality given the availability of a large enough amount of unlabeled EHR and a medical knowledge base.",Healthcare
Using Peer-Customers to Scalably Pair Student Teams with Customers for Hands-on Curriculum Final Projects,"Peer-customer is a mechanism to pair student teams with customers in hands-on curriculum courses. Each student pitches a problem they want someone else in the class to solve for them. The use of peer-customers provides practical and scalable access for students to work with a customer on a real-world need for their final project. The peer-customer, despite being a student in the class, do not work on the project with the team. This dissociation forces a student team to practice customer needs assessment, testing, and surveying that can often be lacking in self-ideated final projects that do not have resources to curate external customers like in capstone courses. We prototyped the use of peer-customers in an introductory physical prototyping course focused on basic embedded systems design and python programming. In this paper, we present a practical guide on how best to use peer-customers, supported by key observations made during two separate offerings of the course with a total of N64 students (N29 Y1 and N35 Y2).","Using Peer-Customers to Scalably Pair Student Teams with Customers for Hands-on Curriculum Final Projects Peer-customer is a mechanism to pair student teams with customers in hands-on curriculum courses. Each student pitches a problem they want someone else in the class to solve for them. The use of peer-customers provides practical and scalable access for students to work with a customer on a real-world need for their final project. The peer-customer, despite being a student in the class, do not work on the project with the team. This dissociation forces a student team to practice customer needs assessment, testing, and surveying that can often be lacking in self-ideated final projects that do not have resources to curate external customers like in capstone courses. We prototyped the use of peer-customers in an introductory physical prototyping course focused on basic embedded systems design and python programming. In this paper, we present a practical guide on how best to use peer-customers, supported by key observations made during two separate offerings of the course with a total of N64 students (N29 Y1 and N35 Y2).",Education
Fisher information and quantum mechanical models for finance,The probability distribution function (PDF) for prices on financial markets is derived by extremization of Fisher information. It is shown how on that basis the quantum-like description for financial markets arises and different financial market models are mapped by quantum mechanical ones.,Fisher information and quantum mechanical models for finance The probability distribution function (PDF) for prices on financial markets is derived by extremization of Fisher information. It is shown how on that basis the quantum-like description for financial markets arises and different financial market models are mapped by quantum mechanical ones.,Finance
Duality and Convergence for Binomial Markets with Friction,"We prove limit theorems for the super-replication cost of European options in a Binomial model with friction. The examples covered are markets with proportional transaction costs and the illiquid markets. The dual representation for the super-replication cost in these models are obtained and used to prove the limit theorems. In particular, the existence of the liquidity premium for the continuous time limit of the model proposed in 6 is proved. Hence, this paper extends the previous convergence result of 13 to the general non-Markovian case. Moreover, the special case of small transaction costs yields, in the continuous limit, the G-expectation of Peng as earlier proved by Kusuoka in 14.","Duality and Convergence for Binomial Markets with Friction We prove limit theorems for the super-replication cost of European options in a Binomial model with friction. The examples covered are markets with proportional transaction costs and the illiquid markets. The dual representation for the super-replication cost in these models are obtained and used to prove the limit theorems. In particular, the existence of the liquidity premium for the continuous time limit of the model proposed in 6 is proved. Hence, this paper extends the previous convergence result of 13 to the general non-Markovian case. Moreover, the special case of small transaction costs yields, in the continuous limit, the G-expectation of Peng as earlier proved by Kusuoka in 14.",Finance
Broadening the perspective for sustainable AI: Comprehensive sustainability criteria and indicators for AI systems,"The increased use of AI systems is associated with multi-faceted societal, environmental, and economic consequences. These include non-transparent decision-making processes, discrimination, increasing inequalities, rising energy consumption and greenhouse gas emissions in AI model development and application, and an increasing concentration of economic power. By considering the multi-dimensionality of sustainability, this paper takes steps towards substantiating the call for an overarching perspective on sustainable AI. It presents the SCAIS Framework (Sustainability Criteria and Indicators for Artificial Intelligence Systems) which contains a set 19 sustainability criteria for sustainable AI and 67 indicators that is based on the results of a critical review and expert workshops. This interdisciplinary approach contributes a unique holistic perspective to facilitate and structure the discourse on sustainable AI. Further, it provides a concrete framework that lays the foundation for developing standards and tools to support the conscious development and application of AI systems.","Broadening the perspective for sustainable AI: Comprehensive sustainability criteria and indicators for AI systems The increased use of AI systems is associated with multi-faceted societal, environmental, and economic consequences. These include non-transparent decision-making processes, discrimination, increasing inequalities, rising energy consumption and greenhouse gas emissions in AI model development and application, and an increasing concentration of economic power. By considering the multi-dimensionality of sustainability, this paper takes steps towards substantiating the call for an overarching perspective on sustainable AI. It presents the SCAIS Framework (Sustainability Criteria and Indicators for Artificial Intelligence Systems) which contains a set 19 sustainability criteria for sustainable AI and 67 indicators that is based on the results of a critical review and expert workshops. This interdisciplinary approach contributes a unique holistic perspective to facilitate and structure the discourse on sustainable AI. Further, it provides a concrete framework that lays the foundation for developing standards and tools to support the conscious development and application of AI systems.",Environment
Less is More: Pre-Training Cross-Lingual Small-Scale Language Models with Cognitively-Plausible Curriculum Learning Strategies,"Curriculum Learning has been a popular strategy to improve the cognitive plausibility of Small-Scale Language Models (SSLMs) in the BabyLM Challenge. However, it has not led to considerable improvements over non-curriculum models. We assess whether theoretical linguistic acquisition theories can be used to specify more fine-grained curriculum learning strategies, creating age-ordered corpora of Child-Directed Speech for four typologically distant language families to implement SSLMs and acquisition-inspired curricula cross-lingually. Comparing the success of three objective curricula (Growing, Inwards and MMM) that precisely replicate the predictions of acquisition theories on a standard SSLM architecture, we find fine-grained acquisition-inspired curricula can outperform non-curriculum baselines and performance benefits of curricula strategies in SSLMs can be derived by specifying fine-grained language-specific curricula that precisely replicate language acquisition theories.","Less is More: Pre-Training Cross-Lingual Small-Scale Language Models with Cognitively-Plausible Curriculum Learning Strategies Curriculum Learning has been a popular strategy to improve the cognitive plausibility of Small-Scale Language Models (SSLMs) in the BabyLM Challenge. However, it has not led to considerable improvements over non-curriculum models. We assess whether theoretical linguistic acquisition theories can be used to specify more fine-grained curriculum learning strategies, creating age-ordered corpora of Child-Directed Speech for four typologically distant language families to implement SSLMs and acquisition-inspired curricula cross-lingually. Comparing the success of three objective curricula (Growing, Inwards and MMM) that precisely replicate the predictions of acquisition theories on a standard SSLM architecture, we find fine-grained acquisition-inspired curricula can outperform non-curriculum baselines and performance benefits of curricula strategies in SSLMs can be derived by specifying fine-grained language-specific curricula that precisely replicate language acquisition theories.",Education
"Predicting Financial Markets: Comparing Survey, News, Twitter and Search Engine Data","Financial market prediction on the basis of online sentiment tracking has drawn a lot of attention recently. However, most results in this emerging domain rely on a unique, particular combination of data sets and sentiment tracking tools. This makes it difficult to disambiguate measurement and instrument effects from factors that are actually involved in the apparent relation between online sentiment and market values. In this paper, we survey a range of online data sets (Twitter feeds, news headlines, and volumes of Google search queries) and sentiment tracking methods (Twitter Investor Sentiment, Negative News Sentiment and Tweet  Google Search volumes of financial terms), and compare their value for financial prediction of market indices such as the Dow Jones Industrial Average, trading volumes, and market volatility (VIX), as well as gold prices. We also compare the predictive power of traditional investor sentiment survey data, i.e. Investor Intelligence and Daily Sentiment Index, against those of the mentioned set of online sentiment indicators. Our results show that traditional surveys of Investor Intelligence are lagging indicators of the financial markets. However, weekly Google Insight Search volumes on financial search queries do have predictive value. An indicator of Twitter Investor Sentiment and the frequency of occurrence of financial terms on Twitter in the previous 1-2 days are also found to be very statistically significant predictors of daily market log return. Survey sentiment indicators are however found not to be statistically significant predictors of financial market values, once we control for all other mood indicators as well as the VIX.","Predicting Financial Markets: Comparing Survey, News, Twitter and Search Engine Data Financial market prediction on the basis of online sentiment tracking has drawn a lot of attention recently. However, most results in this emerging domain rely on a unique, particular combination of data sets and sentiment tracking tools. This makes it difficult to disambiguate measurement and instrument effects from factors that are actually involved in the apparent relation between online sentiment and market values. In this paper, we survey a range of online data sets (Twitter feeds, news headlines, and volumes of Google search queries) and sentiment tracking methods (Twitter Investor Sentiment, Negative News Sentiment and Tweet  Google Search volumes of financial terms), and compare their value for financial prediction of market indices such as the Dow Jones Industrial Average, trading volumes, and market volatility (VIX), as well as gold prices. We also compare the predictive power of traditional investor sentiment survey data, i.e. Investor Intelligence and Daily Sentiment Index, against those of the mentioned set of online sentiment indicators. Our results show that traditional surveys of Investor Intelligence are lagging indicators of the financial markets. However, weekly Google Insight Search volumes on financial search queries do have predictive value. An indicator of Twitter Investor Sentiment and the frequency of occurrence of financial terms on Twitter in the previous 1-2 days are also found to be very statistically significant predictors of daily market log return. Survey sentiment indicators are however found not to be statistically significant predictors of financial market values, once we control for all other mood indicators as well as the VIX.",Finance
Filtering Additive Measurement Noise with Maximum Entropy in the Mean,The purpose of this note is to show how the method of maximum entropy in the mean (MEM) may be used to improve parametric estimation when the measurements are corrupted by large level of noise. The method is developed in the context on a concrete example: that of estimation of the parameter in an exponential distribution. We compare the performance of our method with the bayesian and maximum likelihood approaches.,Filtering Additive Measurement Noise with Maximum Entropy in the Mean The purpose of this note is to show how the method of maximum entropy in the mean (MEM) may be used to improve parametric estimation when the measurements are corrupted by large level of noise. The method is developed in the context on a concrete example: that of estimation of the parameter in an exponential distribution. We compare the performance of our method with the bayesian and maximum likelihood approaches.,Technology
A Divergence Critic for Inductive Proof,"Inductive theorem provers often diverge. This paper describes a simple critic, a computer program which monitors the construction of inductive proofs attempting to identify diverging proof attempts. Divergence is recognized by means of a difference matching procedure. The critic then proposes lemmas and generalizations which ripple these differences away so that the proof can go through without divergence. The critic enables the theorem prover Spike to prove many theorems completely automatically from the definitions alone.","A Divergence Critic for Inductive Proof Inductive theorem provers often diverge. This paper describes a simple critic, a computer program which monitors the construction of inductive proofs attempting to identify diverging proof attempts. Divergence is recognized by means of a difference matching procedure. The critic then proposes lemmas and generalizations which ripple these differences away so that the proof can go through without divergence. The critic enables the theorem prover Spike to prove many theorems completely automatically from the definitions alone.",Technology
Image compression by rectangular wavelet transform,"We study image compression by a separable wavelet basis bigpsi(2k_1x-i)psi(2k_2y-j), phi(x-i)psi(2k_2y-j), psi(2k_1(x-i)phi(y-j), phi(x-i)phi(y-i)big, where k_1, k_2 in mathbbZ_; i,jinmathbbZ; and phi,psi are elements of a standard biorthogonal wavelet basis in L_2(mathbbR). Because k_1ne k_2, the supports of the basis elements are rectangles, and the corresponding transform is known as the em rectangular wavelet transform. We prove that if one-dimensional wavelet basis has M dual vanishing moments then the rate of approximation by N coefficients of rectangular wavelet transform is mathcalO(N-MlogC N) for functions with mixed derivative of order M in each direction. The square wavelet transform yields the approximation rate is mathcalO(N-M2) for functions with all derivatives of the total order M. Thus, the rectangular wavelet transform can outperform the square one if an image has a mixed derivative. We provide experimental comparison of image compression which shows that rectangular wavelet transform outperform the square one.","Image compression by rectangular wavelet transform We study image compression by a separable wavelet basis bigpsi(2k_1x-i)psi(2k_2y-j), phi(x-i)psi(2k_2y-j), psi(2k_1(x-i)phi(y-j), phi(x-i)phi(y-i)big, where k_1, k_2 in mathbbZ_; i,jinmathbbZ; and phi,psi are elements of a standard biorthogonal wavelet basis in L_2(mathbbR). Because k_1ne k_2, the supports of the basis elements are rectangles, and the corresponding transform is known as the em rectangular wavelet transform. We prove that if one-dimensional wavelet basis has M dual vanishing moments then the rate of approximation by N coefficients of rectangular wavelet transform is mathcalO(N-MlogC N) for functions with mixed derivative of order M in each direction. The square wavelet transform yields the approximation rate is mathcalO(N-M2) for functions with all derivatives of the total order M. Thus, the rectangular wavelet transform can outperform the square one if an image has a mixed derivative. We provide experimental comparison of image compression which shows that rectangular wavelet transform outperform the square one.",Technology
On Curating Responsible and Representative Healthcare Video Recommendations for Patient Education and Health Literacy: An Augmented Intelligence Approach,"Studies suggest that one in three US adults use the Internet to diagnose or learn about a health concern. However, such access to health information online could exacerbate the disparities in health information availability and use. Health information seeking behavior (HISB) refers to the ways in which individuals seek information about their health, risks, illnesses, and health-protective behaviors. For patients engaging in searches for health information on digital media platforms, health literacy divides can be exacerbated both by their own lack of knowledge and by algorithmic recommendations, with results that disproportionately impact disadvantaged populations, minorities, and low health literacy users. This study reports on an exploratory investigation of the above challenges by examining whether responsible and representative recommendations can be generated using advanced analytic methods applied to a large corpus of videos and their metadata on a chronic condition (diabetes) from the YouTube social media platform. The paper focusses on biases associated with demographic characters of actors using videos on diabetes that were retrieved and curated for multiple criteria such as encoded medical content and their understandability to address patient education and population health literacy needs. This approach offers an immense opportunity for innovation in human-in-the-loop, augmented-intelligence, bias-aware and responsible algorithmic recommendations by combining the perspectives of health professionals and patients into a scalable and generalizable machine learning framework for patient empowerment and improved health outcomes.","On Curating Responsible and Representative Healthcare Video Recommendations for Patient Education and Health Literacy: An Augmented Intelligence Approach Studies suggest that one in three US adults use the Internet to diagnose or learn about a health concern. However, such access to health information online could exacerbate the disparities in health information availability and use. Health information seeking behavior (HISB) refers to the ways in which individuals seek information about their health, risks, illnesses, and health-protective behaviors. For patients engaging in searches for health information on digital media platforms, health literacy divides can be exacerbated both by their own lack of knowledge and by algorithmic recommendations, with results that disproportionately impact disadvantaged populations, minorities, and low health literacy users. This study reports on an exploratory investigation of the above challenges by examining whether responsible and representative recommendations can be generated using advanced analytic methods applied to a large corpus of videos and their metadata on a chronic condition (diabetes) from the YouTube social media platform. The paper focusses on biases associated with demographic characters of actors using videos on diabetes that were retrieved and curated for multiple criteria such as encoded medical content and their understandability to address patient education and population health literacy needs. This approach offers an immense opportunity for innovation in human-in-the-loop, augmented-intelligence, bias-aware and responsible algorithmic recommendations by combining the perspectives of health professionals and patients into a scalable and generalizable machine learning framework for patient empowerment and improved health outcomes.",Healthcare
Foundation Models for Clean Energy Forecasting: A Comprehensive Review,"As global energy systems transit to clean energy, accurate renewable generation and renewable demand forecasting is imperative for effective grid management. Foundation Models (FMs) can help improve forecasting of renewable generation and demand because FMs can rapidly process complex, high-dimensional time-series data. This review paper focuses on FMs in the realm of renewable energy forecasting, primarily focusing on wind and solar. We present an overview of the architectures, pretraining strategies, finetuning methods, and types of data used in the context of renewable energy forecasting. We emphasize the role of models that are trained at a large scale, domain specific Transformer architectures, where attention is paid to spatial temporal correlations, the embedding of domain knowledge, and also the brief and intermittent nature of renewable generation. We assess recent FM based advancements in forecast accuracy such as reconciling predictions over multiple time scales and quantifying uncertainty in renewable energy forecasting. We also review existing challenges and areas of improvement in long-term and multivariate time series forecasting. In this survey, a distinction between theory and practice is established regarding the use of FMs in the clean energy forecasting domain. Additionally, it critically assesses the strengths and weaknesses of FMs while advancing future research direction in this new and exciting area of forecasting.","Foundation Models for Clean Energy Forecasting: A Comprehensive Review As global energy systems transit to clean energy, accurate renewable generation and renewable demand forecasting is imperative for effective grid management. Foundation Models (FMs) can help improve forecasting of renewable generation and demand because FMs can rapidly process complex, high-dimensional time-series data. This review paper focuses on FMs in the realm of renewable energy forecasting, primarily focusing on wind and solar. We present an overview of the architectures, pretraining strategies, finetuning methods, and types of data used in the context of renewable energy forecasting. We emphasize the role of models that are trained at a large scale, domain specific Transformer architectures, where attention is paid to spatial temporal correlations, the embedding of domain knowledge, and also the brief and intermittent nature of renewable generation. We assess recent FM based advancements in forecast accuracy such as reconciling predictions over multiple time scales and quantifying uncertainty in renewable energy forecasting. We also review existing challenges and areas of improvement in long-term and multivariate time series forecasting. In this survey, a distinction between theory and practice is established regarding the use of FMs in the clean energy forecasting domain. Additionally, it critically assesses the strengths and weaknesses of FMs while advancing future research direction in this new and exciting area of forecasting.",Environment
Efficiency Enhancement of Genetic Algorithms via Building-Block-Wise Fitness Estimation,"This paper studies fitness inheritance as an efficiency enhancement technique for a class of competent genetic algorithms called estimation distribution algorithms. Probabilistic models of important sub-solutions are developed to estimate the fitness of a proportion of individuals in the population, thereby avoiding computationally expensive function evaluations. The effect of fitness inheritance on the convergence time and population sizing are modeled and the speed-up obtained through inheritance is predicted. The results show that a fitness-inheritance mechanism which utilizes information on building-block fitnesses provides significant efficiency enhancement. For additively separable problems, fitness inheritance reduces the number of function evaluations to about half and yields a speed-up of about 1.75--2.25.","Efficiency Enhancement of Genetic Algorithms via Building-Block-Wise Fitness Estimation This paper studies fitness inheritance as an efficiency enhancement technique for a class of competent genetic algorithms called estimation distribution algorithms. Probabilistic models of important sub-solutions are developed to estimate the fitness of a proportion of individuals in the population, thereby avoiding computationally expensive function evaluations. The effect of fitness inheritance on the convergence time and population sizing are modeled and the speed-up obtained through inheritance is predicted. The results show that a fitness-inheritance mechanism which utilizes information on building-block fitnesses provides significant efficiency enhancement. For additively separable problems, fitness inheritance reduces the number of function evaluations to about half and yields a speed-up of about 1.75--2.25.",Technology
Enhancing Green Economy with Artificial Intelligence: Role of Energy Use and FDI in the United States,"The escalating challenge of climate change necessitates an urgent exploration of factors influencing carbon emissions. This study contributes to the discourse by examining the interplay of technological, economic, and demographic factors on environmental sustainability. This study investigates the impact of artificial intelligence (AI) innovation, economic growth, foreign direct investment (FDI), energy consumption, and urbanization on CO2 emissions in the United States from 1990 to 2022. Employing the ARDL framework integrated with the STIRPAT model, the findings reveal a dual narrative: while AI innovation mitigates environmental stress, economic growth, energy use, FDI, and urbanization exacerbate environmental degradation. Unit root tests (ADF, PP, and DF-GLS) confirm mixed integration levels among variables, and the ARDL bounds test establishes long-term co-integration. The analysis highlights that AI innovation positively correlates with CO2 reduction when environmental safeguards are in place, whereas GDP growth, energy consumption, FDI, and urbanization intensify CO2 emissions. Robustness checks using FMOLS, DOLS, and CCR validate the ARDL findings. Additionally, Pairwise Granger causality tests reveal significant one-way causal links between CO2 emissions and economic growth, AI innovation, energy use, FDI, and urbanization. These relationships emphasize the critical role of AI-driven technological advancements, sustainable investments, and green energy in fostering ecological sustainability. The study suggests policy measures such as encouraging green FDI, advancing AI technologies, adopting sustainable energy practices, and implementing eco-friendly urban development to promote sustainable growth in the USA.","Enhancing Green Economy with Artificial Intelligence: Role of Energy Use and FDI in the United States The escalating challenge of climate change necessitates an urgent exploration of factors influencing carbon emissions. This study contributes to the discourse by examining the interplay of technological, economic, and demographic factors on environmental sustainability. This study investigates the impact of artificial intelligence (AI) innovation, economic growth, foreign direct investment (FDI), energy consumption, and urbanization on CO2 emissions in the United States from 1990 to 2022. Employing the ARDL framework integrated with the STIRPAT model, the findings reveal a dual narrative: while AI innovation mitigates environmental stress, economic growth, energy use, FDI, and urbanization exacerbate environmental degradation. Unit root tests (ADF, PP, and DF-GLS) confirm mixed integration levels among variables, and the ARDL bounds test establishes long-term co-integration. The analysis highlights that AI innovation positively correlates with CO2 reduction when environmental safeguards are in place, whereas GDP growth, energy consumption, FDI, and urbanization intensify CO2 emissions. Robustness checks using FMOLS, DOLS, and CCR validate the ARDL findings. Additionally, Pairwise Granger causality tests reveal significant one-way causal links between CO2 emissions and economic growth, AI innovation, energy use, FDI, and urbanization. These relationships emphasize the critical role of AI-driven technological advancements, sustainable investments, and green energy in fostering ecological sustainability. The study suggests policy measures such as encouraging green FDI, advancing AI technologies, adopting sustainable energy practices, and implementing eco-friendly urban development to promote sustainable growth in the USA.",Environment
Modeling News Interactions and Influence for Financial Market Prediction,"The diffusion of financial news into market prices is a complex process, making it challenging to evaluate the connections between news events and market movements. This paper introduces FININ (Financial Interconnected News Influence Network), a novel market prediction model that captures not only the links between news and prices but also the interactions among news items themselves. FININ effectively integrates multi-modal information from both market data and news articles. We conduct extensive experiments on two datasets, encompassing the SP 500 and NASDAQ 100 indices over a 15-year period and over 2.7 million news articles. The results demonstrate FININs effectiveness, outperforming advanced market prediction models with an improvement of 0.429 and 0.341 in the daily Sharpe ratio for the two markets respectively. Moreover, our results reveal insights into the financial news, including the delayed market pricing of news, the long memory effect of news, and the limitations of financial sentiment analysis in fully extracting predictive power from news data.","Modeling News Interactions and Influence for Financial Market Prediction The diffusion of financial news into market prices is a complex process, making it challenging to evaluate the connections between news events and market movements. This paper introduces FININ (Financial Interconnected News Influence Network), a novel market prediction model that captures not only the links between news and prices but also the interactions among news items themselves. FININ effectively integrates multi-modal information from both market data and news articles. We conduct extensive experiments on two datasets, encompassing the SP 500 and NASDAQ 100 indices over a 15-year period and over 2.7 million news articles. The results demonstrate FININs effectiveness, outperforming advanced market prediction models with an improvement of 0.429 and 0.341 in the daily Sharpe ratio for the two markets respectively. Moreover, our results reveal insights into the financial news, including the delayed market pricing of news, the long memory effect of news, and the limitations of financial sentiment analysis in fully extracting predictive power from news data.",Finance
Does ESG and Digital Transformation affects Corporate Sustainability? The Moderating role of Green Innovation,"Recently, environmental, social, and governance (ESG) has become an important factor in companies sustainable development. Artificial intelligence (AI) is also a core digital technology that can create innovative, sustainable, comprehensive, and resilient environments. ESG- and AI-based digital transformation is a relevant strategy for managing business value and sustainability in corporate green management operations. Therefore, this study examines how corporate sustainability relates to ESG- and AI-based digital transformation. Furthermore, it confirms the moderating effect of green innovation on the process of increasing sustainability. To achieve the purpose of this study, 359 data points collected for hypothesis testing were used for statistical analysis and for mobile business platform users. The following conclusions are drawn. (1) ESG activities have become key variables that enable sustainable corporate growth. Companies can implement eco-friendly operating processes through ESG activities. (2) This study verifies the relationship between AI-based digital transformation and corporate sustainability and confirms that digital transformation positively affects corporate sustainability. In addition, societal problems can be identified and environmental accidents prevented through technological innovation. (3) This study does not verify the positive moderating effect of green innovation; however, it emphasizes its necessity and importance. Although green innovation improves performance only in the long term, it is a key factor for companies pursuing sustainable growth. This study reveals that ESG- and AI-based digital transformation is an important tool for promoting corporate sustainability, broadening the literature in related fields and providing insights for corporate management and government policymakers to advance corporate sustainability.","Does ESG and Digital Transformation affects Corporate Sustainability? The Moderating role of Green Innovation Recently, environmental, social, and governance (ESG) has become an important factor in companies sustainable development. Artificial intelligence (AI) is also a core digital technology that can create innovative, sustainable, comprehensive, and resilient environments. ESG- and AI-based digital transformation is a relevant strategy for managing business value and sustainability in corporate green management operations. Therefore, this study examines how corporate sustainability relates to ESG- and AI-based digital transformation. Furthermore, it confirms the moderating effect of green innovation on the process of increasing sustainability. To achieve the purpose of this study, 359 data points collected for hypothesis testing were used for statistical analysis and for mobile business platform users. The following conclusions are drawn. (1) ESG activities have become key variables that enable sustainable corporate growth. Companies can implement eco-friendly operating processes through ESG activities. (2) This study verifies the relationship between AI-based digital transformation and corporate sustainability and confirms that digital transformation positively affects corporate sustainability. In addition, societal problems can be identified and environmental accidents prevented through technological innovation. (3) This study does not verify the positive moderating effect of green innovation; however, it emphasizes its necessity and importance. Although green innovation improves performance only in the long term, it is a key factor for companies pursuing sustainable growth. This study reveals that ESG- and AI-based digital transformation is an important tool for promoting corporate sustainability, broadening the literature in related fields and providing insights for corporate management and government policymakers to advance corporate sustainability.",Environment
Neural Networks with Complex and Quaternion Inputs,"This article investigates Kak neural networks, which can be instantaneously trained, for complex and quaternion inputs. The performance of the basic algorithm has been analyzed and shown how it provides a plausible model of human perception and understanding of images. The motivation for studying quaternion inputs is their use in representing spatial rotations that find applications in computer graphics, robotics, global navigation, computer vision and the spatial orientation of instruments. The problem of efficient mapping of data in quaternion neural networks is examined. Some problems that need to be addressed before quaternion neural networks find applications are identified.","Neural Networks with Complex and Quaternion Inputs This article investigates Kak neural networks, which can be instantaneously trained, for complex and quaternion inputs. The performance of the basic algorithm has been analyzed and shown how it provides a plausible model of human perception and understanding of images. The motivation for studying quaternion inputs is their use in representing spatial rotations that find applications in computer graphics, robotics, global navigation, computer vision and the spatial orientation of instruments. The problem of efficient mapping of data in quaternion neural networks is examined. Some problems that need to be addressed before quaternion neural networks find applications are identified.",Technology
Substructure Discovery Using Minimum Description Length and Background Knowledge,"The ability to identify interesting and repetitive substructures is an essential component to discovering knowledge in structural data. We describe a new version of our SUBDUE substructure discovery system based on the minimum description length principle. The SUBDUE system discovers substructures that compress the original data and represent structural concepts in the data. By replacing previously-discovered substructures in the data, multiple passes of SUBDUE produce a hierarchical description of the structural regularities in the data. SUBDUE uses a computationally-bounded inexact graph match that identifies similar, but not identical, instances of a substructure and finds an approximate measure of closeness of two substructures when under computational constraints. In addition to the minimum description length principle, other background knowledge can be used by SUBDUE to guide the search towards more appropriate substructures. Experiments in a variety of domains demonstrate SUBDUEs ability to find substructures capable of compressing the original data and to discover structural concepts important to the domain. Description of Online Appendix: This is a compressed tar file containing the SUBDUE discovery system, written in C. The program accepts as input databases represented in graph form, and will output discovered substructures with their corresponding value.","Substructure Discovery Using Minimum Description Length and Background Knowledge The ability to identify interesting and repetitive substructures is an essential component to discovering knowledge in structural data. We describe a new version of our SUBDUE substructure discovery system based on the minimum description length principle. The SUBDUE system discovers substructures that compress the original data and represent structural concepts in the data. By replacing previously-discovered substructures in the data, multiple passes of SUBDUE produce a hierarchical description of the structural regularities in the data. SUBDUE uses a computationally-bounded inexact graph match that identifies similar, but not identical, instances of a substructure and finds an approximate measure of closeness of two substructures when under computational constraints. In addition to the minimum description length principle, other background knowledge can be used by SUBDUE to guide the search towards more appropriate substructures. Experiments in a variety of domains demonstrate SUBDUEs ability to find substructures capable of compressing the original data and to discover structural concepts important to the domain. Description of Online Appendix: This is a compressed tar file containing the SUBDUE discovery system, written in C. The program accepts as input databases represented in graph form, and will output discovered substructures with their corresponding value.",Technology
A decision support system for ship identification based on the curvature scale space representation,"In this paper, a decision support system for ship identification is presented. The system receives as input a silhouette of the vessel to be identified, previously extracted from a side view of the object. This view could have been acquired with imaging sensors operating at different spectral ranges (CCD, FLIR, image intensifier). The input silhouette is preprocessed and compared to those stored in a database, retrieving a small number of potential matches ranked by their similarity to the target silhouette. This set of potential matches is presented to the system operator, who makes the final ship identification. This system makes use of an evolved version of the Curvature Scale Space (CSS) representation. In the proposed approach, it is curvature extrema, instead of zero crossings, that are tracked during silhouette evolution, hence improving robustness and enabling to cope successfully with cases where the standard CCS representation is found to be unstable. Also, the use of local curvature was replaced with the more robust concept of lobe concavity, with significant additional gains in performance. Experimental results on actual operational imagery prove the excellent performance and robustness of the developed method.","A decision support system for ship identification based on the curvature scale space representation In this paper, a decision support system for ship identification is presented. The system receives as input a silhouette of the vessel to be identified, previously extracted from a side view of the object. This view could have been acquired with imaging sensors operating at different spectral ranges (CCD, FLIR, image intensifier). The input silhouette is preprocessed and compared to those stored in a database, retrieving a small number of potential matches ranked by their similarity to the target silhouette. This set of potential matches is presented to the system operator, who makes the final ship identification. This system makes use of an evolved version of the Curvature Scale Space (CSS) representation. In the proposed approach, it is curvature extrema, instead of zero crossings, that are tracked during silhouette evolution, hence improving robustness and enabling to cope successfully with cases where the standard CCS representation is found to be unstable. Also, the use of local curvature was replaced with the more robust concept of lobe concavity, with significant additional gains in performance. Experimental results on actual operational imagery prove the excellent performance and robustness of the developed method.",Technology
Improving Connectionist Energy Minimization,"Symmetric networks designed for energy minimization such as Boltzman machines and Hopfield nets are frequently investigated for use in optimization, constraint satisfaction and approximation of NP-hard problems. Nevertheless, finding a global solution (i.e., a global minimum for the energy function) is not guaranteed and even a local solution may take an exponential number of steps. We propose an improvement to the standard local activation function used for such networks. The improved algorithm guarantees that a global minimum is found in linear time for tree-like subnetworks. The algorithm, called activate, is uniform and does not assume that the network is tree-like. It can identify tree-like subnetworks even in cyclic topologies (arbitrary networks) and avoid local minima along these trees. For acyclic networks, the algorithm is guaranteed to converge to a global minimum from any initial state of the system (self-stabilization) and remains correct under various types of schedulers. On the negative side, we show that in the presence of cycles, no uniform algorithm exists that guarantees optimality even under a sequential asynchronous scheduler. An asynchronous scheduler can activate only one unit at a time while a synchronous scheduler can activate any number of units in a single time step. In addition, no uniform algorithm exists to optimize even acyclic networks when the scheduler is synchronous. Finally, we show how the algorithm can be improved using the cycle-cutset scheme. The general algorithm, called activate-with-cutset, improves over activate and has some performance guarantees that are related to the size of the networks cycle-cutset.","Improving Connectionist Energy Minimization Symmetric networks designed for energy minimization such as Boltzman machines and Hopfield nets are frequently investigated for use in optimization, constraint satisfaction and approximation of NP-hard problems. Nevertheless, finding a global solution (i.e., a global minimum for the energy function) is not guaranteed and even a local solution may take an exponential number of steps. We propose an improvement to the standard local activation function used for such networks. The improved algorithm guarantees that a global minimum is found in linear time for tree-like subnetworks. The algorithm, called activate, is uniform and does not assume that the network is tree-like. It can identify tree-like subnetworks even in cyclic topologies (arbitrary networks) and avoid local minima along these trees. For acyclic networks, the algorithm is guaranteed to converge to a global minimum from any initial state of the system (self-stabilization) and remains correct under various types of schedulers. On the negative side, we show that in the presence of cycles, no uniform algorithm exists that guarantees optimality even under a sequential asynchronous scheduler. An asynchronous scheduler can activate only one unit at a time while a synchronous scheduler can activate any number of units in a single time step. In addition, no uniform algorithm exists to optimize even acyclic networks when the scheduler is synchronous. Finally, we show how the algorithm can be improved using the cycle-cutset scheme. The general algorithm, called activate-with-cutset, improves over activate and has some performance guarantees that are related to the size of the networks cycle-cutset.",Technology
The clinical target distribution: a probabilistic alternative to the clinical target volume,"Definition of the clinical target volume (CTV) is one of the weakest links in the radiation therapy chain. In particular, inability to account for uncertainties is a severe limitation in the traditional CTV delineation approach. Here, we introduce and test a new concept for tumor target definition, the clinical target distribution (CTD). The CTD is a continuous distribution of the probability of voxels to be tumorous. We describe an approach to incorporate the CTD in treatment plan optimization algorithms, and implement it in a commercial treatment planning system. We test the approach in two synthetic and two clinical cases, a sarcoma and a glioblastoma case. The CTD is straightforward to implement in treatment planning and comes with several advantages. It allows one to find the most suitable tradeoff between target coverage and sparing of surrounding healthy organs at the treatment planning stage, without having to modify or redraw a CTV. Owing to the variable probabilities afforded by the CTD, a more flexible and more clinically meaningful sparing of critical structure becomes possible. Finally, the CTD is expected to reduce the inter-user variability of defining the traditional CTV.","The clinical target distribution: a probabilistic alternative to the clinical target volume Definition of the clinical target volume (CTV) is one of the weakest links in the radiation therapy chain. In particular, inability to account for uncertainties is a severe limitation in the traditional CTV delineation approach. Here, we introduce and test a new concept for tumor target definition, the clinical target distribution (CTD). The CTD is a continuous distribution of the probability of voxels to be tumorous. We describe an approach to incorporate the CTD in treatment plan optimization algorithms, and implement it in a commercial treatment planning system. We test the approach in two synthetic and two clinical cases, a sarcoma and a glioblastoma case. The CTD is straightforward to implement in treatment planning and comes with several advantages. It allows one to find the most suitable tradeoff between target coverage and sparing of surrounding healthy organs at the treatment planning stage, without having to modify or redraw a CTV. Owing to the variable probabilities afforded by the CTD, a more flexible and more clinically meaningful sparing of critical structure becomes possible. Finally, the CTD is expected to reduce the inter-user variability of defining the traditional CTV.",Healthcare
Financial equilibria in the semimartingale setting: complete markets and markets with withdrawal constraints,"Existence of stochastic financial equilibria giving rise to semimartingale asset prices is established under a general class of assumptions. These equilibria are expressed in real terms and span complete markets or markets with withdrawal constraints.We deal with random endowment density streams which admit jumps and general time-dependent utility functions on which only regularity conditions are imposed. As an integral part of the proof of the main result, we establish a novel characterization of semimartingale functions.","Financial equilibria in the semimartingale setting: complete markets and markets with withdrawal constraints Existence of stochastic financial equilibria giving rise to semimartingale asset prices is established under a general class of assumptions. These equilibria are expressed in real terms and span complete markets or markets with withdrawal constraints.We deal with random endowment density streams which admit jumps and general time-dependent utility functions on which only regularity conditions are imposed. As an integral part of the proof of the main result, we establish a novel characterization of semimartingale functions.",Finance
Identification in models of biochemical reactions,"We introduce, analyze, and implement a new method for parameter identification for system of ordinary differential equations that are used to model sets of biochemical reactions. Our method relies on the integral formulation of the ODE system and a method of linear least squares applied to the integral equations. Certain variants of this method are also introduced in this paper.","Identification in models of biochemical reactions We introduce, analyze, and implement a new method for parameter identification for system of ordinary differential equations that are used to model sets of biochemical reactions. Our method relies on the integral formulation of the ODE system and a method of linear least squares applied to the integral equations. Certain variants of this method are also introduced in this paper.",Healthcare
A complete classification of epistatic two-locus models,"The study of epistasis is of great importance in statistical genetics in fields such as linkage and association analysis and QTL mapping. In an effort to classify the types of epistasis in the case of two biallelic loci Li and Reich listed and described all models in the simplest case of 01 penetrance values. However, they left open the problem of finding a classification of two-locus models with continuous penetrance values. We provide a complete classification of biallelic two-locus models. In addition to solving the classification problem for dichotomous trait disease models, our results apply to any instance where real numbers are assigned to genotypes, and provide a complete framework for studying epistasis in QTL data. Our approach is geometric and we show that there are 387 distinct types of two-locus models, which can be reduced to 69 when symmetry between loci and alleles is accounted for. The model types are defined by 86 circuits, which are linear combinations of genotype values, each of which measures a fundamental unit of interaction. The circuits provide information on epistasis beyond that contained in the additive x add, add x dom, and dom x dom interaction terms. We discuss the connection between our classification and standard epistatic models and demonstrate its utility by analyzing a previously published dataset.","A complete classification of epistatic two-locus models The study of epistasis is of great importance in statistical genetics in fields such as linkage and association analysis and QTL mapping. In an effort to classify the types of epistasis in the case of two biallelic loci Li and Reich listed and described all models in the simplest case of 01 penetrance values. However, they left open the problem of finding a classification of two-locus models with continuous penetrance values. We provide a complete classification of biallelic two-locus models. In addition to solving the classification problem for dichotomous trait disease models, our results apply to any instance where real numbers are assigned to genotypes, and provide a complete framework for studying epistasis in QTL data. Our approach is geometric and we show that there are 387 distinct types of two-locus models, which can be reduced to 69 when symmetry between loci and alleles is accounted for. The model types are defined by 86 circuits, which are linear combinations of genotype values, each of which measures a fundamental unit of interaction. The circuits provide information on epistasis beyond that contained in the additive x add, add x dom, and dom x dom interaction terms. We discuss the connection between our classification and standard epistatic models and demonstrate its utility by analyzing a previously published dataset.",Healthcare
A novel interactive OBE approach in SCM pedagogy using beer game simulation theory,"The primary challenge in SCM pedagogy is the learners interaction with the dynamic nature of supply chain transactions. Once achieved, it is also required to evaluate learners learning experience based on their performance. In this paper, a combination of outcome-based education (OBE) and simulation-based education is proposed focusing on beer game theory. The analysis is based on 336 runs of beer game simulation within a target group of 56 participants divided into 14 subgroups (SG1-SG14).The purpose of the study is mainly to investigate the effect of mutual interactions on students learning process using supply chain total cost and ordering fluctuations as critical measurement criteria.","A novel interactive OBE approach in SCM pedagogy using beer game simulation theory The primary challenge in SCM pedagogy is the learners interaction with the dynamic nature of supply chain transactions. Once achieved, it is also required to evaluate learners learning experience based on their performance. In this paper, a combination of outcome-based education (OBE) and simulation-based education is proposed focusing on beer game theory. The analysis is based on 336 runs of beer game simulation within a target group of 56 participants divided into 14 subgroups (SG1-SG14).The purpose of the study is mainly to investigate the effect of mutual interactions on students learning process using supply chain total cost and ordering fluctuations as critical measurement criteria.",Education
Environmental (in)considerations in the Design of Smartphone Settings,"Designing for sufficiency is one of many approaches that could foster more moderate and sustainable digital practices. Based on the Sustainable Information and Communication Technologies (ICT) and Human-Computer Interaction (HCI) literature, we identify five environmental settings categories. However, our analysis of three mobile OS and nine representative applications shows an overall lack of environmental concerns in settings design, leading us to identify six pervasive anti-patterns. Environmental settings, where they exist, are set on the most intensive option by default. They are not presented as such, are not easily accessible, and offer little explanation of their impact. Instead, they encourage more intensive use. Based on these findings, we create a design workbook that explores design principles for environmental settings: presenting the environmental potential of settings; shifting to environmentally neutral states; previewing effects to encourage moderate use; rethinking defaults; facilitating settings access and; exploring more frugal settings. Building upon this workbook, we discuss how settings can tie individual behaviors to systemic factors.","Environmental (in)considerations in the Design of Smartphone Settings Designing for sufficiency is one of many approaches that could foster more moderate and sustainable digital practices. Based on the Sustainable Information and Communication Technologies (ICT) and Human-Computer Interaction (HCI) literature, we identify five environmental settings categories. However, our analysis of three mobile OS and nine representative applications shows an overall lack of environmental concerns in settings design, leading us to identify six pervasive anti-patterns. Environmental settings, where they exist, are set on the most intensive option by default. They are not presented as such, are not easily accessible, and offer little explanation of their impact. Instead, they encourage more intensive use. Based on these findings, we create a design workbook that explores design principles for environmental settings: presenting the environmental potential of settings; shifting to environmentally neutral states; previewing effects to encourage moderate use; rethinking defaults; facilitating settings access and; exploring more frugal settings. Building upon this workbook, we discuss how settings can tie individual behaviors to systemic factors.",Environment
Enhance your smartphone with a Bluetooth Arduino nano board,"Using smartphones in experimental physics teachings offers many advantages in term of engagement, pedagogy and flexibility. But it presents the drawbacks of possibly endangering the device and also facing the heterogeneity of available sensors on different smartphones. We present a low-cost alternative that preserves the advantages of smartphones: using a microcontroller equipped with a large variety of sensors that transmits data to a smartphone using Bluetooth Low-Energy protocol. This device can be lent to students with little risks and used to perform a wide range of experiments. It opens the way to new types of physics teachings.","Enhance your smartphone with a Bluetooth Arduino nano board Using smartphones in experimental physics teachings offers many advantages in term of engagement, pedagogy and flexibility. But it presents the drawbacks of possibly endangering the device and also facing the heterogeneity of available sensors on different smartphones. We present a low-cost alternative that preserves the advantages of smartphones: using a microcontroller equipped with a large variety of sensors that transmits data to a smartphone using Bluetooth Low-Energy protocol. This device can be lent to students with little risks and used to perform a wide range of experiments. It opens the way to new types of physics teachings.",Education
Dynamics onin financial markets: dynamical decoupling and stylized facts,"Stylized facts can be regarded as constraints for any modeling attempt of price dynamics on a financial market, in that an empirically reasonable model has to reproduce these stylized facts at least qualitatively. The dynamics of market prices is modeled on a macro-level as the result of the dynamic coupling of two dynamical components. The degree of their dynamical decoupling is shown to have a significant impact on the stochastic properties of return trials such as the return distribution, volatility clustering, and the multifractal behavior of time scales of asset returns. Particularly we observe a cross over in the return distribution from a Gaussian-like to a Levy-like shape when the degree of decoupling increases. In parallel, the larger the degree of decoupling is the more pronounced is volatility clustering. These findings suggest that the considerations of time in an economic system, in general, and the coupling of constituting processes is essential for understanding the behavior of a financial market.","Dynamics onin financial markets: dynamical decoupling and stylized facts Stylized facts can be regarded as constraints for any modeling attempt of price dynamics on a financial market, in that an empirically reasonable model has to reproduce these stylized facts at least qualitatively. The dynamics of market prices is modeled on a macro-level as the result of the dynamic coupling of two dynamical components. The degree of their dynamical decoupling is shown to have a significant impact on the stochastic properties of return trials such as the return distribution, volatility clustering, and the multifractal behavior of time scales of asset returns. Particularly we observe a cross over in the return distribution from a Gaussian-like to a Levy-like shape when the degree of decoupling increases. In parallel, the larger the degree of decoupling is the more pronounced is volatility clustering. These findings suggest that the considerations of time in an economic system, in general, and the coupling of constituting processes is essential for understanding the behavior of a financial market.",Finance
Do probabilistic medium-range temperature forecasts need to allow for non-normality?,"The gaussian spread regression model for the calibration of site specific ensemble temperature forecasts depends on the apparently restrictive assumption that the uncertainty around temperature forecasts is normally distributed. We generalise the model using the kernel density to allow for much more flexible distribution shapes. However, we do not find any meaningful improvement in the resulting probabilistic forecast when evaluated using likelihood based scores. We conclude that the distribution of uncertainty is either very close to normal, or if it is not close to normal, then the non-normality is not being predicted by the ensemble forecast that we test.","Do probabilistic medium-range temperature forecasts need to allow for non-normality? The gaussian spread regression model for the calibration of site specific ensemble temperature forecasts depends on the apparently restrictive assumption that the uncertainty around temperature forecasts is normally distributed. We generalise the model using the kernel density to allow for much more flexible distribution shapes. However, we do not find any meaningful improvement in the resulting probabilistic forecast when evaluated using likelihood based scores. We conclude that the distribution of uncertainty is either very close to normal, or if it is not close to normal, then the non-normality is not being predicted by the ensemble forecast that we test.",Environment
Hyper-local sustainable assortment planning,"Assortment planning, an important seasonal activity for any retailer, involves choosing the right subset of products to stock in each store.While existing approaches only maximize the expected revenue, we propose including the environmental impact too, through the Higg Material Sustainability Index. The trade-off between revenue and environmental impact is balanced through a multi-objective optimization approach, that yields a Pareto-front of optimal assortments for merchandisers to choose from. Using the proposed approach on a few product categories of a leading fashion retailer shows that choosing assortments with lower environmental impact with a minimal impact on revenue is possible.","Hyper-local sustainable assortment planning Assortment planning, an important seasonal activity for any retailer, involves choosing the right subset of products to stock in each store.While existing approaches only maximize the expected revenue, we propose including the environmental impact too, through the Higg Material Sustainability Index. The trade-off between revenue and environmental impact is balanced through a multi-objective optimization approach, that yields a Pareto-front of optimal assortments for merchandisers to choose from. Using the proposed approach on a few product categories of a leading fashion retailer shows that choosing assortments with lower environmental impact with a minimal impact on revenue is possible.",Environment
News Cohesiveness: an Indicator of Systemic Risk in Financial Markets,"Motivated by recent financial crises significant research efforts have been put into studying contagion effects and herding behaviour in financial markets. Much less has been said about influence of financial news on financial markets. We propose a novel measure of collective behaviour in financial news on the Web, News Cohesiveness Index (NCI), and show that it can be used as a systemic risk indicator. We evaluate the NCI on financial documents from large Web news sources on a daily basis from October 2011 to July 2013 and analyse the interplay between financial markets and financially related news. We hypothesized that strong cohesion in financial news reflects movements in the financial markets. Cohesiveness is more general and robust measure of systemic risk expressed in news, than measures based on simple occurrences of specific terms. Our results indicate that cohesiveness in the financial news is highly correlated with and driven by volatility on the financial markets.","News Cohesiveness: an Indicator of Systemic Risk in Financial Markets Motivated by recent financial crises significant research efforts have been put into studying contagion effects and herding behaviour in financial markets. Much less has been said about influence of financial news on financial markets. We propose a novel measure of collective behaviour in financial news on the Web, News Cohesiveness Index (NCI), and show that it can be used as a systemic risk indicator. We evaluate the NCI on financial documents from large Web news sources on a daily basis from October 2011 to July 2013 and analyse the interplay between financial markets and financially related news. We hypothesized that strong cohesion in financial news reflects movements in the financial markets. Cohesiveness is more general and robust measure of systemic risk expressed in news, than measures based on simple occurrences of specific terms. Our results indicate that cohesiveness in the financial news is highly correlated with and driven by volatility on the financial markets.",Finance
Preliminary study of metabolic radiotherapy with 188Re via small animal imaging,"188Re is a beta- (Emax  2.12 MeV) and gamma (155 keV) emitter. Since its chemistry is similar to that of the largely employed tracer, 99mTc, molecules of hyaluronic acid (HA) have been labelled with 188Re to produce a target specific radiopharmaceutical. The radiolabeled compound, i.v. injected in healthy mice, is able to accumulate into the liver after a few minutes. To study the effect of metabolic radiotherapy in mice, we have built a small gamma camera based on a matrix of YAP:Ce crystals, with 0.6x0.6x10 mm3 pixels, read out by a R2486 Hamamatsu PSPMT. A high-sensitivity 20 mm thick lead parallel-hole collimator, with hole diameter 1.5 mm and septa of 0.18 mm, is placed in front of the YAP matrix. Preliminary results obtained with various phantoms containing a solution of 188Re and with C57 black mice injected with the 188Re-HA solution are presented. To increase the space resolution and to obtain two orthogonal projections simultaneously we are building in parallel two new cameras to be positioned at 90 degrees. They use a CsI(Tl) matrix with 1x1x5 mm3 pixels read out by H8500 Hamamatsu Flat panel PMT.","Preliminary study of metabolic radiotherapy with 188Re via small animal imaging 188Re is a beta- (Emax  2.12 MeV) and gamma (155 keV) emitter. Since its chemistry is similar to that of the largely employed tracer, 99mTc, molecules of hyaluronic acid (HA) have been labelled with 188Re to produce a target specific radiopharmaceutical. The radiolabeled compound, i.v. injected in healthy mice, is able to accumulate into the liver after a few minutes. To study the effect of metabolic radiotherapy in mice, we have built a small gamma camera based on a matrix of YAP:Ce crystals, with 0.6x0.6x10 mm3 pixels, read out by a R2486 Hamamatsu PSPMT. A high-sensitivity 20 mm thick lead parallel-hole collimator, with hole diameter 1.5 mm and septa of 0.18 mm, is placed in front of the YAP matrix. Preliminary results obtained with various phantoms containing a solution of 188Re and with C57 black mice injected with the 188Re-HA solution are presented. To increase the space resolution and to obtain two orthogonal projections simultaneously we are building in parallel two new cameras to be positioned at 90 degrees. They use a CsI(Tl) matrix with 1x1x5 mm3 pixels read out by H8500 Hamamatsu Flat panel PMT.",Healthcare
Uncertainty in climate science and climate policy,"This essay, written by a statistician and a climate scientist, describes our view of the gap that exists between current practice in mainstream climate science, and the practical needs of policymakers charged with exploring possible interventions in the context of climate change. By mainstream we mean the type of climate science that dominates in universities and research centres, which we will term academic climate science, in contrast to policy climate science; aspects of this distinction will become clearer in what follows. In a nutshell, we do not think that academic climate science equips climate scientists to be as helpful as they might be, when involved in climate policy assessment. Partly, we attribute this to an over-investment in high resolution climate simulators, and partly to a culture that is uncomfortable with the inherently subjective nature of climate uncertainty.","Uncertainty in climate science and climate policy This essay, written by a statistician and a climate scientist, describes our view of the gap that exists between current practice in mainstream climate science, and the practical needs of policymakers charged with exploring possible interventions in the context of climate change. By mainstream we mean the type of climate science that dominates in universities and research centres, which we will term academic climate science, in contrast to policy climate science; aspects of this distinction will become clearer in what follows. In a nutshell, we do not think that academic climate science equips climate scientists to be as helpful as they might be, when involved in climate policy assessment. Partly, we attribute this to an over-investment in high resolution climate simulators, and partly to a culture that is uncomfortable with the inherently subjective nature of climate uncertainty.",Environment
Nonstationary time series analysis of heart rate variability,"An analysis of the RR-interval time series, t_i, is presented for the case in which the average time, bart, changes slowly. In particular, bart and a short-time scale variability parameter, V, are simultaneously measured while bart decreases for subjects in the reclined position. The initial decrease in bart is usually linear with V yielding parameters that can be related to physiological quantities.","Nonstationary time series analysis of heart rate variability An analysis of the RR-interval time series, t_i, is presented for the case in which the average time, bart, changes slowly. In particular, bart and a short-time scale variability parameter, V, are simultaneously measured while bart decreases for subjects in the reclined position. The initial decrease in bart is usually linear with V yielding parameters that can be related to physiological quantities.",Healthcare
Blind Detection and Compensation of Camera Lens Geometric Distortions,"This paper presents a blind detection and compensation technique for camera lens geometric distortions. The lens distortion introduces higher-order correlations in the frequency domain and in turn it can be detected using higher-order spectral analysis tools without assuming any specific calibration target. The existing blind lens distortion removal method only considered a single-coefficient radial distortion model. In this paper, two coefficients are considered to model approximately the geometric distortion. All the models considered have analytical closed-form inverse formulae.","Blind Detection and Compensation of Camera Lens Geometric Distortions This paper presents a blind detection and compensation technique for camera lens geometric distortions. The lens distortion introduces higher-order correlations in the frequency domain and in turn it can be detected using higher-order spectral analysis tools without assuming any specific calibration target. The existing blind lens distortion removal method only considered a single-coefficient radial distortion model. In this paper, two coefficients are considered to model approximately the geometric distortion. All the models considered have analytical closed-form inverse formulae.",Technology
Using Individualized Treatment Effects to Assess Treatment Effect Heterogeneity,"Assessing treatment effect heterogeneity (TEH) in clinical trials is crucial, as it provides insights into the variability of treatment responses among patients, influencing important decisions related to drug development. Furthermore, it can lead to personalized medicine by tailoring treatments to individual patient characteristics. This paper introduces novel methodologies for assessing treatment effects using the individual treatment effect as a basis. To estimate this effect, we use a Double Robust (DR) learner to infer a pseudo-outcome that reflects the causal contrast. This pseudo-outcome is then used to perform three objectives: (1) a global test for heterogeneity, (2) ranking covariates based on their influence on effect modification, and (3) providing estimates of the individualized treatment effect. We compare our DR-learner with various alternatives and competing methods in a simulation study, and also use it to assess heterogeneity in a pooled analysis of five Phase III trials in psoriatic arthritis. By integrating these methods with the recently proposed WATCH workflow (Workflow to Assess Treatment Effect Heterogeneity in Drug Development for Clinical Trial Sponsors), we provide a robust framework for analyzing TEH, offering insights that enable more informed decision-making in this challenging area.","Using Individualized Treatment Effects to Assess Treatment Effect Heterogeneity Assessing treatment effect heterogeneity (TEH) in clinical trials is crucial, as it provides insights into the variability of treatment responses among patients, influencing important decisions related to drug development. Furthermore, it can lead to personalized medicine by tailoring treatments to individual patient characteristics. This paper introduces novel methodologies for assessing treatment effects using the individual treatment effect as a basis. To estimate this effect, we use a Double Robust (DR) learner to infer a pseudo-outcome that reflects the causal contrast. This pseudo-outcome is then used to perform three objectives: (1) a global test for heterogeneity, (2) ranking covariates based on their influence on effect modification, and (3) providing estimates of the individualized treatment effect. We compare our DR-learner with various alternatives and competing methods in a simulation study, and also use it to assess heterogeneity in a pooled analysis of five Phase III trials in psoriatic arthritis. By integrating these methods with the recently proposed WATCH workflow (Workflow to Assess Treatment Effect Heterogeneity in Drug Development for Clinical Trial Sponsors), we provide a robust framework for analyzing TEH, offering insights that enable more informed decision-making in this challenging area.",Healthcare
MarS: a Financial Market Simulation Engine Powered by Generative Foundation Model,"Generative models aim to simulate realistic effects of various actions across different contexts, from text generation to visual effects. Despite significant efforts to build real-world simulators, the application of generative models to virtual worlds, like financial markets, remains under-explored. In financial markets, generative models can simulate complex market effects of participants with various behaviors, enabling interaction under different market conditions, and training strategies without financial risk. This simulation relies on the finest structured data in financial market like orders thus building the finest realistic simulation. We propose Large Market Model (LMM), an order-level generative foundation model, for financial market simulation, akin to language modeling in the digital world. Our financial Market Simulation engine (MarS), powered by LMM, addresses the domain-specific need for realistic, interactive and controllable order generation. Key observations include LMMs strong scalability across data size and model complexity, and MarSs robust and practicable realism in controlled generation with market impact. We showcase MarS as a forecast tool, detection system, analysis platform, and agent training environment, thus demonstrating MarSs paradigm shift potential for a variety of financial applications. We release the code of MarS at https:github.commicrosoftMarS.","MarS: a Financial Market Simulation Engine Powered by Generative Foundation Model Generative models aim to simulate realistic effects of various actions across different contexts, from text generation to visual effects. Despite significant efforts to build real-world simulators, the application of generative models to virtual worlds, like financial markets, remains under-explored. In financial markets, generative models can simulate complex market effects of participants with various behaviors, enabling interaction under different market conditions, and training strategies without financial risk. This simulation relies on the finest structured data in financial market like orders thus building the finest realistic simulation. We propose Large Market Model (LMM), an order-level generative foundation model, for financial market simulation, akin to language modeling in the digital world. Our financial Market Simulation engine (MarS), powered by LMM, addresses the domain-specific need for realistic, interactive and controllable order generation. Key observations include LMMs strong scalability across data size and model complexity, and MarSs robust and practicable realism in controlled generation with market impact. We showcase MarS as a forecast tool, detection system, analysis platform, and agent training environment, thus demonstrating MarSs paradigm shift potential for a variety of financial applications. We release the code of MarS at https:github.commicrosoftMarS.",Finance
Capacity Expansion of High Renewable Penetrated Energy Systems Considering Concentrating Solar Power for Seasonal Energy Balance,"With the increasing proportion of variable renewable energy which owns fluctuation characteristics and the promotion of the Clean Heating policy, the seasonal energy imbalance of the system has been more and more challenging. There is a lack of effective means to mitigate this challenge under the background of gradual compression of the traditional thermal unit construction. Concentrating solar power (CSP) is a promising technology to replace thermal units by integrating emergency boilers to cope with extreme weather, and can meet long-time energy balance as a seasonal peak regulation source. In this paper, we propose a long-term high-resolution expansion planning model of the energy system under high renewable penetration which integrates CSP technology for seasonal energy balance. With the projection to 2050, by taking the energy system in Xinjiang province which is a typical area of the Clean Heating project with rich irradiance as a case study, it shows that the optimal deployment of CSP and electric boiler (EB) can reduce the cost, peak-valley difference of net load and renewable curtailment by 8.73, 19.72 and 58.24 respectively at 65 renewable penetration compared to the base scenario.","Capacity Expansion of High Renewable Penetrated Energy Systems Considering Concentrating Solar Power for Seasonal Energy Balance With the increasing proportion of variable renewable energy which owns fluctuation characteristics and the promotion of the Clean Heating policy, the seasonal energy imbalance of the system has been more and more challenging. There is a lack of effective means to mitigate this challenge under the background of gradual compression of the traditional thermal unit construction. Concentrating solar power (CSP) is a promising technology to replace thermal units by integrating emergency boilers to cope with extreme weather, and can meet long-time energy balance as a seasonal peak regulation source. In this paper, we propose a long-term high-resolution expansion planning model of the energy system under high renewable penetration which integrates CSP technology for seasonal energy balance. With the projection to 2050, by taking the energy system in Xinjiang province which is a typical area of the Clean Heating project with rich irradiance as a case study, it shows that the optimal deployment of CSP and electric boiler (EB) can reduce the cost, peak-valley difference of net load and renewable curtailment by 8.73, 19.72 and 58.24 respectively at 65 renewable penetration compared to the base scenario.",Environment
"Economic complexity and the sustainability transition: A review of data, methods, and literature","Economic Complexity (EC) methods have gained increasing popularity across fields and disciplines. In particular, the EC toolbox has proved particularly promising in the study of complex and interrelated phenomena, such as the transition towards a greener economy. Using the EC approach, scholars have been investigating the relationship between EC and sustainability, proposing to identify the distinguishing characteristics of green products and to assess the readiness of productive and technological structures for the sustainability transition. This article proposes to review and summarize the data, methods, and empirical literature that are relevant to the study of the sustainability transition from an EC perspective. We review three distinct but connected blocks of literature on EC and environmental sustainability. First, we survey the evidence linking measures of EC to indicators related to environmental sustainability. Second, we review articles that strive to assess the green competitiveness of productive systems. Third, we examine evidence on green technological development and its connection to non-green knowledge bases. Finally, we summarize the findings for each block and identify avenues for further research in this recent and growing body of empirical literature.","Economic complexity and the sustainability transition: A review of data, methods, and literature Economic Complexity (EC) methods have gained increasing popularity across fields and disciplines. In particular, the EC toolbox has proved particularly promising in the study of complex and interrelated phenomena, such as the transition towards a greener economy. Using the EC approach, scholars have been investigating the relationship between EC and sustainability, proposing to identify the distinguishing characteristics of green products and to assess the readiness of productive and technological structures for the sustainability transition. This article proposes to review and summarize the data, methods, and empirical literature that are relevant to the study of the sustainability transition from an EC perspective. We review three distinct but connected blocks of literature on EC and environmental sustainability. First, we survey the evidence linking measures of EC to indicators related to environmental sustainability. Second, we review articles that strive to assess the green competitiveness of productive systems. Third, we examine evidence on green technological development and its connection to non-green knowledge bases. Finally, we summarize the findings for each block and identify avenues for further research in this recent and growing body of empirical literature.",Environment
How to predict and avert economic crisis,"Our study shows that many firms would accumulate at zero output level (namely, Bankruptcy status) if a perfectly competitive market reaches full employment (namely, those people who should obtain employment have obtained employment). As a result, appearance of economic crisis is determined by two points; that is, (a). Stock market approaches perfect competition; (b). Society reaches full employment. The empirical research of these two points would lead to early warning of economic crisis. Moreover, it is a surprise that the state of economic crisis would be a feasible equilibrium within the framework of the Arrow-Debreu model. That means that we can not understand the origin of economic crisis within the framework of modern economics, for example, the general equilibrium theory.","How to predict and avert economic crisis Our study shows that many firms would accumulate at zero output level (namely, Bankruptcy status) if a perfectly competitive market reaches full employment (namely, those people who should obtain employment have obtained employment). As a result, appearance of economic crisis is determined by two points; that is, (a). Stock market approaches perfect competition; (b). Society reaches full employment. The empirical research of these two points would lead to early warning of economic crisis. Moreover, it is a surprise that the state of economic crisis would be a feasible equilibrium within the framework of the Arrow-Debreu model. That means that we can not understand the origin of economic crisis within the framework of modern economics, for example, the general equilibrium theory.",Finance
Public and Teacher Response to Einsteinian Physics in Schools,"Einsteinian physics represents a distinct paradigm shift compared to Newtonian physics. There is worldwide interest in introducing Einsteinian physics concepts early in school curriculum and trials have demonstrated that this is feasible. However introducing Einsteinian concepts from an early age requires more than suitable curriculum and teaching resources - it also requires teacher training and public support. This paper describes a pilot study used in an attempt to gauge public and teacher support. This entailed giving teachers, who included the entire staff of a primary school, and self-selected family groups an in-depth understanding of proposed curriculum content through public outreach and professional development workshops. We assessed their attitudes through questionnaires. Comments and opinions from the public were also collected from online resources. Results show overwhelming support from both teachers and the public. We assessed attitudes of children as well as adults and obtained opinions regarding the appropriate age at which to begin to introduce Einsteinian concepts.","Public and Teacher Response to Einsteinian Physics in Schools Einsteinian physics represents a distinct paradigm shift compared to Newtonian physics. There is worldwide interest in introducing Einsteinian physics concepts early in school curriculum and trials have demonstrated that this is feasible. However introducing Einsteinian concepts from an early age requires more than suitable curriculum and teaching resources - it also requires teacher training and public support. This paper describes a pilot study used in an attempt to gauge public and teacher support. This entailed giving teachers, who included the entire staff of a primary school, and self-selected family groups an in-depth understanding of proposed curriculum content through public outreach and professional development workshops. We assessed their attitudes through questionnaires. Comments and opinions from the public were also collected from online resources. Results show overwhelming support from both teachers and the public. We assessed attitudes of children as well as adults and obtained opinions regarding the appropriate age at which to begin to introduce Einsteinian concepts.",Education
Guidance on Individualized Treatment Rule Estimation in High Dimensions,"Individualized treatment rules, cornerstones of precision medicine, inform patient treatment decisions with the goal of optimizing patient outcomes. These rules are generally unknown functions of patients pre-treatment covariates, meaning they must be estimated from clinical or observational study data. Myriad methods have been developed to learn these rules, and these procedures are demonstrably successful in traditional asymptotic settings with moderate number of covariates. The finite-sample performance of these methods in high-dimensional covariate settings, which are increasingly the norm in modern clinical trials, has not been well characterized, however. We perform a comprehensive comparison of state-of-the-art individualized treatment rule estimators, assessing performance on the basis of the estimators accuracy, interpretability, and computational efficiency. Sixteen data-generating processes with continuous outcomes and binary treatment assignments are considered, reflecting a diversity of randomized and observational studies. We summarize our findings and provide succinct advice to practitioners needing to estimate individualized treatment rules in high dimensions. All code is made publicly available, facilitating modifications and extensions to our simulation study. A novel pre-treatment covariate filtering procedure is also proposed and is shown to improve estimators accuracy and interpretability.","Guidance on Individualized Treatment Rule Estimation in High Dimensions Individualized treatment rules, cornerstones of precision medicine, inform patient treatment decisions with the goal of optimizing patient outcomes. These rules are generally unknown functions of patients pre-treatment covariates, meaning they must be estimated from clinical or observational study data. Myriad methods have been developed to learn these rules, and these procedures are demonstrably successful in traditional asymptotic settings with moderate number of covariates. The finite-sample performance of these methods in high-dimensional covariate settings, which are increasingly the norm in modern clinical trials, has not been well characterized, however. We perform a comprehensive comparison of state-of-the-art individualized treatment rule estimators, assessing performance on the basis of the estimators accuracy, interpretability, and computational efficiency. Sixteen data-generating processes with continuous outcomes and binary treatment assignments are considered, reflecting a diversity of randomized and observational studies. We summarize our findings and provide succinct advice to practitioners needing to estimate individualized treatment rules in high dimensions. All code is made publicly available, facilitating modifications and extensions to our simulation study. A novel pre-treatment covariate filtering procedure is also proposed and is shown to improve estimators accuracy and interpretability.",Healthcare
"Employment, labor productivity and environmental sustainability: Firm-level evidence from transition","This paper examines how investment in environmentally sustainable practices impacts employment and labor productivity growth of firms in transition economies. The study considers labor skill composition and geographical differences, shedding light on sustainability dynamics. The empirical analysis relies on the World Bank-s Enterprise Survey 2019 for 24 transition economies, constructing an environmental sustainability index from various indicators through a Principal Components Analysis. To address endogeneity, a battery of fixed effects and instrumental variables are employed. Results reveal the relevance of environmental sustainability for both employment and labor productivity growth. However, the significance diminishes when addressing endogeneity comprehensively, alluding that any relation between environmentally sustainable practices and jobs growth is more complex and needs time to work. The decelerating job-creation effect of sustainability investments is however confirmed for the high-skill firms, while low-skill firms benefit from labor productivity gains spurred by such investment. Geographically, Central Europe sees more pronounced labor productivity impacts, possibly due to its higher development and sustainability-awareness levels as compared to Southeast Europe and the Commonwealth of Independent States.","Employment, labor productivity and environmental sustainability: Firm-level evidence from transition This paper examines how investment in environmentally sustainable practices impacts employment and labor productivity growth of firms in transition economies. The study considers labor skill composition and geographical differences, shedding light on sustainability dynamics. The empirical analysis relies on the World Bank-s Enterprise Survey 2019 for 24 transition economies, constructing an environmental sustainability index from various indicators through a Principal Components Analysis. To address endogeneity, a battery of fixed effects and instrumental variables are employed. Results reveal the relevance of environmental sustainability for both employment and labor productivity growth. However, the significance diminishes when addressing endogeneity comprehensively, alluding that any relation between environmentally sustainable practices and jobs growth is more complex and needs time to work. The decelerating job-creation effect of sustainability investments is however confirmed for the high-skill firms, while low-skill firms benefit from labor productivity gains spurred by such investment. Geographically, Central Europe sees more pronounced labor productivity impacts, possibly due to its higher development and sustainability-awareness levels as compared to Southeast Europe and the Commonwealth of Independent States.",Environment
Better Foreground Segmentation Through Graph Cuts,"For many tracking and surveillance applications, background subtraction provides an effective means of segmenting objects moving in front of a static background. Researchers have traditionally used combinations of morphological operations to remove the noise inherent in the background-subtracted result. Such techniques can effectively isolate foreground objects, but tend to lose fidelity around the borders of the segmentation, especially for noisy input. This paper explores the use of a minimum graph cut algorithm to segment the foreground, resulting in qualitatively and quantitiatively cleaner segmentations. Experiments on both artificial and real data show that the graph-based method reduces the error around segmented foreground objects. A MATLAB code implementation is available at http:www.cs.smith.edunhoweresearchcodefgseg","Better Foreground Segmentation Through Graph Cuts For many tracking and surveillance applications, background subtraction provides an effective means of segmenting objects moving in front of a static background. Researchers have traditionally used combinations of morphological operations to remove the noise inherent in the background-subtracted result. Such techniques can effectively isolate foreground objects, but tend to lose fidelity around the borders of the segmentation, especially for noisy input. This paper explores the use of a minimum graph cut algorithm to segment the foreground, resulting in qualitatively and quantitiatively cleaner segmentations. Experiments on both artificial and real data show that the graph-based method reduces the error around segmented foreground objects. A MATLAB code implementation is available at http:www.cs.smith.edunhoweresearchcodefgseg",Technology
The Risk to Population Health Equity Posed by Automated Decision Systems: A Narrative Review,"Artificial intelligence is already ubiquitous, and is increasingly being used to autonomously make ever more consequential decisions. However, there has been relatively little research into the existing and possible consequences for population health equity. A narrative review was undertaken using a hermeneutic approach to explore current and future uses of narrow AI and automated decision systems (ADS) in medicine and public health, issues that have emerged, and implications for equity. Accounts reveal a tremendous expectation on AI to transform medical and public health practices. Prominent demonstrations of AI capability - particularly in diagnostic decision making, risk prediction, and surveillance - are stimulating rapid adoption, spurred by COVID-19. Automated decisions being made have significant consequences for individual and population health and wellbeing. Meanwhile, it is evident that hazards including bias, incontestability, and privacy erosion have emerged in sensitive domains such as criminal justice where narrow AI and ADS are in common use. Reports of issues arising from their use in health are already appearing. As the use of ADS in health expands, it is probable that these hazards will manifest more widely. Bias, incontestability, and privacy erosion give rise to mechanisms by which existing social, economic and health disparities are perpetuated and amplified. Consequently, there is a significant risk that use of ADS in health will exacerbate existing population health inequities. The industrial scale and rapidity with which ADS can be applied heightens the risk to population health equity. It is incumbent on health practitioners and policy makers therefore to explore the potential implications of using ADS, to ensure the use of artificial intelligence promotes population health and equity.","The Risk to Population Health Equity Posed by Automated Decision Systems: A Narrative Review Artificial intelligence is already ubiquitous, and is increasingly being used to autonomously make ever more consequential decisions. However, there has been relatively little research into the existing and possible consequences for population health equity. A narrative review was undertaken using a hermeneutic approach to explore current and future uses of narrow AI and automated decision systems (ADS) in medicine and public health, issues that have emerged, and implications for equity. Accounts reveal a tremendous expectation on AI to transform medical and public health practices. Prominent demonstrations of AI capability - particularly in diagnostic decision making, risk prediction, and surveillance - are stimulating rapid adoption, spurred by COVID-19. Automated decisions being made have significant consequences for individual and population health and wellbeing. Meanwhile, it is evident that hazards including bias, incontestability, and privacy erosion have emerged in sensitive domains such as criminal justice where narrow AI and ADS are in common use. Reports of issues arising from their use in health are already appearing. As the use of ADS in health expands, it is probable that these hazards will manifest more widely. Bias, incontestability, and privacy erosion give rise to mechanisms by which existing social, economic and health disparities are perpetuated and amplified. Consequently, there is a significant risk that use of ADS in health will exacerbate existing population health inequities. The industrial scale and rapidity with which ADS can be applied heightens the risk to population health equity. It is incumbent on health practitioners and policy makers therefore to explore the potential implications of using ADS, to ensure the use of artificial intelligence promotes population health and equity.",Healthcare
Curriculum in Gradient-Based Meta-Reinforcement Learning,"Gradient-based meta-learners such as Model-Agnostic Meta-Learning (MAML) have shown strong few-shot performance in supervised and reinforcement learning settings. However, specifically in the case of meta-reinforcement learning (meta-RL), we can show that gradient-based meta-learners are sensitive to task distributions. With the wrong curriculum, agents suffer the effects of meta-overfitting, shallow adaptation, and adaptation instability. In this work, we begin by highlighting intriguing failure cases of gradient-based meta-RL and show that task distributions can wildly affect algorithmic outputs, stability, and performance. To address this problem, we leverage insights from recent literature on domain randomization and propose meta Active Domain Randomization (meta-ADR), which learns a curriculum of tasks for gradient-based meta-RL in a similar as ADR does for sim2real transfer. We show that this approach induces more stable policies on a variety of simulated locomotion and navigation tasks. We assess in- and out-of-distribution generalization and find that the learned task distributions, even in an unstructured task space, greatly improve the adaptation performance of MAML. Finally, we motivate the need for better benchmarking in meta-RL that prioritizes textitgeneralization over single-task adaption performance.","Curriculum in Gradient-Based Meta-Reinforcement Learning Gradient-based meta-learners such as Model-Agnostic Meta-Learning (MAML) have shown strong few-shot performance in supervised and reinforcement learning settings. However, specifically in the case of meta-reinforcement learning (meta-RL), we can show that gradient-based meta-learners are sensitive to task distributions. With the wrong curriculum, agents suffer the effects of meta-overfitting, shallow adaptation, and adaptation instability. In this work, we begin by highlighting intriguing failure cases of gradient-based meta-RL and show that task distributions can wildly affect algorithmic outputs, stability, and performance. To address this problem, we leverage insights from recent literature on domain randomization and propose meta Active Domain Randomization (meta-ADR), which learns a curriculum of tasks for gradient-based meta-RL in a similar as ADR does for sim2real transfer. We show that this approach induces more stable policies on a variety of simulated locomotion and navigation tasks. We assess in- and out-of-distribution generalization and find that the learned task distributions, even in an unstructured task space, greatly improve the adaptation performance of MAML. Finally, we motivate the need for better benchmarking in meta-RL that prioritizes textitgeneralization over single-task adaption performance.",Education
ESG-coherent risk measures for sustainable investing,"The growing interest in sustainable investing calls for an axiomatic approach to measures of risk and reward that focus not only on financial returns, but also on measures of environmental and social sustainability, i.e. environmental, social, and governance (ESG) scores. We propose definitions for ESG-coherent risk measures and ESG reward-risk ratios based on functions of bivariate random variables that are applied to financial returns and ESG scores, extending the traditional univariate measures to the ESG case. We provide examples and present an empirical analysis in which the ESG-coherent risk measures and ESG reward-risk ratios are used to rank stocks.","ESG-coherent risk measures for sustainable investing The growing interest in sustainable investing calls for an axiomatic approach to measures of risk and reward that focus not only on financial returns, but also on measures of environmental and social sustainability, i.e. environmental, social, and governance (ESG) scores. We propose definitions for ESG-coherent risk measures and ESG reward-risk ratios based on functions of bivariate random variables that are applied to financial returns and ESG scores, extending the traditional univariate measures to the ESG case. We provide examples and present an empirical analysis in which the ESG-coherent risk measures and ESG reward-risk ratios are used to rank stocks.",Environment
Complex Systems: From Nuclear Physics to Financial Markets,"We compare correlations and coherent structures in nuclei and financial markets. In the nuclear physics part we review giant resonances which can be interpreted as a coherent structure embedded in chaos. With similar methods we investigate the financial empirical correlation matrix of the DAX and Dow Jones. We will show, that if the time-zone delay is properly accounted for, the two distinct markets largely merge into one. This is reflected by the largest eigenvalue that develops a gap relative to the remaining, chaotic eigenvalues. By extending investigations of the specific character of financial collectivity we also discuss the criticality-analog phenomenon of the financial log-periodicity and show specific examples.","Complex Systems: From Nuclear Physics to Financial Markets We compare correlations and coherent structures in nuclei and financial markets. In the nuclear physics part we review giant resonances which can be interpreted as a coherent structure embedded in chaos. With similar methods we investigate the financial empirical correlation matrix of the DAX and Dow Jones. We will show, that if the time-zone delay is properly accounted for, the two distinct markets largely merge into one. This is reflected by the largest eigenvalue that develops a gap relative to the remaining, chaotic eigenvalues. By extending investigations of the specific character of financial collectivity we also discuss the criticality-analog phenomenon of the financial log-periodicity and show specific examples.",Finance
Clinical Trial Active Learning,"This paper presents a novel approach to active learning that takes into account the non-independent and identically distributed (non-i.i.d.) structure of a clinical trial setting. There exists two types of clinical trials: retrospective and prospective. Retrospective clinical trials analyze data after treatment has been performed; prospective clinical trials collect data as treatment is ongoing. Typically, active learning approaches assume the dataset is i.i.d. when selecting training samples; however, in the case of clinical trials, treatment results in a dependency between the data collected at the current and past visits. Thus, we propose prospective active learning to overcome the limitations present in traditional active learning methods and apply it to disease detection in optical coherence tomography (OCT) images, where we condition on the time an image was collected to enforce the i.i.d. assumption. We compare our proposed method to the traditional active learning paradigm, which we refer to as retrospective in nature. We demonstrate that prospective active learning outperforms retrospective active learning in two different types of test settings.","Clinical Trial Active Learning This paper presents a novel approach to active learning that takes into account the non-independent and identically distributed (non-i.i.d.) structure of a clinical trial setting. There exists two types of clinical trials: retrospective and prospective. Retrospective clinical trials analyze data after treatment has been performed; prospective clinical trials collect data as treatment is ongoing. Typically, active learning approaches assume the dataset is i.i.d. when selecting training samples; however, in the case of clinical trials, treatment results in a dependency between the data collected at the current and past visits. Thus, we propose prospective active learning to overcome the limitations present in traditional active learning methods and apply it to disease detection in optical coherence tomography (OCT) images, where we condition on the time an image was collected to enforce the i.i.d. assumption. We compare our proposed method to the traditional active learning paradigm, which we refer to as retrospective in nature. We demonstrate that prospective active learning outperforms retrospective active learning in two different types of test settings.",Healthcare
Aspects of Evolutionary Design by Computers,"This paper examines the four main types of Evolutionary Design by computers: Evolutionary Design Optimisation, Evolutionary Art, Evolutionary Artificial Life Forms and Creative Evolutionary Design. Definitions for all four areas are provided. A review of current work in each of these areas is given, with examples of the types of applications that have been tackled. The different properties and requirements of each are examined. Descriptions of typical representations and evolutionary algorithms are provided and examples of designs evolved using these techniques are shown. The paper then discusses how the boundaries of these areas are beginning to merge, resulting in four new overlapping types of Evolutionary Design: Integral Evolutionary Design, Artificial Life Based Evolutionary Design, Aesthetic Evolutionary AL and Aesthetic Evolutionary Design. Finally, the last part of the paper discusses some common problems faced by creators of Evolutionary Design systems, including: interdependent elements in designs, epistasis, and constraint handling.","Aspects of Evolutionary Design by Computers This paper examines the four main types of Evolutionary Design by computers: Evolutionary Design Optimisation, Evolutionary Art, Evolutionary Artificial Life Forms and Creative Evolutionary Design. Definitions for all four areas are provided. A review of current work in each of these areas is given, with examples of the types of applications that have been tackled. The different properties and requirements of each are examined. Descriptions of typical representations and evolutionary algorithms are provided and examples of designs evolved using these techniques are shown. The paper then discusses how the boundaries of these areas are beginning to merge, resulting in four new overlapping types of Evolutionary Design: Integral Evolutionary Design, Artificial Life Based Evolutionary Design, Aesthetic Evolutionary AL and Aesthetic Evolutionary Design. Finally, the last part of the paper discusses some common problems faced by creators of Evolutionary Design systems, including: interdependent elements in designs, epistasis, and constraint handling.",Technology
CORN: Correlation-Driven Nonparametric Learning Approach for Portfolio Selection -- an Online Appendix,"This appendix proves CORNs universal consistency. One of Bins PhD thesis examiner (Special thanks to Vladimir Vovk from Royal Holloway, University of London) suggested that CORN is universal and provided sketch proof of Lemma 1.6, which is the key of this proof. Based on the proof in Gyprfi et al. 2006, we thus prove CORNs universal consistency. Note that the notations in this appendix follows Gyorfi et al. 2006.","CORN: Correlation-Driven Nonparametric Learning Approach for Portfolio Selection -- an Online Appendix This appendix proves CORNs universal consistency. One of Bins PhD thesis examiner (Special thanks to Vladimir Vovk from Royal Holloway, University of London) suggested that CORN is universal and provided sketch proof of Lemma 1.6, which is the key of this proof. Based on the proof in Gyprfi et al. 2006, we thus prove CORNs universal consistency. Note that the notations in this appendix follows Gyorfi et al. 2006.",Finance
A Family of Simplified Geometric Distortion Models for Camera Calibration,"The commonly used radial distortion model for camera calibration is in fact an assumption or a restriction. In practice, camera distortion could happen in a general geometrical manner that is not limited to the radial sense. This paper proposes a simplified geometrical distortion modeling method by using two different radial distortion functions in the two image axes. A family of simplified geometric distortion models is proposed, which are either simple polynomials or the rational functions of polynomials. Analytical geometric undistortion is possible using two of the distortion functions discussed in this paper and their performance can be improved by applying a piecewise fitting idea. Our experimental results show that the geometrical distortion models always perform better than their radial distortion counterparts. Furthermore, the proposed geometric modeling method is more appropriate for cameras whose distortion is not perfectly radially symmetric around the center of distortion.","A Family of Simplified Geometric Distortion Models for Camera Calibration The commonly used radial distortion model for camera calibration is in fact an assumption or a restriction. In practice, camera distortion could happen in a general geometrical manner that is not limited to the radial sense. This paper proposes a simplified geometrical distortion modeling method by using two different radial distortion functions in the two image axes. A family of simplified geometric distortion models is proposed, which are either simple polynomials or the rational functions of polynomials. Analytical geometric undistortion is possible using two of the distortion functions discussed in this paper and their performance can be improved by applying a piecewise fitting idea. Our experimental results show that the geometrical distortion models always perform better than their radial distortion counterparts. Furthermore, the proposed geometric modeling method is more appropriate for cameras whose distortion is not perfectly radially symmetric around the center of distortion.",Technology
Tailoring Education with GenAI: A New Horizon in Lesson Planning,"The advent of Generative AI (GenAI) in education presents a transformative approach to traditional teaching methodologies, which often overlook the diverse needs of individual students. This study introduces a GenAI tool, based on advanced natural language processing, designed as a digital assistant for educators, enabling the creation of customized lesson plans. The tool utilizes an innovative feature termed interactive mega-prompt, a comprehensive query system that allows educators to input detailed classroom specifics such as student demographics, learning objectives, and preferred teaching styles. This input is then processed by the GenAI to generate tailored lesson plans. To evaluate the tools effectiveness, a comprehensive methodology incorporating both quantitative (i.e.,  of time savings) and qualitative (i.e., user satisfaction) criteria was implemented, spanning various subjects and educational levels, with continuous feedback collected from educators through a structured evaluation form. Preliminary results show that educators find the GenAI-generated lesson plans effective, significantly reducing lesson planning time and enhancing the learning experience by accommodating diverse student needs. This AI-driven approach signifies a paradigm shift in education, suggesting its potential applicability in broader educational contexts, including special education needs (SEN), where individualized attention and specific learning aids are paramount","Tailoring Education with GenAI: A New Horizon in Lesson Planning The advent of Generative AI (GenAI) in education presents a transformative approach to traditional teaching methodologies, which often overlook the diverse needs of individual students. This study introduces a GenAI tool, based on advanced natural language processing, designed as a digital assistant for educators, enabling the creation of customized lesson plans. The tool utilizes an innovative feature termed interactive mega-prompt, a comprehensive query system that allows educators to input detailed classroom specifics such as student demographics, learning objectives, and preferred teaching styles. This input is then processed by the GenAI to generate tailored lesson plans. To evaluate the tools effectiveness, a comprehensive methodology incorporating both quantitative (i.e.,  of time savings) and qualitative (i.e., user satisfaction) criteria was implemented, spanning various subjects and educational levels, with continuous feedback collected from educators through a structured evaluation form. Preliminary results show that educators find the GenAI-generated lesson plans effective, significantly reducing lesson planning time and enhancing the learning experience by accommodating diverse student needs. This AI-driven approach signifies a paradigm shift in education, suggesting its potential applicability in broader educational contexts, including special education needs (SEN), where individualized attention and specific learning aids are paramount",Education
On the effect of small-scale oceanic variability on topography-generated currents,"Small-scale oceanic motions, in combination with bottom topography, induce mean large-scale along-isobaths flows. The direction of these mean flows is usually found to be anticyclonic (cyclonic) over bumps (depressions). Here we employ a quasigeostrophic model to show that the current direction of these topographically induced large-scale flows can be reversed by the small-scale variability. This result addresses the existence of a new bulk effect from the small-scale activity that could have strong consequences on the circulation of the worlds ocean.","On the effect of small-scale oceanic variability on topography-generated currents Small-scale oceanic motions, in combination with bottom topography, induce mean large-scale along-isobaths flows. The direction of these mean flows is usually found to be anticyclonic (cyclonic) over bumps (depressions). Here we employ a quasigeostrophic model to show that the current direction of these topographically induced large-scale flows can be reversed by the small-scale variability. This result addresses the existence of a new bulk effect from the small-scale activity that could have strong consequences on the circulation of the worlds ocean.",Environment
On Interference of Signals and Generalization in Feedforward Neural Networks,This paper studies how the generalization ability of neurons can be affected by mutual processing of different signals. This study is done on the basis of a feedforward artificial neural network. The mutual processing of signals can possibly be a good model of patterns in a set generalized by a neural network and in effect may improve generalization. In this paper it is discussed that the interference may also cause a highly random generalization. Adaptive activation functions are discussed as a way of reducing that type of generalization. A test of a feedforward neural network is performed that shows the discussed random generalization.,On Interference of Signals and Generalization in Feedforward Neural Networks This paper studies how the generalization ability of neurons can be affected by mutual processing of different signals. This study is done on the basis of a feedforward artificial neural network. The mutual processing of signals can possibly be a good model of patterns in a set generalized by a neural network and in effect may improve generalization. In this paper it is discussed that the interference may also cause a highly random generalization. Adaptive activation functions are discussed as a way of reducing that type of generalization. A test of a feedforward neural network is performed that shows the discussed random generalization.,Technology
Evidence-based teaching: how do we all get there?,"There are compelling reasons to shift our pedagogy toward evidence-based active learning methods that substantially improve student success, and now plenty of resources to aid in that shift. These include the recent CBMS Statement on Active Learning, MAA Instructional Practices Guide (IPG), and MIT Electronic Seminar on Mathematics Education. But implementation is neither quick nor easy. There are still plenty of individual, institutional, cultural, and professional obstacles, along with wonderful opportunities. At the 2019 Joint Mathematics Meetings we co-organized a guided discussion -- an un-panel -- sponsored by the American Mathematical Societys Committee on Education in order to stimulate the process of our community moving toward active learning in our teaching pedagogy. Seventy participants with fifteen discussion leaders expanded an initial list of issues, and considered questions around both challenges and opportunities. Here we summarize from these discussions, suggesting areas for collaborative efforts ranging from local colleagues and educational institutions to national and global professional societies.","Evidence-based teaching: how do we all get there? There are compelling reasons to shift our pedagogy toward evidence-based active learning methods that substantially improve student success, and now plenty of resources to aid in that shift. These include the recent CBMS Statement on Active Learning, MAA Instructional Practices Guide (IPG), and MIT Electronic Seminar on Mathematics Education. But implementation is neither quick nor easy. There are still plenty of individual, institutional, cultural, and professional obstacles, along with wonderful opportunities. At the 2019 Joint Mathematics Meetings we co-organized a guided discussion -- an un-panel -- sponsored by the American Mathematical Societys Committee on Education in order to stimulate the process of our community moving toward active learning in our teaching pedagogy. Seventy participants with fifteen discussion leaders expanded an initial list of issues, and considered questions around both challenges and opportunities. Here we summarize from these discussions, suggesting areas for collaborative efforts ranging from local colleagues and educational institutions to national and global professional societies.",Education
Statistical Properties of the Interbeat Interval Cascade in Human Subjects,"Statistical properties of interbeat intervals cascade are evaluated by considering the joint probability distribution P(Delta x_2,tau_2;Delta x_1,tau_1) for two interbeat increments Delta x_1 and Delta x_2 of different time scales tau_1 and tau_2. We present evidence that the conditional probability distribution P(Delta x_2,tau_2Delta x_1,tau_1) may obey a Chapman-Kolmogorov equation. The corresponding Kramers-Moyal (KM) coefficients are evaluated. It is shown that while the first and second KM coefficients, i.e., the drift and diffusion coefficients, take on well-defined and significant values, the higher-order coefficients in the KM expansion are very small. As a result, the joint probability distributions of the increments in the interbeat intervals obey a Fokker-Planck equation. The method provides a novel technique for distinguishing the two classes of subjects in terms of the drift and diffusion coefficients, which behave differently for two classes of the subjects, namely, healthy subjects and those with congestive heart failure.","Statistical Properties of the Interbeat Interval Cascade in Human Subjects Statistical properties of interbeat intervals cascade are evaluated by considering the joint probability distribution P(Delta x_2,tau_2;Delta x_1,tau_1) for two interbeat increments Delta x_1 and Delta x_2 of different time scales tau_1 and tau_2. We present evidence that the conditional probability distribution P(Delta x_2,tau_2Delta x_1,tau_1) may obey a Chapman-Kolmogorov equation. The corresponding Kramers-Moyal (KM) coefficients are evaluated. It is shown that while the first and second KM coefficients, i.e., the drift and diffusion coefficients, take on well-defined and significant values, the higher-order coefficients in the KM expansion are very small. As a result, the joint probability distributions of the increments in the interbeat intervals obey a Fokker-Planck equation. The method provides a novel technique for distinguishing the two classes of subjects in terms of the drift and diffusion coefficients, which behave differently for two classes of the subjects, namely, healthy subjects and those with congestive heart failure.",Healthcare
A simple model of energy expenditure in human locomotion,"A simple harmonic oscillator model is proposed to describe the mechanical power involved in human locomotion. In this framework, by taking into account the anthropometric parameters of a standard individual, we are able to calculate the speed-power curves in human walking. The proposed model accounts for the well known Margarias law in which the cost ofthe human running (independent from the speed) is fixed to 1 Kcal(Kg Km). The model includes the effects of a gentle slope (either positive or negative) and the effect due to the mechanical response of the walking surface. The model results obtained in the presence of a slope are in qualitative agreement with the experimental data obtained by A. Leonardi et al.","A simple model of energy expenditure in human locomotion A simple harmonic oscillator model is proposed to describe the mechanical power involved in human locomotion. In this framework, by taking into account the anthropometric parameters of a standard individual, we are able to calculate the speed-power curves in human walking. The proposed model accounts for the well known Margarias law in which the cost ofthe human running (independent from the speed) is fixed to 1 Kcal(Kg Km). The model includes the effects of a gentle slope (either positive or negative) and the effect due to the mechanical response of the walking surface. The model results obtained in the presence of a slope are in qualitative agreement with the experimental data obtained by A. Leonardi et al.",Healthcare
Reliability and Market Price of Energy in the Presence of Intermittent and Non-Dispatchable Renewable Energies,"The intermittent nature of the renewable energies increases the operation costs of conventional generators. As the share of energy supplied by renewable sources increases, these costs also increase. In this paper, we quantify these costs by developing a market clearing price of energy in the presence of renewable energy and congestion constraints. We consider an electricity market where generators propose their asking price per unit of energy to an independent system operator (ISO). The ISO solve an optimization problem to dispatch energy from each generator to minimize the total cost of energy purchased on behalf of the consumers. To ensure that the generators are able to meet the load within a desired confidence level, we incorporate the notion of load variance using the Conditional Value-at-Risk (CVAR) measure in an electricity market and we derive the amount of committed power and market clearing price of energy as a function of CVAR. It is shown that a higher penetration of renewable energies may increase the committed power, market clearing price of energy and consumer cost of energy due to renewable generation uncertainties. We also obtain an upper-bound on the amount that congestion constraints can affect the committed power. We present descriptive simulations to illustrate the impact of renewable energy penetration and reliability levels on committed power by the non-renewable generators, difference between the dispatched and committed power, market price of energy and profit of renewable and non-renewable generators.","Reliability and Market Price of Energy in the Presence of Intermittent and Non-Dispatchable Renewable Energies The intermittent nature of the renewable energies increases the operation costs of conventional generators. As the share of energy supplied by renewable sources increases, these costs also increase. In this paper, we quantify these costs by developing a market clearing price of energy in the presence of renewable energy and congestion constraints. We consider an electricity market where generators propose their asking price per unit of energy to an independent system operator (ISO). The ISO solve an optimization problem to dispatch energy from each generator to minimize the total cost of energy purchased on behalf of the consumers. To ensure that the generators are able to meet the load within a desired confidence level, we incorporate the notion of load variance using the Conditional Value-at-Risk (CVAR) measure in an electricity market and we derive the amount of committed power and market clearing price of energy as a function of CVAR. It is shown that a higher penetration of renewable energies may increase the committed power, market clearing price of energy and consumer cost of energy due to renewable generation uncertainties. We also obtain an upper-bound on the amount that congestion constraints can affect the committed power. We present descriptive simulations to illustrate the impact of renewable energy penetration and reliability levels on committed power by the non-renewable generators, difference between the dispatched and committed power, market price of energy and profit of renewable and non-renewable generators.",Environment
Medical Concept Representation Learning from Electronic Health Records and its Application on Heart Failure Prediction,"Objective: To transform heterogeneous clinical data from electronic health records into clinically meaningful constructed features using data driven method that rely, in part, on temporal relations among data. Materials and Methods: The clinically meaningful representations of medical concepts and patients are the key for health analytic applications. Most of existing approaches directly construct features mapped to raw data (e.g., ICD or CPT codes), or utilize some ontology mapping such as SNOMED codes. However, none of the existing approaches leverage EHR data directly for learning such concept representation. We propose a new way to represent heterogeneous medical concepts (e.g., diagnoses, medications and procedures) based on co-occurrence patterns in longitudinal electronic health records. The intuition behind the method is to map medical concepts that are co-occuring closely in time to similar concept vectors so that their distance will be small. We also derive a simple method to construct patient vectors from the related medical concept vectors. Results: For qualitative evaluation, we study similar medical concepts across diagnosis, medication and procedure. In quantitative evaluation, our proposed representation significantly improves the predictive modeling performance for onset of heart failure (HF), where classification methods (e.g. logistic regression, neural network, support vector machine and K-nearest neighbors) achieve up to 23 improvement in area under the ROC curve (AUC) using this proposed representation. Conclusion: We proposed an effective method for patient and medical concept representation learning. The resulting representation can map relevant concepts together and also improves predictive modeling performance.","Medical Concept Representation Learning from Electronic Health Records and its Application on Heart Failure Prediction Objective: To transform heterogeneous clinical data from electronic health records into clinically meaningful constructed features using data driven method that rely, in part, on temporal relations among data. Materials and Methods: The clinically meaningful representations of medical concepts and patients are the key for health analytic applications. Most of existing approaches directly construct features mapped to raw data (e.g., ICD or CPT codes), or utilize some ontology mapping such as SNOMED codes. However, none of the existing approaches leverage EHR data directly for learning such concept representation. We propose a new way to represent heterogeneous medical concepts (e.g., diagnoses, medications and procedures) based on co-occurrence patterns in longitudinal electronic health records. The intuition behind the method is to map medical concepts that are co-occuring closely in time to similar concept vectors so that their distance will be small. We also derive a simple method to construct patient vectors from the related medical concept vectors. Results: For qualitative evaluation, we study similar medical concepts across diagnosis, medication and procedure. In quantitative evaluation, our proposed representation significantly improves the predictive modeling performance for onset of heart failure (HF), where classification methods (e.g. logistic regression, neural network, support vector machine and K-nearest neighbors) achieve up to 23 improvement in area under the ROC curve (AUC) using this proposed representation. Conclusion: We proposed an effective method for patient and medical concept representation learning. The resulting representation can map relevant concepts together and also improves predictive modeling performance.",Healthcare
Clustering with Transitive Distance and K-Means Duality,"Recent spectral clustering methods are a propular and powerful technique for data clustering. These methods need to solve the eigenproblem whose computational complexity is O(n3), where n is the number of data samples. In this paper, a non-eigenproblem based clustering method is proposed to deal with the clustering problem. Its performance is comparable to the spectral clustering algorithms but it is more efficient with computational complexity O(n2). We show that with a transitive distance and an observed property, called K-means duality, our algorithm can be used to handle data sets with complex cluster shapes, multi-scale clusters, and noise. Moreover, no parameters except the number of clusters need to be set in our algorithm.","Clustering with Transitive Distance and K-Means Duality Recent spectral clustering methods are a propular and powerful technique for data clustering. These methods need to solve the eigenproblem whose computational complexity is O(n3), where n is the number of data samples. In this paper, a non-eigenproblem based clustering method is proposed to deal with the clustering problem. Its performance is comparable to the spectral clustering algorithms but it is more efficient with computational complexity O(n2). We show that with a transitive distance and an observed property, called K-means duality, our algorithm can be used to handle data sets with complex cluster shapes, multi-scale clusters, and noise. Moreover, no parameters except the number of clusters need to be set in our algorithm.",Technology
A philosophical essay on life and its connections with genetic algorithms,"This paper makes a number of connections between life and various facets of genetic and evolutionary algorithms research. Specifically, it addresses the topics of adaptation, multiobjective optimization, decision making, deception, and search operators, among others. It argues that human life, from birth to death, is an adaptive or dynamic optimization problem where people are continuously searching for happiness. More important, the paper speculates that genetic algorithms can be used as a source of inspiration for helping people make decisions in their everyday life.","A philosophical essay on life and its connections with genetic algorithms This paper makes a number of connections between life and various facets of genetic and evolutionary algorithms research. Specifically, it addresses the topics of adaptation, multiobjective optimization, decision making, deception, and search operators, among others. It argues that human life, from birth to death, is an adaptive or dynamic optimization problem where people are continuously searching for happiness. More important, the paper speculates that genetic algorithms can be used as a source of inspiration for helping people make decisions in their everyday life.",Technology
LLM Agents for Education: Advances and Applications,"Large Language Model (LLM) agents have demonstrated remarkable capabilities in automating tasks and driving innovation across diverse educational applications. In this survey, we provide a systematic review of state-of-the-art research on LLM agents in education, categorizing them into two broad classes: (1) emphPedagogical Agents, which focus on automating complex pedagogical tasks to support both teachers and students; and (2) emphDomain-Specific Educational Agents, which are tailored for specialized fields such as science education, language learning, and professional development. We comprehensively examine the technological advancements underlying these LLM agents, including key datasets, benchmarks, and algorithmic frameworks that drive their effectiveness. Furthermore, we discuss critical challenges such as privacy, bias and fairness concerns, hallucination mitigation, and integration with existing educational ecosystems. This survey aims to provide a comprehensive technological overview of LLM agents for education, fostering further research and collaboration to enhance their impact for the greater good of learners and educators alike.","LLM Agents for Education: Advances and Applications Large Language Model (LLM) agents have demonstrated remarkable capabilities in automating tasks and driving innovation across diverse educational applications. In this survey, we provide a systematic review of state-of-the-art research on LLM agents in education, categorizing them into two broad classes: (1) emphPedagogical Agents, which focus on automating complex pedagogical tasks to support both teachers and students; and (2) emphDomain-Specific Educational Agents, which are tailored for specialized fields such as science education, language learning, and professional development. We comprehensively examine the technological advancements underlying these LLM agents, including key datasets, benchmarks, and algorithmic frameworks that drive their effectiveness. Furthermore, we discuss critical challenges such as privacy, bias and fairness concerns, hallucination mitigation, and integration with existing educational ecosystems. This survey aims to provide a comprehensive technological overview of LLM agents for education, fostering further research and collaboration to enhance their impact for the greater good of learners and educators alike.",Education
Optimization of supply diversity for the self-assembly of simple objects in two and three dimensions,"The field of algorithmic self-assembly is concerned with the design and analysis of self-assembly systems from a computational perspective, that is, from the perspective of mathematical problems whose study may give insight into the natural processes through which elementary objects self-assemble into more complex ones. One of the main problems of algorithmic self-assembly is the minimum tile set problem (MTSP), which asks for a collection of types of elementary objects (called tiles) to be found for the self-assembly of an object having a pre-established shape. Such a collection is to be as concise as possible, thus minimizing supply diversity, while satisfying a set of stringent constraints having to do with the termination and other properties of the self-assembly process from its tile types. We present a study of what we think is the first practical approach to MTSP. Our study starts with the introduction of an evolutionary heuristic to tackle MTSP and includes results from extensive experimentation with the heuristic on the self-assembly of simple objects in two and three dimensions. The heuristic we introduce combines classic elements from the field of evolutionary computation with a problem-specific variant of Pareto dominance into a multi-objective approach to MTSP.","Optimization of supply diversity for the self-assembly of simple objects in two and three dimensions The field of algorithmic self-assembly is concerned with the design and analysis of self-assembly systems from a computational perspective, that is, from the perspective of mathematical problems whose study may give insight into the natural processes through which elementary objects self-assemble into more complex ones. One of the main problems of algorithmic self-assembly is the minimum tile set problem (MTSP), which asks for a collection of types of elementary objects (called tiles) to be found for the self-assembly of an object having a pre-established shape. Such a collection is to be as concise as possible, thus minimizing supply diversity, while satisfying a set of stringent constraints having to do with the termination and other properties of the self-assembly process from its tile types. We present a study of what we think is the first practical approach to MTSP. Our study starts with the introduction of an evolutionary heuristic to tackle MTSP and includes results from extensive experimentation with the heuristic on the self-assembly of simple objects in two and three dimensions. The heuristic we introduce combines classic elements from the field of evolutionary computation with a problem-specific variant of Pareto dominance into a multi-objective approach to MTSP.",Technology
Pricing Energy in the Presence of Renewables,"At present, electricity markets largely ignore the fact that renewable power producers impose significant externalities on non-renewable energy producers. This is because consumers are generally guaranteed electricity within certain load parameters. The intermittent nature of production by renewable energy producers implies that they rely on non-renewable producers so that the aggregate power delivered meets the promised quality of service. This implicit insurance provided by the non-renewable power sector to consumers is not currently priced and leads to an often ignored, hidden monetary transfer from non-renewable producers to renewable producers. As the fraction of energy supplied by renewable resources increases, these externalities also increase. In this paper, we quantify these externalities by developing the market clearing price of energy in the presence of renewable energy. We consider a day-ahead electricity market where renewable and non-renewable generators bid by proposing their asking price per unit of energy to an independent system operator (ISO). The ISOs problem is a multi-stage stochastic optimization problem to dispatch energy from each generator to minimize the cost of purchased energy on behalf of the consumers. We incorporate the notion of load variance using the Conditional Value-at-Risk (CVAR) measure in the day-ahead electricity market to ensure that the generators are able to meet the load within a desired confidence level. We analytically derive the market clearing price of energy as a function of CVAR. It is shown that a higher penetration level of the renewable energies may increase the market clearing price of energy.","Pricing Energy in the Presence of Renewables At present, electricity markets largely ignore the fact that renewable power producers impose significant externalities on non-renewable energy producers. This is because consumers are generally guaranteed electricity within certain load parameters. The intermittent nature of production by renewable energy producers implies that they rely on non-renewable producers so that the aggregate power delivered meets the promised quality of service. This implicit insurance provided by the non-renewable power sector to consumers is not currently priced and leads to an often ignored, hidden monetary transfer from non-renewable producers to renewable producers. As the fraction of energy supplied by renewable resources increases, these externalities also increase. In this paper, we quantify these externalities by developing the market clearing price of energy in the presence of renewable energy. We consider a day-ahead electricity market where renewable and non-renewable generators bid by proposing their asking price per unit of energy to an independent system operator (ISO). The ISOs problem is a multi-stage stochastic optimization problem to dispatch energy from each generator to minimize the cost of purchased energy on behalf of the consumers. We incorporate the notion of load variance using the Conditional Value-at-Risk (CVAR) measure in the day-ahead electricity market to ensure that the generators are able to meet the load within a desired confidence level. We analytically derive the market clearing price of energy as a function of CVAR. It is shown that a higher penetration level of the renewable energies may increase the market clearing price of energy.",Environment
Electronic Medication Prescribing Support System for Diagnosing Tropical Diseases,This paper presents the development of an e-prescription system for diagnosing tropical diseases.Results after testing the developed system by medical experts indicated that the e-prescription systems is more efficient and less susceptible to common errors associated with the conventional handwritten medical prescription and can also go a long way to help to improve patients health outcome in the health industry especially in the tropics.,Electronic Medication Prescribing Support System for Diagnosing Tropical Diseases This paper presents the development of an e-prescription system for diagnosing tropical diseases.Results after testing the developed system by medical experts indicated that the e-prescription systems is more efficient and less susceptible to common errors associated with the conventional handwritten medical prescription and can also go a long way to help to improve patients health outcome in the health industry especially in the tropics.,Healthcare
Video Analysis and Modeling Tool for Physics Education: A workshop for Redesigning Pedagogy,"This workshop aims to demonstrate how the Tracker Video Analysis and Modeling Tool engages, enables and empowers teachers to be learners so that we can be leaders in our teaching practice. Through this workshop, the kinematics of a falling ball and a projectile motion are explored using video analysis and in the later video modeling. We hope to lead and inspire other teachers by facilitating their experiences with this ICT-enabled video modeling pedagogy (Brown, 2008) and free tool for facilitating students-centered active learning, thus motivate students to be more self-directed.","Video Analysis and Modeling Tool for Physics Education: A workshop for Redesigning Pedagogy This workshop aims to demonstrate how the Tracker Video Analysis and Modeling Tool engages, enables and empowers teachers to be learners so that we can be leaders in our teaching practice. Through this workshop, the kinematics of a falling ball and a projectile motion are explored using video analysis and in the later video modeling. We hope to lead and inspire other teachers by facilitating their experiences with this ICT-enabled video modeling pedagogy (Brown, 2008) and free tool for facilitating students-centered active learning, thus motivate students to be more self-directed.",Education
SWAF: Swarm Algorithm Framework for Numerical Optimization,"A swarm algorithm framework (SWAF), realized by agent-based modeling, is presented to solve numerical optimization problems. Each agent is a bare bones cognitive architecture, which learns knowledge by appropriately deploying a set of simple rules in fast and frugal heuristics. Two essential categories of rules, the generate-and-test and the problem-formulation rules, are implemented, and both of the macro rules by simple combination and subsymbolic deploying of multiple rules among them are also studied. Experimental results on benchmark problems are presented, and performance comparison between SWAF and other existing algorithms indicates that it is efficiently.","SWAF: Swarm Algorithm Framework for Numerical Optimization A swarm algorithm framework (SWAF), realized by agent-based modeling, is presented to solve numerical optimization problems. Each agent is a bare bones cognitive architecture, which learns knowledge by appropriately deploying a set of simple rules in fast and frugal heuristics. Two essential categories of rules, the generate-and-test and the problem-formulation rules, are implemented, and both of the macro rules by simple combination and subsymbolic deploying of multiple rules among them are also studied. Experimental results on benchmark problems are presented, and performance comparison between SWAF and other existing algorithms indicates that it is efficiently.",Technology
Financial markets with volatility uncertainty,We investigate financial markets under model risk caused by uncertain volatilities. For this purpose we consider a financial market that features volatility uncertainty. To have a mathematical consistent framework we use the notion of G-expectation and its corresponding G-Brownian motion recently introduced by Peng (2007). Our financial market consists of a riskless asset and a risky stock with price process modeled by a geometric G-Brownian motion. We adapt the notion of arbitrage to this more complex situation and consider stock price dynamics which exclude arbitrage opportunities. Due to volatility uncertainty the market is not complete any more. We establish the interval of no-arbitrage prices for general European contingent claims and deduce explicit results in a Markovian setting.,Financial markets with volatility uncertainty We investigate financial markets under model risk caused by uncertain volatilities. For this purpose we consider a financial market that features volatility uncertainty. To have a mathematical consistent framework we use the notion of G-expectation and its corresponding G-Brownian motion recently introduced by Peng (2007). Our financial market consists of a riskless asset and a risky stock with price process modeled by a geometric G-Brownian motion. We adapt the notion of arbitrage to this more complex situation and consider stock price dynamics which exclude arbitrage opportunities. Due to volatility uncertainty the market is not complete any more. We establish the interval of no-arbitrage prices for general European contingent claims and deduce explicit results in a Markovian setting.,Finance
A Market-Oriented Programming Environment and its Application to Distributed Multicommodity Flow Problems,"Market price systems constitute a well-understood class of mechanisms that under certain conditions provide effective decentralization of decision making with minimal communication overhead. In a market-oriented programming approach to distributed problem solving, we derive the activities and resource allocations for a set of computational agents by computing the competitive equilibrium of an artificial economy. WALRAS provides basic constructs for defining computational market structures, and protocols for deriving their corresponding price equilibria. In a particular realization of this approach for a form of multicommodity flow problem, we see that careful construction of the decision process according to economic principles can lead to efficient distributed resource allocation, and that the behavior of the system can be meaningfully analyzed in economic terms.","A Market-Oriented Programming Environment and its Application to Distributed Multicommodity Flow Problems Market price systems constitute a well-understood class of mechanisms that under certain conditions provide effective decentralization of decision making with minimal communication overhead. In a market-oriented programming approach to distributed problem solving, we derive the activities and resource allocations for a set of computational agents by computing the competitive equilibrium of an artificial economy. WALRAS provides basic constructs for defining computational market structures, and protocols for deriving their corresponding price equilibria. In a particular realization of this approach for a form of multicommodity flow problem, we see that careful construction of the decision process according to economic principles can lead to efficient distributed resource allocation, and that the behavior of the system can be meaningfully analyzed in economic terms.",Technology
A Unified View of TD Algorithms; Introducing Full-Gradient TD and Equi-Gradient Descent TD,"This paper addresses the issue of policy evaluation in Markov Decision Processes, using linear function approximation. It provides a unified view of algorithms such as TD(lambda), LSTD(lambda), iLSTD, residual-gradient TD. It is asserted that they all consist in minimizing a gradient function and differ by the form of this function and their means of minimizing it. Two new schemes are introduced in that framework: Full-gradient TD which uses a generalization of the principle introduced in iLSTD, and EGD TD, which reduces the gradient by successive equi-gradient descents. These three algorithms form a new intermediate family with the interesting property of making much better use of the samples than TD while keeping a gradient descent scheme, which is useful for complexity issues and optimistic policy iteration.","A Unified View of TD Algorithms; Introducing Full-Gradient TD and Equi-Gradient Descent TD This paper addresses the issue of policy evaluation in Markov Decision Processes, using linear function approximation. It provides a unified view of algorithms such as TD(lambda), LSTD(lambda), iLSTD, residual-gradient TD. It is asserted that they all consist in minimizing a gradient function and differ by the form of this function and their means of minimizing it. Two new schemes are introduced in that framework: Full-gradient TD which uses a generalization of the principle introduced in iLSTD, and EGD TD, which reduces the gradient by successive equi-gradient descents. These three algorithms form a new intermediate family with the interesting property of making much better use of the samples than TD while keeping a gradient descent scheme, which is useful for complexity issues and optimistic policy iteration.",Technology
EU Economic Modelling System,"This is the first study that attempts to assess the regional economic impacts of the European Institute of Innovation and Technology (EIT) investments in a spatially explicit macroeconomic model, which allows us to take into account all key direct, indirect and spatial spillover effects of EIT investments via inter-regional trade and investment linkages and a spatial diffusion of technology via an endogenously determined global knowledge frontier with endogenous growth engines driven by investments in knowledge and human capital. Our simulation results of highly detailed EIT expenditure data suggest that, besides sizable direct effects in those regions that receive the EIT investment support, there are also significant spatial spillover effects to other (non-supported) EU regions. Taking into account all key indirect and spatial spillover effects is a particular strength of the adopted spatial general equilibrium methodology; our results suggest that they are important indeed and need to be taken into account when assessing the impacts of EIT investment policies on regional economies.","EU Economic Modelling System This is the first study that attempts to assess the regional economic impacts of the European Institute of Innovation and Technology (EIT) investments in a spatially explicit macroeconomic model, which allows us to take into account all key direct, indirect and spatial spillover effects of EIT investments via inter-regional trade and investment linkages and a spatial diffusion of technology via an endogenously determined global knowledge frontier with endogenous growth engines driven by investments in knowledge and human capital. Our simulation results of highly detailed EIT expenditure data suggest that, besides sizable direct effects in those regions that receive the EIT investment support, there are also significant spatial spillover effects to other (non-supported) EU regions. Taking into account all key indirect and spatial spillover effects is a particular strength of the adopted spatial general equilibrium methodology; our results suggest that they are important indeed and need to be taken into account when assessing the impacts of EIT investment policies on regional economies.",Finance
Question-Answering (QA) Model for a Personalized Learning Assistant for Arabic Language,"This paper describes the creation, optimization, and assessment of a question-answering (QA) model for a personalized learning assistant that uses BERT transformers customized for the Arabic language. The model was particularly finetuned on science textbooks in Palestinian curriculum. Our approach uses BERTs brilliant capabilities to automatically produce correct answers to questions in the field of science education. The models ability to understand and extract pertinent information is improved by finetuning it using 11th and 12th grade biology book in Palestinian curriculum. This increases the models efficacy in producing enlightening responses. Exact match (EM) and F1 score metrics are used to assess the models performance; the results show an EM score of 20 and an F1 score of 51. These findings show that the model can comprehend and react to questions in the context of Palestinian science book. The results demonstrate the potential of BERT-based QA models to support learning and understanding Arabic students questions.","Question-Answering (QA) Model for a Personalized Learning Assistant for Arabic Language This paper describes the creation, optimization, and assessment of a question-answering (QA) model for a personalized learning assistant that uses BERT transformers customized for the Arabic language. The model was particularly finetuned on science textbooks in Palestinian curriculum. Our approach uses BERTs brilliant capabilities to automatically produce correct answers to questions in the field of science education. The models ability to understand and extract pertinent information is improved by finetuning it using 11th and 12th grade biology book in Palestinian curriculum. This increases the models efficacy in producing enlightening responses. Exact match (EM) and F1 score metrics are used to assess the models performance; the results show an EM score of 20 and an F1 score of 51. These findings show that the model can comprehend and react to questions in the context of Palestinian science book. The results demonstrate the potential of BERT-based QA models to support learning and understanding Arabic students questions.",Education
Climate Prediction through Statistical Methods,"Climate change is a reality of today. Paleoclimatic proxies and climate predictions based on coupled atmosphere-ocean general circulation models provide us with temperature data. Using Detrended Fluctuation Analysis, we are investigating the statistical connection between the climate types of the present and these local temperatures. We are relating this issue to some well-known historic climate shifts. Our main result is that the temperature fluctuations with or without a temperature scale attached to them, can be used to classify climates in the absence of other indicators such as pan evaporation and precipitation.","Climate Prediction through Statistical Methods Climate change is a reality of today. Paleoclimatic proxies and climate predictions based on coupled atmosphere-ocean general circulation models provide us with temperature data. Using Detrended Fluctuation Analysis, we are investigating the statistical connection between the climate types of the present and these local temperatures. We are relating this issue to some well-known historic climate shifts. Our main result is that the temperature fluctuations with or without a temperature scale attached to them, can be used to classify climates in the absence of other indicators such as pan evaporation and precipitation.",Environment
Medical World Model: Generative Simulation of Tumor Evolution for Treatment Planning,"Providing effective treatment and making informed clinical decisions are essential goals of modern medicine and clinical care. We are interested in simulating disease dynamics for clinical decision-making, leveraging recent advances in large generative models. To this end, we introduce the Medical World Model (MeWM), the first world model in medicine that visually predicts future disease states based on clinical decisions. MeWM comprises (i) vision-language models to serve as policy models, and (ii) tumor generative models as dynamics models. The policy model generates action plans, such as clinical treatments, while the dynamics model simulates tumor progression or regression under given treatment conditions. Building on this, we propose the inverse dynamics model that applies survival analysis to the simulated post-treatment tumor, enabling the evaluation of treatment efficacy and the selection of the optimal clinical action plan. As a result, the proposed MeWM simulates disease dynamics by synthesizing post-treatment tumors, with state-of-the-art specificity in Turing tests evaluated by radiologists. Simultaneously, its inverse dynamics model outperforms medical-specialized GPTs in optimizing individualized treatment protocols across all metrics. Notably, MeWM improves clinical decision-making for interventional physicians, boosting F1-score in selecting the optimal TACE protocol by 13, paving the way for future integration of medical world models as the second readers.","Medical World Model: Generative Simulation of Tumor Evolution for Treatment Planning Providing effective treatment and making informed clinical decisions are essential goals of modern medicine and clinical care. We are interested in simulating disease dynamics for clinical decision-making, leveraging recent advances in large generative models. To this end, we introduce the Medical World Model (MeWM), the first world model in medicine that visually predicts future disease states based on clinical decisions. MeWM comprises (i) vision-language models to serve as policy models, and (ii) tumor generative models as dynamics models. The policy model generates action plans, such as clinical treatments, while the dynamics model simulates tumor progression or regression under given treatment conditions. Building on this, we propose the inverse dynamics model that applies survival analysis to the simulated post-treatment tumor, enabling the evaluation of treatment efficacy and the selection of the optimal clinical action plan. As a result, the proposed MeWM simulates disease dynamics by synthesizing post-treatment tumors, with state-of-the-art specificity in Turing tests evaluated by radiologists. Simultaneously, its inverse dynamics model outperforms medical-specialized GPTs in optimizing individualized treatment protocols across all metrics. Notably, MeWM improves clinical decision-making for interventional physicians, boosting F1-score in selecting the optimal TACE protocol by 13, paving the way for future integration of medical world models as the second readers.",Healthcare
Algorithmic Complexity of Real Financial Markets,"A new approach to the understanding of the complex behavior of financial markets index using tools from thermodynamics and statistical physics is developed. Physical complexity, a magnitude rooted in the Kolmogorov-Chaitin theory is applied to binary sequences built up from real time series of financial markets indices. The study is based on NASDAQ and Mexican IPC data. Different behaviors of this magnitude are shown when applied to the intervals of series placed before crashes and in intervals when no financial turbulence is observed. The connection between our results and The Efficient Market Hypothesis is discussed.","Algorithmic Complexity of Real Financial Markets A new approach to the understanding of the complex behavior of financial markets index using tools from thermodynamics and statistical physics is developed. Physical complexity, a magnitude rooted in the Kolmogorov-Chaitin theory is applied to binary sequences built up from real time series of financial markets indices. The study is based on NASDAQ and Mexican IPC data. Different behaviors of this magnitude are shown when applied to the intervals of series placed before crashes and in intervals when no financial turbulence is observed. The connection between our results and The Efficient Market Hypothesis is discussed.",Finance
Efficiency is Not Enough: A Critical Perspective of Environmentally Sustainable AI,"Artificial intelligence (AI) is currently spearheaded by machine learning (ML) methods such as deep learning which have accelerated progress on many tasks thought to be out of reach of AI. These recent ML methods are often compute hungry, energy intensive, and result in significant green house gas emissions, a known driver of anthropogenic climate change. Additionally, the platforms on which ML systems run are associated with environmental impacts that go beyond the energy consumption driven carbon emissions. The primary solution lionized by both industry and the ML community to improve the environmental sustainability of ML is to increase the compute and energy efficiency with which ML systems operate. In this perspective, we argue that it is time to look beyond efficiency in order to make ML more environmentally sustainable. We present three high-level discrepancies between the many variables that influence the efficiency of ML and the environmental sustainability of ML. Firstly, we discuss how compute efficiency does not imply energy efficiency or carbon efficiency. Second, we present the unexpected effects of efficiency on operational emissions throughout the ML model life cycle. And, finally, we explore the broader environmental impacts that are not accounted by efficiency. These discrepancies show as to why efficiency alone is not enough to remedy the adverse environmental impacts of ML. Instead, we argue for systems thinking as the next step towards holistically improving the environmental sustainability of ML.","Efficiency is Not Enough: A Critical Perspective of Environmentally Sustainable AI Artificial intelligence (AI) is currently spearheaded by machine learning (ML) methods such as deep learning which have accelerated progress on many tasks thought to be out of reach of AI. These recent ML methods are often compute hungry, energy intensive, and result in significant green house gas emissions, a known driver of anthropogenic climate change. Additionally, the platforms on which ML systems run are associated with environmental impacts that go beyond the energy consumption driven carbon emissions. The primary solution lionized by both industry and the ML community to improve the environmental sustainability of ML is to increase the compute and energy efficiency with which ML systems operate. In this perspective, we argue that it is time to look beyond efficiency in order to make ML more environmentally sustainable. We present three high-level discrepancies between the many variables that influence the efficiency of ML and the environmental sustainability of ML. Firstly, we discuss how compute efficiency does not imply energy efficiency or carbon efficiency. Second, we present the unexpected effects of efficiency on operational emissions throughout the ML model life cycle. And, finally, we explore the broader environmental impacts that are not accounted by efficiency. These discrepancies show as to why efficiency alone is not enough to remedy the adverse environmental impacts of ML. Instead, we argue for systems thinking as the next step towards holistically improving the environmental sustainability of ML.",Environment
Non-linear quantization for arbitrary distributions and applications to Medical Image Processing,"We report the development of a scalar quantization approach that helps build tables of decision and reconstruction levels for any probability density function (pdf). Several example pdfs are used for illustration: Uniform, Gaussian, Laplace, one-sided Rayleigh, and Gamma (One sided and double-sided symmetrical). The main applications of the methodology are principally aimed at Multiresolution Image compression where generally the Stretched Exponential pdf is encountered. Specialising to this important case, we perform quantization and information entropy calculations from selected medical MRI (Magnetic Resonance Imaging) pictures of the human brain. The image histograms are fitted to a Stretched exponential model and the corresponding entropies are compared.","Non-linear quantization for arbitrary distributions and applications to Medical Image Processing We report the development of a scalar quantization approach that helps build tables of decision and reconstruction levels for any probability density function (pdf). Several example pdfs are used for illustration: Uniform, Gaussian, Laplace, one-sided Rayleigh, and Gamma (One sided and double-sided symmetrical). The main applications of the methodology are principally aimed at Multiresolution Image compression where generally the Stretched Exponential pdf is encountered. Specialising to this important case, we perform quantization and information entropy calculations from selected medical MRI (Magnetic Resonance Imaging) pictures of the human brain. The image histograms are fitted to a Stretched exponential model and the corresponding entropies are compared.",Healthcare
Could short selling make financial markets tumble?,"It is suggested to consider long term trends of financial markets as a growth phenomenon. The question that is asked is what conditions are needed for a long term sustainable growth or contraction in a financial market? The paper discuss the role of traditional market players of long only mutual funds versus hedge funds which take both short and long positions. It will be argued that financial markets since their very origin and only till very recently, have been in a state of broken symmetry which favored long term growth instead of contraction. The reason for this broken symmetry into a long term bull phase is the historical almost complete dominance by long only players in financial markets. Dangers connected to short trading are illustrated by the appearence of long term bearish trends seen in analytical results and by simulation results of an agent based market model. Recent short trade data of the Nasdaq Composite index show an increase in the short activity prior to or at the same time as dips in the market, and reveal an steadily increase in the short trading activity, reaching levels never seen before.","Could short selling make financial markets tumble? It is suggested to consider long term trends of financial markets as a growth phenomenon. The question that is asked is what conditions are needed for a long term sustainable growth or contraction in a financial market? The paper discuss the role of traditional market players of long only mutual funds versus hedge funds which take both short and long positions. It will be argued that financial markets since their very origin and only till very recently, have been in a state of broken symmetry which favored long term growth instead of contraction. The reason for this broken symmetry into a long term bull phase is the historical almost complete dominance by long only players in financial markets. Dangers connected to short trading are illustrated by the appearence of long term bearish trends seen in analytical results and by simulation results of an agent based market model. Recent short trade data of the Nasdaq Composite index show an increase in the short activity prior to or at the same time as dips in the market, and reveal an steadily increase in the short trading activity, reaching levels never seen before.",Finance
Ranking of different of investment risk in high-tech projects using TOPSIS method in fuzzy environment based on linguistic variables,"Examining the trend of the global economy shows that global trade is moving towards high-tech products. Given that these products generate very high added value, countries that can produce and export these products will have high growth in the industrial sector. The importance of investing in advanced technologies for economic and social growth and development is so great that it is mentioned as one of the strong levers to achieve development. It should be noted that the policy of developing advanced technologies requires consideration of various performance aspects, risks and future risks in the investment phase. Risk related to high-tech investment projects has a meaning other than financial concepts only. In recent years, researchers have focused on identifying, analyzing, and prioritizing risk. There are two important components in measuring investment risk in high-tech industries, which include identifying the characteristics and criteria for measuring system risk and how to measure them. This study tries to evaluate and rank the investment risks in advanced industries using fuzzy TOPSIS technique based on verbal variables.","Ranking of different of investment risk in high-tech projects using TOPSIS method in fuzzy environment based on linguistic variables Examining the trend of the global economy shows that global trade is moving towards high-tech products. Given that these products generate very high added value, countries that can produce and export these products will have high growth in the industrial sector. The importance of investing in advanced technologies for economic and social growth and development is so great that it is mentioned as one of the strong levers to achieve development. It should be noted that the policy of developing advanced technologies requires consideration of various performance aspects, risks and future risks in the investment phase. Risk related to high-tech investment projects has a meaning other than financial concepts only. In recent years, researchers have focused on identifying, analyzing, and prioritizing risk. There are two important components in measuring investment risk in high-tech industries, which include identifying the characteristics and criteria for measuring system risk and how to measure them. This study tries to evaluate and rank the investment risks in advanced industries using fuzzy TOPSIS technique based on verbal variables.",Finance
On the mechanism of decadal oscillations in a coarse resolution ocean model,"The mechanism that causes an interdecadal oscillation in a coarse resolution sector ocean model forced by mixed boundary conditions is studied. The oscillation is characterized by large fluctuations in convective activity and airsea heat exchange on a decadal time scale. When the convective activity is large, a strengthening of the southeastward surface flow advects more relatively fresh water from the northwestern part of the basin into the convective area, which reduces the convective activity. Similarly, when the convective activity is small, the flow of relatively fresh water is weak, which enables the expansion of the convective area. The oscillation critically depends on how the ocean circulation, and especially the surface circulation, responds to anomalous convective activity. Horizontal boundaries turn out to play an important role in the dynamical response of the ocean circulation. That the dynamical reponse is essential to the oscillation is confirmed with two simple (conceptual) models, and some idealized ocean experiments.","On the mechanism of decadal oscillations in a coarse resolution ocean model The mechanism that causes an interdecadal oscillation in a coarse resolution sector ocean model forced by mixed boundary conditions is studied. The oscillation is characterized by large fluctuations in convective activity and airsea heat exchange on a decadal time scale. When the convective activity is large, a strengthening of the southeastward surface flow advects more relatively fresh water from the northwestern part of the basin into the convective area, which reduces the convective activity. Similarly, when the convective activity is small, the flow of relatively fresh water is weak, which enables the expansion of the convective area. The oscillation critically depends on how the ocean circulation, and especially the surface circulation, responds to anomalous convective activity. Horizontal boundaries turn out to play an important role in the dynamical response of the ocean circulation. That the dynamical reponse is essential to the oscillation is confirmed with two simple (conceptual) models, and some idealized ocean experiments.",Environment
Evaluating the Financial Market Function in Prewar Japan using a Time-Varying Parameter Model,"This paper explores when the financial market lost the price formation function in prewar Japan in the sense of Famas (1970) semi-strong form market efficiency using a new dataset. We particularly focus on the relationship between the prewar Japanese financial market and several government policy interventions to explore whether the semi-strong form market efficiency evolves over time. To capture the long-run impact of government policy interventions against the markets, we measure the time-varying joint degree of market efficiency and the time-varying impulse responses based on Ito et al.s (2014; 2017) generalized least squares-based time-varying vector autoregressive model. The empirical results reveal that (1) the joint degree of market efficiency in the prewar Japanese financial market fluctuated over time because of external events such as policy changes and wars, (2) the semi-strong form EMH is almost supported in the prewar Japanese financial market, (3) Los (2004) adaptive market hypothesis is supported in the prewar Japanese financial market even if we consider that the public information affects the financial markets, and (4) the prewar Japanese financial markets lost the price formation function in 1932 and that was a turning point in the market.","Evaluating the Financial Market Function in Prewar Japan using a Time-Varying Parameter Model This paper explores when the financial market lost the price formation function in prewar Japan in the sense of Famas (1970) semi-strong form market efficiency using a new dataset. We particularly focus on the relationship between the prewar Japanese financial market and several government policy interventions to explore whether the semi-strong form market efficiency evolves over time. To capture the long-run impact of government policy interventions against the markets, we measure the time-varying joint degree of market efficiency and the time-varying impulse responses based on Ito et al.s (2014; 2017) generalized least squares-based time-varying vector autoregressive model. The empirical results reveal that (1) the joint degree of market efficiency in the prewar Japanese financial market fluctuated over time because of external events such as policy changes and wars, (2) the semi-strong form EMH is almost supported in the prewar Japanese financial market, (3) Los (2004) adaptive market hypothesis is supported in the prewar Japanese financial market even if we consider that the public information affects the financial markets, and (4) the prewar Japanese financial markets lost the price formation function in 1932 and that was a turning point in the market.",Finance
An agent-based model for designing a financial market that works well,"Designing a financial market that works well is very important for developing and maintaining an advanced economy, but is not easy because changing detailed rules, even ones that seem trivial, sometimes causes unexpected large impacts and side effects. A computer simulation using an agent-based model can directly treat and clearly explain such complex systems where micro processes and macro phenomena interact. Many effective agent-based models investigating human behavior have already been developed. Recently, an artificial market model, which is an agent-based model for a financial market, has started to contribute to discussions on rules and regulations of actual financial markets. I introduce an artificial market model to design financial markets that work well and describe a previous study investigating tick size reduction. I hope that more artificial market models will contribute to designing financial markets that work well to further develop and maintain advanced economies.","An agent-based model for designing a financial market that works well Designing a financial market that works well is very important for developing and maintaining an advanced economy, but is not easy because changing detailed rules, even ones that seem trivial, sometimes causes unexpected large impacts and side effects. A computer simulation using an agent-based model can directly treat and clearly explain such complex systems where micro processes and macro phenomena interact. Many effective agent-based models investigating human behavior have already been developed. Recently, an artificial market model, which is an agent-based model for a financial market, has started to contribute to discussions on rules and regulations of actual financial markets. I introduce an artificial market model to design financial markets that work well and describe a previous study investigating tick size reduction. I hope that more artificial market models will contribute to designing financial markets that work well to further develop and maintain advanced economies.",Finance
Measuring the Dunkelflaute: How (not) to analyze variable renewable energy shortage,"As variable renewable energy sources increasingly gain importance in global energy systems, there is a growing interest in understanding periods of variable renewable energy shortage (Dunkelflauten). Defining, quantifying, and comparing such shortage events across different renewable generation technologies and locations presents a surprisingly intricate challenge. Various methodological approaches exist in different bodies of literature, which have been applied to single technologies in specific locations or technology portfolios across multiple regions. We provide an overview of various methods for quantifying variable renewable energy shortage, focusing either on supply from variable renewables or its mismatch with electricity demand. We explain and critically discuss the merits and challenges of different approaches for defining and identifying shortage events and propose further methodological improvements for more accurate shortage determination. Additionally, we elaborate on comparability requirements for multi-technological and multi-regional energy shortage analysis. In doing so, we aim to contribute to unifying disparate methodologies, harmonizing terminologies, and providing guidance for future research.","Measuring the Dunkelflaute: How (not) to analyze variable renewable energy shortage As variable renewable energy sources increasingly gain importance in global energy systems, there is a growing interest in understanding periods of variable renewable energy shortage (Dunkelflauten). Defining, quantifying, and comparing such shortage events across different renewable generation technologies and locations presents a surprisingly intricate challenge. Various methodological approaches exist in different bodies of literature, which have been applied to single technologies in specific locations or technology portfolios across multiple regions. We provide an overview of various methods for quantifying variable renewable energy shortage, focusing either on supply from variable renewables or its mismatch with electricity demand. We explain and critically discuss the merits and challenges of different approaches for defining and identifying shortage events and propose further methodological improvements for more accurate shortage determination. Additionally, we elaborate on comparability requirements for multi-technological and multi-regional energy shortage analysis. In doing so, we aim to contribute to unifying disparate methodologies, harmonizing terminologies, and providing guidance for future research.",Environment
"Energy-Sustainable IoT Connectivity: Vision, Technological Enablers, Challenges, and Future Directions","Technology solutions must effectively balance economic growth, social equity, and environmental integrity to achieve a sustainable society. Notably, although the Internet of Things (IoT) paradigm constitutes a key sustainability enabler, critical issues such as the increasing maintenance operations, energy consumption, and manufacturingdisposal of IoT devices have long-term negative economic, societal, and environmental impacts and must be efficiently addressed. This calls for self-sustainable IoT ecosystems requiring minimal external resources and intervention, effectively utilizing renewable energy sources, and recycling materials whenever possible, thus encompassing energy sustainability. In this work, we focus on energy-sustainable IoT during the operation phase, although our discussions sometimes extend to other sustainability aspects and IoT lifecycle phases. Specifically, we provide a fresh look at energy-sustainable IoT and identify energy provision, transfer, and energy efficiency as the three main energy-related processes whose harmonious coexistence pushes toward realizing self-sustainable IoT systems. Their main related technologies, recent advances, challenges, and research directions are also discussed. Moreover, we overview relevant performance metrics to assess the energy-sustainability potential of a certain technique, technology, device, or network and list some target values for the next generation of wireless systems. Overall, this paper offers insights that are valuable for advancing sustainability goals for present and future generations.","Energy-Sustainable IoT Connectivity: Vision, Technological Enablers, Challenges, and Future Directions Technology solutions must effectively balance economic growth, social equity, and environmental integrity to achieve a sustainable society. Notably, although the Internet of Things (IoT) paradigm constitutes a key sustainability enabler, critical issues such as the increasing maintenance operations, energy consumption, and manufacturingdisposal of IoT devices have long-term negative economic, societal, and environmental impacts and must be efficiently addressed. This calls for self-sustainable IoT ecosystems requiring minimal external resources and intervention, effectively utilizing renewable energy sources, and recycling materials whenever possible, thus encompassing energy sustainability. In this work, we focus on energy-sustainable IoT during the operation phase, although our discussions sometimes extend to other sustainability aspects and IoT lifecycle phases. Specifically, we provide a fresh look at energy-sustainable IoT and identify energy provision, transfer, and energy efficiency as the three main energy-related processes whose harmonious coexistence pushes toward realizing self-sustainable IoT systems. Their main related technologies, recent advances, challenges, and research directions are also discussed. Moreover, we overview relevant performance metrics to assess the energy-sustainability potential of a certain technique, technology, device, or network and list some target values for the next generation of wireless systems. Overall, this paper offers insights that are valuable for advancing sustainability goals for present and future generations.",Environment
Automating Turkish Educational Quiz Generation Using Large Language Models,"Crafting quizzes from educational content is a pivotal activity that benefits both teachers and students by reinforcing learning and evaluating understanding. In this study, we introduce a novel approach to generate quizzes from Turkish educational texts, marking a pioneering endeavor in educational technology specifically tailored to the Turkish educational context. We present a specialized dataset, named the Turkish-Quiz-Instruct, comprising an extensive collection of Turkish educational texts accompanied by multiple-choice and short-answer quizzes. This research leverages the capabilities of Large Language Models (LLMs), including GPT-4-Turbo, GPT-3.5-Turbo, Llama-2-7b-chat-hf, and Llama-2-13b-chat-hf, to automatically generate quiz questions and answers from the Turkish educational content. Our work delineates the methodology for employing these LLMs in the context of Turkish educational material, thereby opening new avenues for automated Turkish quiz generation. The study not only demonstrates the efficacy of using such models for generating coherent and relevant quiz content but also sets a precedent for future research in the domain of automated educational content creation for languages other than English. The Turkish-Quiz-Instruct dataset is introduced as a valuable resource for researchers and practitioners aiming to explore the boundaries of educational technology and language-specific applications of LLMs in Turkish. By addressing the challenges of quiz generation in a non-English context specifically Turkish, this study contributes significantly to the field of Turkish educational technology, providing insights into the potential of leveraging LLMs for educational purposes across diverse linguistic landscapes.","Automating Turkish Educational Quiz Generation Using Large Language Models Crafting quizzes from educational content is a pivotal activity that benefits both teachers and students by reinforcing learning and evaluating understanding. In this study, we introduce a novel approach to generate quizzes from Turkish educational texts, marking a pioneering endeavor in educational technology specifically tailored to the Turkish educational context. We present a specialized dataset, named the Turkish-Quiz-Instruct, comprising an extensive collection of Turkish educational texts accompanied by multiple-choice and short-answer quizzes. This research leverages the capabilities of Large Language Models (LLMs), including GPT-4-Turbo, GPT-3.5-Turbo, Llama-2-7b-chat-hf, and Llama-2-13b-chat-hf, to automatically generate quiz questions and answers from the Turkish educational content. Our work delineates the methodology for employing these LLMs in the context of Turkish educational material, thereby opening new avenues for automated Turkish quiz generation. The study not only demonstrates the efficacy of using such models for generating coherent and relevant quiz content but also sets a precedent for future research in the domain of automated educational content creation for languages other than English. The Turkish-Quiz-Instruct dataset is introduced as a valuable resource for researchers and practitioners aiming to explore the boundaries of educational technology and language-specific applications of LLMs in Turkish. By addressing the challenges of quiz generation in a non-English context specifically Turkish, this study contributes significantly to the field of Turkish educational technology, providing insights into the potential of leveraging LLMs for educational purposes across diverse linguistic landscapes.",Education
Robust market-adjusted systemic risk measures,"In this note we consider a system of financial institutions and study systemic risk measures in the presence of a financial market and in a robust setting, namely, where no reference probability is assigned. We obtain a dual representation for convex robust systemic risk measures adjusted to the financial market and show its relation to some appropriate no-arbitrage conditions.","Robust market-adjusted systemic risk measures In this note we consider a system of financial institutions and study systemic risk measures in the presence of a financial market and in a robust setting, namely, where no reference probability is assigned. We obtain a dual representation for convex robust systemic risk measures adjusted to the financial market and show its relation to some appropriate no-arbitrage conditions.",Finance
Do medium range ensemble forecasts give useful predictions of temporal correlations?,Medium range ensemble forecasts are typically used to derive predictions of the conditional marginal distributions of future events on individual days. We assess whether they can also be used to predict the conditional correlations between different days.,Do medium range ensemble forecasts give useful predictions of temporal correlations? Medium range ensemble forecasts are typically used to derive predictions of the conditional marginal distributions of future events on individual days. We assess whether they can also be used to predict the conditional correlations between different days.,Environment
Equilibrium notions and framing effects,"Experimental economics has repeatedly demonstrated that the Nash equilibrium makes inaccurate predictions for a vast set of games. Instead, several alternative theoretical concepts predict behavior that is much more in tune with observed data, with the quantal response equilibrium as the most prominent example. However, here we show that this equilibrium notion itself, like any other concept that varies smoothly with the payoffs, is necessarily subject to framing effects: If the same economic problem is represented in a different but equivalent way, the predicted results will differ. As a consequence, we argue that tools and methods that are successful in explaining human behavior in laboratory experiments may be unsuitable for doing theory.","Equilibrium notions and framing effects Experimental economics has repeatedly demonstrated that the Nash equilibrium makes inaccurate predictions for a vast set of games. Instead, several alternative theoretical concepts predict behavior that is much more in tune with observed data, with the quantal response equilibrium as the most prominent example. However, here we show that this equilibrium notion itself, like any other concept that varies smoothly with the payoffs, is necessarily subject to framing effects: If the same economic problem is represented in a different but equivalent way, the predicted results will differ. As a consequence, we argue that tools and methods that are successful in explaining human behavior in laboratory experiments may be unsuitable for doing theory.",Finance
Impact of Public and Private Investments on Economic Growth of Developing Countries,"This paper aims to study the impact of public and private investments on the economic growth of developing countries. The study uses the panel data of 39 developing countries covering the periods 1990-2019. The study was based on the neoclassical growth models or exogenous growth models state in which land, labor, capital accumulation, etc., and technology proved substantial for economic growth. The paper finds that public investment has a strong positive impact on economic growth than private investment. Gross capital formation, labor growth, and government final consumption expenditure were found significant in explaining the economic growth. Overall, both public and private investments are substantial for the economic growth and development of developing countries.","Impact of Public and Private Investments on Economic Growth of Developing Countries This paper aims to study the impact of public and private investments on the economic growth of developing countries. The study uses the panel data of 39 developing countries covering the periods 1990-2019. The study was based on the neoclassical growth models or exogenous growth models state in which land, labor, capital accumulation, etc., and technology proved substantial for economic growth. The paper finds that public investment has a strong positive impact on economic growth than private investment. Gross capital formation, labor growth, and government final consumption expenditure were found significant in explaining the economic growth. Overall, both public and private investments are substantial for the economic growth and development of developing countries.",Finance
Stability of central finite difference schemes for the Heston PDE,"This paper deals with stability in the numerical solution of the prominent Heston partial differential equation from mathematical finance. We study the well-known central second-order finite difference discretization, which leads to large semi-discrete systems with non-normal matrices A. By employing the logarithmic spectral norm we prove practical, rigorous stability bounds. Our theoretical stability results are illustrated by ample numerical experiments.","Stability of central finite difference schemes for the Heston PDE This paper deals with stability in the numerical solution of the prominent Heston partial differential equation from mathematical finance. We study the well-known central second-order finite difference discretization, which leads to large semi-discrete systems with non-normal matrices A. By employing the logarithmic spectral norm we prove practical, rigorous stability bounds. Our theoretical stability results are illustrated by ample numerical experiments.",Finance
Weighted Trade Network in a Model of Preferential Bipartite Transactions,"Using a model of wealth distribution where traders are characterized by quenched random saving propensities and trade among themselves by bipartite transactions, we mimic the enhanced rates of trading of the rich by introducing the preferential selection rule using a pair of continuously tunable parameters. The bipartite trading defines a growing trade network of traders linked by their mutual trade relationships. With the preferential selection rule this network appears to be highly heterogeneous characterized by the scale-free nodal degree and the link weight distributions and presents signatures of non-trivial strength-degree correlations. With detailed numerical simulations and using finite-size scaling analysis we present evidence that the associated critical exponents are continuous functions of the tuning parameters. However the wealth distribution has been observed to follow the well-known Pareto law robustly for all positive values of the tuning parameters.","Weighted Trade Network in a Model of Preferential Bipartite Transactions Using a model of wealth distribution where traders are characterized by quenched random saving propensities and trade among themselves by bipartite transactions, we mimic the enhanced rates of trading of the rich by introducing the preferential selection rule using a pair of continuously tunable parameters. The bipartite trading defines a growing trade network of traders linked by their mutual trade relationships. With the preferential selection rule this network appears to be highly heterogeneous characterized by the scale-free nodal degree and the link weight distributions and presents signatures of non-trivial strength-degree correlations. With detailed numerical simulations and using finite-size scaling analysis we present evidence that the associated critical exponents are continuous functions of the tuning parameters. However the wealth distribution has been observed to follow the well-known Pareto law robustly for all positive values of the tuning parameters.",Finance
Dynamic Financial Analysis (DFA) of General Insurers under Climate Change,"Climate change is expected to significantly affect the physical, financial, and economic environments over the long term, posing risks to the financial health of general insurers. While general insurers typically use Dynamic Financial Analysis (DFA) for a comprehensive view of financial impacts, traditional DFA as presented in the literature does not consider the impact of climate change. To address this gap, we introduce a climate-dependent DFA approach that integrates climate risk into DFA, providing a holistic assessment of the long-term impact of climate change on the general insurance industry. The proposed framework has three key features. First, it captures the long-term impact of climate change on the assets and liabilities of general insurers by considering both physical and economic dimensions across different climate scenarios within an interconnected structure. Second, it addresses the uncertainty of climate change impacts using stochastic simulations within climate scenario analysis that are useful for actuarial applications. Finally, the framework is tailored to the general insurance sector by addressing its unique characteristics. To demonstrate the practical application of our model, we conduct an extensive empirical study using Australian data to assess the long-term financial impact of climate change on the general insurance market under various climate scenarios. The results show that the interaction between economic growth and physical risk plays a key role in shaping general insurers risk-return profiles. Limitations of our framework are thoroughly discussed.","Dynamic Financial Analysis (DFA) of General Insurers under Climate Change Climate change is expected to significantly affect the physical, financial, and economic environments over the long term, posing risks to the financial health of general insurers. While general insurers typically use Dynamic Financial Analysis (DFA) for a comprehensive view of financial impacts, traditional DFA as presented in the literature does not consider the impact of climate change. To address this gap, we introduce a climate-dependent DFA approach that integrates climate risk into DFA, providing a holistic assessment of the long-term impact of climate change on the general insurance industry. The proposed framework has three key features. First, it captures the long-term impact of climate change on the assets and liabilities of general insurers by considering both physical and economic dimensions across different climate scenarios within an interconnected structure. Second, it addresses the uncertainty of climate change impacts using stochastic simulations within climate scenario analysis that are useful for actuarial applications. Finally, the framework is tailored to the general insurance sector by addressing its unique characteristics. To demonstrate the practical application of our model, we conduct an extensive empirical study using Australian data to assess the long-term financial impact of climate change on the general insurance market under various climate scenarios. The results show that the interaction between economic growth and physical risk plays a key role in shaping general insurers risk-return profiles. Limitations of our framework are thoroughly discussed.",Environment
A discussion of stock market speculation by Pierre-Joseph Proudhon,"The object of this contribution is to present the ideas behind the thinking of the French economist Pierre-Joseph Proudhon (1809-1865) in relation to the causes and effects of Stock market speculation. It is based upon the works of this author but particularly on his Manuel du speculateur a la Bourse (Stock Market Speculator Manual) edited in 1857 in Paris. Compared to the markets of today, however, the stock market described by Proudhon appears embryonic. Nevertheless it represents the location for transactions in financial assets, commodities, precious metals and even some transactions involving options. This contribution is organised in the following manner - the first section is devoted to the development of Proudhons thought in relation to speculation. It is divided into two parts. The first part is dedicated to Pierre-Joseph Proudhons definitions of stock market speculation or gambling with shares that for him served no purpose either from a human or economic perspective and was therefore condemnable and to be contrasted with entrepreneurial speculation that, even though it is a highly-risky activity, involves the spirit of enterprise and provides the lifeblood of economic growth. The second part allows us to present Pierre-Joseph Proudhons propositions in relation to restricting the speculation that he considers obnoxious. The second section has two objectives: one part places in perspective the views of Proudhon and the characteristics of stock market activity under the Second Empire whilst the other part examines current-day aspects of the characteristics evoked by Proudhon. We are interested especially in the question of the regulation and that of the relevance today of certain accounting practices.","A discussion of stock market speculation by Pierre-Joseph Proudhon The object of this contribution is to present the ideas behind the thinking of the French economist Pierre-Joseph Proudhon (1809-1865) in relation to the causes and effects of Stock market speculation. It is based upon the works of this author but particularly on his Manuel du speculateur a la Bourse (Stock Market Speculator Manual) edited in 1857 in Paris. Compared to the markets of today, however, the stock market described by Proudhon appears embryonic. Nevertheless it represents the location for transactions in financial assets, commodities, precious metals and even some transactions involving options. This contribution is organised in the following manner - the first section is devoted to the development of Proudhons thought in relation to speculation. It is divided into two parts. The first part is dedicated to Pierre-Joseph Proudhons definitions of stock market speculation or gambling with shares that for him served no purpose either from a human or economic perspective and was therefore condemnable and to be contrasted with entrepreneurial speculation that, even though it is a highly-risky activity, involves the spirit of enterprise and provides the lifeblood of economic growth. The second part allows us to present Pierre-Joseph Proudhons propositions in relation to restricting the speculation that he considers obnoxious. The second section has two objectives: one part places in perspective the views of Proudhon and the characteristics of stock market activity under the Second Empire whilst the other part examines current-day aspects of the characteristics evoked by Proudhon. We are interested especially in the question of the regulation and that of the relevance today of certain accounting practices.",Finance
Revitalizing Sex Education for Chinese Children: A Formative Study,"Sex education helps children obtain knowledge and awareness of sexuality, and protects them against sexually transmitted diseases, pregnancy, and sexual abuse. Sex education is not well taught to children in China -- both school-based education and parental communication on this topic are limited. To interrogate the status quo of sex education in China and explore suitable interventions, we conducted a series of formative studies including interviews and social media analysis. Multiple stakeholders such as children, parents, education practitioners, and the general public were engaged for an in-depth understanding of their unique needs regarding teaching and learning sex education. We found that school-based sex education for Chinese children was currently insufficient and restrictive. Involving parents in sex education posed several challenges, such as a lack of sexuality and pedagogy knowledge, and embarrassment in initiating sex education conversations. Culture and politics were major hurdles to effective sex education. Based on the findings, we reflect on the complex interactions between culture, politics, education policy, and pedagogy, and discuss situated design of sex education in broader cultural and social contexts.","Revitalizing Sex Education for Chinese Children: A Formative Study Sex education helps children obtain knowledge and awareness of sexuality, and protects them against sexually transmitted diseases, pregnancy, and sexual abuse. Sex education is not well taught to children in China -- both school-based education and parental communication on this topic are limited. To interrogate the status quo of sex education in China and explore suitable interventions, we conducted a series of formative studies including interviews and social media analysis. Multiple stakeholders such as children, parents, education practitioners, and the general public were engaged for an in-depth understanding of their unique needs regarding teaching and learning sex education. We found that school-based sex education for Chinese children was currently insufficient and restrictive. Involving parents in sex education posed several challenges, such as a lack of sexuality and pedagogy knowledge, and embarrassment in initiating sex education conversations. Culture and politics were major hurdles to effective sex education. Based on the findings, we reflect on the complex interactions between culture, politics, education policy, and pedagogy, and discuss situated design of sex education in broader cultural and social contexts.",Education
Towards Sustainable Artificial Intelligence: An Overview of Environmental Protection Uses and Issues,"Artificial Intelligence (AI) is used to create more sustainable production methods and model climate change, making it a valuable tool in the fight against environmental degradation. This paper describes the paradox of an energy-consuming technology serving the ecological challenges of tomorrow. The study provides an overview of the sectors that use AI-based solutions for environmental protection. It draws on numerous examples from AI for Green players to present use cases and concrete examples. In the second part of the study, the negative impacts of AI on the environment and the emerging technological solutions to support Green AI are examined. It is also shown that the research on less energy-consuming AI is motivated more by cost and energy autonomy constraints than by environmental considerations. This leads to a rebound effect that favors an increase in the complexity of models. Finally, the need to integrate environmental indicators into algorithms is discussed. The environmental dimension is part of the broader ethical problem of AI, and addressing it is crucial for ensuring the sustainability of AI in the long term.","Towards Sustainable Artificial Intelligence: An Overview of Environmental Protection Uses and Issues Artificial Intelligence (AI) is used to create more sustainable production methods and model climate change, making it a valuable tool in the fight against environmental degradation. This paper describes the paradox of an energy-consuming technology serving the ecological challenges of tomorrow. The study provides an overview of the sectors that use AI-based solutions for environmental protection. It draws on numerous examples from AI for Green players to present use cases and concrete examples. In the second part of the study, the negative impacts of AI on the environment and the emerging technological solutions to support Green AI are examined. It is also shown that the research on less energy-consuming AI is motivated more by cost and energy autonomy constraints than by environmental considerations. This leads to a rebound effect that favors an increase in the complexity of models. Finally, the need to integrate environmental indicators into algorithms is discussed. The environmental dimension is part of the broader ethical problem of AI, and addressing it is crucial for ensuring the sustainability of AI in the long term.",Environment
Leveraging External Data for Testing Experimental Therapies with Biomarker Interactions in Randomized Clinical Trials,"In oncology the efficacy of novel therapeutics often differs across patient subgroups, and these variations are difficult to predict during the initial phases of the drug development process. The relation between the power of randomized clinical trials and heterogeneous treatment effects has been discussed by several authors. In particular, false negative results are likely to occur when the treatment effects concentrate in a subpopulation but the study design did not account for potential heterogeneous treatment effects. The use of external data from completed clinical studies and electronic health records has the potential to improve decision-making throughout the development of new therapeutics, from early-stage trials to registration. Here we discuss the use of external data to evaluate experimental treatments with potential heterogeneous treatment effects. We introduce a permutation procedure to test, at the completion of a randomized clinical trial, the null hypothesis that the experimental therapy does not improve the primary outcomes in any subpopulation. The permutation test leverages the available external data to increase power. Also, the procedure controls the false positive rate at the desired alpha-level without restrictive assumptions on the external data, for example, in scenarios with unmeasured confounders, different pre-treatment patient profiles in the trial population compared to the external data, and other discrepancies between the trial and the external data. We illustrate that the permutation test is optimal according to an interpretable criteria and discuss examples based on asymptotic results and simulations, followed by a retrospective analysis of individual patient-level data from a collection of glioblastoma clinical trials.","Leveraging External Data for Testing Experimental Therapies with Biomarker Interactions in Randomized Clinical Trials In oncology the efficacy of novel therapeutics often differs across patient subgroups, and these variations are difficult to predict during the initial phases of the drug development process. The relation between the power of randomized clinical trials and heterogeneous treatment effects has been discussed by several authors. In particular, false negative results are likely to occur when the treatment effects concentrate in a subpopulation but the study design did not account for potential heterogeneous treatment effects. The use of external data from completed clinical studies and electronic health records has the potential to improve decision-making throughout the development of new therapeutics, from early-stage trials to registration. Here we discuss the use of external data to evaluate experimental treatments with potential heterogeneous treatment effects. We introduce a permutation procedure to test, at the completion of a randomized clinical trial, the null hypothesis that the experimental therapy does not improve the primary outcomes in any subpopulation. The permutation test leverages the available external data to increase power. Also, the procedure controls the false positive rate at the desired alpha-level without restrictive assumptions on the external data, for example, in scenarios with unmeasured confounders, different pre-treatment patient profiles in the trial population compared to the external data, and other discrepancies between the trial and the external data. We illustrate that the permutation test is optimal according to an interpretable criteria and discuss examples based on asymptotic results and simulations, followed by a retrospective analysis of individual patient-level data from a collection of glioblastoma clinical trials.",Healthcare
Processing of Test Matrices with Guessing Correction,"It is suggested to insert into test matrix 1s for correct responses, 0s for response refusals, and negative corrective elements for incorrect responses. With the classical test theory approach test scores of examinees and items are calculated traditionally as sums of matrix elements, organized in rows and columns. Correlation coefficients are estimated using correction coefficients. In item response theory approach examinee and item logits are estimated using maximum likelihood method and probabilities of all matrix elements.","Processing of Test Matrices with Guessing Correction It is suggested to insert into test matrix 1s for correct responses, 0s for response refusals, and negative corrective elements for incorrect responses. With the classical test theory approach test scores of examinees and items are calculated traditionally as sums of matrix elements, organized in rows and columns. Correlation coefficients are estimated using correction coefficients. In item response theory approach examinee and item logits are estimated using maximum likelihood method and probabilities of all matrix elements.",Technology
The problem with the Brier score,"The Brier score is frequently used by meteorologists to measure the skill of binary probabilistic forecasts. We show, however, that in simple idealised cases it gives counterintuitive results. We advocate the use of an alternative measure that has a more compelling intuitive justification.","The problem with the Brier score The Brier score is frequently used by meteorologists to measure the skill of binary probabilistic forecasts. We show, however, that in simple idealised cases it gives counterintuitive results. We advocate the use of an alternative measure that has a more compelling intuitive justification.",Environment
Defaultable bonds with an infinite number of Levy factors,A market with defaultable bonds where the bond dynamics is in a Heath-Jarrow-Morton setting and the forward rates are driven by an infinite number of Levy factors is considered. The setting includes rating migrations driven by a Markov chain. All basic types of recovery are investigated. We formulate necessary and sufficient conditions (generalized HJM conditions) under which the market is arbitrage free. Connections with consistency conditions are discussed.,Defaultable bonds with an infinite number of Levy factors A market with defaultable bonds where the bond dynamics is in a Heath-Jarrow-Morton setting and the forward rates are driven by an infinite number of Levy factors is considered. The setting includes rating migrations driven by a Markov chain. All basic types of recovery are investigated. We formulate necessary and sufficient conditions (generalized HJM conditions) under which the market is arbitrage free. Connections with consistency conditions are discussed.,Finance
Cyber-Security Investment in the Context of Disruptive Technologies: Extension of the Gordon-Loeb Model,"Cyber-security breaches inflict significant costs on organizations. Hence, the development of an information-systems defense capability through cyber-security investment is a prerequisite. The question of how to determine the optimal amount to invest in cyber-security has been widely investigated in the literature. In this respect, the Gordon-Loeb model and its extensions received wide-scale acceptance. However, such models predominantly rely on restrictive assumptions that are not adapted for analyzing dynamic aspects of cyber-security investment. Yet, understanding such dynamic aspects is a key feature for studying cyber-security investment in the context of a fast-paced and continuously evolving technological landscape. We propose an extension of the Gordon-Loeb model by considering multi-period and relaxing the assumption of a continuous security-breach probability function. Such theoretical adaptations enable to capture dynamic aspects of cyber-security investment such as the advent of a disruptive technology and its investment consequences. Such a proposed extension of the Gordon-Loeb model gives room for a hypothetical decrease of the optimal level of cyber-security investment, due to a potential technological shift. While we believe our framework should be generalizable across the cyber-security milieu, we illustrate our approach in the context of critical-infrastructure protection, where security-cost reductions related to risk events are of paramount importance as potential losses reach unaffordable proportions. Moreover, despite the fact that some technologies are considered as disruptive and thus promising for critical-infrastructure protection, their effects on cyber-security investment have been discussed little.","Cyber-Security Investment in the Context of Disruptive Technologies: Extension of the Gordon-Loeb Model Cyber-security breaches inflict significant costs on organizations. Hence, the development of an information-systems defense capability through cyber-security investment is a prerequisite. The question of how to determine the optimal amount to invest in cyber-security has been widely investigated in the literature. In this respect, the Gordon-Loeb model and its extensions received wide-scale acceptance. However, such models predominantly rely on restrictive assumptions that are not adapted for analyzing dynamic aspects of cyber-security investment. Yet, understanding such dynamic aspects is a key feature for studying cyber-security investment in the context of a fast-paced and continuously evolving technological landscape. We propose an extension of the Gordon-Loeb model by considering multi-period and relaxing the assumption of a continuous security-breach probability function. Such theoretical adaptations enable to capture dynamic aspects of cyber-security investment such as the advent of a disruptive technology and its investment consequences. Such a proposed extension of the Gordon-Loeb model gives room for a hypothetical decrease of the optimal level of cyber-security investment, due to a potential technological shift. While we believe our framework should be generalizable across the cyber-security milieu, we illustrate our approach in the context of critical-infrastructure protection, where security-cost reductions related to risk events are of paramount importance as potential losses reach unaffordable proportions. Moreover, despite the fact that some technologies are considered as disruptive and thus promising for critical-infrastructure protection, their effects on cyber-security investment have been discussed little.",Finance
A review of Design of Experiments courses offered to undergraduate students at American universities,"Design of Experiments (DoE) is a relevant class to undergraduate students in the sciences, because it teaches them how to plan, conduct, and analyze experiments. In the literature on DoE, there are several contributions to its pedagogy, such as easy-to-use class experiments, virtual experiments, and software to construct experimental designs. However, there are virtually no systematic evaluations of the actual DoE pedagogy. To address this issue, we build the first database of DoE courses offered to undergraduate students in the United States. The database has records on courses offered from 2019 to 2022 at the best universities in the US News Best National Universities ranking of 2022. Specifically, it has data on 18 general and content-specific features of 206 courses. To study the DoE pedagogy, we analyze the database using descriptive statistics and text mining. Based on our analysis, we provide instructors with recommendations and teaching material to enhance their DoE courses. The database and material are included in the supplement of this article.","A review of Design of Experiments courses offered to undergraduate students at American universities Design of Experiments (DoE) is a relevant class to undergraduate students in the sciences, because it teaches them how to plan, conduct, and analyze experiments. In the literature on DoE, there are several contributions to its pedagogy, such as easy-to-use class experiments, virtual experiments, and software to construct experimental designs. However, there are virtually no systematic evaluations of the actual DoE pedagogy. To address this issue, we build the first database of DoE courses offered to undergraduate students in the United States. The database has records on courses offered from 2019 to 2022 at the best universities in the US News Best National Universities ranking of 2022. Specifically, it has data on 18 general and content-specific features of 206 courses. To study the DoE pedagogy, we analyze the database using descriptive statistics and text mining. Based on our analysis, we provide instructors with recommendations and teaching material to enhance their DoE courses. The database and material are included in the supplement of this article.",Education
Whats the worth of having a single CS teacher program aimed at teachers with heterogeneous profiles?,"There is consensus regarding the relevance of including Computer Science (CS) in official school curricula. However, this discipline cannot be taught on a large scale until there are enough trained teachers who can effectively lead a class. In this article, we discuss the results of a 400-hour teacher training program conducted in Argentina aimed at K-12 teachers with no CS background. The only requirement to sign up was to be an in-service teacher, and therefore there were a plethora of different teacher profiles that attended the courses. Our research aims at understanding whether a single teacher training program can be effective in teaching CS contents and specific pedagogy to teachers with very heterogeneous profiles. Also, we investigate what teachers expect to do with the contents they learn. To assess these questions anonymous examinations and questionnaires were given and interviews were conducted with course attendees. Even though the majority reached the expected minimum bar regarding CS contents and pedagogy, significant differences appear in their self-perception as regards career opportunities in CS teaching. Our conclusion is that carrying out CS teacher training for a broad spectrum of profiles may be effective for promoting CS contents. However, if the goal is to boost teachers confidence in teaching a CS subject, then having a program which focuses on a more restricted selection of profiles would be a better strategy.","Whats the worth of having a single CS teacher program aimed at teachers with heterogeneous profiles? There is consensus regarding the relevance of including Computer Science (CS) in official school curricula. However, this discipline cannot be taught on a large scale until there are enough trained teachers who can effectively lead a class. In this article, we discuss the results of a 400-hour teacher training program conducted in Argentina aimed at K-12 teachers with no CS background. The only requirement to sign up was to be an in-service teacher, and therefore there were a plethora of different teacher profiles that attended the courses. Our research aims at understanding whether a single teacher training program can be effective in teaching CS contents and specific pedagogy to teachers with very heterogeneous profiles. Also, we investigate what teachers expect to do with the contents they learn. To assess these questions anonymous examinations and questionnaires were given and interviews were conducted with course attendees. Even though the majority reached the expected minimum bar regarding CS contents and pedagogy, significant differences appear in their self-perception as regards career opportunities in CS teaching. Our conclusion is that carrying out CS teacher training for a broad spectrum of profiles may be effective for promoting CS contents. However, if the goal is to boost teachers confidence in teaching a CS subject, then having a program which focuses on a more restricted selection of profiles would be a better strategy.",Education
Hurricanes Increase Climate Change Conversations on Twitter,"The public understanding of climate change plays a critical role in translating climate science into climate action. In the public discourse, climate impacts are often discussed in the context of extreme weather events. Here, we analyse 65 million Twitter posts and 240 thousand news media articles related to 18 major hurricanes from 2010 to 2022 to clarify how hurricanes impact the public discussion around climate change. First, we analyse news content and show that climate change is the most prominent non-hurricane specific topic discussed by the news media in relation to hurricanes. Second, we perform a comparative analysis between reliable and questionable news media outlets, finding that the language around climate change varies between news media providers. Finally, using geolocated data, we show that accounts in regions affected by hurricanes discuss climate change at a significantly higher rate than accounts in unaffected areas, with references to climate change increasing by, on average, 80 after impact, and up to 200 for the largest hurricanes. Our findings demonstrate how hurricanes have a key impact on the public awareness of climate change.","Hurricanes Increase Climate Change Conversations on Twitter The public understanding of climate change plays a critical role in translating climate science into climate action. In the public discourse, climate impacts are often discussed in the context of extreme weather events. Here, we analyse 65 million Twitter posts and 240 thousand news media articles related to 18 major hurricanes from 2010 to 2022 to clarify how hurricanes impact the public discussion around climate change. First, we analyse news content and show that climate change is the most prominent non-hurricane specific topic discussed by the news media in relation to hurricanes. Second, we perform a comparative analysis between reliable and questionable news media outlets, finding that the language around climate change varies between news media providers. Finally, using geolocated data, we show that accounts in regions affected by hurricanes discuss climate change at a significantly higher rate than accounts in unaffected areas, with references to climate change increasing by, on average, 80 after impact, and up to 200 for the largest hurricanes. Our findings demonstrate how hurricanes have a key impact on the public awareness of climate change.",Environment
Designing Competent Mutation Operators via Probabilistic Model Building of Neighborhoods,"This paper presents a competent selectomutative genetic algorithm (GA), that adapts linkage and solves hard problems quickly, reliably, and accurately. A probabilistic model building process is used to automatically identify key building blocks (BBs) of the search problem. The mutation operator uses the probabilistic model of linkage groups to find the best among competing building blocks. The competent selectomutative GA successfully solves additively separable problems of bounded difficulty, requiring only subquadratic number of function evaluations. The results show that for additively separable problems the probabilistic model building BB-wise mutation scales as O(2km1.5), and requires O(k0.5logm) less function evaluations than its selectorecombinative counterpart, confirming theoretical results reported elsewhere (Sastry  Goldberg, 2004).","Designing Competent Mutation Operators via Probabilistic Model Building of Neighborhoods This paper presents a competent selectomutative genetic algorithm (GA), that adapts linkage and solves hard problems quickly, reliably, and accurately. A probabilistic model building process is used to automatically identify key building blocks (BBs) of the search problem. The mutation operator uses the probabilistic model of linkage groups to find the best among competing building blocks. The competent selectomutative GA successfully solves additively separable problems of bounded difficulty, requiring only subquadratic number of function evaluations. The results show that for additively separable problems the probabilistic model building BB-wise mutation scales as O(2km1.5), and requires O(k0.5logm) less function evaluations than its selectorecombinative counterpart, confirming theoretical results reported elsewhere (Sastry  Goldberg, 2004).",Technology
BlockIoT: Blockchain-based Health Data Integration using IoT Devices,"The development and adoption of Electronic Health Records (EHR) and health monitoring Internet of Things (IoT) Devices have enabled digitization of patient records and has also substantially transformed the healthcare delivery system in aspects such as remote patient monitoring, healthcare decision making, and medical research. However, data tends to be fragmented among health infrastructures and prevents interoperability of medical data at the point of care. In order to address this gap, we introduce BlockIoT that uses blockchain technology to transfer previously inaccessible and centralized data from medical devices to EHR systems, which provides greater insight to providers who can, in turn, provide better outcomes for patients. This notion of interoperability of medical device data is possible through an Application Programming Interface (API), which serves as a versatile endpoint for all incoming medical device data, a distributed file system that ensures data resilience, and knowledge templates that analyze, identify, and represent medical device data to providers. Our participatory design survey on BlockIoT demonstrates that BlockIoT is a suitable system to supplement physicians clinical practice and increases efficiency in most healthcare specialties, including cardiology, pulmonology, endocrinology, and primary care.","BlockIoT: Blockchain-based Health Data Integration using IoT Devices The development and adoption of Electronic Health Records (EHR) and health monitoring Internet of Things (IoT) Devices have enabled digitization of patient records and has also substantially transformed the healthcare delivery system in aspects such as remote patient monitoring, healthcare decision making, and medical research. However, data tends to be fragmented among health infrastructures and prevents interoperability of medical data at the point of care. In order to address this gap, we introduce BlockIoT that uses blockchain technology to transfer previously inaccessible and centralized data from medical devices to EHR systems, which provides greater insight to providers who can, in turn, provide better outcomes for patients. This notion of interoperability of medical device data is possible through an Application Programming Interface (API), which serves as a versatile endpoint for all incoming medical device data, a distributed file system that ensures data resilience, and knowledge templates that analyze, identify, and represent medical device data to providers. Our participatory design survey on BlockIoT demonstrates that BlockIoT is a suitable system to supplement physicians clinical practice and increases efficiency in most healthcare specialties, including cardiology, pulmonology, endocrinology, and primary care.",Healthcare
"Metaverse in Education: Vision, Opportunities, and Challenges","Traditional education has been updated with the development of information technology in human history. Within big data and cyber-physical systems, the Metaverse has generated strong interest in various applications (e.g., entertainment, business, and cultural travel) over the last decade. As a novel social work idea, the Metaverse consists of many kinds of technologies, e.g., big data, interaction, artificial intelligence, game design, Internet computing, Internet of Things, and blockchain. It is foreseeable that the usage of Metaverse will contribute to educational development. However, the architectures of the Metaverse in education are not yet mature enough. There are many questions we should address for the Metaverse in education. To this end, this paper aims to provide a systematic literature review of Metaverse in education. This paper is a comprehensive survey of the Metaverse in education, with a focus on current technologies, challenges, opportunities, and future directions. First, we present a brief overview of the Metaverse in education, as well as the motivation behind its integration. Then, we survey some important characteristics for the Metaverse in education, including the personal teaching environment and the personal learning environment. Next, we envisage what variations of this combination will bring to education in the future and discuss their strengths and weaknesses. We also review the state-of-the-art case studies (including technical companies and educational institutions) for Metaverse in education. Finally, we point out several challenges and issues in this promising area.","Metaverse in Education: Vision, Opportunities, and Challenges Traditional education has been updated with the development of information technology in human history. Within big data and cyber-physical systems, the Metaverse has generated strong interest in various applications (e.g., entertainment, business, and cultural travel) over the last decade. As a novel social work idea, the Metaverse consists of many kinds of technologies, e.g., big data, interaction, artificial intelligence, game design, Internet computing, Internet of Things, and blockchain. It is foreseeable that the usage of Metaverse will contribute to educational development. However, the architectures of the Metaverse in education are not yet mature enough. There are many questions we should address for the Metaverse in education. To this end, this paper aims to provide a systematic literature review of Metaverse in education. This paper is a comprehensive survey of the Metaverse in education, with a focus on current technologies, challenges, opportunities, and future directions. First, we present a brief overview of the Metaverse in education, as well as the motivation behind its integration. Then, we survey some important characteristics for the Metaverse in education, including the personal teaching environment and the personal learning environment. Next, we envisage what variations of this combination will bring to education in the future and discuss their strengths and weaknesses. We also review the state-of-the-art case studies (including technical companies and educational institutions) for Metaverse in education. Finally, we point out several challenges and issues in this promising area.",Education
Modeling Pedagogical Learning Environment with Hybrid Model based on ICT,"Pedagogy is a method that handles the ethos and culture of instruction from educators and the learning of learners. Pedagogy of Information and Communications Technology (ICT) refers to the interactions among the teacher, children, and learning environment based on ICT. It is a discipline that deals with the theory and practice of teaching strategies, teaching actions, teaching judgments, and decisions. It is also the understanding and needs of students as well as the background and interests of an individual one. In this paper, we have designed the pedagogical learning environment from the perspective of ICT education. In our methodology of the pedagogy for ICT, education includes the interaction among different elements. The methodology improves to propagate convenience differently into the educational environment. We are also building a hybrid model for the ICT development program. The hybrid model represents the combination of standards, stages, year level, and class level as well as brings it into one umbrella. We have constructed the pedagogical learning environment theoretically from the perspective of ICT education to the consideration of outcome-based ICT learning. Outcome-based education is a fundamental element for building any nation completely around the globe.","Modeling Pedagogical Learning Environment with Hybrid Model based on ICT Pedagogy is a method that handles the ethos and culture of instruction from educators and the learning of learners. Pedagogy of Information and Communications Technology (ICT) refers to the interactions among the teacher, children, and learning environment based on ICT. It is a discipline that deals with the theory and practice of teaching strategies, teaching actions, teaching judgments, and decisions. It is also the understanding and needs of students as well as the background and interests of an individual one. In this paper, we have designed the pedagogical learning environment from the perspective of ICT education. In our methodology of the pedagogy for ICT, education includes the interaction among different elements. The methodology improves to propagate convenience differently into the educational environment. We are also building a hybrid model for the ICT development program. The hybrid model represents the combination of standards, stages, year level, and class level as well as brings it into one umbrella. We have constructed the pedagogical learning environment theoretically from the perspective of ICT education to the consideration of outcome-based ICT learning. Outcome-based education is a fundamental element for building any nation completely around the globe.",Education
"Investigating the effects of housing instability on depression, anxiety, and mental health treatment in childhood and adolescence","Housing instability is a widespread phenomenon in the United States. In combination with other social determinants of health, housing instability affects childrens overall health and development. Drawing on data from the 2022 National Survey of Childrens Health, we employed multiple logistic regression models to understand how sociodemographic factors, especially housing instability, affect mental health outcomes and treatment access for youth aged 6-17 years. Our results show that youth facing housing instability have a higher likelihood of experiencing anxiety (OR: 1.42, p0.001) and depression (OR: 1.57, p0.001). Furthermore, youth experiencing both mental health conditions and housing instability are significantly less likely to receive mental health services in the past year, indicating the substantial barriers they face in accessing mental health care. Based on our findings, we highlight opportunities for digital mental health interventions to provide children experiencing housing instability with more accessible and consistent mental health services.","Investigating the effects of housing instability on depression, anxiety, and mental health treatment in childhood and adolescence Housing instability is a widespread phenomenon in the United States. In combination with other social determinants of health, housing instability affects childrens overall health and development. Drawing on data from the 2022 National Survey of Childrens Health, we employed multiple logistic regression models to understand how sociodemographic factors, especially housing instability, affect mental health outcomes and treatment access for youth aged 6-17 years. Our results show that youth facing housing instability have a higher likelihood of experiencing anxiety (OR: 1.42, p0.001) and depression (OR: 1.57, p0.001). Furthermore, youth experiencing both mental health conditions and housing instability are significantly less likely to receive mental health services in the past year, indicating the substantial barriers they face in accessing mental health care. Based on our findings, we highlight opportunities for digital mental health interventions to provide children experiencing housing instability with more accessible and consistent mental health services.",Healthcare
A System Framework for Smart Class System to Boost Education and Management,"The large number of reasonably priced computers, Internet broadband connectivity and rich education content has created a global phenomenon by which information and communication technology (ICT) has used to remodel education. E-learning can be explained as the use of available information, computational and communication technologies to assist learning practice. In the modern world, education has become more universal, and people are looking for learning with simplicity and interest. Students are looking for more interactive and attractive learning style rather than old traditional style. Using technological learning, we can enhance the education system. We can deliver quality education to students as well as we can ease and uniform the process of education by using the modern technologies and methods. In this paper, we propose a smart class model to manage the entire educational activities and hence to enhance the quality of education.","A System Framework for Smart Class System to Boost Education and Management The large number of reasonably priced computers, Internet broadband connectivity and rich education content has created a global phenomenon by which information and communication technology (ICT) has used to remodel education. E-learning can be explained as the use of available information, computational and communication technologies to assist learning practice. In the modern world, education has become more universal, and people are looking for learning with simplicity and interest. Students are looking for more interactive and attractive learning style rather than old traditional style. Using technological learning, we can enhance the education system. We can deliver quality education to students as well as we can ease and uniform the process of education by using the modern technologies and methods. In this paper, we propose a smart class model to manage the entire educational activities and hence to enhance the quality of education.",Education
From Exploration to End of Life: Unpacking Sustainability in Physicalization Practices,"Data physicalizations have gained prominence across domains, but their environmental impact has been largely overlooked. This work addresses this gap by investigating the interplay between sustainability and physicalization practices. We conducted interviews with experts from diverse backgrounds, followed by a survey to gather insights into how they approach physicalization projects and reflect on sustainability. Our thematic analysis revealed sustainability considerations throughout the entire physicalization life cycle -- a framework that encompasses various stages in a physicalizations existence. Notably, we found no single agreed-upon definition for sustainable physicalizations, highlighting the complexity of integrating sustainability into physicalization practices. We outline sustainability challenges and strategies based on participants experiences and propose the Sustainable Physicalization Practices (SuPPra) Matrix, providing a structured approach for designers to reflect on and enhance the environmental impact of their future physicalizations.","From Exploration to End of Life: Unpacking Sustainability in Physicalization Practices Data physicalizations have gained prominence across domains, but their environmental impact has been largely overlooked. This work addresses this gap by investigating the interplay between sustainability and physicalization practices. We conducted interviews with experts from diverse backgrounds, followed by a survey to gather insights into how they approach physicalization projects and reflect on sustainability. Our thematic analysis revealed sustainability considerations throughout the entire physicalization life cycle -- a framework that encompasses various stages in a physicalizations existence. Notably, we found no single agreed-upon definition for sustainable physicalizations, highlighting the complexity of integrating sustainability into physicalization practices. We outline sustainability challenges and strategies based on participants experiences and propose the Sustainable Physicalization Practices (SuPPra) Matrix, providing a structured approach for designers to reflect on and enhance the environmental impact of their future physicalizations.",Environment
Estimate of drainage water behaviour in shallow lakes,"A theoretical estimate of the explicit time dependence of a drainage water of shallow lakes is presented as an important contribution for understanding the lake dynamics. This information can be obtained from a sum of functions, largely used in fitting of experimental data. These functions were chosen because their centre and weight yield a good description of the water basin behaviour. The coefficients of these functions are here extracted using results of measured and  or calculated data for the state variables describing the shallow West Lake, Hangzou. This procedure can also be applied to other shallow lakes, generating geological information about their drainage basin, which is one of the most important parameters to describe their micrometeorological behaviour. One concludes this work emphasizing the relevance of the explicit time dependence of the drainage variables and the requirement of measured data to validate this approach.","Estimate of drainage water behaviour in shallow lakes A theoretical estimate of the explicit time dependence of a drainage water of shallow lakes is presented as an important contribution for understanding the lake dynamics. This information can be obtained from a sum of functions, largely used in fitting of experimental data. These functions were chosen because their centre and weight yield a good description of the water basin behaviour. The coefficients of these functions are here extracted using results of measured and  or calculated data for the state variables describing the shallow West Lake, Hangzou. This procedure can also be applied to other shallow lakes, generating geological information about their drainage basin, which is one of the most important parameters to describe their micrometeorological behaviour. One concludes this work emphasizing the relevance of the explicit time dependence of the drainage variables and the requirement of measured data to validate this approach.",Environment
Dual-CLVSA: a Novel Deep Learning Approach to Predict Financial Markets with Sentiment Measurements,"It is a challenging task to predict financial markets. The complexity of this task is mainly due to the interaction between financial markets and market participants, who are not able to keep rational all the time, and often affected by emotions such as fear and ecstasy. Based on the state-of-the-art approach particularly for financial market predictions, a hybrid convolutional LSTM Based variational sequence-to-sequence model with attention (CLVSA), we propose a novel deep learning approach, named dual-CLVSA, to predict financial market movement with both trading data and the corresponding social sentiment measurements, each through a separate sequence-to-sequence channel. We evaluate the performance of our approach with backtesting on historical trading data of SPDR SP 500 Trust ETF over eight years. The experiment results show that dual-CLVSA can effectively fuse the two types of data, and verify that sentiment measurements are not only informative for financial market predictions, but they also contain extra profitable features to boost the performance of our predicting system.","Dual-CLVSA: a Novel Deep Learning Approach to Predict Financial Markets with Sentiment Measurements It is a challenging task to predict financial markets. The complexity of this task is mainly due to the interaction between financial markets and market participants, who are not able to keep rational all the time, and often affected by emotions such as fear and ecstasy. Based on the state-of-the-art approach particularly for financial market predictions, a hybrid convolutional LSTM Based variational sequence-to-sequence model with attention (CLVSA), we propose a novel deep learning approach, named dual-CLVSA, to predict financial market movement with both trading data and the corresponding social sentiment measurements, each through a separate sequence-to-sequence channel. We evaluate the performance of our approach with backtesting on historical trading data of SPDR SP 500 Trust ETF over eight years. The experiment results show that dual-CLVSA can effectively fuse the two types of data, and verify that sentiment measurements are not only informative for financial market predictions, but they also contain extra profitable features to boost the performance of our predicting system.",Finance
A hybrid approach for the implementation of the Heston model,"We propose a hybrid tree-finite difference method in order to approximate the Heston model. We prove the convergence by embedding the procedure in a bivariate Markov chain and we study the convergence of European and American option prices. We finally provide numerical experiments that give accurate option prices in the Heston model, showing the reliability and the efficiency of the algorithm.","A hybrid approach for the implementation of the Heston model We propose a hybrid tree-finite difference method in order to approximate the Heston model. We prove the convergence by embedding the procedure in a bivariate Markov chain and we study the convergence of European and American option prices. We finally provide numerical experiments that give accurate option prices in the Heston model, showing the reliability and the efficiency of the algorithm.",Finance
Revealing Differences Between Curricula Using the Colorado Upper-Division Electrostatics Diagnostic,"The Colorado Upper-Division Electrostatics (CUE) Diagnostic is an exam developed as part of the curriculum reform at the University of Colorado, Boulder (CU). It was designed to assess conceptual learning within upper-division electricity and magnetism (EM). Using the CUE, we have been documenting students understanding of EM at Oregon State University (OSU) over a period of 5 years. Our analysis indicates that the CUE identifies concepts that are generally difficult for students, regardless of the curriculum. The overall pattern of OSU students scores reproduces the pattern reported by Chasteen et al. at CU. There are, however, some important differences that we will address. In particular, our students struggle with the CUE problems involving separation of variables and boundary conditions. We will discuss the possible causes for this, as well as steps that may rectify the situation.","Revealing Differences Between Curricula Using the Colorado Upper-Division Electrostatics Diagnostic The Colorado Upper-Division Electrostatics (CUE) Diagnostic is an exam developed as part of the curriculum reform at the University of Colorado, Boulder (CU). It was designed to assess conceptual learning within upper-division electricity and magnetism (EM). Using the CUE, we have been documenting students understanding of EM at Oregon State University (OSU) over a period of 5 years. Our analysis indicates that the CUE identifies concepts that are generally difficult for students, regardless of the curriculum. The overall pattern of OSU students scores reproduces the pattern reported by Chasteen et al. at CU. There are, however, some important differences that we will address. In particular, our students struggle with the CUE problems involving separation of variables and boundary conditions. We will discuss the possible causes for this, as well as steps that may rectify the situation.",Education
Auctioning Corporate Bonds: A Uniform-Price under Investment Mandates,"This paper examines how risk and budget limits on investment mandates affect the bidding strategy in a uniform-price auction for issuing corporate bonds. I prove the existence of symmetric Bayesian Nash equilibrium and explore how the risk limits imposed on the mandate may mitigate severe underpricing, as the symmetric equilibriums yield positively relates to the risk limit. Investment mandates with low-risk acceptance inversely affect the equilibrium bid. The equilibrium bid provides insights into the optimal mechanism for pricing corporate bonds conveying information about the bonds valuation, market power, and the number of bidders. These findings contribute to auction theory and have implications for empirical research in the corporate bond market.","Auctioning Corporate Bonds: A Uniform-Price under Investment Mandates This paper examines how risk and budget limits on investment mandates affect the bidding strategy in a uniform-price auction for issuing corporate bonds. I prove the existence of symmetric Bayesian Nash equilibrium and explore how the risk limits imposed on the mandate may mitigate severe underpricing, as the symmetric equilibriums yield positively relates to the risk limit. Investment mandates with low-risk acceptance inversely affect the equilibrium bid. The equilibrium bid provides insights into the optimal mechanism for pricing corporate bonds conveying information about the bonds valuation, market power, and the number of bidders. These findings contribute to auction theory and have implications for empirical research in the corporate bond market.",Finance
Does Infrastructure Investment Lead to Economic Growth or Economic Fragility? Evidence from China,"The prevalent view in the economics literature is that a high level of infrastructure investment is a precursor to economic growth. China is especially held up as a model to emulate. Based on the largest dataset of its kind, this paper punctures the twin myths that, first, infrastructure creates economic value, and, second, China has a distinct advantage in its delivery. Far from being an engine of economic growth, the typical infrastructure investment fails to deliver a positive risk adjusted return. Moreover, Chinas track record in delivering infrastructure is no better than that of rich democracies. Where investments are debt-financed, overinvesting in unproductive projects results in the buildup of debt, monetary expansion, instability in financial markets, and economic fragility, exactly as we see in China today. We conclude that poorly managed infrastructure investments are a main explanation of surfacing economic and financial problems in China. We predict that, unless China shifts to a lower level of higher-quality infrastructure investments, the country is headed for an infrastructure-led national financial and economic crisis, which is likely also to be a crisis for the international economy. Chinas infrastructure investment model is not one to follow for other countries but one to avoid.","Does Infrastructure Investment Lead to Economic Growth or Economic Fragility? Evidence from China The prevalent view in the economics literature is that a high level of infrastructure investment is a precursor to economic growth. China is especially held up as a model to emulate. Based on the largest dataset of its kind, this paper punctures the twin myths that, first, infrastructure creates economic value, and, second, China has a distinct advantage in its delivery. Far from being an engine of economic growth, the typical infrastructure investment fails to deliver a positive risk adjusted return. Moreover, Chinas track record in delivering infrastructure is no better than that of rich democracies. Where investments are debt-financed, overinvesting in unproductive projects results in the buildup of debt, monetary expansion, instability in financial markets, and economic fragility, exactly as we see in China today. We conclude that poorly managed infrastructure investments are a main explanation of surfacing economic and financial problems in China. We predict that, unless China shifts to a lower level of higher-quality infrastructure investments, the country is headed for an infrastructure-led national financial and economic crisis, which is likely also to be a crisis for the international economy. Chinas infrastructure investment model is not one to follow for other countries but one to avoid.",Finance
On Sequence Prediction for Arbitrary Measures,"Suppose we are given two probability measures on the set of one-way infinite finite-alphabet sequences and consider the question when one of the measures predicts the other, that is, when conditional probabilities converge (in a certain sense) when one of the measures is chosen to generate the sequence. This question may be considered a refinement of the problem of sequence prediction in its most general formulation: for a given class of probability measures, does there exist a measure which predicts all of the measures in the class? To address this problem, we find some conditions on local absolute continuity which are sufficient for prediction and which generalize several different notions which are known to be sufficient for prediction. We also formulate some open questions to outline a direction for finding the conditions on classes of measures for which prediction is possible.","On Sequence Prediction for Arbitrary Measures Suppose we are given two probability measures on the set of one-way infinite finite-alphabet sequences and consider the question when one of the measures predicts the other, that is, when conditional probabilities converge (in a certain sense) when one of the measures is chosen to generate the sequence. This question may be considered a refinement of the problem of sequence prediction in its most general formulation: for a given class of probability measures, does there exist a measure which predicts all of the measures in the class? To address this problem, we find some conditions on local absolute continuity which are sufficient for prediction and which generalize several different notions which are known to be sufficient for prediction. We also formulate some open questions to outline a direction for finding the conditions on classes of measures for which prediction is possible.",Technology
Desperately seeking the impact of learning analytics in education at scale: Marrying data analysis with teaching and learning,"Learning analytics (LA) is argued to be able to improve learning outcomes, learner support and teaching. However, despite an increasingly expanding amount of student (digital) data accessible from various online education and learning platforms and the growing interest in LA worldwide as well as considerable research efforts already made, there is still little empirical evidence of impact on practice that shows the effectiveness of LA in education settings. Based on a selection of theoretical and empirical research, this chapter provides a critical discussion about the possibilities of collecting and using student data as well as barriers and challenges to overcome in providing data-informed support to educators everyday teaching practices. We argue that in order to increase the impact of data-driven decision-making aimed at students improved learning in education at scale, we need to better understand educators needs, their teaching practices and the context in which these practices occur, and how to support them in developing relevant knowledge, strategies and skills to facilitate the data-informed process of digitalization of education.","Desperately seeking the impact of learning analytics in education at scale: Marrying data analysis with teaching and learning Learning analytics (LA) is argued to be able to improve learning outcomes, learner support and teaching. However, despite an increasingly expanding amount of student (digital) data accessible from various online education and learning platforms and the growing interest in LA worldwide as well as considerable research efforts already made, there is still little empirical evidence of impact on practice that shows the effectiveness of LA in education settings. Based on a selection of theoretical and empirical research, this chapter provides a critical discussion about the possibilities of collecting and using student data as well as barriers and challenges to overcome in providing data-informed support to educators everyday teaching practices. We argue that in order to increase the impact of data-driven decision-making aimed at students improved learning in education at scale, we need to better understand educators needs, their teaching practices and the context in which these practices occur, and how to support them in developing relevant knowledge, strategies and skills to facilitate the data-informed process of digitalization of education.",Education
Medical Data Augmentation via ChatGPT: A Case Study on Medication Identification and Medication Event Classification,"The identification of key factors such as medications, diseases, and relationships within electronic health records and clinical notes has a wide range of applications in the clinical field. In the N2C2 2022 competitions, various tasks were presented to promote the identification of key factors in electronic health records (EHRs) using the Contextualized Medication Event Dataset (CMED). Pretrained large language models (LLMs) demonstrated exceptional performance in these tasks. This study aims to explore the utilization of LLMs, specifically ChatGPT, for data augmentation to overcome the limited availability of annotated data for identifying the key factors in EHRs. Additionally, different pre-trained BERT models, initially trained on extensive datasets like Wikipedia and MIMIC, were employed to develop models for identifying these key variables in EHRs through fine-tuning on augmented datasets. The experimental results of two EHR analysis tasks, namely medication identification and medication event classification, indicate that data augmentation based on ChatGPT proves beneficial in improving performance for both medication identification and medication event classification.","Medical Data Augmentation via ChatGPT: A Case Study on Medication Identification and Medication Event Classification The identification of key factors such as medications, diseases, and relationships within electronic health records and clinical notes has a wide range of applications in the clinical field. In the N2C2 2022 competitions, various tasks were presented to promote the identification of key factors in electronic health records (EHRs) using the Contextualized Medication Event Dataset (CMED). Pretrained large language models (LLMs) demonstrated exceptional performance in these tasks. This study aims to explore the utilization of LLMs, specifically ChatGPT, for data augmentation to overcome the limited availability of annotated data for identifying the key factors in EHRs. Additionally, different pre-trained BERT models, initially trained on extensive datasets like Wikipedia and MIMIC, were employed to develop models for identifying these key variables in EHRs through fine-tuning on augmented datasets. The experimental results of two EHR analysis tasks, namely medication identification and medication event classification, indicate that data augmentation based on ChatGPT proves beneficial in improving performance for both medication identification and medication event classification.",Healthcare
The Reality Game,"We introduce an evolutionary game with feedback between perception and reality, which we call the reality game. It is a game of chance in which the probabilities for different objective outcomes (e.g., heads or tails in a coin toss) depend on the amount wagered on those outcomes. By varying the reality map, which relates the amount wagered to the probability of the outcome, it is possible to move continuously from a purely objective game in which probabilities have no dependence on wagers to a purely subjective game in which probabilities equal the amount wagered. We study self-reinforcing games, in which betting more on an outcome increases its odds, and self-defeating games, in which the opposite is true. This is investigated in and out of equilibrium, with and without rational players, and both numerically and analytically. We introduce a method of measuring the inefficiency of the game, similar to measuring the magnitude of the arbitrage opportunities in a financial market. We prove that convergence to equilibrium is is a power law with an extremely slow rate of convergence: The more subjective the game, the slower the convergence.","The Reality Game We introduce an evolutionary game with feedback between perception and reality, which we call the reality game. It is a game of chance in which the probabilities for different objective outcomes (e.g., heads or tails in a coin toss) depend on the amount wagered on those outcomes. By varying the reality map, which relates the amount wagered to the probability of the outcome, it is possible to move continuously from a purely objective game in which probabilities have no dependence on wagers to a purely subjective game in which probabilities equal the amount wagered. We study self-reinforcing games, in which betting more on an outcome increases its odds, and self-defeating games, in which the opposite is true. This is investigated in and out of equilibrium, with and without rational players, and both numerically and analytically. We introduce a method of measuring the inefficiency of the game, similar to measuring the magnitude of the arbitrage opportunities in a financial market. We prove that convergence to equilibrium is is a power law with an extremely slow rate of convergence: The more subjective the game, the slower the convergence.",Finance
Leveraging machine learning to enhance climate models: a review,"Recent achievements in machine learning (Ml) have had a significant impact on various fields, including climate science. Climate modeling is very important and plays a crucial role in shaping the decisions of governments and individuals in mitigating the impact of climate change. Climate change poses a serious threat to humanity, however, current climate models are limited by computational costs, uncertainties, and biases, affecting their prediction accuracy. The vast amount of climate data generated by satellites, radars, and earth system models (ESMS) poses a significant challenge. ML techniques can be effectively employed to analyze this data and extract valuable insights that aid in our understanding of the earth climate. This review paper focuses on how ml has been utilized in the last 5 years to boost the current state-of-the-art climate models. We invite the ml community to join in the global effort to accurately model the earth climate by collaborating with other fields to leverage ml as a powerful tool in this endeavor.","Leveraging machine learning to enhance climate models: a review Recent achievements in machine learning (Ml) have had a significant impact on various fields, including climate science. Climate modeling is very important and plays a crucial role in shaping the decisions of governments and individuals in mitigating the impact of climate change. Climate change poses a serious threat to humanity, however, current climate models are limited by computational costs, uncertainties, and biases, affecting their prediction accuracy. The vast amount of climate data generated by satellites, radars, and earth system models (ESMS) poses a significant challenge. ML techniques can be effectively employed to analyze this data and extract valuable insights that aid in our understanding of the earth climate. This review paper focuses on how ml has been utilized in the last 5 years to boost the current state-of-the-art climate models. We invite the ml community to join in the global effort to accurately model the earth climate by collaborating with other fields to leverage ml as a powerful tool in this endeavor.",Environment
A review of effects of climate change on Agriculture in Africa,"Currently, agriculture in Africa contributes only a tenth to global Green House Gas (GHG) emissions from agriculture. Despite its relatively low contribution to GHG, a conundrum of climate justice, adverse impacts of climate change disproportionately threaten Africas agriculture, the Continents main economic sector. Consequently, we seek to review the effects of climate change on Agriculture.","A review of effects of climate change on Agriculture in Africa Currently, agriculture in Africa contributes only a tenth to global Green House Gas (GHG) emissions from agriculture. Despite its relatively low contribution to GHG, a conundrum of climate justice, adverse impacts of climate change disproportionately threaten Africas agriculture, the Continents main economic sector. Consequently, we seek to review the effects of climate change on Agriculture.",Environment
Introductory quantum information science coursework at US institutions: Content coverage,"Despite rapid growth of quantum information science and engineering (QISQISE) workforce development initiatives, perceived lack of agreement among faculty on core content has made prior research-based curriculum and assessment development initiatives difficult to scale. To identify areas if consensus on content coverage, we report findings from a survey of N63 instructors teaching introductory QISE courses at US institutions of higher learning. We identify a subset of content items common across a large fraction (80) of introductory QISE courses that are potentially amenable to research-based curriculum development, with an emphasis on foundational skills in mathematics, physics, and engineering. As a further guide for curriculum development, we also examine differences in content coverage by level (undergraduategraduate) and discipline. Finally, we briefly discuss the implications of our findings for the development of a research-based QISE assessment at the postsecondary level.","Introductory quantum information science coursework at US institutions: Content coverage Despite rapid growth of quantum information science and engineering (QISQISE) workforce development initiatives, perceived lack of agreement among faculty on core content has made prior research-based curriculum and assessment development initiatives difficult to scale. To identify areas if consensus on content coverage, we report findings from a survey of N63 instructors teaching introductory QISE courses at US institutions of higher learning. We identify a subset of content items common across a large fraction (80) of introductory QISE courses that are potentially amenable to research-based curriculum development, with an emphasis on foundational skills in mathematics, physics, and engineering. As a further guide for curriculum development, we also examine differences in content coverage by level (undergraduategraduate) and discipline. Finally, we briefly discuss the implications of our findings for the development of a research-based QISE assessment at the postsecondary level.",Education
Assessing Pedagogical Readiness for Digital Innovation: A Mixed-Methods Study,"Digital innovation in education has revolutionized teaching and learning processes, demanding a rethink of pedagogical competence among educators. This study evaluates the preparation of instructors to use digital technologies into their educational practices. The study used a mixed-methods approach, integrating both qualitative interviews and quantitative surveys to evaluate teachers institutional support systems, beliefs, and technical proficiency. The results show that even while a large number of educators acknowledge the benefits of digital tools, problems including poor professional development and change aversion still exist. In order to improve digital pedagogical preparation, the study emphasizes the necessity of focused training initiatives and encouraging institutional regulations. There is discussion on the implications for educational institutions and policymakers.","Assessing Pedagogical Readiness for Digital Innovation: A Mixed-Methods Study Digital innovation in education has revolutionized teaching and learning processes, demanding a rethink of pedagogical competence among educators. This study evaluates the preparation of instructors to use digital technologies into their educational practices. The study used a mixed-methods approach, integrating both qualitative interviews and quantitative surveys to evaluate teachers institutional support systems, beliefs, and technical proficiency. The results show that even while a large number of educators acknowledge the benefits of digital tools, problems including poor professional development and change aversion still exist. In order to improve digital pedagogical preparation, the study emphasizes the necessity of focused training initiatives and encouraging institutional regulations. There is discussion on the implications for educational institutions and policymakers.",Education
Parameter-less Optimization with the Extended Compact Genetic Algorithm and Iterated Local Search,"This paper presents a parameter-less optimization framework that uses the extended compact genetic algorithm (ECGA) and iterated local search (ILS), but is not restricted to these algorithms. The presented optimization algorithm (ILSECGA) comes as an extension of the parameter-less genetic algorithm (GA), where the parameters of a selecto-recombinative GA are eliminated. The approach that we propose is tested on several well known problems. In the absence of domain knowledge, it is shown that ILSECGA is a robust and easy-to-use optimization method.","Parameter-less Optimization with the Extended Compact Genetic Algorithm and Iterated Local Search This paper presents a parameter-less optimization framework that uses the extended compact genetic algorithm (ECGA) and iterated local search (ILS), but is not restricted to these algorithms. The presented optimization algorithm (ILSECGA) comes as an extension of the parameter-less genetic algorithm (GA), where the parameters of a selecto-recombinative GA are eliminated. The approach that we propose is tested on several well known problems. In the absence of domain knowledge, it is shown that ILSECGA is a robust and easy-to-use optimization method.",Technology
Geometric Morphology of Granular Materials,"We present a new method to transform the spectral pixel information of a micrograph into an affine geometric description, which allows us to analyze the morphology of granular materials. We use spectral and pulse-coupled neural network based segmentation techniques to generate blobs, and a newly developed algorithm to extract dilated contours. A constrained Delaunay tesselation of the contour points results in a triangular mesh. This mesh is the basic ingredient of the Chodal Axis Transform, which provides a morphological decomposition of shapes. Such decomposition allows for grain separation and the efficient computation of the statistical features of granular materials.","Geometric Morphology of Granular Materials We present a new method to transform the spectral pixel information of a micrograph into an affine geometric description, which allows us to analyze the morphology of granular materials. We use spectral and pulse-coupled neural network based segmentation techniques to generate blobs, and a newly developed algorithm to extract dilated contours. A constrained Delaunay tesselation of the contour points results in a triangular mesh. This mesh is the basic ingredient of the Chodal Axis Transform, which provides a morphological decomposition of shapes. Such decomposition allows for grain separation and the efficient computation of the statistical features of granular materials.",Technology
ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer Climate Change Queries,"As the issue of global climate change becomes increasingly severe, the demand for research in climate science continues to grow. Natural language processing technologies, represented by Large Language Models (LLMs), have been widely applied to climate change-specific research, providing essential information support for decision-makers and the public. Some studies have improved model performance on relevant tasks by constructing climate change-related instruction data and instruction-tuning LLMs. However, current research remains inadequate in efficiently producing large volumes of high-precision instruction data for climate change, which limits further development of climate change LLMs. This study introduces an automated method for constructing instruction data. The method generates instructions using facts and background knowledge from documents and enhances the diversity of the instruction data through web scraping and the collection of seed instructions. Using this method, we constructed a climate change instruction dataset, named ClimateChat-Corpus, which was used to fine-tune open-source LLMs, resulting in an LLM named ClimateChat. Evaluation results show that ClimateChat significantly improves performance on climate change question-and-answer tasks. Additionally, we evaluated the impact of different base models and instruction data on LLM performance and demonstrated its capability to adapt to a wide range of climate change scientific discovery tasks, emphasizing the importance of selecting an appropriate base model for instruction tuning. This research provides valuable references and empirical support for constructing climate change instruction data and training climate change-specific LLMs.","ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer Climate Change Queries As the issue of global climate change becomes increasingly severe, the demand for research in climate science continues to grow. Natural language processing technologies, represented by Large Language Models (LLMs), have been widely applied to climate change-specific research, providing essential information support for decision-makers and the public. Some studies have improved model performance on relevant tasks by constructing climate change-related instruction data and instruction-tuning LLMs. However, current research remains inadequate in efficiently producing large volumes of high-precision instruction data for climate change, which limits further development of climate change LLMs. This study introduces an automated method for constructing instruction data. The method generates instructions using facts and background knowledge from documents and enhances the diversity of the instruction data through web scraping and the collection of seed instructions. Using this method, we constructed a climate change instruction dataset, named ClimateChat-Corpus, which was used to fine-tune open-source LLMs, resulting in an LLM named ClimateChat. Evaluation results show that ClimateChat significantly improves performance on climate change question-and-answer tasks. Additionally, we evaluated the impact of different base models and instruction data on LLM performance and demonstrated its capability to adapt to a wide range of climate change scientific discovery tasks, emphasizing the importance of selecting an appropriate base model for instruction tuning. This research provides valuable references and empirical support for constructing climate change instruction data and training climate change-specific LLMs.",Environment
Vision-Based Road Detection in Automotive Systems: A Real-Time Expectation-Driven Approach,"The main aim of this work is the development of a vision-based road detection system fast enough to cope with the difficult real-time constraints imposed by moving vehicle applications. The hardware platform, a special-purpose massively parallel system, has been chosen to minimize system production and operational costs. This paper presents a novel approach to expectation-driven low-level image segmentation, which can be mapped naturally onto mesh-connected massively parallel SIMD architectures capable of handling hierarchical data structures. The input image is assumed to contain a distorted version of a given template; a multiresolution stretching process is used to reshape the original template in accordance with the acquired image content, minimizing a potential function. The distorted template is the process output.","Vision-Based Road Detection in Automotive Systems: A Real-Time Expectation-Driven Approach The main aim of this work is the development of a vision-based road detection system fast enough to cope with the difficult real-time constraints imposed by moving vehicle applications. The hardware platform, a special-purpose massively parallel system, has been chosen to minimize system production and operational costs. This paper presents a novel approach to expectation-driven low-level image segmentation, which can be mapped naturally onto mesh-connected massively parallel SIMD architectures capable of handling hierarchical data structures. The input image is assumed to contain a distorted version of a given template; a multiresolution stretching process is used to reshape the original template in accordance with the acquired image content, minimizing a potential function. The distorted template is the process output.",Technology
A theory of growth by differential sedimentation with application to snowflake formation,"A simple model of irreversible aggregation under differential sedimentation of particles in a fluid is presented. The structure of the aggregates produced by this process is found to feed back on the dynamics in such a way as to stabilise both the exponents controlling the growth rate, and the fractal dimension of the clusters produced at readily predictable values. The aggregation of ice crystals to form snowflakes is considered as a potential application of the model.","A theory of growth by differential sedimentation with application to snowflake formation A simple model of irreversible aggregation under differential sedimentation of particles in a fluid is presented. The structure of the aggregates produced by this process is found to feed back on the dynamics in such a way as to stabilise both the exponents controlling the growth rate, and the fractal dimension of the clusters produced at readily predictable values. The aggregation of ice crystals to form snowflakes is considered as a potential application of the model.",Environment
Theoretical Properties of Projection Based Multilayer Perceptrons with Functional Inputs,"Many real world data are sampled functions. As shown by Functional Data Analysis (FDA) methods, spectra, time series, images, gesture recognition data, etc. can be processed more efficiently if their functional nature is taken into account during the data analysis process. This is done by extending standard data analysis methods so that they can apply to functional inputs. A general way to achieve this goal is to compute projections of the functional data onto a finite dimensional sub-space of the functional space. The coordinates of the data on a basis of this sub-space provide standard vector representations of the functions. The obtained vectors can be processed by any standard method. In our previous work, this general approach has been used to define projection based Multilayer Perceptrons (MLPs) with functional inputs. We study in this paper important theoretical properties of the proposed model. We show in particular that MLPs with functional inputs are universal approximators: they can approximate to arbitrary accuracy any continuous mapping from a compact sub-space of a functional space to R. Moreover, we provide a consistency result that shows that any mapping from a functional space to R can be learned thanks to examples by a projection based MLP: the generalization mean square error of the MLP decreases to the smallest possible mean square error on the data when the number of examples goes to infinity.","Theoretical Properties of Projection Based Multilayer Perceptrons with Functional Inputs Many real world data are sampled functions. As shown by Functional Data Analysis (FDA) methods, spectra, time series, images, gesture recognition data, etc. can be processed more efficiently if their functional nature is taken into account during the data analysis process. This is done by extending standard data analysis methods so that they can apply to functional inputs. A general way to achieve this goal is to compute projections of the functional data onto a finite dimensional sub-space of the functional space. The coordinates of the data on a basis of this sub-space provide standard vector representations of the functions. The obtained vectors can be processed by any standard method. In our previous work, this general approach has been used to define projection based Multilayer Perceptrons (MLPs) with functional inputs. We study in this paper important theoretical properties of the proposed model. We show in particular that MLPs with functional inputs are universal approximators: they can approximate to arbitrary accuracy any continuous mapping from a compact sub-space of a functional space to R. Moreover, we provide a consistency result that shows that any mapping from a functional space to R can be learned thanks to examples by a projection based MLP: the generalization mean square error of the MLP decreases to the smallest possible mean square error on the data when the number of examples goes to infinity.",Technology
Inferring medication adherence from time-varying health measures,"Medication adherence is a problem of widespread concern in clinical care. Poor adherence is a particular problem for patients with chronic diseases requiring long-term medication because poor adherence can result in less successful treatment outcomes and even preventable deaths. Existing methods to collect information about patient adherence are resource-intensive or do not successfully detect low-adherers with high accuracy. Acknowledging that health measures recorded at clinic visits are more reliably recorded than a patients adherence, we have developed an approach to infer medication adherence rates based on longitudinally recorded health measures that are likely impacted by time-varying adherence behaviors. Our framework permits the inclusion of baseline health characteristics and socio-demographic data. We employ a modular inferential approach. First, we fit a two-component model on a training set of patients who have detailed adherence data obtained from electronic medication monitoring. One model component predicts adherence behaviors only from baseline health and socio-demographic information, and the other predicts longitudinal health measures given the adherence and baseline health measures. Posterior draws of relevant model parameters are simulated from this model using Markov chain Monte Carlo methods. Second, we develop an approach to infer medication adherence from the time-varying health measures using a Sequential Monte Carlo algorithm applied to a new set of patients for whom no adherence data are available. We apply and evaluate the method on a cohort of hypertensive patients, using baseline health comorbidities, socio-demographic measures, and blood pressure measured over time to infer patients adherence to antihypertensive medication.","Inferring medication adherence from time-varying health measures Medication adherence is a problem of widespread concern in clinical care. Poor adherence is a particular problem for patients with chronic diseases requiring long-term medication because poor adherence can result in less successful treatment outcomes and even preventable deaths. Existing methods to collect information about patient adherence are resource-intensive or do not successfully detect low-adherers with high accuracy. Acknowledging that health measures recorded at clinic visits are more reliably recorded than a patients adherence, we have developed an approach to infer medication adherence rates based on longitudinally recorded health measures that are likely impacted by time-varying adherence behaviors. Our framework permits the inclusion of baseline health characteristics and socio-demographic data. We employ a modular inferential approach. First, we fit a two-component model on a training set of patients who have detailed adherence data obtained from electronic medication monitoring. One model component predicts adherence behaviors only from baseline health and socio-demographic information, and the other predicts longitudinal health measures given the adherence and baseline health measures. Posterior draws of relevant model parameters are simulated from this model using Markov chain Monte Carlo methods. Second, we develop an approach to infer medication adherence from the time-varying health measures using a Sequential Monte Carlo algorithm applied to a new set of patients for whom no adherence data are available. We apply and evaluate the method on a cohort of hypertensive patients, using baseline health comorbidities, socio-demographic measures, and blood pressure measured over time to infer patients adherence to antihypertensive medication.",Healthcare
Towards a model for protein production rates,This submission is a duplicate of arXiv:q-bio0602024 and has been removed.,Towards a model for protein production rates This submission is a duplicate of arXiv:q-bio0602024 and has been removed.,Healthcare
From Witchs Shot to Music Making Bones -- Resources for Medical Laymen to Technical Language and Vice Versa,"Many people share information in social media or forums, like food they eat, sports activities they do or events which have been visited. This also applies to information about a persons health status. Information we share online unveils directly or indirectly information about our lifestyle and health situation and thus provides a valuable data resource. If we can make advantage of that data, applications can be created that enable e.g. the detection of possible risk factors of diseases or adverse drug reactions of medications. However, as most people are not medical experts, language used might be more descriptive rather than the precise medical expression as medics do. To detect and use those relevant information, laymen language has to be translated andor linked to the corresponding medical concept. This work presents baseline data sources in order to address this challenge for German. We introduce a new data set which annotates medical laymen and technical expressions in a patient forum, along with a set of medical synonyms and definitions, and present first baseline results on the data.","From Witchs Shot to Music Making Bones -- Resources for Medical Laymen to Technical Language and Vice Versa Many people share information in social media or forums, like food they eat, sports activities they do or events which have been visited. This also applies to information about a persons health status. Information we share online unveils directly or indirectly information about our lifestyle and health situation and thus provides a valuable data resource. If we can make advantage of that data, applications can be created that enable e.g. the detection of possible risk factors of diseases or adverse drug reactions of medications. However, as most people are not medical experts, language used might be more descriptive rather than the precise medical expression as medics do. To detect and use those relevant information, laymen language has to be translated andor linked to the corresponding medical concept. This work presents baseline data sources in order to address this challenge for German. We introduce a new data set which annotates medical laymen and technical expressions in a patient forum, along with a set of medical synonyms and definitions, and present first baseline results on the data.",Healthcare
The Medical Authority of AI: A Study of AI-enabled Consumer-facing Health Technology,"Recently, consumer-facing health technologies such as Artificial Intelligence (AI)-based symptom checkers (AISCs) have sprung up in everyday healthcare practice. AISCs solicit symptom information from users and provide medical suggestions and possible diagnoses, a responsibility that people usually entrust with real-person authorities such as physicians and expert patients. Thus, the advent of AISCs begs a question of whether and how they transform the notion of medical authority in everyday healthcare practice. To answer this question, we conducted an interview study with thirty AISC users. We found that users assess the medical authority of AISCs using various factors including automated decisions and interaction design patterns of AISC apps, associations with established medical authorities like hospitals, and comparisons with other health technologies. We reveal how AISCs are used in healthcare delivery, discuss how AI transforms conventional understandings of medical authority, and derive implications for designing AI-enabled health technology.","The Medical Authority of AI: A Study of AI-enabled Consumer-facing Health Technology Recently, consumer-facing health technologies such as Artificial Intelligence (AI)-based symptom checkers (AISCs) have sprung up in everyday healthcare practice. AISCs solicit symptom information from users and provide medical suggestions and possible diagnoses, a responsibility that people usually entrust with real-person authorities such as physicians and expert patients. Thus, the advent of AISCs begs a question of whether and how they transform the notion of medical authority in everyday healthcare practice. To answer this question, we conducted an interview study with thirty AISC users. We found that users assess the medical authority of AISCs using various factors including automated decisions and interaction design patterns of AISC apps, associations with established medical authorities like hospitals, and comparisons with other health technologies. We reveal how AISCs are used in healthcare delivery, discuss how AI transforms conventional understandings of medical authority, and derive implications for designing AI-enabled health technology.",Healthcare
Quasi-Monte Carlo methods for the Heston model,"In this paper, we discuss the application of quasi-Monte Carlo methods to the Heston model. We base our algorithms on the Broadie-Kaya algorithm, an exact simulation scheme for the Heston model. As the joint transition densities are not available in closed-form, the Linear Transformation method due to Imai and Tan, a popular and widely applicable method to improve the effectiveness of quasi-Monte Carlo methods, cannot be employed in the context of path-dependent options when the underlying price process follows the Heston model. Consequently, we tailor quasi-Monte Carlo methods directly to the Heston model. The contributions of the paper are threefold: We firstly show how to apply quasi-Monte Carlo methods in the context of the Heston model and the SVJ model, secondly that quasi-Monte Carlo methods improve on Monte Carlo methods, and thirdly how to improve the effectiveness of quasi-Monte Carlo methods by using bridge constructions tailored to the Heston and SVJ models. Finally, we provide some extensions for computing greeks, barrier options, multidimensional and multi-asset pricing, and the 32 model.","Quasi-Monte Carlo methods for the Heston model In this paper, we discuss the application of quasi-Monte Carlo methods to the Heston model. We base our algorithms on the Broadie-Kaya algorithm, an exact simulation scheme for the Heston model. As the joint transition densities are not available in closed-form, the Linear Transformation method due to Imai and Tan, a popular and widely applicable method to improve the effectiveness of quasi-Monte Carlo methods, cannot be employed in the context of path-dependent options when the underlying price process follows the Heston model. Consequently, we tailor quasi-Monte Carlo methods directly to the Heston model. The contributions of the paper are threefold: We firstly show how to apply quasi-Monte Carlo methods in the context of the Heston model and the SVJ model, secondly that quasi-Monte Carlo methods improve on Monte Carlo methods, and thirdly how to improve the effectiveness of quasi-Monte Carlo methods by using bridge constructions tailored to the Heston and SVJ models. Finally, we provide some extensions for computing greeks, barrier options, multidimensional and multi-asset pricing, and the 32 model.",Finance
Climate-Contingent Finance,"Climate adaptation could yield significant benefits. However, the uncertainty of which future climate scenarios will occur decreases the feasibility of proactively adapting. Climate adaptation projects could be underwritten by benefits paid for in the climate scenarios that each adaptation project is designed to address because other entities would like to hedge the financial risk of those scenarios. Because the return on investment is a function of the level of climate change, it is optimal for the adapting entity to finance adaptation with repayment as a function of the climate. It is also optimal for entities with more financial downside under a more extreme climate to serve as an investing counterparty because they can obtain higher than market rates of return when they need it most. In this way, parties proactively adapting would reduce the risk they over-prepare, while their investors would reduce the risk they under-prepare. This is superior to typical insurance because, by investing in climate-contingent mechanisms, investors are not merely financially hedging but also outright preventing physical damage, and therefore creating economic value. This coordinates capital through time and place according to parties risk reduction capabilities and financial profiles, while also providing a diversifying investment return. Climate-contingent finance can be generalized to any situation where entities share exposure to a risk where they lack direct control over whether it occurs (e.g., climate change, or a natural pandemic), and one type of entity can take proactive actions to benefit from addressing the effects of the risk if it occurs (e.g., through innovating on crops that would do well under extreme climate change or vaccination technology that could address particular viruses) with funding from another type of entity that seeks a targeted return to ameliorate the downside.","Climate-Contingent Finance Climate adaptation could yield significant benefits. However, the uncertainty of which future climate scenarios will occur decreases the feasibility of proactively adapting. Climate adaptation projects could be underwritten by benefits paid for in the climate scenarios that each adaptation project is designed to address because other entities would like to hedge the financial risk of those scenarios. Because the return on investment is a function of the level of climate change, it is optimal for the adapting entity to finance adaptation with repayment as a function of the climate. It is also optimal for entities with more financial downside under a more extreme climate to serve as an investing counterparty because they can obtain higher than market rates of return when they need it most. In this way, parties proactively adapting would reduce the risk they over-prepare, while their investors would reduce the risk they under-prepare. This is superior to typical insurance because, by investing in climate-contingent mechanisms, investors are not merely financially hedging but also outright preventing physical damage, and therefore creating economic value. This coordinates capital through time and place according to parties risk reduction capabilities and financial profiles, while also providing a diversifying investment return. Climate-contingent finance can be generalized to any situation where entities share exposure to a risk where they lack direct control over whether it occurs (e.g., climate change, or a natural pandemic), and one type of entity can take proactive actions to benefit from addressing the effects of the risk if it occurs (e.g., through innovating on crops that would do well under extreme climate change or vaccination technology that could address particular viruses) with funding from another type of entity that seeks a targeted return to ameliorate the downside.",Environment
Note on log-periodic description of 2008 financial crash,"We analyze the financial crash in 2008 for different financial markets from the point of view of log-periodic function model. In particular, we consider Dow Jones index, DAX index and Hang Seng index. We shortly discuss the possible relation of the theory of critical phenomena in physics to financial markets.","Note on log-periodic description of 2008 financial crash We analyze the financial crash in 2008 for different financial markets from the point of view of log-periodic function model. In particular, we consider Dow Jones index, DAX index and Hang Seng index. We shortly discuss the possible relation of the theory of critical phenomena in physics to financial markets.",Finance
Heterogeneous Beliefs with Partial Observations,This paper examines a heterogeneous beliefs model in which there is a process that is only partially observed by the agents. The economy contains a risky asset producing dividends continuously in time. The dividends are observed by the agents. The dividends are assumed to be a known function of some other unobserved process. The agents use filtering to estimate the value of this unobserved process. The agents have different beliefs about the dynamics of the unobserved process and therefore form different estimates. We analyse this model and derive the state price density. We use this to derive the riskless rate. We also characterise the price of the risky asset in terms of the solution of a series of differential equations.,Heterogeneous Beliefs with Partial Observations This paper examines a heterogeneous beliefs model in which there is a process that is only partially observed by the agents. The economy contains a risky asset producing dividends continuously in time. The dividends are observed by the agents. The dividends are assumed to be a known function of some other unobserved process. The agents use filtering to estimate the value of this unobserved process. The agents have different beliefs about the dynamics of the unobserved process and therefore form different estimates. We analyse this model and derive the state price density. We use this to derive the riskless rate. We also characterise the price of the risky asset in terms of the solution of a series of differential equations.,Finance
Multiplicative Algorithm for Orthgonal Groups and Independent Component Analysis,"The multiplicative Newton-like method developed by the author et al. is extended to the situation where the dynamics is restricted to the orthogonal group. A general framework is constructed without specifying the cost function. Though the restriction to the orthogonal groups makes the problem somewhat complicated, an explicit expression for the amount of individual jumps is obtained. This algorithm is exactly second-order-convergent. The global instability inherent in the Newton method is remedied by a Levenberg-Marquardt-type variation. The method thus constructed can readily be applied to the independent component analysis. Its remarkable performance is illustrated by a numerical simulation.","Multiplicative Algorithm for Orthgonal Groups and Independent Component Analysis The multiplicative Newton-like method developed by the author et al. is extended to the situation where the dynamics is restricted to the orthogonal group. A general framework is constructed without specifying the cost function. Though the restriction to the orthogonal groups makes the problem somewhat complicated, an explicit expression for the amount of individual jumps is obtained. This algorithm is exactly second-order-convergent. The global instability inherent in the Newton method is remedied by a Levenberg-Marquardt-type variation. The method thus constructed can readily be applied to the independent component analysis. Its remarkable performance is illustrated by a numerical simulation.",Technology
Pattern Matching and Discourse Processing in Information Extraction from Japanese Text,"Information extraction is the task of automatically picking up information of interest from an unconstrained text. Information of interest is usually extracted in two steps. First, sentence level processing locates relevant pieces of information scattered throughout the text; second, discourse processing merges coreferential information to generate the output. In the first step, pieces of information are locally identified without recognizing any relationships among them. A key word search or simple pattern search can achieve this purpose. The second step requires deeper knowledge in order to understand relationships among separately identified pieces of information. Previous information extraction systems focused on the first step, partly because they were not required to link up each piece of information with other pieces. To link the extracted pieces of information and map them onto a structured output format, complex discourse processing is essential. This paper reports on a Japanese information extraction system that merges information using a pattern matcher and discourse processor. Evaluation results show a high level of system performance which approaches human performance.","Pattern Matching and Discourse Processing in Information Extraction from Japanese Text Information extraction is the task of automatically picking up information of interest from an unconstrained text. Information of interest is usually extracted in two steps. First, sentence level processing locates relevant pieces of information scattered throughout the text; second, discourse processing merges coreferential information to generate the output. In the first step, pieces of information are locally identified without recognizing any relationships among them. A key word search or simple pattern search can achieve this purpose. The second step requires deeper knowledge in order to understand relationships among separately identified pieces of information. Previous information extraction systems focused on the first step, partly because they were not required to link up each piece of information with other pieces. To link the extracted pieces of information and map them onto a structured output format, complex discourse processing is essential. This paper reports on a Japanese information extraction system that merges information using a pattern matcher and discourse processor. Evaluation results show a high level of system performance which approaches human performance.",Technology
From Opinion Polarization to Climate Action: A Social-Climate Model of the Opinion Spectrum,"We developed a coupled social-climate network model to understand the interaction between climate change opinion spread and the climate system and determine the role of this interaction in shaping collective actions and global temperature changes. In contrast to previous social-climate models that discretized opinions, we assumed opinions on climate change form a continuum, and were thereby able to capture more nuanced interactions. The model shows that resistance to behaviour change, elevated mitigation costs, and slow response to climate events can result in a global temperature anomaly in excess of 2degC. However, this outcome could be avoided by lowering mitigation costs and increasing the rate of interactions between individuals with differing opinions (social learning). Our model is the first to demonstrate the emergence of opinion polarization in a human-environment system. We predict that polarization of opinions in a population can be extinguished, and the population will adopt mitigation practices, when the response to temperature change is sensitive, even at higher mitigation costs. It also indicates that even with polarized opinion, an average pro-mitigative opinion in the population can reduce emissions. Finally, our model underscores how frequent and unexpected social or environmental changes, such as policy changes or extreme weather events, can slow climate change mitigation. This analysis helps identify the factors that support achieving international climate goals, such as leveraging peer influence and decreasing stubbornness in individuals, reducing mitigation costs, and encouraging climate-friendly lifestyles. Our model offers a valuable new framework for exploring the integration of social and natural sciences, particularly in the domain of human behavioural change.","From Opinion Polarization to Climate Action: A Social-Climate Model of the Opinion Spectrum We developed a coupled social-climate network model to understand the interaction between climate change opinion spread and the climate system and determine the role of this interaction in shaping collective actions and global temperature changes. In contrast to previous social-climate models that discretized opinions, we assumed opinions on climate change form a continuum, and were thereby able to capture more nuanced interactions. The model shows that resistance to behaviour change, elevated mitigation costs, and slow response to climate events can result in a global temperature anomaly in excess of 2degC. However, this outcome could be avoided by lowering mitigation costs and increasing the rate of interactions between individuals with differing opinions (social learning). Our model is the first to demonstrate the emergence of opinion polarization in a human-environment system. We predict that polarization of opinions in a population can be extinguished, and the population will adopt mitigation practices, when the response to temperature change is sensitive, even at higher mitigation costs. It also indicates that even with polarized opinion, an average pro-mitigative opinion in the population can reduce emissions. Finally, our model underscores how frequent and unexpected social or environmental changes, such as policy changes or extreme weather events, can slow climate change mitigation. This analysis helps identify the factors that support achieving international climate goals, such as leveraging peer influence and decreasing stubbornness in individuals, reducing mitigation costs, and encouraging climate-friendly lifestyles. Our model offers a valuable new framework for exploring the integration of social and natural sciences, particularly in the domain of human behavioural change.",Environment
Pareto Optimal Projection Search (POPS): Automated Radiation Therapy Treatment Planning by Direct Search of the Pareto Surface,"Objective: Radiation therapy treatment planning is a time-consuming, iterative process with potentially high inter-planner variability. Fully automated treatment planning processes could reduce a planners active treatment planning time and remove inter-planner variability, with the potential to tremendously improve patient turnover and quality of care. In developing fully automated algorithms for treatment planning, we have two main objectives: to produce plans that are 1) Pareto optimal and 2) clinically acceptable. Here, we propose the Pareto optimal projection search (POPS) algorithm, which provides a general framework for directly searching the Pareto front. Methods: Our POPS algorithm is a novel automated planning method that combines two main search processes: 1) gradient-free search in the decision variable space and 2) projection of decision variables to the Pareto front using the bisection method. We demonstrate the performance of POPS by comparing with clinical treatment plans. As one possible quantitative measure of treatment plan quality, we construct a clinical acceptability scoring function (SF) modified from the previously developed general evaluation metric (GEM). Results: On a dataset of 21 prostate cases collected as part of clinical workflow, our proposed POPS algorithm produces Pareto optimal plans that are clinically acceptable in regards to dose conformity, dose homogeneity, and sparing of organs-at-risk. Conclusion: Our proposed POPS algorithm provides a general framework for fully automated treatment planning that achieves clinically acceptable dosimetric quality without requiring active planning from human planners. Significance: Our fully automated POPS algorithm addresses many key limitations of other automated planning approaches, and we anticipate that it will substantially improve treatment planning workflow.","Pareto Optimal Projection Search (POPS): Automated Radiation Therapy Treatment Planning by Direct Search of the Pareto Surface Objective: Radiation therapy treatment planning is a time-consuming, iterative process with potentially high inter-planner variability. Fully automated treatment planning processes could reduce a planners active treatment planning time and remove inter-planner variability, with the potential to tremendously improve patient turnover and quality of care. In developing fully automated algorithms for treatment planning, we have two main objectives: to produce plans that are 1) Pareto optimal and 2) clinically acceptable. Here, we propose the Pareto optimal projection search (POPS) algorithm, which provides a general framework for directly searching the Pareto front. Methods: Our POPS algorithm is a novel automated planning method that combines two main search processes: 1) gradient-free search in the decision variable space and 2) projection of decision variables to the Pareto front using the bisection method. We demonstrate the performance of POPS by comparing with clinical treatment plans. As one possible quantitative measure of treatment plan quality, we construct a clinical acceptability scoring function (SF) modified from the previously developed general evaluation metric (GEM). Results: On a dataset of 21 prostate cases collected as part of clinical workflow, our proposed POPS algorithm produces Pareto optimal plans that are clinically acceptable in regards to dose conformity, dose homogeneity, and sparing of organs-at-risk. Conclusion: Our proposed POPS algorithm provides a general framework for fully automated treatment planning that achieves clinically acceptable dosimetric quality without requiring active planning from human planners. Significance: Our fully automated POPS algorithm addresses many key limitations of other automated planning approaches, and we anticipate that it will substantially improve treatment planning workflow.",Healthcare
Personal Health Knowledge Graph for Clinically Relevant Diet Recommendations,"We propose a knowledge model for capturing dietary preferences and personal context to provide personalized dietary recommendations. We develop a knowledge model called the Personal Health Ontology, which is grounded in semantic technologies, and represents a patients combined medical information, social determinants of health, and observations of daily living elicited from interviews with diabetic patients. We then generate a personal health knowledge graph that captures temporal patterns from synthetic food logs, annotated with concepts from the Personal Health Ontology. We further discuss how lifestyle guidelines grounded in semantic technologies can be reasoned with the generated personal health knowledge graph to provide appropriate dietary recommendations that satisfy the users medical and other lifestyle needs.","Personal Health Knowledge Graph for Clinically Relevant Diet Recommendations We propose a knowledge model for capturing dietary preferences and personal context to provide personalized dietary recommendations. We develop a knowledge model called the Personal Health Ontology, which is grounded in semantic technologies, and represents a patients combined medical information, social determinants of health, and observations of daily living elicited from interviews with diabetic patients. We then generate a personal health knowledge graph that captures temporal patterns from synthetic food logs, annotated with concepts from the Personal Health Ontology. We further discuss how lifestyle guidelines grounded in semantic technologies can be reasoned with the generated personal health knowledge graph to provide appropriate dietary recommendations that satisfy the users medical and other lifestyle needs.",Healthcare
Training Reinforcement Neurocontrollers Using the Polytope Algorithm,A new training algorithm is presented for delayed reinforcement learning problems that does not assume the existence of a critic model and employs the polytope optimization algorithm to adjust the weights of the action network so that a simple direct measure of the training performance is maximized. Experimental results from the application of the method to the pole balancing problem indicate improved training performance compared with critic-based and genetic reinforcement approaches.,Training Reinforcement Neurocontrollers Using the Polytope Algorithm A new training algorithm is presented for delayed reinforcement learning problems that does not assume the existence of a critic model and employs the polytope optimization algorithm to adjust the weights of the action network so that a simple direct measure of the training performance is maximized. Experimental results from the application of the method to the pole balancing problem indicate improved training performance compared with critic-based and genetic reinforcement approaches.,Technology
Exploring cultural challenges to implementing Educational Technology in the higher education sector in India,"When learning technologies are introduced in educational environments, it is assumed that the educational environment is culture neutral i.e, all educational environments have the same challenges, problems and cultural norms. However, it can be observed that cultural factors can influence the successful implementation and use of learning technologies. In this study the aims were to explore contextual challenges to implementing different educational technologies and to explore the effects of culture. The results of this survey suggest that Hofstedes cultural measures of uncertainty avoidance, power distance and Individualistcollectivist measures, and Duckworths Grit measure of passion and perseverance have a strong impact on the culture of technology use in India.","Exploring cultural challenges to implementing Educational Technology in the higher education sector in India When learning technologies are introduced in educational environments, it is assumed that the educational environment is culture neutral i.e, all educational environments have the same challenges, problems and cultural norms. However, it can be observed that cultural factors can influence the successful implementation and use of learning technologies. In this study the aims were to explore contextual challenges to implementing different educational technologies and to explore the effects of culture. The results of this survey suggest that Hofstedes cultural measures of uncertainty avoidance, power distance and Individualistcollectivist measures, and Duckworths Grit measure of passion and perseverance have a strong impact on the culture of technology use in India.",Education
Optimizing semiconductor devices by self-organizing particle swarm,"A self-organizing particle swarm is presented. It works in dissipative state by employing the small inertia weight, according to experimental analysis on a simplified model, which with fast convergence. Then by recognizing and replacing inactive particles according to the process deviation information of device parameters, the fluctuation is introduced so as to driving the irreversible evolution process with better fitness. The testing on benchmark functions and an application example for device optimization with designed fitness function indicates it improves the performance effectively.","Optimizing semiconductor devices by self-organizing particle swarm A self-organizing particle swarm is presented. It works in dissipative state by employing the small inertia weight, according to experimental analysis on a simplified model, which with fast convergence. Then by recognizing and replacing inactive particles according to the process deviation information of device parameters, the fluctuation is introduced so as to driving the irreversible evolution process with better fitness. The testing on benchmark functions and an application example for device optimization with designed fitness function indicates it improves the performance effectively.",Technology
Augmenting deep neural networks with symbolic knowledge: Towards trustworthy and interpretable AI for education,"Artificial neural networks (ANNs) have shown to be amongst the most important artificial intelligence (AI) techniques in educational applications, providing adaptive educational services. However, their educational potential is limited in practice due to three major challenges: i) difficulty in incorporating symbolic educational knowledge (e.g., causal relationships, and practitioners knowledge) in their development, ii) learning and reflecting biases, and iii) lack of interpretability. Given the high-risk nature of education, the integration of educational knowledge into ANNs becomes crucial for developing AI applications that adhere to essential educational restrictions, and provide interpretability over the predictions. This research argues that the neural-symbolic family of AI has the potential to address the named challenges. To this end, it adapts a neural-symbolic AI framework and accordingly develops an approach called NSAI, that injects and extracts educational knowledge into and from deep neural networks, for modelling learners computational thinking. Our findings reveal that the NSAI approach has better generalizability compared to deep neural networks trained merely on training data, as well as training data augmented by SMOTE and autoencoder methods. More importantly, unlike the other models, the NSAI approach prioritises robust representations that capture causal relationships between input features and output labels, ensuring safety in learning to avoid spurious correlations and control biases in training data. Furthermore, the NSAI approach enables the extraction of rules from the learned network, facilitating interpretation and reasoning about the path to predictions, as well as refining the initial educational knowledge. These findings imply that neural-symbolic AI can overcome the limitations of ANNs in education, enabling trustworthy and interpretable applications.","Augmenting deep neural networks with symbolic knowledge: Towards trustworthy and interpretable AI for education Artificial neural networks (ANNs) have shown to be amongst the most important artificial intelligence (AI) techniques in educational applications, providing adaptive educational services. However, their educational potential is limited in practice due to three major challenges: i) difficulty in incorporating symbolic educational knowledge (e.g., causal relationships, and practitioners knowledge) in their development, ii) learning and reflecting biases, and iii) lack of interpretability. Given the high-risk nature of education, the integration of educational knowledge into ANNs becomes crucial for developing AI applications that adhere to essential educational restrictions, and provide interpretability over the predictions. This research argues that the neural-symbolic family of AI has the potential to address the named challenges. To this end, it adapts a neural-symbolic AI framework and accordingly develops an approach called NSAI, that injects and extracts educational knowledge into and from deep neural networks, for modelling learners computational thinking. Our findings reveal that the NSAI approach has better generalizability compared to deep neural networks trained merely on training data, as well as training data augmented by SMOTE and autoencoder methods. More importantly, unlike the other models, the NSAI approach prioritises robust representations that capture causal relationships between input features and output labels, ensuring safety in learning to avoid spurious correlations and control biases in training data. Furthermore, the NSAI approach enables the extraction of rules from the learned network, facilitating interpretation and reasoning about the path to predictions, as well as refining the initial educational knowledge. These findings imply that neural-symbolic AI can overcome the limitations of ANNs in education, enabling trustworthy and interpretable applications.",Education
The Role of Uncertainty in Controlling Climate Change,"Integrated Assessment Models (IAMs) of the climate and economy aim to analyze the impact and efficacy of policies that aim to control climate change, such as carbon taxes and subsidies. A major characteristic of IAMs is that their geophysical sector determines the mean surface temperature increase over the preindustrial level, which in turn determines the damage function. Most of the existing IAMs are perfect-foresight forward-looking models, assuming that we know all of the future information. However, there are significant uncertainties in the climate and economic system, including parameter uncertainty, model uncertainty, climate tipping risks, economic risks, and ambiguity. For example, climate damages are uncertain: some researchers assume that climate damages are proportional to instantaneous output, while others assume that climate damages have a more persistent impact on economic growth. Climate tipping risks represent (nearly) irreversible climate events that may lead to significant changes in the climate system, such as the Greenland ice sheet collapse, while the conditions, probability of tipping, duration, and associated damage are also uncertain. Technological progress in carbon capture and storage, adaptation, renewable energy, and energy efficiency are uncertain too. In the face of these uncertainties, policymakers have to provide a decision that considers important factors such as risk aversion, inequality aversion, and sustainability of the economy and ecosystem. Solving this problem may require richer and more realistic models than standard IAMs, and advanced computational methods. The recent literature has shown that these uncertainties can be incorporated into IAMs and may change optimal climate policies significantly.","The Role of Uncertainty in Controlling Climate Change Integrated Assessment Models (IAMs) of the climate and economy aim to analyze the impact and efficacy of policies that aim to control climate change, such as carbon taxes and subsidies. A major characteristic of IAMs is that their geophysical sector determines the mean surface temperature increase over the preindustrial level, which in turn determines the damage function. Most of the existing IAMs are perfect-foresight forward-looking models, assuming that we know all of the future information. However, there are significant uncertainties in the climate and economic system, including parameter uncertainty, model uncertainty, climate tipping risks, economic risks, and ambiguity. For example, climate damages are uncertain: some researchers assume that climate damages are proportional to instantaneous output, while others assume that climate damages have a more persistent impact on economic growth. Climate tipping risks represent (nearly) irreversible climate events that may lead to significant changes in the climate system, such as the Greenland ice sheet collapse, while the conditions, probability of tipping, duration, and associated damage are also uncertain. Technological progress in carbon capture and storage, adaptation, renewable energy, and energy efficiency are uncertain too. In the face of these uncertainties, policymakers have to provide a decision that considers important factors such as risk aversion, inequality aversion, and sustainability of the economy and ecosystem. Solving this problem may require richer and more realistic models than standard IAMs, and advanced computational methods. The recent literature has shown that these uncertainties can be incorporated into IAMs and may change optimal climate policies significantly.",Environment
The Two Subcultures: The teaching of theory and Physics place in the college curriculum,"During recent collaboration with colleagues to revise our institutions general-education curriculum, I encountered many perceptions of what we mean by the Natural Sciences. I was surprised to find that perceptions of scientific pedagogy varied significantly among the scientific disciplines, especially concerning issues of philosophy of science and epistemology, manifested in the approaches to teaching theoretical concepts and their development. These realizations suggest that Physics occupies a singular role in college curricula, introducing students, even at the introductory level, to the acquisition of knowledge by theoretical means and the assessment of theory based on experimental evidence.","The Two Subcultures: The teaching of theory and Physics place in the college curriculum During recent collaboration with colleagues to revise our institutions general-education curriculum, I encountered many perceptions of what we mean by the Natural Sciences. I was surprised to find that perceptions of scientific pedagogy varied significantly among the scientific disciplines, especially concerning issues of philosophy of science and epistemology, manifested in the approaches to teaching theoretical concepts and their development. These realizations suggest that Physics occupies a singular role in college curricula, introducing students, even at the introductory level, to the acquisition of knowledge by theoretical means and the assessment of theory based on experimental evidence.",Education
CADe tools for early detection of breast cancer,"A breast neoplasia is often marked by the presence of microcalcifications and massive lesions in the mammogram: hence the need for tools able to recognize such lesions at an early stage. Our collaboration, among italian physicists and radiologists, has built a large distributed database of digitized mammographic images and has developed a Computer Aided Detection (CADe) system for the automatic analysis of mammographic images and installed it in some Italian hospitals by a GRID connection. Regarding microcalcifications, in our CADe digital mammogram is divided into wide windows which are processed by a convolution filter; after a self-organizing map analyzes each window and produces 8 principal components which are used as input of a neural network (FFNN) able to classify the windows matched to a threshold. Regarding massive lesions we select all important maximum intensity position and define the ROI radius. From each ROI found we extract the parameters which are used as input in a FFNN to distinguish between pathological and non-pathological ROI. We present here a test of our CADe system, used as a second reader and a comparison with another (commercial) CADe system.","CADe tools for early detection of breast cancer A breast neoplasia is often marked by the presence of microcalcifications and massive lesions in the mammogram: hence the need for tools able to recognize such lesions at an early stage. Our collaboration, among italian physicists and radiologists, has built a large distributed database of digitized mammographic images and has developed a Computer Aided Detection (CADe) system for the automatic analysis of mammographic images and installed it in some Italian hospitals by a GRID connection. Regarding microcalcifications, in our CADe digital mammogram is divided into wide windows which are processed by a convolution filter; after a self-organizing map analyzes each window and produces 8 principal components which are used as input of a neural network (FFNN) able to classify the windows matched to a threshold. Regarding massive lesions we select all important maximum intensity position and define the ROI radius. From each ROI found we extract the parameters which are used as input in a FFNN to distinguish between pathological and non-pathological ROI. We present here a test of our CADe system, used as a second reader and a comparison with another (commercial) CADe system.",Healthcare
Can ESG Investment and the Implementation of the New Environmental Protection Law Enhance Public Subjective Well-being?,"Air pollution has emerged as a serious challenge for China, posing a threat to public health and hindering the progress of sustainable economic development. In response to air pollution and other environmental issues, the Chinese government introduced a new Environmental Protection Law in 2015. This paper investigates the impact of the new Environmental Protection Laws implementation and corporate Environmental, Social, and Governance (ESG) investments on air pollution and public subjective well-being. Using panel data at the macro level, we employ a difference-in-differences (DID) model, with Chinese provinces and municipalities as units of analysis, to examine the combined effects of the new Environmental Protection Law and changes in corporate ESG investment intensity. The study evaluates their impacts on air quality and public subjective well-being. Findings indicate that these policies and investment behaviors significantly improve public subjective well-being by reducing air pollution. Notably, an increase in ESG investment significantly reduces air pollution levels and is positively associated with enhanced well-being. These results underscore the critical role of environmental legislation and corporate social responsibility in improving public quality of life and provide empirical support for promoting sustainable development in China and beyond.","Can ESG Investment and the Implementation of the New Environmental Protection Law Enhance Public Subjective Well-being? Air pollution has emerged as a serious challenge for China, posing a threat to public health and hindering the progress of sustainable economic development. In response to air pollution and other environmental issues, the Chinese government introduced a new Environmental Protection Law in 2015. This paper investigates the impact of the new Environmental Protection Laws implementation and corporate Environmental, Social, and Governance (ESG) investments on air pollution and public subjective well-being. Using panel data at the macro level, we employ a difference-in-differences (DID) model, with Chinese provinces and municipalities as units of analysis, to examine the combined effects of the new Environmental Protection Law and changes in corporate ESG investment intensity. The study evaluates their impacts on air quality and public subjective well-being. Findings indicate that these policies and investment behaviors significantly improve public subjective well-being by reducing air pollution. Notably, an increase in ESG investment significantly reduces air pollution levels and is positively associated with enhanced well-being. These results underscore the critical role of environmental legislation and corporate social responsibility in improving public quality of life and provide empirical support for promoting sustainable development in China and beyond.",Finance
Graph kernels based on tree patterns for molecules,"Motivated by chemical applications, we revisit and extend a family of positive definite kernels for graphs based on the detection of common subtrees, initially proposed by Ramon et al. (2003). We propose new kernels with a parameter to control the complexity of the subtrees used as features to represent the graphs. This parameter allows to smoothly interpolate between classical graph kernels based on the count of common walks, on the one hand, and kernels that emphasize the detection of large common subtrees, on the other hand. We also propose two modular extensions to this formulation. The first extension increases the number of subtrees that define the feature space, and the second one removes noisy features from the graph representations. We validate experimentally these new kernels on binary classification tasks consisting in discriminating toxic and non-toxic molecules with support vector machines.","Graph kernels based on tree patterns for molecules Motivated by chemical applications, we revisit and extend a family of positive definite kernels for graphs based on the detection of common subtrees, initially proposed by Ramon et al. (2003). We propose new kernels with a parameter to control the complexity of the subtrees used as features to represent the graphs. This parameter allows to smoothly interpolate between classical graph kernels based on the count of common walks, on the one hand, and kernels that emphasize the detection of large common subtrees, on the other hand. We also propose two modular extensions to this formulation. The first extension increases the number of subtrees that define the feature space, and the second one removes noisy features from the graph representations. We validate experimentally these new kernels on binary classification tasks consisting in discriminating toxic and non-toxic molecules with support vector machines.",Healthcare
On the Use of a Wider Class of Linear Systems for the Design of Constant-Coefficients Semi-Implicit Time-Schemes in NWP,"The linearization of the meteorological equations around a specified reference state, usually applied in NWP to define the linear system of constant-coefficients semi-implicit schemes, is outlined as an unnecessarily restrictive approach which may be detrimental in terms of stability. It is shown theoretically that an increased robustness can sometimes be obtained by choosing the reference linear system in a wider set of possibilities. The potential benefits of this new approach are illustrated in two simple examples. The advantage in robustness is not obtained at the price of an increased error or complexity.","On the Use of a Wider Class of Linear Systems for the Design of Constant-Coefficients Semi-Implicit Time-Schemes in NWP The linearization of the meteorological equations around a specified reference state, usually applied in NWP to define the linear system of constant-coefficients semi-implicit schemes, is outlined as an unnecessarily restrictive approach which may be detrimental in terms of stability. It is shown theoretically that an increased robustness can sometimes be obtained by choosing the reference linear system in a wider set of possibilities. The potential benefits of this new approach are illustrated in two simple examples. The advantage in robustness is not obtained at the price of an increased error or complexity.",Environment
Towards Outcome-Driven Patient Subgroups: A Machine Learning Analysis Across Six Depression Treatment Studies,"Major depressive disorder (MDD) is a heterogeneous condition; multiple underlying neurobiological substrates could be associated with treatment response variability. Understanding the sources of this variability and predicting outcomes has been elusive. Machine learning has shown promise in predicting treatment response in MDD, but one limitation has been the lack of clinical interpretability of machine learning models. We analyzed data from six clinical trials of pharmacological treatment for depression (total n  5438) using the Differential Prototypes Neural Network (DPNN), a neural network model that derives patient prototypes which can be used to derive treatment-relevant patient clusters while learning to generate probabilities for differential treatment response. A model classifying remission and outputting individual remission probabilities for five first-line monotherapies and three combination treatments was trained using clinical and demographic data. Model validity and clinical utility were measured based on area under the curve (AUC) and expected improvement in sample remission rate with model-guided treatment, respectively. Post-hoc analyses yielded clusters (subgroups) based on patient prototypes learned during training. Prototypes were evaluated for interpretability by assessing differences in feature distributions and treatment-specific outcomes. A 3-prototype model achieved an AUC of 0.66 and an expected absolute improvement in population remission rate compared to the sample remission rate. We identified three treatment-relevant patient clusters which were clinically interpretable. It is possible to produce novel treatment-relevant patient profiles using machine learning models; doing so may improve precision medicine for depression. Note: This model is not currently the subject of any active clinical trials and is not intended for clinical use.","Towards Outcome-Driven Patient Subgroups: A Machine Learning Analysis Across Six Depression Treatment Studies Major depressive disorder (MDD) is a heterogeneous condition; multiple underlying neurobiological substrates could be associated with treatment response variability. Understanding the sources of this variability and predicting outcomes has been elusive. Machine learning has shown promise in predicting treatment response in MDD, but one limitation has been the lack of clinical interpretability of machine learning models. We analyzed data from six clinical trials of pharmacological treatment for depression (total n  5438) using the Differential Prototypes Neural Network (DPNN), a neural network model that derives patient prototypes which can be used to derive treatment-relevant patient clusters while learning to generate probabilities for differential treatment response. A model classifying remission and outputting individual remission probabilities for five first-line monotherapies and three combination treatments was trained using clinical and demographic data. Model validity and clinical utility were measured based on area under the curve (AUC) and expected improvement in sample remission rate with model-guided treatment, respectively. Post-hoc analyses yielded clusters (subgroups) based on patient prototypes learned during training. Prototypes were evaluated for interpretability by assessing differences in feature distributions and treatment-specific outcomes. A 3-prototype model achieved an AUC of 0.66 and an expected absolute improvement in population remission rate compared to the sample remission rate. We identified three treatment-relevant patient clusters which were clinically interpretable. It is possible to produce novel treatment-relevant patient profiles using machine learning models; doing so may improve precision medicine for depression. Note: This model is not currently the subject of any active clinical trials and is not intended for clinical use.",Healthcare
"Uncovering shifts in the history of Physics education: a systematic, NLP-based, thematic analysis of articles from The Physics Teacher and Physics Education journals (1966-2019)","This study explores the thematic evolution of articles in The Physics Teacher and Physics Education journals, over a critical period in modern history, from the Cold War era to the pre-pandemic world (1966 - 2019). Using an NLP-based inductive topic modeling approach, we identify recurring themes that have shaped the physics education literature, including content-based topics, teaching methodologies, laboratory practices, curriculum development, and the influence of Physics Education Research (PER). Our findings reveal both overarching trends and distinct thematic preferences between the journals. Physics Education has historically emphasized curriculum structures, social aspects of education, and interdisciplinary connections, whereas The Physics Teacher has focused more on pedagogical strategies, demonstrations, and practical teaching tools. Over the past three decades, both journals have increasingly incorporated discussions on technology, computation, and PER-driven instructional practices. By tracing these developments over five decades, this study provides a broader perspective on how physics education has responded to changing educational priorities, technological advancements, and research developments.","Uncovering shifts in the history of Physics education: a systematic, NLP-based, thematic analysis of articles from The Physics Teacher and Physics Education journals (1966-2019) This study explores the thematic evolution of articles in The Physics Teacher and Physics Education journals, over a critical period in modern history, from the Cold War era to the pre-pandemic world (1966 - 2019). Using an NLP-based inductive topic modeling approach, we identify recurring themes that have shaped the physics education literature, including content-based topics, teaching methodologies, laboratory practices, curriculum development, and the influence of Physics Education Research (PER). Our findings reveal both overarching trends and distinct thematic preferences between the journals. Physics Education has historically emphasized curriculum structures, social aspects of education, and interdisciplinary connections, whereas The Physics Teacher has focused more on pedagogical strategies, demonstrations, and practical teaching tools. Over the past three decades, both journals have increasingly incorporated discussions on technology, computation, and PER-driven instructional practices. By tracing these developments over five decades, this study provides a broader perspective on how physics education has responded to changing educational priorities, technological advancements, and research developments.",Education
Combinatorial rules of icosahedral capsid growth,"A model of growth of icosahedral viral capsids is proposed. It takes into account the diversity of hexamers compositions, leading to definite capsid size. We show that the observed yield of capsid production implies a very high level of self-organization of elementary building blocks. The exact number of different protein dimers composing hexamers is related to the size of a given capsid, labeled by its T-number. Simple rules determining these numbers for each value of T are deduced and certain consequences are discussed.","Combinatorial rules of icosahedral capsid growth A model of growth of icosahedral viral capsids is proposed. It takes into account the diversity of hexamers compositions, leading to definite capsid size. We show that the observed yield of capsid production implies a very high level of self-organization of elementary building blocks. The exact number of different protein dimers composing hexamers is related to the size of a given capsid, labeled by its T-number. Simple rules determining these numbers for each value of T are deduced and certain consequences are discussed.",Healthcare
Global warming: What does the data tell us?,"We analyze global surface temperature data obtained at 13472 weather stations from the year 1702 to 1990. The mean annual temperature of a station fluctuates from year to year by typically -0.6oC (one standard deviation). Superimposed on this fluctuation is a linear increase of the temperature by typically 0.40oC per century ever since reliable data is available, i.e. since 1702. The world population has doubled from 1952 to 1990, yet we see no statistically significant acceleration of global warming in this period. We conclude that the effect of humankind on global warming up to 1990 is 0.0 - 0.1oC.","Global warming: What does the data tell us? We analyze global surface temperature data obtained at 13472 weather stations from the year 1702 to 1990. The mean annual temperature of a station fluctuates from year to year by typically -0.6oC (one standard deviation). Superimposed on this fluctuation is a linear increase of the temperature by typically 0.40oC per century ever since reliable data is available, i.e. since 1702. The world population has doubled from 1952 to 1990, yet we see no statistically significant acceleration of global warming in this period. We conclude that the effect of humankind on global warming up to 1990 is 0.0 - 0.1oC.",Environment
An architecture for massive parallelization of the compact genetic algorithm,"This paper presents an architecture which is suitable for a massive parallelization of the compact genetic algorithm. The resulting scheme has three major advantages. First, it has low synchronization costs. Second, it is fault tolerant, and third, it is scalable. The paper argues that the benefits that can be obtained with the proposed approach is potentially higher than those obtained with traditional parallel genetic algorithms. In addition, the ideas suggested in the paper may also be relevant towards parallelizing more complex probabilistic model building genetic algorithms.","An architecture for massive parallelization of the compact genetic algorithm This paper presents an architecture which is suitable for a massive parallelization of the compact genetic algorithm. The resulting scheme has three major advantages. First, it has low synchronization costs. Second, it is fault tolerant, and third, it is scalable. The paper argues that the benefits that can be obtained with the proposed approach is potentially higher than those obtained with traditional parallel genetic algorithms. In addition, the ideas suggested in the paper may also be relevant towards parallelizing more complex probabilistic model building genetic algorithms.",Technology
"The Perceptron Algorithm: Image and Signal Decomposition, Compression, and Analysis by Iterative Gaussian Blurring","A novel algorithm for tunable compression to within the precision of reproduction targets, or storage, is proposed. The new algorithm is termed the Perceptron Algorithm, which utilises simple existing concepts in a novel way, has multiple immediate commercial application aspects as well as it opens up a multitude of fronts in computational science and technology. The aims of this paper are to present the concepts underlying the algorithm, observations by its application to some example cases, and the identification of a multitude of potential areas of applications such as: image compression by orders of magnitude, signal compression including sound as well, image analysis in a multilayered detailed analysis, pattern recognition and matching and rapid database searching (e.g. face recognition), motion analysis, biomedical applications e.g. in MRI and CAT scan image analysis and compression, as well as hints on the link of these ideas to the way how biological memory might work leading to new points of view in neural computation. Commercial applications of immediate interest are the compression of images at the source (e.g. photographic equipment, scanners, satellite imaging systems), DVD film compression, pay-per-view downloads acceleration and many others identified in the present paper at its conclusion and future work section.","The Perceptron Algorithm: Image and Signal Decomposition, Compression, and Analysis by Iterative Gaussian Blurring A novel algorithm for tunable compression to within the precision of reproduction targets, or storage, is proposed. The new algorithm is termed the Perceptron Algorithm, which utilises simple existing concepts in a novel way, has multiple immediate commercial application aspects as well as it opens up a multitude of fronts in computational science and technology. The aims of this paper are to present the concepts underlying the algorithm, observations by its application to some example cases, and the identification of a multitude of potential areas of applications such as: image compression by orders of magnitude, signal compression including sound as well, image analysis in a multilayered detailed analysis, pattern recognition and matching and rapid database searching (e.g. face recognition), motion analysis, biomedical applications e.g. in MRI and CAT scan image analysis and compression, as well as hints on the link of these ideas to the way how biological memory might work leading to new points of view in neural computation. Commercial applications of immediate interest are the compression of images at the source (e.g. photographic equipment, scanners, satellite imaging systems), DVD film compression, pay-per-view downloads acceleration and many others identified in the present paper at its conclusion and future work section.",Technology
Modeling of the Labour Force Redistribution in Investment Projects with Account of their Delay,"The mathematical model of the labour force redistribution in investment projects is presented in the article. The redistribution mode of funds, labour force in particular, according to the equal risk approach applied to the loss of some assets due to delay in all the investment projects is provided in the model. The sample of the developed model for three investment projects with the specified labour force volumes and their defined unit costs at the particular moment is given.","Modeling of the Labour Force Redistribution in Investment Projects with Account of their Delay The mathematical model of the labour force redistribution in investment projects is presented in the article. The redistribution mode of funds, labour force in particular, according to the equal risk approach applied to the loss of some assets due to delay in all the investment projects is provided in the model. The sample of the developed model for three investment projects with the specified labour force volumes and their defined unit costs at the particular moment is given.",Finance
Knowledge Base Completion for Constructing Problem-Oriented Medical Records,"Both electronic health records and personal health records are typically organized by data type, with medical problems, medications, procedures, and laboratory results chronologically sorted in separate areas of the chart. As a result, it can be difficult to find all of the relevant information for answering a clinical question about a given medical problem. A promising alternative is to instead organize by problems, with related medications, procedures, and other pertinent information all grouped together. A recent effort by Buchanan (2017) manually defined, through expert consensus, 11 medical problems and the relevant labs and medications for each. We show how to use machine learning on electronic health records to instead automatically construct these problem-based groupings of relevant medications, procedures, and laboratory tests. We formulate the learning task as one of knowledge base completion, and annotate a dataset that expands the set of problems from 11 to 32. We develop a model architecture that exploits both pre-trained concept embeddings and usage data relating the concepts contained in a longitudinal dataset from a large health system. We evaluate our algorithms ability to suggest relevant medications, procedures, and lab tests, and find that the approach provides feasible suggestions even for problems that are hidden during training. The dataset, along with code to reproduce our results, is available at https:github.comasappresearchkbc-pomr.","Knowledge Base Completion for Constructing Problem-Oriented Medical Records Both electronic health records and personal health records are typically organized by data type, with medical problems, medications, procedures, and laboratory results chronologically sorted in separate areas of the chart. As a result, it can be difficult to find all of the relevant information for answering a clinical question about a given medical problem. A promising alternative is to instead organize by problems, with related medications, procedures, and other pertinent information all grouped together. A recent effort by Buchanan (2017) manually defined, through expert consensus, 11 medical problems and the relevant labs and medications for each. We show how to use machine learning on electronic health records to instead automatically construct these problem-based groupings of relevant medications, procedures, and laboratory tests. We formulate the learning task as one of knowledge base completion, and annotate a dataset that expands the set of problems from 11 to 32. We develop a model architecture that exploits both pre-trained concept embeddings and usage data relating the concepts contained in a longitudinal dataset from a large health system. We evaluate our algorithms ability to suggest relevant medications, procedures, and lab tests, and find that the approach provides feasible suggestions even for problems that are hidden during training. The dataset, along with code to reproduce our results, is available at https:github.comasappresearchkbc-pomr.",Healthcare
On Evaluation of Risky Investment Projects. Investment Certainty Equivalence,"The purpose of the study is to propose a methodology for evaluation and ranking of risky investment projects.An investment certainty equivalence approach dual to the conventional separation of riskless and risky contributions based on cash flow certainty equivalence is introduced. Proposed ranking of investment projects is based on gauging them with the Omega measure, which is defined as the ratio of chances to obtain profitreturn greater than some critical (minimal acceptable) profitability over the chances to obtain the profitreturn less than the critical one.Detailed consideration of alternative riskless investment is presented. Various performance measures characterizing investment projects with a special focus on the role of reinvestment are discussed. Relation between the proposed methodology and the conventional approach based on utilization of risk-adjusted discount rate (RADR) is discussed. Findings are supported with an illustrative example.The methodology proposed can be used to rank projects of different nature, scale and lifespan. In contrast to the conventional RADR approach for investment project evaluation, in the proposed method a risk profile of a specific project is explicitly analyzed in terms of appropriate performance measure distribution. No ad-hoc assumption about suitable risk-premium is made.","On Evaluation of Risky Investment Projects. Investment Certainty Equivalence The purpose of the study is to propose a methodology for evaluation and ranking of risky investment projects.An investment certainty equivalence approach dual to the conventional separation of riskless and risky contributions based on cash flow certainty equivalence is introduced. Proposed ranking of investment projects is based on gauging them with the Omega measure, which is defined as the ratio of chances to obtain profitreturn greater than some critical (minimal acceptable) profitability over the chances to obtain the profitreturn less than the critical one.Detailed consideration of alternative riskless investment is presented. Various performance measures characterizing investment projects with a special focus on the role of reinvestment are discussed. Relation between the proposed methodology and the conventional approach based on utilization of risk-adjusted discount rate (RADR) is discussed. Findings are supported with an illustrative example.The methodology proposed can be used to rank projects of different nature, scale and lifespan. In contrast to the conventional RADR approach for investment project evaluation, in the proposed method a risk profile of a specific project is explicitly analyzed in terms of appropriate performance measure distribution. No ad-hoc assumption about suitable risk-premium is made.",Finance
Implementing Sustainable Tourism practices in luxury resorts of Maldives: Sustainability principles  Tripple Bottomline Approach,"The aim of the research paper is to understand the sustainability challenges faced by resorts mainly luxury in Maldives and to implement the sustainable tourism practices. The Maldives economy is dependent mostly on the fishing, boat building, boat repairing and tourism. Over recent years there is a drastic change that has took place in Maldives in tourism industry. Maldives has progressed to be the upper middle-income country and luxury resorts are the reason for increased GDP in the country. Although there are some practices associated with the luxury resorts to follow in terms of environmental concerns. Present study focuses on the triple bottom line approach and the 12 major Sustainable Tourism Principles as a framework for sustainability practices and its implementation including the challenges associated in Maldives. The paper suggests some recommendations on several paradigm of enforcing laws and regulations, waste management facilities, fostering collaboration along with promoting local agriculture. The study also contemplates on several other areas such as on the impact of sustainability initiatives, coral restoration, and the use of sustainable supply chains. The intent of the current research is to suggest methods to promote the sustainable practices in luxury resort in Maldives.","Implementing Sustainable Tourism practices in luxury resorts of Maldives: Sustainability principles  Tripple Bottomline Approach The aim of the research paper is to understand the sustainability challenges faced by resorts mainly luxury in Maldives and to implement the sustainable tourism practices. The Maldives economy is dependent mostly on the fishing, boat building, boat repairing and tourism. Over recent years there is a drastic change that has took place in Maldives in tourism industry. Maldives has progressed to be the upper middle-income country and luxury resorts are the reason for increased GDP in the country. Although there are some practices associated with the luxury resorts to follow in terms of environmental concerns. Present study focuses on the triple bottom line approach and the 12 major Sustainable Tourism Principles as a framework for sustainability practices and its implementation including the challenges associated in Maldives. The paper suggests some recommendations on several paradigm of enforcing laws and regulations, waste management facilities, fostering collaboration along with promoting local agriculture. The study also contemplates on several other areas such as on the impact of sustainability initiatives, coral restoration, and the use of sustainable supply chains. The intent of the current research is to suggest methods to promote the sustainable practices in luxury resort in Maldives.",Environment
Appraisal of a contour integral method for the Black-Scholes and Heston equations,"A contour integral method recently proposed by Weideman IMA J. Numer. Anal., to appear for integrating semi-discrete advection-diffusion PDEs, is extended for application to some of the important equations of mathematical finance. Using estimates for the numerical range of the spatial operator, optimal contour parameters are derived theoretically and tested numerically. Test examples presented are the Black-Scholes PDE in one space dimension and the Heston PDE in two dimensions. In the latter case efficiency is compared to ADI splitting schemes for solving this problem. In the examples it is found that the contour integral method is superior for the range of medium to high accuracy requirements. Further improvements to the current implementation of the contour integral method are suggested.","Appraisal of a contour integral method for the Black-Scholes and Heston equations A contour integral method recently proposed by Weideman IMA J. Numer. Anal., to appear for integrating semi-discrete advection-diffusion PDEs, is extended for application to some of the important equations of mathematical finance. Using estimates for the numerical range of the spatial operator, optimal contour parameters are derived theoretically and tested numerically. Test examples presented are the Black-Scholes PDE in one space dimension and the Heston PDE in two dimensions. In the latter case efficiency is compared to ADI splitting schemes for solving this problem. In the examples it is found that the contour integral method is superior for the range of medium to high accuracy requirements. Further improvements to the current implementation of the contour integral method are suggested.",Finance
Linear waves and baroclinic instability in an inhomogeneous-density layered primitive-equation ocean model,"We consider a multilayer generalization of Ripas inhomogeneous-density single-layer primitive-equation model. In addition to vary arbitrarily in horizontal position and time, the horizontal velocity and buoyancy fields are allowed to vary linearly with depth within each layer of the model. Preliminary results on linear waves and baroclinic instability suggest that a configuration involving a few layers may set the basis for a quite accurate and numerically efficient ocean model.","Linear waves and baroclinic instability in an inhomogeneous-density layered primitive-equation ocean model We consider a multilayer generalization of Ripas inhomogeneous-density single-layer primitive-equation model. In addition to vary arbitrarily in horizontal position and time, the horizontal velocity and buoyancy fields are allowed to vary linearly with depth within each layer of the model. Preliminary results on linear waves and baroclinic instability suggest that a configuration involving a few layers may set the basis for a quite accurate and numerically efficient ocean model.",Environment
Singular vector ensemble forecasting systems and the prediction of flow dependent uncertainty,The ECMWF ensemble weather forecasts are generated by perturbing the initial conditions of the forecast using a subset of the singular vectors of the linearised propagator. Previous results show that when creating probabilistic forecasts from this ensemble better forecasts are obtained if the mean of the spread and the variability of the spread are calibrated separately. We show results from a simple linear model that suggest that this may be a generic property for all singular vector based ensemble forecasting systems based on only a subset of the full set of singular vectors.,Singular vector ensemble forecasting systems and the prediction of flow dependent uncertainty The ECMWF ensemble weather forecasts are generated by perturbing the initial conditions of the forecast using a subset of the singular vectors of the linearised propagator. Previous results show that when creating probabilistic forecasts from this ensemble better forecasts are obtained if the mean of the spread and the variability of the spread are calibrated separately. We show results from a simple linear model that suggest that this may be a generic property for all singular vector based ensemble forecasting systems based on only a subset of the full set of singular vectors.,Environment
Sustainability in Software Product Lines: Report on Discussion Panel at SPLC 2014,"Sustainability (defined as the capacity to keep up) encompasses a wide set of aims: ranging from energy efficient software products (environmental sustainability), reduction of software development and maintenance costs (economic sustainability), to employee and end-user wellbeing (social sustainability). In this report we explore the role that sustainability plays in software product line engineering (SPL). The report is based on the Sustainability in Software Product Lines panel held at SPLC 2014.","Sustainability in Software Product Lines: Report on Discussion Panel at SPLC 2014 Sustainability (defined as the capacity to keep up) encompasses a wide set of aims: ranging from energy efficient software products (environmental sustainability), reduction of software development and maintenance costs (economic sustainability), to employee and end-user wellbeing (social sustainability). In this report we explore the role that sustainability plays in software product line engineering (SPL). The report is based on the Sustainability in Software Product Lines panel held at SPLC 2014.",Environment
Lets Get Ready to Rumble: Crossover Versus Mutation Head to Head,"This paper analyzes the relative advantages between crossover and mutation on a class of deterministic and stochastic additively separable problems. This study assumes that the recombination and mutation operators have the knowledge of the building blocks (BBs) and effectively exchange or search among competing BBs. Facetwise models of convergence time and population sizing have been used to determine the scalability of each algorithm. The analysis shows that for additively separable deterministic problems, the BB-wise mutation is more efficient than crossover, while the crossover outperforms the mutation on additively separable problems perturbed with additive Gaussian noise. The results show that the speed-up of using BB-wise mutation on deterministic problems is O(k0.5logm), where k is the BB size, and m is the number of BBs. Likewise, the speed-up of using crossover on stochastic problems with fixed noise variance is O(mk0.5log m).","Lets Get Ready to Rumble: Crossover Versus Mutation Head to Head This paper analyzes the relative advantages between crossover and mutation on a class of deterministic and stochastic additively separable problems. This study assumes that the recombination and mutation operators have the knowledge of the building blocks (BBs) and effectively exchange or search among competing BBs. Facetwise models of convergence time and population sizing have been used to determine the scalability of each algorithm. The analysis shows that for additively separable deterministic problems, the BB-wise mutation is more efficient than crossover, while the crossover outperforms the mutation on additively separable problems perturbed with additive Gaussian noise. The results show that the speed-up of using BB-wise mutation on deterministic problems is O(k0.5logm), where k is the BB size, and m is the number of BBs. Likewise, the speed-up of using crossover on stochastic problems with fixed noise variance is O(mk0.5log m).",Technology
An Integrated Framework for Learning and Reasoning,"Learning and reasoning are both aspects of what is considered to be intelligence. Their studies within AI have been separated historically, learning being the topic of machine learning and neural networks, and reasoning falling under classical (or symbolic) AI. However, learning and reasoning are in many ways interdependent. This paper discusses the nature of some of these interdependencies and proposes a general framework called FLARE, that combines inductive learning using prior knowledge together with reasoning in a propositional setting. Several examples that test the framework are presented, including classical induction, many important reasoning protocols and two simple expert systems.","An Integrated Framework for Learning and Reasoning Learning and reasoning are both aspects of what is considered to be intelligence. Their studies within AI have been separated historically, learning being the topic of machine learning and neural networks, and reasoning falling under classical (or symbolic) AI. However, learning and reasoning are in many ways interdependent. This paper discusses the nature of some of these interdependencies and proposes a general framework called FLARE, that combines inductive learning using prior knowledge together with reasoning in a propositional setting. Several examples that test the framework are presented, including classical induction, many important reasoning protocols and two simple expert systems.",Technology
Pre-Training With Scientific Text Improves Educational Question Generation,"With the boom of digital educational materials and scalable e-learning systems, the potential for realising AI-assisted personalised learning has skyrocketed. In this landscape, the automatic generation of educational questions will play a key role, enabling scalable self-assessment when a global population is manoeuvring their personalised learning journeys. We develop EduQG, a novel educational question generation model built by adapting a large language model. Our initial experiments demonstrate that EduQG can produce superior educational questions by pre-training on scientific text.","Pre-Training With Scientific Text Improves Educational Question Generation With the boom of digital educational materials and scalable e-learning systems, the potential for realising AI-assisted personalised learning has skyrocketed. In this landscape, the automatic generation of educational questions will play a key role, enabling scalable self-assessment when a global population is manoeuvring their personalised learning journeys. We develop EduQG, a novel educational question generation model built by adapting a large language model. Our initial experiments demonstrate that EduQG can produce superior educational questions by pre-training on scientific text.",Education
The Problem of Modeling of Economic Dynamics,"The correctness of Harrods model in the differential form is studied. The inadequacy of exponential growth of economy is shown; an alternative result is obtained. By example of Phillips model, an approach to correction of macroeconomic models (in terms of initial prerequisites) is generalized. A methodology based on balance relations for modelling of economic dynamics, including obtaining forecast estimates, is developed. The problems thus considered are reduced to the solution of Volterra and Fredholm integral equations of the second kind.","The Problem of Modeling of Economic Dynamics The correctness of Harrods model in the differential form is studied. The inadequacy of exponential growth of economy is shown; an alternative result is obtained. By example of Phillips model, an approach to correction of macroeconomic models (in terms of initial prerequisites) is generalized. A methodology based on balance relations for modelling of economic dynamics, including obtaining forecast estimates, is developed. The problems thus considered are reduced to the solution of Volterra and Fredholm integral equations of the second kind.",Finance
Reducing BESS Capacity for Accommodating Renewables in Subtransmission Systems with Power Flow Routers,"Widespread utilization of renewable energy sources (RESs) in subtransmission systems causes serious problems on power quality, such as voltage violations, leading to significant curtailment of renewables. This is due to the inherent variability of renewables and the high RX ratio of the subtransmission system. To achieve full utilization of renewables, battery energy storage systems (BESSs) are commonly used to mitigate the negative effects of massive fluctuations of RESs. Power flow router (PFR), which can be regarded as a general type of network-side controller, has also been verified to enhance the grid flexibility for accommodating renewables. In this paper, we investigate the value of PFR in helping BESSs for renewable power accommodation. The performance of PFR is evaluated with the minimum BESS capacity required for zero renewable power curtailment with and without PFRs. The operational constraints of BESSs and the terminal voltage property of PFRs are considered in a multi-period optimization model. The proposed model is tested through numerical simulations on a modified IEEE 30-bus subtransmission system and a remarkable result shows that 15 reduction of BESS capacity can be achieved by installing PFRs on a single line.","Reducing BESS Capacity for Accommodating Renewables in Subtransmission Systems with Power Flow Routers Widespread utilization of renewable energy sources (RESs) in subtransmission systems causes serious problems on power quality, such as voltage violations, leading to significant curtailment of renewables. This is due to the inherent variability of renewables and the high RX ratio of the subtransmission system. To achieve full utilization of renewables, battery energy storage systems (BESSs) are commonly used to mitigate the negative effects of massive fluctuations of RESs. Power flow router (PFR), which can be regarded as a general type of network-side controller, has also been verified to enhance the grid flexibility for accommodating renewables. In this paper, we investigate the value of PFR in helping BESSs for renewable power accommodation. The performance of PFR is evaluated with the minimum BESS capacity required for zero renewable power curtailment with and without PFRs. The operational constraints of BESSs and the terminal voltage property of PFRs are considered in a multi-period optimization model. The proposed model is tested through numerical simulations on a modified IEEE 30-bus subtransmission system and a remarkable result shows that 15 reduction of BESS capacity can be achieved by installing PFRs on a single line.",Environment
Bounds on Query Convergence,"The problem of finding an optimum using noisy evaluations of a smooth cost function arises in many contexts, including economics, business, medicine, experiment design, and foraging theory. We derive an asymptotic bound E (x_t - x)2   O(1sqrt(t)) on the rate of convergence of a sequence (x_0, x_1, ...) generated by an unbiased feedback process observing noisy evaluations of an unknown quadratic function maximised at x. The bound is tight, as the proof leads to a simple algorithm which meets it. We further establish a bound on the total regret, E sum_i1..t (x_i - x)2   O(sqrt(t)) These bounds may impose practical limitations on an agents performance, as O(eps-4) queries are made before the queries converge to x with eps accuracy.","Bounds on Query Convergence The problem of finding an optimum using noisy evaluations of a smooth cost function arises in many contexts, including economics, business, medicine, experiment design, and foraging theory. We derive an asymptotic bound E (x_t - x)2   O(1sqrt(t)) on the rate of convergence of a sequence (x_0, x_1, ...) generated by an unbiased feedback process observing noisy evaluations of an unknown quadratic function maximised at x. The bound is tight, as the proof leads to a simple algorithm which meets it. We further establish a bound on the total regret, E sum_i1..t (x_i - x)2   O(sqrt(t)) These bounds may impose practical limitations on an agents performance, as O(eps-4) queries are made before the queries converge to x with eps accuracy.",Technology
"Transforming the content, pedagogy and structure of an introductory physics course for life sciences majors","In this paper, we describe how we transformed our large-enrollment introductory physics sequence for life-science students to a LectureStudio format and aligned the physics concepts with authentic biological applications. We have reformed the pedagogy to include research-validated practices in interactive engagement, and accomplished our goals of enhanced learning gains, sustainability, and adoptability of our course reforms. The active engagement at the heart of the LectureStudio format results in comparable or enhanced learning gains (as measured by validated concept surveys) when compared to traditional instruction. When coupled with appropriate instructor preparation the format is sustainable, requiring no greater financial or human resources than does the traditional mode of teaching such courses. We have developed a complete suite of active-engagement instructional materials and made them available to the physics education community for adoption outside our institution.","Transforming the content, pedagogy and structure of an introductory physics course for life sciences majors In this paper, we describe how we transformed our large-enrollment introductory physics sequence for life-science students to a LectureStudio format and aligned the physics concepts with authentic biological applications. We have reformed the pedagogy to include research-validated practices in interactive engagement, and accomplished our goals of enhanced learning gains, sustainability, and adoptability of our course reforms. The active engagement at the heart of the LectureStudio format results in comparable or enhanced learning gains (as measured by validated concept surveys) when compared to traditional instruction. When coupled with appropriate instructor preparation the format is sustainable, requiring no greater financial or human resources than does the traditional mode of teaching such courses. We have developed a complete suite of active-engagement instructional materials and made them available to the physics education community for adoption outside our institution.",Education
On the Performance of Delta Hedging Strategies in Exponential Lvy Models,"We consider the performance of non-optimal hedging strategies in exponential Levy models. Given that both the payoff of the contingent claim and the hedging strategy admit suitable integral representations, we use the Laplace transform approach of Hubalek et al. (2006) to derive semi-explicit formulas for the resulting mean squared hedging error in terms of the cumulant generating function of the underlying Levy process. In two numerical examples, we apply these results to compare the efficiency of the Black-Scholes hedge and the model delta to the mean-variance optimal hedge in a normal inverse Gaussian and a diffusion-extended CGMY Levy model.","On the Performance of Delta Hedging Strategies in Exponential Lvy Models We consider the performance of non-optimal hedging strategies in exponential Levy models. Given that both the payoff of the contingent claim and the hedging strategy admit suitable integral representations, we use the Laplace transform approach of Hubalek et al. (2006) to derive semi-explicit formulas for the resulting mean squared hedging error in terms of the cumulant generating function of the underlying Levy process. In two numerical examples, we apply these results to compare the efficiency of the Black-Scholes hedge and the model delta to the mean-variance optimal hedge in a normal inverse Gaussian and a diffusion-extended CGMY Levy model.",Finance
A Set of Essentials for Online Learning : CSE-SET,"Distance learning is not a novel concept. Education or learning conducted online is a form of distance education. Online learning presents a convenient alternative to traditional learning. Numerous researchers have investigated the usage of online education in educational institutions and across nations. A set of essentials for effective online learning are elaborated in this study to ensure stakeholders would not get demotivated in the online learning process. Also, the study lists a set of factors that motivate students and other stakeholders to engage in online learning with enthusiasm and work towards online learning.","A Set of Essentials for Online Learning : CSE-SET Distance learning is not a novel concept. Education or learning conducted online is a form of distance education. Online learning presents a convenient alternative to traditional learning. Numerous researchers have investigated the usage of online education in educational institutions and across nations. A set of essentials for effective online learning are elaborated in this study to ensure stakeholders would not get demotivated in the online learning process. Also, the study lists a set of factors that motivate students and other stakeholders to engage in online learning with enthusiasm and work towards online learning.",Education
Where are Bottlenecks in NK Fitness Landscapes?,"Usually the offspring-parent fitness correlation is used to visualize and analyze some caracteristics of fitness landscapes such as evolvability. In this paper, we introduce a more general representation of this correlation, the Fitness Cloud (FC). We use the bottleneck metaphor to emphasise fitness levels in landscape that cause local search process to slow down. For a local search heuristic such as hill-climbing or simulated annealing, FC allows to visualize bottleneck and neutrality of landscapes. To confirm the relevance of the FC representation we show where the bottlenecks are in the well-know NK fitness landscape and also how to use neutrality information from the FC to combine some neutral operator with local search heuristic.","Where are Bottlenecks in NK Fitness Landscapes? Usually the offspring-parent fitness correlation is used to visualize and analyze some caracteristics of fitness landscapes such as evolvability. In this paper, we introduce a more general representation of this correlation, the Fitness Cloud (FC). We use the bottleneck metaphor to emphasise fitness levels in landscape that cause local search process to slow down. For a local search heuristic such as hill-climbing or simulated annealing, FC allows to visualize bottleneck and neutrality of landscapes. To confirm the relevance of the FC representation we show where the bottlenecks are in the well-know NK fitness landscape and also how to use neutrality information from the FC to combine some neutral operator with local search heuristic.",Technology
Random Worlds and Maximum Entropy,"Given a knowledge base KB containing first-order and statistical facts, we consider a principled method, called the random-worlds method, for computing a degree of belief that some formula Phi holds given KB. If we are reasoning about a world or system consisting of N individuals, then we can consider all possible worlds, or first-order models, with domain 1,...,N that satisfy KB, and compute the fraction of them in which Phi is true. We define the degree of belief to be the asymptotic value of this fraction as N grows large. We show that when the vocabulary underlying Phi and KB uses constants and unary predicates only, we can naturally associate an entropy with each world. As N grows larger, there are many more worlds with higher entropy. Therefore, we can use a maximum-entropy computation to compute the degree of belief. This result is in a similar spirit to previous work in physics and artificial intelligence, but is far more general. Of equal interest to the result itself are the limitations on its scope. Most importantly, the restriction to unary predicates seems necessary. Although the random-worlds method makes sense in general, the connection to maximum entropy seems to disappear in the non-unary case. These observations suggest unexpected limitations to the applicability of maximum-entropy methods.","Random Worlds and Maximum Entropy Given a knowledge base KB containing first-order and statistical facts, we consider a principled method, called the random-worlds method, for computing a degree of belief that some formula Phi holds given KB. If we are reasoning about a world or system consisting of N individuals, then we can consider all possible worlds, or first-order models, with domain 1,...,N that satisfy KB, and compute the fraction of them in which Phi is true. We define the degree of belief to be the asymptotic value of this fraction as N grows large. We show that when the vocabulary underlying Phi and KB uses constants and unary predicates only, we can naturally associate an entropy with each world. As N grows larger, there are many more worlds with higher entropy. Therefore, we can use a maximum-entropy computation to compute the degree of belief. This result is in a similar spirit to previous work in physics and artificial intelligence, but is far more general. Of equal interest to the result itself are the limitations on its scope. Most importantly, the restriction to unary predicates seems necessary. Although the random-worlds method makes sense in general, the connection to maximum entropy seems to disappear in the non-unary case. These observations suggest unexpected limitations to the applicability of maximum-entropy methods.",Technology
The Uncertainty Relationship In Magnetic Resonance Imaging (MRI),The uncertainty relationship in MRI is shown. The result of uncertainty relationship is compared with other factors influencing the resolution of MRI. Our estimations show that the uncertainty relationship is of no significance in practice.,The Uncertainty Relationship In Magnetic Resonance Imaging (MRI) The uncertainty relationship in MRI is shown. The result of uncertainty relationship is compared with other factors influencing the resolution of MRI. Our estimations show that the uncertainty relationship is of no significance in practice.,Healthcare
Analysis of multilevel Monte Carlo path simulation using the Milstein discretisation,"The multilevel Monte Carlo path simulation method introduced by Giles (it Operations Research, 56(3):607-617, 2008) exploits strong convergence properties to improve the computational complexity by combining simulations with different levels of resolution. In this paper we analyse its efficiency when using the Milstein discretisation; this has an improved order of strong convergence compared to the standard Euler-Maruyama method, and it is proved that this leads to an improved order of convergence of the variance of the multilevel estimator. Numerical results are also given for basket options to illustrate the relevance of the analysis.","Analysis of multilevel Monte Carlo path simulation using the Milstein discretisation The multilevel Monte Carlo path simulation method introduced by Giles (it Operations Research, 56(3):607-617, 2008) exploits strong convergence properties to improve the computational complexity by combining simulations with different levels of resolution. In this paper we analyse its efficiency when using the Milstein discretisation; this has an improved order of strong convergence compared to the standard Euler-Maruyama method, and it is proved that this leads to an improved order of convergence of the variance of the multilevel estimator. Numerical results are also given for basket options to illustrate the relevance of the analysis.",Finance
Characterizing Transgender Health Issues in Twitter,"Although there are millions of transgender people in the world, a lack of information exists about their health issues. This issue has consequences for the medical field, which only has a nascent understanding of how to identify and meet this populations health-related needs. Social media sites like Twitter provide new opportunities for transgender people to overcome these barriers by sharing their personal health experiences. Our research employs a computational framework to collect tweets from self-identified transgender users, detect those that are health-related, and identify their information needs. This framework is significant because it provides a macro-scale perspective on an issue that lacks investigation at national or demographic levels. Our findings identified 54 distinct health-related topics that we grouped into 7 broader categories. Further, we found both linguistic and topical differences in the health-related information shared by transgender men (TM) as com-pared to transgender women (TW). These findings can help inform medical and policy-based strategies for health interventions within transgender communities. Also, our proposed approach can inform the development of computational strategies to identify the health-related information needs of other marginalized populations.","Characterizing Transgender Health Issues in Twitter Although there are millions of transgender people in the world, a lack of information exists about their health issues. This issue has consequences for the medical field, which only has a nascent understanding of how to identify and meet this populations health-related needs. Social media sites like Twitter provide new opportunities for transgender people to overcome these barriers by sharing their personal health experiences. Our research employs a computational framework to collect tweets from self-identified transgender users, detect those that are health-related, and identify their information needs. This framework is significant because it provides a macro-scale perspective on an issue that lacks investigation at national or demographic levels. Our findings identified 54 distinct health-related topics that we grouped into 7 broader categories. Further, we found both linguistic and topical differences in the health-related information shared by transgender men (TM) as com-pared to transgender women (TW). These findings can help inform medical and policy-based strategies for health interventions within transgender communities. Also, our proposed approach can inform the development of computational strategies to identify the health-related information needs of other marginalized populations.",Healthcare
Fourier Analysis and Holographic Representations of 1D and 2D Signals,"In this paper, we focus on Fourier analysis and holographic transforms for signal representation. For instance, in the case of image processing, the holographic representation has the property that an arbitrary portion of the transformed image enables reconstruction of the whole image with details missing. We focus on holographic representation defined through the Fourier Transforms. Thus, We firstly review some results in Fourier transform and Fourier series. Next, we review the Discrete Holographic Fourier Transform (DHFT) for image representation. Then, we describe the contributions of our work. We show a simple scheme for progressive transmission based on the DHFT. Next, we propose the Continuous Holographic Fourier Transform (CHFT) and discuss some theoretical aspects of it for 1D signals. Finally, some testes are presented in the experimental results","Fourier Analysis and Holographic Representations of 1D and 2D Signals In this paper, we focus on Fourier analysis and holographic transforms for signal representation. For instance, in the case of image processing, the holographic representation has the property that an arbitrary portion of the transformed image enables reconstruction of the whole image with details missing. We focus on holographic representation defined through the Fourier Transforms. Thus, We firstly review some results in Fourier transform and Fourier series. Next, we review the Discrete Holographic Fourier Transform (DHFT) for image representation. Then, we describe the contributions of our work. We show a simple scheme for progressive transmission based on the DHFT. Next, we propose the Continuous Holographic Fourier Transform (CHFT) and discuss some theoretical aspects of it for 1D signals. Finally, some testes are presented in the experimental results",Technology
A Participatory Strategy for AI Ethics in Education and Rehabilitation grounded in the Capability Approach,"AI-based technologies have significant potential to enhance inclusive education and clinical-rehabilitative contexts for children with Special Educational Needs and Disabilities. AI can enhance learning experiences, empower students, and support both teachers and rehabilitators. However, their usage presents challenges that require a systemic-ecological vision, ethical considerations, and participatory research. Therefore, research and technological development must be rooted in a strong ethical-theoretical framework. The Capability Approach - a theoretical model of disability, human vulnerability, and inclusion - offers a more relevant perspective on functionality, effectiveness, and technological adequacy in inclusive learning environments. In this paper, we propose a participatory research strategy with different stakeholders through a case study on the ARTIS Project, which develops an AI-enriched interface to support children with text comprehension difficulties. Our research strategy integrates ethical, educational, clinical, and technological expertise in designing and implementing AI-based technologies for childrens learning environments through focus groups and collaborative design sessions. We believe that this holistic approach to AI adoption in education can help bridge the gap between technological innovation and ethical responsibility.","A Participatory Strategy for AI Ethics in Education and Rehabilitation grounded in the Capability Approach AI-based technologies have significant potential to enhance inclusive education and clinical-rehabilitative contexts for children with Special Educational Needs and Disabilities. AI can enhance learning experiences, empower students, and support both teachers and rehabilitators. However, their usage presents challenges that require a systemic-ecological vision, ethical considerations, and participatory research. Therefore, research and technological development must be rooted in a strong ethical-theoretical framework. The Capability Approach - a theoretical model of disability, human vulnerability, and inclusion - offers a more relevant perspective on functionality, effectiveness, and technological adequacy in inclusive learning environments. In this paper, we propose a participatory research strategy with different stakeholders through a case study on the ARTIS Project, which develops an AI-enriched interface to support children with text comprehension difficulties. Our research strategy integrates ethical, educational, clinical, and technological expertise in designing and implementing AI-based technologies for childrens learning environments through focus groups and collaborative design sessions. We believe that this holistic approach to AI adoption in education can help bridge the gap between technological innovation and ethical responsibility.",Education
Solving Multiclass Learning Problems via Error-Correcting Output Codes,"Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a discrete set containing k gt 2 values (i.e., k classes). The definition is acquired by studying collections of training examples of the form x_i, f (x_i). Existing approaches to multiclass learning problems include direct application of multiclass algorithms such as the decision-tree algorithms C4.5 and CART, application of binary concept learning algorithms to learn individual binary functions for each of the k classes, and application of binary concept learning algorithms with distributed output representations. This paper compares these three approaches to a new technique in which error-correcting codes are employed as a distributed output representation. We show that these output representations improve the generalization performance of both C4.5 and backpropagation on a wide range of multiclass learning tasks. We also demonstrate that this approach is robust with respect to changes in the size of the training sample, the assignment of distributed representations to particular classes, and the application of overfitting avoidance techniques such as decision-tree pruning. Finally, we show that---like the other methods---the error-correcting code technique can provide reliable class probability estimates. Taken together, these results demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems.","Solving Multiclass Learning Problems via Error-Correcting Output Codes Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a discrete set containing k gt 2 values (i.e., k classes). The definition is acquired by studying collections of training examples of the form x_i, f (x_i). Existing approaches to multiclass learning problems include direct application of multiclass algorithms such as the decision-tree algorithms C4.5 and CART, application of binary concept learning algorithms to learn individual binary functions for each of the k classes, and application of binary concept learning algorithms with distributed output representations. This paper compares these three approaches to a new technique in which error-correcting codes are employed as a distributed output representation. We show that these output representations improve the generalization performance of both C4.5 and backpropagation on a wide range of multiclass learning tasks. We also demonstrate that this approach is robust with respect to changes in the size of the training sample, the assignment of distributed representations to particular classes, and the application of overfitting avoidance techniques such as decision-tree pruning. Finally, we show that---like the other methods---the error-correcting code technique can provide reliable class probability estimates. Taken together, these results demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems.",Technology
Emergence of world-stock-market network,"In the age of globalization, it is natural that the stock market of each country is not independent form the other markets. In this case, collective behavior could be emerged form their dependency together. This article studies the collective behavior of a set of forty influential markets in the world economy with the aim of exploring a global financial structure that could be called world-stock-market network. Towards this end, we analyze the cross-correlation matrix of the indices of these forty markets using Random Matrix Theory (RMT). We find the degree of collective behavior among the markets and the share of each market in their structural formation. This finding together with the results obtained from the same calculation on four stock markets reinforce the idea of a world financial market. Finally, we draw the dendrogram of the cross-correlation matrix to make communities in this abstract global market visible. The dendrogram, drawn by at least thirty percent of correlation, shows that the world financial market comprises three communities each of which includes stock markets with geographical proximity.","Emergence of world-stock-market network In the age of globalization, it is natural that the stock market of each country is not independent form the other markets. In this case, collective behavior could be emerged form their dependency together. This article studies the collective behavior of a set of forty influential markets in the world economy with the aim of exploring a global financial structure that could be called world-stock-market network. Towards this end, we analyze the cross-correlation matrix of the indices of these forty markets using Random Matrix Theory (RMT). We find the degree of collective behavior among the markets and the share of each market in their structural formation. This finding together with the results obtained from the same calculation on four stock markets reinforce the idea of a world financial market. Finally, we draw the dendrogram of the cross-correlation matrix to make communities in this abstract global market visible. The dendrogram, drawn by at least thirty percent of correlation, shows that the world financial market comprises three communities each of which includes stock markets with geographical proximity.",Finance
Ecosystem transformations in response to environmental fluctuations,"Ecosystems, which are intricate amalgams of biological communities and their surrounding environments, continually evolve under the influence of their myriad interactions. The world is currently facing intensifying environmental fluctuations. Understanding general trends in ecosystem transformations in response to environmental fluctuations and elucidating the underlying mechanisms are thus critical challenges. In this study, we used a model ecosystem approach to investigate ecosystem alterations caused by escalating environmental fluctuations. We analyzed two distinct models: a stochastic ecosystem model with a spatial structure, and a differential equation model for resource competition. We found that environmental fluctuations tend to shift multi-species coexistence toward the dominance of specific species. We also categorized biological species as specialists or generalists and discovered that which of these groups becomes the dominant species depends on the intensity and frequency of environmental fluctuations. We also determined that a qualitative change in the diversity-stability relationship depends on the period of environmental fluctuations. These results underscore the need to explicitly consider the type of perturbation when discussing ecological transitions and the stability of ecosystems. Our findings advance understanding of the mechanisms underlying how environmental changes reshape ecosystems and offer insights into ecosystem sustainability in the face of future environmental perturbations.","Ecosystem transformations in response to environmental fluctuations Ecosystems, which are intricate amalgams of biological communities and their surrounding environments, continually evolve under the influence of their myriad interactions. The world is currently facing intensifying environmental fluctuations. Understanding general trends in ecosystem transformations in response to environmental fluctuations and elucidating the underlying mechanisms are thus critical challenges. In this study, we used a model ecosystem approach to investigate ecosystem alterations caused by escalating environmental fluctuations. We analyzed two distinct models: a stochastic ecosystem model with a spatial structure, and a differential equation model for resource competition. We found that environmental fluctuations tend to shift multi-species coexistence toward the dominance of specific species. We also categorized biological species as specialists or generalists and discovered that which of these groups becomes the dominant species depends on the intensity and frequency of environmental fluctuations. We also determined that a qualitative change in the diversity-stability relationship depends on the period of environmental fluctuations. These results underscore the need to explicitly consider the type of perturbation when discussing ecological transitions and the stability of ecosystems. Our findings advance understanding of the mechanisms underlying how environmental changes reshape ecosystems and offer insights into ecosystem sustainability in the face of future environmental perturbations.",Environment
School management information systems: Challenges to educational decision-making in the big data era,"Despite the benefits of school management information systems (SMIS), the concept of data-driven school culture failed to materialize for many educational institutions. Challenges posed by the quality of data in the big data era have prevented many schools from realizing the real potential of the SMIS. The paper analyses the uses, features, and inhibiting factors of SMIS. The paper proposes a five-phase conceptual model that assist administrators with making timely, quality decisions. The paper enriches the theoretical landscape of SMIS usage in the era of big data and lays a foundation for the future by establishing an educational decision-making model.","School management information systems: Challenges to educational decision-making in the big data era Despite the benefits of school management information systems (SMIS), the concept of data-driven school culture failed to materialize for many educational institutions. Challenges posed by the quality of data in the big data era have prevented many schools from realizing the real potential of the SMIS. The paper analyses the uses, features, and inhibiting factors of SMIS. The paper proposes a five-phase conceptual model that assist administrators with making timely, quality decisions. The paper enriches the theoretical landscape of SMIS usage in the era of big data and lays a foundation for the future by establishing an educational decision-making model.",Education
Markov Chain Monte Carlo on Asymmetric GARCH Model Using the Adaptive Construction Scheme,We perform Markov chain Monte Carlo simulations for a Bayesian inference of the GJR-GARCH model which is one of asymmetric GARCH models. The adaptive construction scheme is used for the construction of the proposal density in the Metropolis-Hastings algorithm and the parameters of the proposal density are determined adaptively by using the data sampled by the Markov chain Monte Carlo simulation. We study the performance of the scheme with the artificial GJR-GARCH data. We find that the adaptive construction scheme samples GJR-GARCH parameters effectively and conclude that the Metropolis-Hastings algorithm with the adaptive construction scheme is an efficient method to the Bayesian inference of the GJR-GARCH model.,Markov Chain Monte Carlo on Asymmetric GARCH Model Using the Adaptive Construction Scheme We perform Markov chain Monte Carlo simulations for a Bayesian inference of the GJR-GARCH model which is one of asymmetric GARCH models. The adaptive construction scheme is used for the construction of the proposal density in the Metropolis-Hastings algorithm and the parameters of the proposal density are determined adaptively by using the data sampled by the Markov chain Monte Carlo simulation. We study the performance of the scheme with the artificial GJR-GARCH data. We find that the adaptive construction scheme samples GJR-GARCH parameters effectively and conclude that the Metropolis-Hastings algorithm with the adaptive construction scheme is an efficient method to the Bayesian inference of the GJR-GARCH model.,Finance
Optimal heterogeneity in a simplified highly renewable European electricity system,"The resource quality and the temporal generation pattern of variable renewable energy sources vary significantly across Europe. In this paper spatial distributions of renewable assets are explored which exploit this heterogeneity to lower the total system costs for a high level of renewable electricity in Europe. Several intuitive heuristic algorithms, optimal portfolio theory and a local search algorithm are used to find optimal distributions of renewable generation capacities that minimise the total costs of backup, transmission and renewable capacity simultaneously. Using current cost projections, an optimal heterogeneous distribution favours onshore wind, particularly in countries bordering the North Sea, which results in average electricity costs that are up to 11 lower than for a homogeneous reference distribution of renewables proportional to each countrys mean load. The reduction becomes even larger, namely 18, once the transmission capacities are put to zero in the homogeneous reference distribution. Heuristic algorithms to distribute renewable capacity based on each countrys wind and solar capacity factors are shown to provide a satisfactory approximation to fully optimised renewable distributions, while maintaining the benefits of transparency and comprehensibility. The sensitivities of the results to changing costs of solar generation and gas supply as well as to the possible cross-sectoral usage of unavoidable curtailment energy are also examined.","Optimal heterogeneity in a simplified highly renewable European electricity system The resource quality and the temporal generation pattern of variable renewable energy sources vary significantly across Europe. In this paper spatial distributions of renewable assets are explored which exploit this heterogeneity to lower the total system costs for a high level of renewable electricity in Europe. Several intuitive heuristic algorithms, optimal portfolio theory and a local search algorithm are used to find optimal distributions of renewable generation capacities that minimise the total costs of backup, transmission and renewable capacity simultaneously. Using current cost projections, an optimal heterogeneous distribution favours onshore wind, particularly in countries bordering the North Sea, which results in average electricity costs that are up to 11 lower than for a homogeneous reference distribution of renewables proportional to each countrys mean load. The reduction becomes even larger, namely 18, once the transmission capacities are put to zero in the homogeneous reference distribution. Heuristic algorithms to distribute renewable capacity based on each countrys wind and solar capacity factors are shown to provide a satisfactory approximation to fully optimised renewable distributions, while maintaining the benefits of transparency and comprehensibility. The sensitivities of the results to changing costs of solar generation and gas supply as well as to the possible cross-sectoral usage of unavoidable curtailment energy are also examined.",Environment
Analyzing the Crowding-Out Effect of Investment Herding on Consumption: An Optimal Control Theory Approach,"Investment herding, a phenomenon where households mimic the decisions of others rather than relying on their own analysis, has significant effects on financial markets and household behavior. Excessive investment herding may reduce investments and lead to a depletion of household consumption, which is called the crowding-out effect. While existing research has qualitatively examined the impact of investment herding on consumption, quantitative studies in this area remain limited. In this work, we investigate the optimal investment and consumption decisions of households under the impact of investment herding. We formulate an optimization problem to model how investment herding influences household decisions over time. Based on the optimal control theory, we solve for the analytical solutions of optimal investment and consumption decisions. We theoretically analyze the impact of investment herding on household consumption decisions and demonstrate the existence of the crowding-out effect. We further explore how parameters, such as interest rate, excess return rate, and volatility, influence the crowding-out effect. Finally, we conduct a real data test to validate our theoretical analysis of the crowding-out effect. This study is crucial to understanding the impact of investment herding on household consumption and offering valuable insights for policymakers seeking to stimulate consumption and mitigate the negative effects of investment herding on economic growth.","Analyzing the Crowding-Out Effect of Investment Herding on Consumption: An Optimal Control Theory Approach Investment herding, a phenomenon where households mimic the decisions of others rather than relying on their own analysis, has significant effects on financial markets and household behavior. Excessive investment herding may reduce investments and lead to a depletion of household consumption, which is called the crowding-out effect. While existing research has qualitatively examined the impact of investment herding on consumption, quantitative studies in this area remain limited. In this work, we investigate the optimal investment and consumption decisions of households under the impact of investment herding. We formulate an optimization problem to model how investment herding influences household decisions over time. Based on the optimal control theory, we solve for the analytical solutions of optimal investment and consumption decisions. We theoretically analyze the impact of investment herding on household consumption decisions and demonstrate the existence of the crowding-out effect. We further explore how parameters, such as interest rate, excess return rate, and volatility, influence the crowding-out effect. Finally, we conduct a real data test to validate our theoretical analysis of the crowding-out effect. This study is crucial to understanding the impact of investment herding on household consumption and offering valuable insights for policymakers seeking to stimulate consumption and mitigate the negative effects of investment herding on economic growth.",Finance
Auto-Evaluation: A Critical Measure in Driving Improvements in Quality and Safety of AI-Generated Lesson Resources,"As a publicly funded body in the UK, Oak National Academy is in a unique position to innovate within this field as we have a comprehensive curriculum of approximately 13,000 open education resources (OER) for all National Curriculum subjects, designed and quality-assured by expert, human teachers. This has provided the corpus of content needed for building a high-quality AI-powered lesson planning tool, Aila, that is free to use and, therefore, accessible to all teachers across the country. Furthermore, using our evidence-informed curriculum principles, we have codified and exemplified each component of lesson design. To assess the quality of lessons produced by Aila at scale, we have developed an AI-powered auto-evaluation agent,facilitating informed improvements to enhance output quality. Through comparisons between human and auto-evaluations, we have begun to refine this agent further to increase its accuracy, measured by its alignment with an expert human evaluator. In this paper we present this iterative evaluation process through an illustrative case study focused on one quality benchmark - the level of challenge within multiple-choice quizzes. We also explore the contribution that this may make to similar projects and the wider sector.","Auto-Evaluation: A Critical Measure in Driving Improvements in Quality and Safety of AI-Generated Lesson Resources As a publicly funded body in the UK, Oak National Academy is in a unique position to innovate within this field as we have a comprehensive curriculum of approximately 13,000 open education resources (OER) for all National Curriculum subjects, designed and quality-assured by expert, human teachers. This has provided the corpus of content needed for building a high-quality AI-powered lesson planning tool, Aila, that is free to use and, therefore, accessible to all teachers across the country. Furthermore, using our evidence-informed curriculum principles, we have codified and exemplified each component of lesson design. To assess the quality of lessons produced by Aila at scale, we have developed an AI-powered auto-evaluation agent,facilitating informed improvements to enhance output quality. Through comparisons between human and auto-evaluations, we have begun to refine this agent further to increase its accuracy, measured by its alignment with an expert human evaluator. In this paper we present this iterative evaluation process through an illustrative case study focused on one quality benchmark - the level of challenge within multiple-choice quizzes. We also explore the contribution that this may make to similar projects and the wider sector.",Education
Alife Model of Evolutionary Emergence of Purposeful Adaptive Behavior,"The process of evolutionary emergence of purposeful adaptive behavior is investigated by means of computer simulations. The model proposed implies that there is an evolving population of simple agents, which have two natural needs: energy and reproduction. Any need is characterized quantitatively by a corresponding motivation. Motivations determine goal-directed behavior of agents. The model demonstrates that purposeful behavior does emerge in the simulated evolutionary processes. Emergence of purposefulness is accompanied by origin of a simple hierarchy in the control system of agents.","Alife Model of Evolutionary Emergence of Purposeful Adaptive Behavior The process of evolutionary emergence of purposeful adaptive behavior is investigated by means of computer simulations. The model proposed implies that there is an evolving population of simple agents, which have two natural needs: energy and reproduction. Any need is characterized quantitatively by a corresponding motivation. Motivations determine goal-directed behavior of agents. The model demonstrates that purposeful behavior does emerge in the simulated evolutionary processes. Emergence of purposefulness is accompanied by origin of a simple hierarchy in the control system of agents.",Technology
On the El-Nino Teleconnection to Spring Precipitation in Europe,"In a statistical analysis of more than a century of data we find a strong connection between strong warm El Nino winter events and high spring precipitation in a band from Southern England eastwards into Asia. This relationship is an extension of the connection mentioned by Kiladis and Diaz (1989), and much stronger than the winter season teleconnection that has been the subject of other studies. Linear correlation coefficients between DJF NINO3 indices and MAM precipitation are higher than r0.3 for individual stations, and as high as r0.49 for an index of precipitation anomalies around 50N from 5W to 35E. The lagged correlation suggests that south-east Asian surface temperature anomalies may act as intermediate variables.","On the El-Nino Teleconnection to Spring Precipitation in Europe In a statistical analysis of more than a century of data we find a strong connection between strong warm El Nino winter events and high spring precipitation in a band from Southern England eastwards into Asia. This relationship is an extension of the connection mentioned by Kiladis and Diaz (1989), and much stronger than the winter season teleconnection that has been the subject of other studies. Linear correlation coefficients between DJF NINO3 indices and MAM precipitation are higher than r0.3 for individual stations, and as high as r0.49 for an index of precipitation anomalies around 50N from 5W to 35E. The lagged correlation suggests that south-east Asian surface temperature anomalies may act as intermediate variables.",Environment
Renewable Power Trades and Network Congestion Externalities,"Integrating renewable energy production into the electricity grid is an important policy goal to address climate change. However, such an integration faces economic and technological challenges. As power generation by renewable sources increases, power transmission patterns over the electric grid change. Due to physical laws, these new transmission patterns lead to non-intuitive grid congestion externalities. We derive the conditions under which negative network externalities due to power trades occur. Calibration using a stylized framework and data from Europe shows that each additional unit of power traded between northern and western Europe reduces transmission capacity for the southern and eastern regions by 27 per unit traded. Such externalities suggest that new investments in the electric grid infrastructure cannot be made piecemeal. In our example, power infrastructure investment in northern and western Europe needs an accompanying investment in southern and eastern Europe as well. An economic challenge is regions facing externalities do not always have the financial ability to invest in infrastructure. Power transit fares can help finance power infrastructure investment in regions facing network congestion externalities. The resulting investment in the overall electricity grid facilitates integration of renewable energy production.","Renewable Power Trades and Network Congestion Externalities Integrating renewable energy production into the electricity grid is an important policy goal to address climate change. However, such an integration faces economic and technological challenges. As power generation by renewable sources increases, power transmission patterns over the electric grid change. Due to physical laws, these new transmission patterns lead to non-intuitive grid congestion externalities. We derive the conditions under which negative network externalities due to power trades occur. Calibration using a stylized framework and data from Europe shows that each additional unit of power traded between northern and western Europe reduces transmission capacity for the southern and eastern regions by 27 per unit traded. Such externalities suggest that new investments in the electric grid infrastructure cannot be made piecemeal. In our example, power infrastructure investment in northern and western Europe needs an accompanying investment in southern and eastern Europe as well. An economic challenge is regions facing externalities do not always have the financial ability to invest in infrastructure. Power transit fares can help finance power infrastructure investment in regions facing network congestion externalities. The resulting investment in the overall electricity grid facilitates integration of renewable energy production.",Finance
What are the limits on Commercial Bank Lending?,"Analysis of the 2007-8 credit crisis has concentrated on issues of relaxed lending standards, and the perception of irrational behaviour by speculative investors in real estate and other assets. Asset backed securities have been extensively criticised for creating a moral hazard in loan issuance and an associated increase in default risk, by removing the immediate lenders incentive to ensure that the underlying loans could be repaid. However significant monetary issues can accompany any form of increased commercial bank lending, and these appear to have been overlooked by this analysis. In this paper we propose a general explanation for credit crises based on an examination of the mechanics of the banking system, and in particular its internal controls on the supply of credit. We suggest that the current credit crisis is the result of multiple failures in the Basel regulatory framework, including the removal of central bank reserve requirements from some classes of deposit accounts within the banking system, allowing financial instruments representing debt to be used as regulatory capital, and in particular the introduction of securitized lending which effectively removed a previously implicit control over the total quantity of lending originating from the banking system. We further argue that the interaction of these problems has led to a destabilising imbalance between total money and loan supply growth, in that total lending sourced from the commercial bank sector increased at a faster rate than accompanying growth in the money supply. This not only created a multi-decade macro-economic debt spiral, but by increasing the ratio of debt to money within the monetary system acted to increase the risk of loan defaults, and consequentially reduce the overall stability of the banking system.","What are the limits on Commercial Bank Lending? Analysis of the 2007-8 credit crisis has concentrated on issues of relaxed lending standards, and the perception of irrational behaviour by speculative investors in real estate and other assets. Asset backed securities have been extensively criticised for creating a moral hazard in loan issuance and an associated increase in default risk, by removing the immediate lenders incentive to ensure that the underlying loans could be repaid. However significant monetary issues can accompany any form of increased commercial bank lending, and these appear to have been overlooked by this analysis. In this paper we propose a general explanation for credit crises based on an examination of the mechanics of the banking system, and in particular its internal controls on the supply of credit. We suggest that the current credit crisis is the result of multiple failures in the Basel regulatory framework, including the removal of central bank reserve requirements from some classes of deposit accounts within the banking system, allowing financial instruments representing debt to be used as regulatory capital, and in particular the introduction of securitized lending which effectively removed a previously implicit control over the total quantity of lending originating from the banking system. We further argue that the interaction of these problems has led to a destabilising imbalance between total money and loan supply growth, in that total lending sourced from the commercial bank sector increased at a faster rate than accompanying growth in the money supply. This not only created a multi-decade macro-economic debt spiral, but by increasing the ratio of debt to money within the monetary system acted to increase the risk of loan defaults, and consequentially reduce the overall stability of the banking system.",Finance
Augmented CARDS: A machine learning approach to identifying triggers of climate change misinformation on Twitter,"Misinformation about climate change poses a significant threat to societal well-being, prompting the urgent need for effective mitigation strategies. However, the rapid proliferation of online misinformation on social media platforms outpaces the ability of fact-checkers to debunk false claims. Automated detection of climate change misinformation offers a promising solution. In this study, we address this gap by developing a two-step hierarchical model, the Augmented CARDS model, specifically designed for detecting contrarian climate claims on Twitter. Furthermore, we apply the Augmented CARDS model to five million climate-themed tweets over a six-month period in 2022. We find that over half of contrarian climate claims on Twitter involve attacks on climate actors or conspiracy theories. Spikes in climate contrarianism coincide with one of four stimuli: political events, natural events, contrarian influencers, or convinced influencers. Implications for automated responses to climate misinformation are discussed.","Augmented CARDS: A machine learning approach to identifying triggers of climate change misinformation on Twitter Misinformation about climate change poses a significant threat to societal well-being, prompting the urgent need for effective mitigation strategies. However, the rapid proliferation of online misinformation on social media platforms outpaces the ability of fact-checkers to debunk false claims. Automated detection of climate change misinformation offers a promising solution. In this study, we address this gap by developing a two-step hierarchical model, the Augmented CARDS model, specifically designed for detecting contrarian climate claims on Twitter. Furthermore, we apply the Augmented CARDS model to five million climate-themed tweets over a six-month period in 2022. We find that over half of contrarian climate claims on Twitter involve attacks on climate actors or conspiracy theories. Spikes in climate contrarianism coincide with one of four stimuli: political events, natural events, contrarian influencers, or convinced influencers. Implications for automated responses to climate misinformation are discussed.",Environment
Incorporating External Data into the Analysis of Clinical Trials via Bayesian Additive Regression Trees,"Most clinical trials involve the comparison of a new treatment to a control arm (e.g., the standard of care) and the estimation of a treatment effect. External data, including historical clinical trial data and real-world observational data, are commonly available for the control arm. Borrowing information from external data holds the promise of improving the estimation of relevant parameters and increasing the power of detecting a treatment effect if it exists. In this paper, we propose to use Bayesian additive regression trees (BART) for incorporating external data into the analysis of clinical trials, with a specific goal of estimating the conditional or population average treatment effect. BART naturally adjusts for patient-level covariates and captures potentially heterogeneous treatment effects across different data sources, achieving flexible borrowing. Simulation studies demonstrate that BART compares favorably to a hierarchical linear model and a normal-normal hierarchical model. We illustrate the proposed method with an acupuncture trial.","Incorporating External Data into the Analysis of Clinical Trials via Bayesian Additive Regression Trees Most clinical trials involve the comparison of a new treatment to a control arm (e.g., the standard of care) and the estimation of a treatment effect. External data, including historical clinical trial data and real-world observational data, are commonly available for the control arm. Borrowing information from external data holds the promise of improving the estimation of relevant parameters and increasing the power of detecting a treatment effect if it exists. In this paper, we propose to use Bayesian additive regression trees (BART) for incorporating external data into the analysis of clinical trials, with a specific goal of estimating the conditional or population average treatment effect. BART naturally adjusts for patient-level covariates and captures potentially heterogeneous treatment effects across different data sources, achieving flexible borrowing. Simulation studies demonstrate that BART compares favorably to a hierarchical linear model and a normal-normal hierarchical model. We illustrate the proposed method with an acupuncture trial.",Healthcare
Show or Tell? Demonstration is More Robust to Changes in Shared Perception than Explanation,"Successful teaching entails a complex interaction between a teacher and a learner. The teacher must select and convey information based on what they think the learner perceives and believes. Teaching always involves misaligned beliefs, but studies of pedagogy often focus on situations where teachers and learners share perceptions. Nonetheless, a teacher and learner may not always experience or attend to the same aspects of the environment. Here, we study how misaligned perceptions influence communication. We hypothesize that the efficacy of different forms of communication depends on the shared perceptual state between teacher and learner. We develop a cooperative teaching game to test whether concrete mediums (demonstrations, or showing) are more robust than abstract ones (language, or telling) when the teacher and learner are not perceptually aligned. We find evidence that (1) language-based teaching is more affected by perceptual misalignment, but (2) demonstration-based teaching is less likely to convey nuanced information. We discuss implications for human pedagogy and machine learning.","Show or Tell? Demonstration is More Robust to Changes in Shared Perception than Explanation Successful teaching entails a complex interaction between a teacher and a learner. The teacher must select and convey information based on what they think the learner perceives and believes. Teaching always involves misaligned beliefs, but studies of pedagogy often focus on situations where teachers and learners share perceptions. Nonetheless, a teacher and learner may not always experience or attend to the same aspects of the environment. Here, we study how misaligned perceptions influence communication. We hypothesize that the efficacy of different forms of communication depends on the shared perceptual state between teacher and learner. We develop a cooperative teaching game to test whether concrete mediums (demonstrations, or showing) are more robust than abstract ones (language, or telling) when the teacher and learner are not perceptually aligned. We find evidence that (1) language-based teaching is more affected by perceptual misalignment, but (2) demonstration-based teaching is less likely to convey nuanced information. We discuss implications for human pedagogy and machine learning.",Education
Smart Education: Higher Education Instruction and the Internet of Things (IoT),"The Internet of Things (IoT) has many applications in our daily lives. One aspect in particular is how the IoT is making a substantial impact on education and learning; as we move into the Smart Educational era. This article explores how the IoT continues to transform the education landscape, from classrooms and assessments to culture and attitudes. Smart Education is a pivotal tool in the fight to meet the educational challenges of tomorrow. The IoT tools are getting used more and more often in the area of education, aiming to increase student engagement, satisfaction and quality of learning. IoT will reshape student culture and habits beyond belief. As Smart Education is more than just using technologies, it involves a whole range of factors, from the educational management through to the pedagogical techniques and effectiveness. Educators in the 21st century now have access to gamification, smart devices, data management, and immersive technologies. Enabling academics to gather a variety of information from students. Ranging from monitoring student engagement to adapting the learning strategies for improved learning effectiveness. Through Smart Education, educators will be able to better monitor the needs of individual students and adjust their learning load correspondingly (i.e., optimal learning environmentworkload to support and prevent students failing). One of the biggest challenges for educators is how new technologies will address growing problems (engagement and achievement). The scale and pace of change (technological IoT era) is unprecedented. Typically, jobs students are trained for today will not be here tomorrow. Education is not just about knowledge acquisition, but also the digital skills, adaptability and creativity (essential, if students are to thrive in the new world).","Smart Education: Higher Education Instruction and the Internet of Things (IoT) The Internet of Things (IoT) has many applications in our daily lives. One aspect in particular is how the IoT is making a substantial impact on education and learning; as we move into the Smart Educational era. This article explores how the IoT continues to transform the education landscape, from classrooms and assessments to culture and attitudes. Smart Education is a pivotal tool in the fight to meet the educational challenges of tomorrow. The IoT tools are getting used more and more often in the area of education, aiming to increase student engagement, satisfaction and quality of learning. IoT will reshape student culture and habits beyond belief. As Smart Education is more than just using technologies, it involves a whole range of factors, from the educational management through to the pedagogical techniques and effectiveness. Educators in the 21st century now have access to gamification, smart devices, data management, and immersive technologies. Enabling academics to gather a variety of information from students. Ranging from monitoring student engagement to adapting the learning strategies for improved learning effectiveness. Through Smart Education, educators will be able to better monitor the needs of individual students and adjust their learning load correspondingly (i.e., optimal learning environmentworkload to support and prevent students failing). One of the biggest challenges for educators is how new technologies will address growing problems (engagement and achievement). The scale and pace of change (technological IoT era) is unprecedented. Typically, jobs students are trained for today will not be here tomorrow. Education is not just about knowledge acquisition, but also the digital skills, adaptability and creativity (essential, if students are to thrive in the new world).",Education
Solvent content of protein crystals from diffraction intensities by Independent Component Analysis,"An analysis of the protein content of several crystal forms of proteins has been performed. We apply a new numerical technique, the Independent Component Analysis (ICA), to determine the volume fraction of the asymmetric unit occupied by the protein. This technique requires only the crystallographic data of structure factors as input.","Solvent content of protein crystals from diffraction intensities by Independent Component Analysis An analysis of the protein content of several crystal forms of proteins has been performed. We apply a new numerical technique, the Independent Component Analysis (ICA), to determine the volume fraction of the asymmetric unit occupied by the protein. This technique requires only the crystallographic data of structure factors as input.",Healthcare
A Domain-Independent Algorithm for Plan Adaptation,"The paradigms of transformational planning, case-based planning, and plan debugging all involve a process known as plan adaptation - modifying or repairing an old plan so it solves a new problem. In this paper we provide a domain-independent algorithm for plan adaptation, demonstrate that it is sound, complete, and systematic, and compare it to other adaptation algorithms in the literature. Our approach is based on a view of planning as searching a graph of partial plans. Generative planning starts at the graphs root and moves from node to node using plan-refinement operators. In planning by adaptation, a library plan - an arbitrary node in the plan graph - is the starting point for the search, and the plan-adaptation algorithm can apply both the same refinement operators available to a generative planner and can also retract constraints and steps from the plan. Our algorithms completeness ensures that the adaptation algorithm will eventually search the entire graph and its systematicity ensures that it will do so without redundantly searching any parts of the graph.","A Domain-Independent Algorithm for Plan Adaptation The paradigms of transformational planning, case-based planning, and plan debugging all involve a process known as plan adaptation - modifying or repairing an old plan so it solves a new problem. In this paper we provide a domain-independent algorithm for plan adaptation, demonstrate that it is sound, complete, and systematic, and compare it to other adaptation algorithms in the literature. Our approach is based on a view of planning as searching a graph of partial plans. Generative planning starts at the graphs root and moves from node to node using plan-refinement operators. In planning by adaptation, a library plan - an arbitrary node in the plan graph - is the starting point for the search, and the plan-adaptation algorithm can apply both the same refinement operators available to a generative planner and can also retract constraints and steps from the plan. Our algorithms completeness ensures that the adaptation algorithm will eventually search the entire graph and its systematicity ensures that it will do so without redundantly searching any parts of the graph.",Technology
Training and Validating a Treatment Recommender with Partial Verification Evidence,"Current clinical decision support systems (DSS) are trained and validated on observational data from the target clinic. This is problematic for treatments validated in a randomized clinical trial (RCT), but not yet introduced in any clinic. In this work, we report on a method for training and validating the DSS using the RCT data. The key challenges we address are of missingness -- missing rationale for treatment assignment (the assignment is at random), and missing verification evidence, since the effectiveness of a treatment for a patient can only be verified (ground truth) for treatments what were actually assigned to a patient. We use data from a multi-armed RCT that investigated the effectiveness of single- and combination- treatments for 240 tinnitus patients recruited and treated in 5 clinical centers. To deal with the missing rationale challenge, we re-model the target variable (outcome) in order to suppress the effect of the randomly-assigned treatment, and control on the effect of treatment in general. Our methods are also robust to missing values in features and with a small number of patients per RCT arm. We deal with missing verification evidence by using counterfactual treatment verification, which compares the effectiveness of the DSS recommendations to the effectiveness of the RCT assignments when they are aligned vs not aligned. We demonstrate that our approach leverages the RCT data for learning and verification, by showing that the DSS suggests treatments that improve the outcome. The results are limited through the small number of patients per treatment; while our ensemble is designed to mitigate this effect, the predictive performance of the methods is affected by the smallness of the data. We provide a basis for the establishment of decision supporting routines on treatments that have been tested in RCTs but have not yet been deployed clinically.","Training and Validating a Treatment Recommender with Partial Verification Evidence Current clinical decision support systems (DSS) are trained and validated on observational data from the target clinic. This is problematic for treatments validated in a randomized clinical trial (RCT), but not yet introduced in any clinic. In this work, we report on a method for training and validating the DSS using the RCT data. The key challenges we address are of missingness -- missing rationale for treatment assignment (the assignment is at random), and missing verification evidence, since the effectiveness of a treatment for a patient can only be verified (ground truth) for treatments what were actually assigned to a patient. We use data from a multi-armed RCT that investigated the effectiveness of single- and combination- treatments for 240 tinnitus patients recruited and treated in 5 clinical centers. To deal with the missing rationale challenge, we re-model the target variable (outcome) in order to suppress the effect of the randomly-assigned treatment, and control on the effect of treatment in general. Our methods are also robust to missing values in features and with a small number of patients per RCT arm. We deal with missing verification evidence by using counterfactual treatment verification, which compares the effectiveness of the DSS recommendations to the effectiveness of the RCT assignments when they are aligned vs not aligned. We demonstrate that our approach leverages the RCT data for learning and verification, by showing that the DSS suggests treatments that improve the outcome. The results are limited through the small number of patients per treatment; while our ensemble is designed to mitigate this effect, the predictive performance of the methods is affected by the smallness of the data. We provide a basis for the establishment of decision supporting routines on treatments that have been tested in RCTs but have not yet been deployed clinically.",Healthcare
Technoeconomic Supplement of P2G Clusters with Hydrogen Pipeline for Coordinated Renewable Energy and HVDC Systems,"Under the downward tendency of prices of renewable energy generators and upward trend of hydrogen demand, this paper studies the technoeconomic supplement of P2G clusters with hydrogen pipeline for HVDC to jointly consume renewable energy. First, the planning and operation constraints of large-capacity P2G clusters is established. On this basis, the multistage coordinated planning model of renewable energy, HVDCs, P2Gs and hydrogen pipelines is proposed considering both variability and uncertainty, rendering a distributionally robust chance-constrained (DRCC) program. Then this model is applied in the case study based on the real Inner Mongolia-Shandong system. Compared with energy transmission via HVDC only, P2G can provide operation supplement with its operational flexibility and long term economic supplement with increasing demand in high-valued transportation sector, which stimulates an extra 24 GW renewable energy exploration. Sensitivity analysis for both technical and economic factors further verifies the advantages of P2G in the presence of high variability due to renewable energy and downward tendency of prices of renewable energy generators. However, since the additional levelized cost of the P2G (0.04 RMBkWh) is approximately twice the HVDC (0.02 RMBkWh), P2G is more sensitive to uncertainty from both renewable energy and hydrogen demand.","Technoeconomic Supplement of P2G Clusters with Hydrogen Pipeline for Coordinated Renewable Energy and HVDC Systems Under the downward tendency of prices of renewable energy generators and upward trend of hydrogen demand, this paper studies the technoeconomic supplement of P2G clusters with hydrogen pipeline for HVDC to jointly consume renewable energy. First, the planning and operation constraints of large-capacity P2G clusters is established. On this basis, the multistage coordinated planning model of renewable energy, HVDCs, P2Gs and hydrogen pipelines is proposed considering both variability and uncertainty, rendering a distributionally robust chance-constrained (DRCC) program. Then this model is applied in the case study based on the real Inner Mongolia-Shandong system. Compared with energy transmission via HVDC only, P2G can provide operation supplement with its operational flexibility and long term economic supplement with increasing demand in high-valued transportation sector, which stimulates an extra 24 GW renewable energy exploration. Sensitivity analysis for both technical and economic factors further verifies the advantages of P2G in the presence of high variability due to renewable energy and downward tendency of prices of renewable energy generators. However, since the additional levelized cost of the P2G (0.04 RMBkWh) is approximately twice the HVDC (0.02 RMBkWh), P2G is more sensitive to uncertainty from both renewable energy and hydrogen demand.",Environment
Information based clustering,"In an age of increasingly large data sets, investigators in many different disciplines have turned to clustering as a tool for data analysis and exploration. Existing clustering methods, however, typically depend on several nontrivial assumptions about the structure of data. Here we reformulate the clustering problem from an information theoretic perspective which avoids many of these assumptions. In particular, our formulation obviates the need for defining a cluster prototype, does not require an a priori similarity metric, is invariant to changes in the representation of the data, and naturally captures non-linear relations. We apply this approach to different domains and find that it consistently produces clusters that are more coherent than those extracted by existing algorithms. Finally, our approach provides a way of clustering based on collective notions of similarity rather than the traditional pairwise measures.","Information based clustering In an age of increasingly large data sets, investigators in many different disciplines have turned to clustering as a tool for data analysis and exploration. Existing clustering methods, however, typically depend on several nontrivial assumptions about the structure of data. Here we reformulate the clustering problem from an information theoretic perspective which avoids many of these assumptions. In particular, our formulation obviates the need for defining a cluster prototype, does not require an a priori similarity metric, is invariant to changes in the representation of the data, and naturally captures non-linear relations. We apply this approach to different domains and find that it consistently produces clusters that are more coherent than those extracted by existing algorithms. Finally, our approach provides a way of clustering based on collective notions of similarity rather than the traditional pairwise measures.",Healthcare
Reinforced Curriculum Learning on Pre-trained Neural Machine Translation Models,"The competitive performance of neural machine translation (NMT) critically relies on large amounts of training data. However, acquiring high-quality translation pairs requires expert knowledge and is costly. Therefore, how to best utilize a given dataset of samples with diverse quality and characteristics becomes an important yet understudied question in NMT. Curriculum learning methods have been introduced to NMT to optimize a models performance by prescribing the data input order, based on heuristics such as the assessment of noise and difficulty levels. However, existing methods require training from scratch, while in practice most NMT models are pre-trained on big data already. Moreover, as heuristics, they do not generalize well. In this paper, we aim to learn a curriculum for improving a pre-trained NMT model by re-selecting influential data samples from the original training set and formulate this task as a reinforcement learning problem. Specifically, we propose a data selection framework based on Deterministic Actor-Critic, in which a critic network predicts the expected change of model performance due to a certain sample, while an actor network learns to select the best sample out of a random batch of samples presented to it. Experiments on several translation datasets show that our method can further improve the performance of NMT when original batch training reaches its ceiling, without using additional new training data, and significantly outperforms several strong baseline methods.","Reinforced Curriculum Learning on Pre-trained Neural Machine Translation Models The competitive performance of neural machine translation (NMT) critically relies on large amounts of training data. However, acquiring high-quality translation pairs requires expert knowledge and is costly. Therefore, how to best utilize a given dataset of samples with diverse quality and characteristics becomes an important yet understudied question in NMT. Curriculum learning methods have been introduced to NMT to optimize a models performance by prescribing the data input order, based on heuristics such as the assessment of noise and difficulty levels. However, existing methods require training from scratch, while in practice most NMT models are pre-trained on big data already. Moreover, as heuristics, they do not generalize well. In this paper, we aim to learn a curriculum for improving a pre-trained NMT model by re-selecting influential data samples from the original training set and formulate this task as a reinforcement learning problem. Specifically, we propose a data selection framework based on Deterministic Actor-Critic, in which a critic network predicts the expected change of model performance due to a certain sample, while an actor network learns to select the best sample out of a random batch of samples presented to it. Experiments on several translation datasets show that our method can further improve the performance of NMT when original batch training reaches its ceiling, without using additional new training data, and significantly outperforms several strong baseline methods.",Education
"Health Insurance Coverage Rule Interpretation Corpus: Law, Policy, and Medical Guidance for Health Insurance Coverage Understanding","U.S. health insurance is complex, and inadequate understanding and limited access to justice have dire implications for the most vulnerable. Advances in natural language processing present an opportunity to support efficient, case-specific understanding, and to improve access to justice and healthcare. Yet existing corpora lack context necessary for assessing even simple cases. We collect and release a corpus of reputable legal and medical text related to U.S. health insurance. We also introduce an outcome prediction task for health insurance appeals designed to support regulatory and patient self-help applications, and release a labeled benchmark for our task, and models trained on it.","Health Insurance Coverage Rule Interpretation Corpus: Law, Policy, and Medical Guidance for Health Insurance Coverage Understanding U.S. health insurance is complex, and inadequate understanding and limited access to justice have dire implications for the most vulnerable. Advances in natural language processing present an opportunity to support efficient, case-specific understanding, and to improve access to justice and healthcare. Yet existing corpora lack context necessary for assessing even simple cases. We collect and release a corpus of reputable legal and medical text related to U.S. health insurance. We also introduce an outcome prediction task for health insurance appeals designed to support regulatory and patient self-help applications, and release a labeled benchmark for our task, and models trained on it.",Healthcare
"Experience in applying remote technology in the secondary education institutions in Russia, located in rural areas (From the experience of Podolsky municipal district schools)","This article examines the experience of distance education technologies in Podolsky municipal district, Moscow region.","Experience in applying remote technology in the secondary education institutions in Russia, located in rural areas (From the experience of Podolsky municipal district schools) This article examines the experience of distance education technologies in Podolsky municipal district, Moscow region.",Education
Pneumatic capillary gun for ballistic delivery of microparticles,"A pneumatic gun for ballistic delivery of microparticles to soft targets is proposed and demonstrated. The particles are accelerated by a high speed flow of Helium in a capillary tube. Vacuum suction applied to a concentric, larger diameter tube is used to divert substantially all of the flow of Helium from the gun nozzle, thereby preventing the gas from hitting and damaging the target. Speed of ejection of micron-sized gold particles from the gun nozzle, and their depth of penetration into agarose gels are reported.","Pneumatic capillary gun for ballistic delivery of microparticles A pneumatic gun for ballistic delivery of microparticles to soft targets is proposed and demonstrated. The particles are accelerated by a high speed flow of Helium in a capillary tube. Vacuum suction applied to a concentric, larger diameter tube is used to divert substantially all of the flow of Helium from the gun nozzle, thereby preventing the gas from hitting and damaging the target. Speed of ejection of micron-sized gold particles from the gun nozzle, and their depth of penetration into agarose gels are reported.",Healthcare
Data-Mining Research in Education,"As an interdisciplinary discipline, data mining (DM) is popular in education area especially when examining students learning performances. It focuses on analyzing educational related data to develop models for improving learners learning experiences and enhancing institutional effectiveness. Therefore, DM does help education institutions provide high-quality education for its learners. Applying data mining in education also known as educational data mining (EDM), which enables to better understand how students learn and identify how improve educational outcomes. Present paper is designed to justify the capabilities of data mining approaches in the filed of education. The latest trends on EDM research are introduced in this review. Several specific algorithms, methods, applications and gaps in the current literature and future insights are discussed here.","Data-Mining Research in Education As an interdisciplinary discipline, data mining (DM) is popular in education area especially when examining students learning performances. It focuses on analyzing educational related data to develop models for improving learners learning experiences and enhancing institutional effectiveness. Therefore, DM does help education institutions provide high-quality education for its learners. Applying data mining in education also known as educational data mining (EDM), which enables to better understand how students learn and identify how improve educational outcomes. Present paper is designed to justify the capabilities of data mining approaches in the filed of education. The latest trends on EDM research are introduced in this review. Several specific algorithms, methods, applications and gaps in the current literature and future insights are discussed here.",Education
Assisted Video Sequences Indexing : Motion Analysis Based on Interest Points,"This work deals with content-based video indexing. Our viewpoint is semi-automatic analysis of compressed video. We consider the possible applications of motion analysis and moving object detection : assisting moving object indexing, summarising videos, and allowing image and motion queries. We propose an approach based on interest points. As first results, we test and compare the stability of different types of interest point detectors in compressed sequences.","Assisted Video Sequences Indexing : Motion Analysis Based on Interest Points This work deals with content-based video indexing. Our viewpoint is semi-automatic analysis of compressed video. We consider the possible applications of motion analysis and moving object detection : assisting moving object indexing, summarising videos, and allowing image and motion queries. We propose an approach based on interest points. As first results, we test and compare the stability of different types of interest point detectors in compressed sequences.",Technology
Transforming student learning with classroom communication systems,"Since 1993, the University of Massachusetts Physics Education Research Group (UMPERG) has developed curriculum and pedagogic techniques for use with classroom communication systems (CCSs) and has researched the effectiveness of CCS-based teaching. This bulletin describes how CCSs can influence interactive pedagogy and fundamentally transform the learning process. It includes advice drawn from lessons learned through a decade of experience.","Transforming student learning with classroom communication systems Since 1993, the University of Massachusetts Physics Education Research Group (UMPERG) has developed curriculum and pedagogic techniques for use with classroom communication systems (CCSs) and has researched the effectiveness of CCS-based teaching. This bulletin describes how CCSs can influence interactive pedagogy and fundamentally transform the learning process. It includes advice drawn from lessons learned through a decade of experience.",Education
Multidimensional spatiotemporal clustering -- An application to environmental sustainability scores in Europe,"The assessment of corporate sustainability performance is extremely relevant in facilitating the transition to a green and low-carbon intensity economy. However, companies located in different areas may be subject to different sustainability and environmental risks and policies. Henceforth, the main objective of this paper is to investigate the spatial and temporal pattern of the sustainability evaluations of European firms. We leverage on a large dataset containing information about companies sustainability performances, measured by MSCI ESG ratings, and geographical coordinates of firms in Western Europe between 2013 and 2023. By means of a modified version of the Chavent et al. (2018) hierarchical algorithm, we conduct a spatial clustering analysis, combining sustainability and spatial information, and a spatiotemporal clustering analysis, which combines the time dynamics of multiple sustainability features and spatial dissimilarities, to detect groups of firms with homogeneous sustainability performance. We are able to build cross-national and cross-industry clusters with remarkable differences in terms of sustainability scores. Among other results, in the spatio-temporal analysis, we observe a high degree of geographical overlap among clusters, indicating that the temporal dynamics in sustainability assessment are relevant within a multidimensional approach. Our findings help to capture the diversity of ESG ratings across Western Europe and may assist practitioners and policymakers in evaluating companies facing different sustainability-linked risks in different areas.","Multidimensional spatiotemporal clustering -- An application to environmental sustainability scores in Europe The assessment of corporate sustainability performance is extremely relevant in facilitating the transition to a green and low-carbon intensity economy. However, companies located in different areas may be subject to different sustainability and environmental risks and policies. Henceforth, the main objective of this paper is to investigate the spatial and temporal pattern of the sustainability evaluations of European firms. We leverage on a large dataset containing information about companies sustainability performances, measured by MSCI ESG ratings, and geographical coordinates of firms in Western Europe between 2013 and 2023. By means of a modified version of the Chavent et al. (2018) hierarchical algorithm, we conduct a spatial clustering analysis, combining sustainability and spatial information, and a spatiotemporal clustering analysis, which combines the time dynamics of multiple sustainability features and spatial dissimilarities, to detect groups of firms with homogeneous sustainability performance. We are able to build cross-national and cross-industry clusters with remarkable differences in terms of sustainability scores. Among other results, in the spatio-temporal analysis, we observe a high degree of geographical overlap among clusters, indicating that the temporal dynamics in sustainability assessment are relevant within a multidimensional approach. Our findings help to capture the diversity of ESG ratings across Western Europe and may assist practitioners and policymakers in evaluating companies facing different sustainability-linked risks in different areas.",Environment
FPL Analysis for Adaptive Bandits,"A main problem of Follow the Perturbed Leader strategies for online decision problems is that regret bounds are typically proven against oblivious adversary. In partial observation cases, it was not clear how to obtain performance guarantees against adaptive adversary, without worsening the bounds. We propose a conceptually simple argument to resolve this problem. Using this, a regret bound of O(t(23)) for FPL in the adversarial multi-armed bandit problem is shown. This bound holds for the common FPL variant using only the observations from designated exploration rounds. Using all observations allows for the stronger bound of O(t(12)), matching the best bound known so far (and essentially the known lower bound) for adversarial bandits. Surprisingly, this variant does not even need explicit exploration, it is self-stabilizing. However the sampling probabilities have to be either externally provided or approximated to sufficient accuracy, using O(t2 log t) samples in each step.","FPL Analysis for Adaptive Bandits A main problem of Follow the Perturbed Leader strategies for online decision problems is that regret bounds are typically proven against oblivious adversary. In partial observation cases, it was not clear how to obtain performance guarantees against adaptive adversary, without worsening the bounds. We propose a conceptually simple argument to resolve this problem. Using this, a regret bound of O(t(23)) for FPL in the adversarial multi-armed bandit problem is shown. This bound holds for the common FPL variant using only the observations from designated exploration rounds. Using all observations allows for the stronger bound of O(t(12)), matching the best bound known so far (and essentially the known lower bound) for adversarial bandits. Surprisingly, this variant does not even need explicit exploration, it is self-stabilizing. However the sampling probabilities have to be either externally provided or approximated to sufficient accuracy, using O(t2 log t) samples in each step.",Technology
The influence of scattered photons on the accurate determination of microcalcification thickness in digital mammography,"Our interest has been to study the effect that scattered radiation has on contrast, signal-to-noise ratio and thickness reconstruction in digital mammographies. Using the GEANT code we have performed Monte-Carlo simulations of 25 kVp MoMo photons, through a breast phantom which contains a 0.2-1.0 mm thick microcalcifications incident on a 20x106 mm2 pixelized detector. The data have been analyzed assuming 6 different shapes of the incident beam: a 0.2x0.2 mm2 narrow beam, 4 different 20 mm long scanning beams of various widths, and a 20x100 mm2 beam with no scatter reduction mechanisms (NSR) . Since the image of a point depends on scattered photons which passed up to 2 cm away from the object (for 4 cm thick phantom), we identify the background definition as a main source of systematic uncertainty in the image quality analysis. We propose the use of two dimensional functions (a polynomial for the background and Gaussians for the signal) for total photon transmission description. Our main results indicate the possible calcification thickness reconstruction with an accuracy of the order of 6 using 3 mm wide scanning beam. Signal-to-noise ratio with the 3 mm wide beam gets improved by 20 with respect to NSR, a figure similar to that obtained with the narrow beam. Thickness reconstruction is shown to be an alternative to signal-to-noise ratio for microcalcification detection.","The influence of scattered photons on the accurate determination of microcalcification thickness in digital mammography Our interest has been to study the effect that scattered radiation has on contrast, signal-to-noise ratio and thickness reconstruction in digital mammographies. Using the GEANT code we have performed Monte-Carlo simulations of 25 kVp MoMo photons, through a breast phantom which contains a 0.2-1.0 mm thick microcalcifications incident on a 20x106 mm2 pixelized detector. The data have been analyzed assuming 6 different shapes of the incident beam: a 0.2x0.2 mm2 narrow beam, 4 different 20 mm long scanning beams of various widths, and a 20x100 mm2 beam with no scatter reduction mechanisms (NSR) . Since the image of a point depends on scattered photons which passed up to 2 cm away from the object (for 4 cm thick phantom), we identify the background definition as a main source of systematic uncertainty in the image quality analysis. We propose the use of two dimensional functions (a polynomial for the background and Gaussians for the signal) for total photon transmission description. Our main results indicate the possible calcification thickness reconstruction with an accuracy of the order of 6 using 3 mm wide scanning beam. Signal-to-noise ratio with the 3 mm wide beam gets improved by 20 with respect to NSR, a figure similar to that obtained with the narrow beam. Thickness reconstruction is shown to be an alternative to signal-to-noise ratio for microcalcification detection.",Healthcare
Set-valued dynamic treatment regimes for competing outcomes,"Dynamic treatment regimes operationalize the clinical decision process as a sequence of functions, one for each clinical decision, where each function takes as input up-to-date patient information and gives as output a single recommended treatment. Current methods for estimating optimal dynamic treatment regimes, for example Q-learning, require the specification of a single outcome by which the goodness of competing dynamic treatment regimes are measured. However, this is an over-simplification of the goal of clinical decision making, which aims to balance several potentially competing outcomes. For example, often a balance must be struck between treatment effectiveness and side-effect burden. We propose a method for constructing dynamic treatment regimes that accommodates competing outcomes by recommending sets of treatments at each decision point. Formally, we construct a sequence of set-valued functions that take as input up-to-date patient information and give as output a recommended subset of the possible treatments. For a given patient history, the recommended set of treatments contains all treatments that are not inferior according to any of the competing outcomes. When there is more than one decision point, constructing these set-valued functions requires solving a non-trivial enumeration problem. We offer an exact enumeration algorithm by recasting the problem as a linear mixed integer program. The proposed methods are illustrated using data from a depression study and the CATIE schizophrenia study.","Set-valued dynamic treatment regimes for competing outcomes Dynamic treatment regimes operationalize the clinical decision process as a sequence of functions, one for each clinical decision, where each function takes as input up-to-date patient information and gives as output a single recommended treatment. Current methods for estimating optimal dynamic treatment regimes, for example Q-learning, require the specification of a single outcome by which the goodness of competing dynamic treatment regimes are measured. However, this is an over-simplification of the goal of clinical decision making, which aims to balance several potentially competing outcomes. For example, often a balance must be struck between treatment effectiveness and side-effect burden. We propose a method for constructing dynamic treatment regimes that accommodates competing outcomes by recommending sets of treatments at each decision point. Formally, we construct a sequence of set-valued functions that take as input up-to-date patient information and give as output a recommended subset of the possible treatments. For a given patient history, the recommended set of treatments contains all treatments that are not inferior according to any of the competing outcomes. When there is more than one decision point, constructing these set-valued functions requires solving a non-trivial enumeration problem. We offer an exact enumeration algorithm by recasting the problem as a linear mixed integer program. The proposed methods are illustrated using data from a depression study and the CATIE schizophrenia study.",Healthcare
Consistency of trace norm minimization,"Regularization by the sum of singular values, also referred to as the trace norm, is a popular technique for estimating low rank rectangular matrices. In this paper, we extend some of the consistency results of the Lasso to provide necessary and sufficient conditions for rank consistency of trace norm minimization with the square loss. We also provide an adaptive version that is rank consistent even when the necessary condition for the non adaptive version is not fulfilled.","Consistency of trace norm minimization Regularization by the sum of singular values, also referred to as the trace norm, is a popular technique for estimating low rank rectangular matrices. In this paper, we extend some of the consistency results of the Lasso to provide necessary and sufficient conditions for rank consistency of trace norm minimization with the square loss. We also provide an adaptive version that is rank consistent even when the necessary condition for the non adaptive version is not fulfilled.",Technology
Indexing and Visualization of Climate Change Narratives Using BERT and Causal Extraction,"In this study, we propose a methodology to extract, index, and visualize climate change narratives (stories about the connection between causal and consequential events related to climate change). We use two natural language processing methods, BERT (Bidirectional Encoder Representations from Transformers) and causal extraction, to textually analyze newspaper articles on climate change to extract climate change narratives. The novelty of the methodology could extract and quantify the causal relationships assumed by the newspapers writers. Looking at the extracted climate change narratives over time, we find that since 2018, an increasing number of narratives suggest the impact of the development of climate change policy discussion and the implementation of climate change-related policies on corporate behaviors, macroeconomics, and price dynamics. We also observed the recent emergence of narratives focusing on the linkages between climate change-related policies and monetary policy. Furthermore, there is a growing awareness of the negative impacts of natural disasters (e.g., abnormal weather and severe floods) related to climate change on economic activities, and this issue might be perceived as a new challenge for companies and governments. The methodology of this study is expected to be applied to a wide range of fields, as it can analyze causal relationships among various economic topics, including analysis of inflation expectation or monetary policy communication strategy.","Indexing and Visualization of Climate Change Narratives Using BERT and Causal Extraction In this study, we propose a methodology to extract, index, and visualize climate change narratives (stories about the connection between causal and consequential events related to climate change). We use two natural language processing methods, BERT (Bidirectional Encoder Representations from Transformers) and causal extraction, to textually analyze newspaper articles on climate change to extract climate change narratives. The novelty of the methodology could extract and quantify the causal relationships assumed by the newspapers writers. Looking at the extracted climate change narratives over time, we find that since 2018, an increasing number of narratives suggest the impact of the development of climate change policy discussion and the implementation of climate change-related policies on corporate behaviors, macroeconomics, and price dynamics. We also observed the recent emergence of narratives focusing on the linkages between climate change-related policies and monetary policy. Furthermore, there is a growing awareness of the negative impacts of natural disasters (e.g., abnormal weather and severe floods) related to climate change on economic activities, and this issue might be perceived as a new challenge for companies and governments. The methodology of this study is expected to be applied to a wide range of fields, as it can analyze causal relationships among various economic topics, including analysis of inflation expectation or monetary policy communication strategy.",Environment
Exploring the current applications and potential of extended reality for environmental sustainability in manufacturing,"In response to the transformation towards Industry 5.0, there is a growing call for manufacturing systems that prioritize environmental sustainability, alongside the emerging application of digital tools. Extended Reality (XR) - including Virtual Reality (VR), Augmented Reality (AR) and Mixed Reality (MR) - is one of the technologies identified as an enabler for Industry 5.0. XR could potentially also be a driver for more sustainable manufacturing: however, its potential environmental benefits have received limited attention. This paper aims to explore the current manufacturing applications and research within the field of XR technology connected to the environmental sustainability principle. The objectives of this paper are two-fold: (1) Identify the currently explored use cases of XR technology in literature and research, addressing environmental sustainability in manufacturing; (2) Provide guidance and references for industry and companies to use cases, toolboxes, methodologies, and workflows for implementing XR in environmental sustainable manufacturing practices. Based on the categorization of sustainability indicators, developed by the National Institute of Standards and Technology (NIST), the authors analyzed and mapped the current literature, with criteria of pragmatic XR use cases for manufacturing. The exploration resulted in a mapping of the current applications and use cases of XR technology within manufacturing that has the potential to drive environmental sustainability. The results are presented as stated use-cases with reference to the literature, contributing as guidance and inspiration for future researchers or implementations in industry, using XR as a driver for environmental sustainability. Furthermore, the authors open up the discussion for future work and research to increase the attention of XR as a driver for environmental sustainability.","Exploring the current applications and potential of extended reality for environmental sustainability in manufacturing In response to the transformation towards Industry 5.0, there is a growing call for manufacturing systems that prioritize environmental sustainability, alongside the emerging application of digital tools. Extended Reality (XR) - including Virtual Reality (VR), Augmented Reality (AR) and Mixed Reality (MR) - is one of the technologies identified as an enabler for Industry 5.0. XR could potentially also be a driver for more sustainable manufacturing: however, its potential environmental benefits have received limited attention. This paper aims to explore the current manufacturing applications and research within the field of XR technology connected to the environmental sustainability principle. The objectives of this paper are two-fold: (1) Identify the currently explored use cases of XR technology in literature and research, addressing environmental sustainability in manufacturing; (2) Provide guidance and references for industry and companies to use cases, toolboxes, methodologies, and workflows for implementing XR in environmental sustainable manufacturing practices. Based on the categorization of sustainability indicators, developed by the National Institute of Standards and Technology (NIST), the authors analyzed and mapped the current literature, with criteria of pragmatic XR use cases for manufacturing. The exploration resulted in a mapping of the current applications and use cases of XR technology within manufacturing that has the potential to drive environmental sustainability. The results are presented as stated use-cases with reference to the literature, contributing as guidance and inspiration for future researchers or implementations in industry, using XR as a driver for environmental sustainability. Furthermore, the authors open up the discussion for future work and research to increase the attention of XR as a driver for environmental sustainability.",Environment
The Role of Central Banks in Advancing Sustainable Finance,"This paper examines the pivotal role central banks play in advancing sustainable finance, a crucial component in addressing global environmental and social challenges. As supervisors of financial stability and economic growth, central banks have dominance over the financial system to influence how a country moves towards sustainable economy. The chapter explores how central banks integrate sustainability into their monetary policies, regulatory frameworks, and financial market operations. It highlights the ways in which central banks can promote green finance through sustainable investment principles, climate risk assessments, and green bond markets. Additionally, the chapter examines the collaborative efforts between central banks, governments, and international institutions to align financial systems with sustainability goals. By investigating case studies and best practices, the chapter provides a comprehensive understanding of the strategies central banks employ to foster a resilient and sustainable financial landscape. The findings underscore the imperative for central banks to balance traditional mandates with the emerging necessity to support sustainable development, ultimately contributing to the broader agenda of achieving global sustainability targets.","The Role of Central Banks in Advancing Sustainable Finance This paper examines the pivotal role central banks play in advancing sustainable finance, a crucial component in addressing global environmental and social challenges. As supervisors of financial stability and economic growth, central banks have dominance over the financial system to influence how a country moves towards sustainable economy. The chapter explores how central banks integrate sustainability into their monetary policies, regulatory frameworks, and financial market operations. It highlights the ways in which central banks can promote green finance through sustainable investment principles, climate risk assessments, and green bond markets. Additionally, the chapter examines the collaborative efforts between central banks, governments, and international institutions to align financial systems with sustainability goals. By investigating case studies and best practices, the chapter provides a comprehensive understanding of the strategies central banks employ to foster a resilient and sustainable financial landscape. The findings underscore the imperative for central banks to balance traditional mandates with the emerging necessity to support sustainable development, ultimately contributing to the broader agenda of achieving global sustainability targets.",Environment
Teaching Fluid Mechanics for Undergraduate Students in Applied Industrial Biology: from Theory to Atypical Experiments,"EBI is a further education establishment which provides education in applied industrial biology at level of MSc engineering degree. Fluid mechanics at EBI was considered by students as difficult who seemed somewhat unmotivated. In order to motivate them, we applied a new play-based pedagogy. Students were asked to draw inspiration from everyday life situations to find applications of fluid mechanics and to do experiments to verify and validate some theoretical results obtained in course. In this paper, we present an innovative teachinglearning pedagogy which includes the concept of learning through play and its implications in fluid mechanics for engineering. Examples of atypical experiments in fluid mechanics made by students are presented. Based on teaching evaluation by students, it is possible to know how students feel the course. The effectiveness of this approach to motivate students is presented through an analysis of students teaching assessment. Learning through play proved a great success in fluid mechanics where course evaluations increased substantially. Fluid mechanics has been progressively perceived as interesting, useful, pleasant and easy to assimilate. It is shown that this pedagogy which includes educational gaming presents benefits for students. These experiments seem therefore to be a very effective tool for improving teachinglearning activities in higher education.","Teaching Fluid Mechanics for Undergraduate Students in Applied Industrial Biology: from Theory to Atypical Experiments EBI is a further education establishment which provides education in applied industrial biology at level of MSc engineering degree. Fluid mechanics at EBI was considered by students as difficult who seemed somewhat unmotivated. In order to motivate them, we applied a new play-based pedagogy. Students were asked to draw inspiration from everyday life situations to find applications of fluid mechanics and to do experiments to verify and validate some theoretical results obtained in course. In this paper, we present an innovative teachinglearning pedagogy which includes the concept of learning through play and its implications in fluid mechanics for engineering. Examples of atypical experiments in fluid mechanics made by students are presented. Based on teaching evaluation by students, it is possible to know how students feel the course. The effectiveness of this approach to motivate students is presented through an analysis of students teaching assessment. Learning through play proved a great success in fluid mechanics where course evaluations increased substantially. Fluid mechanics has been progressively perceived as interesting, useful, pleasant and easy to assimilate. It is shown that this pedagogy which includes educational gaming presents benefits for students. These experiments seem therefore to be a very effective tool for improving teachinglearning activities in higher education.",Education
Sustainability: Delivering Agilitys Promise,"Sustainability is a promise by agile development, as it is part of both the Agile Alliances and the Scrum Alliances vision. Thus far, not much has been delivered on this promise. This paper explores the Agile Manifesto and points out how agility could contribute to sustainability in its three dimensions - social, economic, and environmental. Additionally, this paper provides some sample cases of companies focusing on both sustainability (partially or holistically) and agile development.","Sustainability: Delivering Agilitys Promise Sustainability is a promise by agile development, as it is part of both the Agile Alliances and the Scrum Alliances vision. Thus far, not much has been delivered on this promise. This paper explores the Agile Manifesto and points out how agility could contribute to sustainability in its three dimensions - social, economic, and environmental. Additionally, this paper provides some sample cases of companies focusing on both sustainability (partially or holistically) and agile development.",Environment
