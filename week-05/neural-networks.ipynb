{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b92d6d23",
   "metadata": {},
   "source": [
    "### Neural Networks.. what are they?\n",
    "\n",
    "Neural networks are computational models inspired by the human brain, consisting of layers of interconnected nodes, or neurons. Each neuron processes input data through weighted connections, adds a bias, and applies an activation function to produce an output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2cc9a3",
   "metadata": {},
   "source": [
    "**Key Components**:  \n",
    "- Neurons: process inputs to produce outputs, organized in layers.\n",
    "- Weights: determine the strength of connections between neurons, adjusted during training.\n",
    "- Biases: allow neurons to shift the activation function, increasing flexibility in modeling data.\n",
    "- Activation Functions: introduce non-linearity, enabling the network to learn complex relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f25b927d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights and biases:\n",
      "W1: [[0.37454012 0.95071431]\n",
      " [0.73199394 0.59865848]]\n",
      "b1: [0.15601864 0.15599452]\n",
      "W2: [[0.05808361]\n",
      " [0.86617615]]\n",
      "b2: [0.60111501]\n",
      "Predictions before training:\n",
      "[[0.7501134 ]\n",
      " [0.7740691 ]\n",
      " [0.78391515]\n",
      " [0.79889097]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# XOR dataset: inputs and outputs\n",
    "X = np.array([[0, 0],[0, 1],[1, 0],[1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Initialize weights and biases\n",
    "np.random.seed(42)  \n",
    "W1 = np.random.rand(2, 2)  # Weights: input (2) to hidden (2)\n",
    "b1 = np.random.rand(2)     # Biases: hidden layer\n",
    "W2 = np.random.rand(2, 1)  # Weights: hidden (2) to output (1)\n",
    "b2 = np.random.rand(1)     # Bias: output layer\n",
    "\n",
    "print(\"Initial weights and biases:\")\n",
    "print(\"W1:\", W1)\n",
    "print(\"b1:\", b1)\n",
    "print(\"W2:\", W2)\n",
    "print(\"b2:\", b2)\n",
    "\n",
    "# Forward propagation\n",
    "Z1 = X @ W1 + b1    # Linear combination for hidden layer\n",
    "A1 = sigmoid(Z1)    # Activation for hidden layer\n",
    "Z2 = A1 @ W2 + b2   # Linear combination for output layer\n",
    "A2 = sigmoid(Z2)    # Activation for output layer\n",
    "print(\"Predictions before training:\")\n",
    "print(A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae43cf",
   "metadata": {},
   "source": [
    "**Key Note**: Initial predictions are poor because the network hasn't learned the XOR pattern. Training would adjust weights and biases to minimize the error between predictions(`A2`) and true outputs (`Y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df2384b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions with tanh hidden layer activation, sigmoid for the output layer:\n",
      "[[0.67789997]\n",
      " [0.767621  ]\n",
      " [0.78997617]\n",
      " [0.81174609]]\n"
     ]
    }
   ],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Forward propagation\n",
    "Z1 = X @ W1 + b1            # Weighted sum for hidden layer\n",
    "A1 = tanh(Z1)               # Activation for hidden layer\n",
    "Z2 = A1 @ W2 + b2           # Weighted sum for output layer\n",
    "A2 = sigmoid(Z2)            # Output prediction\n",
    "print(\"Predictions with tanh hidden layer activation, sigmoid for the output layer:\")\n",
    "print(A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f28f37",
   "metadata": {},
   "source": [
    "The **XOR problem** is a fundamental example in machine learning, demonstrating the limitations of linear models and the power of multi-layer neural networks. Its resolution through hidden layers and non-linear activations was a key milestone in the development of deep learning, showing that complex problems require complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a154b12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid activations: [[0.53892573 0.53891974]\n",
      " [0.70847987 0.68019172]\n",
      " [0.62961342 0.75151503]\n",
      " [0.77946523 0.84623444]]\n",
      "ReLU activations: [[0.15601864 0.15599452]\n",
      " [0.88801258 0.754653  ]\n",
      " [0.53055876 1.10670883]\n",
      " [1.2625527  1.70536731]]\n",
      "Tanh activations: [[0.15476492 0.15474138]\n",
      " [0.71041072 0.63791667]\n",
      " [0.48580809 0.80289593]\n",
      " [0.85176634 0.93607668]]\n"
     ]
    }
   ],
   "source": [
    "# Define activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Forward pass with different activations\n",
    "Z1 = X @ W1 + b1\n",
    "A1_sigmoid = sigmoid(Z1)\n",
    "A1_relu = relu(Z1)\n",
    "A1_tanh = tanh(Z1)\n",
    "print(\"Sigmoid activations:\", A1_sigmoid)\n",
    "print(\"ReLU activations:\", A1_relu)\n",
    "print(\"Tanh activations:\", A1_tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66279051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.3247\n",
      "Epoch 1000, Loss: 0.2473\n",
      "Epoch 2000, Loss: 0.2406\n",
      "Epoch 3000, Loss: 0.2233\n",
      "Epoch 4000, Loss: 0.1960\n",
      "Epoch 5000, Loss: 0.1676\n",
      "Epoch 6000, Loss: 0.1206\n",
      "Epoch 7000, Loss: 0.0605\n",
      "Epoch 8000, Loss: 0.0304\n",
      "Epoch 9000, Loss: 0.0183\n",
      "Predictions after training:\n",
      "[[0.10801367]\n",
      " [0.8918913 ]\n",
      " [0.89154907]\n",
      " [0.12260958]]\n"
     ]
    }
   ],
   "source": [
    "# Define sigmoid derivative\n",
    "def sigmoid_deriv(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Training loop\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    Z1 = X @ W1 + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = A1 @ W2 + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    # Compute loss (mean squared error)\n",
    "    loss = np.mean((A2 - y)**2)\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    # Backpropagation\n",
    "    dA2 = 2 * (A2 - y) / len(y)  # Derivative of loss\n",
    "    dZ2 = dA2 * sigmoid_deriv(Z2)\n",
    "    dW2 = A1.T @ dZ2\n",
    "    db2 = np.sum(dZ2, axis=0)\n",
    "    \n",
    "    dA1 = dZ2 @ W2.T\n",
    "    dZ1 = dA1 * sigmoid_deriv(Z1)\n",
    "    dW1 = X.T @ dZ1\n",
    "    db1 = np.sum(dZ1, axis=0)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "\n",
    "# Test after training\n",
    "predictions = sigmoid(np.dot(sigmoid(X @ W1 + b1), W2) + b2)\n",
    "print(\"Predictions after training:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c07e85b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xavier-initialized W1: [[-0.41074287 -0.37135113]\n",
      " [-0.40402679 -0.65342524]]\n"
     ]
    }
   ],
   "source": [
    "# Xavier initialization\n",
    "W1 = np.random.randn(2, 2) * np.sqrt(1/2)\n",
    "W2 = np.random.randn(2, 1) * np.sqrt(1/2)\n",
    "print(\"Xavier-initialized W1:\", W1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
